{
    "human_text": [
        "State-space models are successfully used in many areas of science,\nengineering and economics to model time series and dynamical systems. We\npresent a fully Bayesian approach to inference \\emph{and learning} (i.e. state\nestimation and system identification) in nonlinear nonparametric state-space\nmodels. We place a Gaussian process prior over the state transition dynamics,\nresulting in a flexible model able to capture complex dynamical phenomena. To\nenable efficient inference, we marginalize over the transition dynamics\nfunction and infer directly the joint smoothing distribution using specially\ntailored Particle Markov Chain Monte Carlo samplers. Once a sample from the\nsmoothing distribution is computed, the state transition predictive\ndistribution can be formulated analytically. Our approach preserves the full\nnonparametric expressivity of the model and can make use of sparse Gaussian\nprocesses to greatly reduce computational complexity.",
        "Searching for small objects in large images is a task that is both\nchallenging for current deep learning systems and important in numerous\nreal-world applications, such as remote sensing and medical imaging. Thorough\nscanning of very large images is computationally expensive, particularly at\nresolutions sufficient to capture small objects. The smaller an object of\ninterest, the more likely it is to be obscured by clutter or otherwise deemed\ninsignificant. We examine these issues in the context of two complementary\nproblems: closed-set object detection and open-set target search. First, we\npresent a method for predicting pixel-level objectness from a low resolution\ngist image, which we then use to select regions for performing object detection\nlocally at high resolution. This approach has the benefit of not being fixed to\na predetermined grid, thereby requiring fewer costly high-resolution glimpses\nthan existing methods. Second, we propose a novel strategy for open-set visual\nsearch that seeks to find all instances of a target class which may be\npreviously unseen and is defined by a single image. We interpret both detection\nproblems through a probabilistic, Bayesian lens, whereby the objectness maps\nproduced by our method serve as priors in a maximum-a-posteriori approach to\nthe detection step. We evaluate the end-to-end performance of both the\ncombination of our patch selection strategy with this target search approach\nand the combination of our patch selection strategy with standard object\ndetection methods. Both elements of our approach are seen to significantly\noutperform baseline strategies.",
        "Dynamic graphs are rife with higher-order interactions, such as co-authorship\nrelationships and protein-protein interactions in biological networks, that\nnaturally arise between more than two nodes at once. In spite of the ubiquitous\npresence of such higher-order interactions, limited attention has been paid to\nthe higher-order counterpart of the popular pairwise link prediction problem.\nExisting higher-order structure prediction methods are mostly based on\nheuristic feature extraction procedures, which work well in practice but lack\ntheoretical guarantees. Such heuristics are primarily focused on predicting\nlinks in a static snapshot of the graph. Moreover, these heuristic-based\nmethods fail to effectively utilize and benefit from the knowledge of latent\nsubstructures already present within the higher-order structures. In this\npaper, we overcome these obstacles by capturing higher-order interactions\nsuccinctly as \\textit{simplices}, model their neighborhood by face-vectors, and\ndevelop a nonparametric kernel estimator for simplices that views the evolving\ngraph from the perspective of a time process (i.e., a sequence of graph\nsnapshots). Our method substantially outperforms several baseline higher-order\nprediction methods. As a theoretical achievement, we prove the consistency and\nasymptotic normality in terms of the Wasserstein distance of our estimator\nusing Stein's method.",
        "Grid R-CNN is a well-performed objection detection framework. It transforms\nthe traditional box offset regression problem into a grid point estimation\nproblem. With the guidance of the grid points, it can obtain high-quality\nlocalization results. However, the speed of Grid R-CNN is not so satisfactory.\nIn this technical report we present Grid R-CNN Plus, a better and faster\nversion of Grid R-CNN. We have made several updates that significantly speed up\nthe framework and simultaneously improve the accuracy. On COCO dataset, the\nRes50-FPN based Grid R-CNN Plus detector achieves an mAP of 40.4%,\noutperforming the baseline on the same model by 3.0 points with similar\ninference time. Code is available at https://github.com/STVIR/Grid-R-CNN .",
        "Following considerable development in 3D scanning technologies, many studies\nhave recently been proposed with various approaches for 3D vision tasks,\nincluding some methods that utilize 2D convolutional neural networks (CNNs).\nHowever, even though 2D CNNs have achieved high performance in many 2D vision\ntasks, existing works have not effectively applied them onto 3D vision tasks.\nIn particular, segmentation has not been well studied because of the difficulty\nof dense prediction for each point, which requires rich feature representation.\nIn this paper, we propose a simple and efficient architecture named point\nprojection and back-projection network (PBP-Net), which leverages 2D CNNs for\nthe 3D point cloud segmentation. 3 modules are introduced, each of which\nprojects 3D point cloud onto 2D planes, extracts features using a 2D CNN\nbackbone, and back-projects features onto the original 3D point cloud. To\ndemonstrate effective 3D feature extraction using 2D CNN, we perform various\nexperiments including comparison to recent methods. We analyze the proposed\nmodules through ablation studies and perform experiments on object part\nsegmentation (ShapeNet-Part dataset) and indoor scene semantic segmentation\n(S3DIS dataset). The experimental results show that proposed PBP-Net achieves\ncomparable performance to existing state-of-the-art methods.",
        "Finding an optimal matching in a weighted graph is a standard combinatorial\nproblem. We consider its semi-bandit version where either a pair or a full\nmatching is sampled sequentially. We prove that it is possible to leverage a\nrank-1 assumption on the adjacency matrix to reduce the sample complexity and\nthe regret of off-the-shelf algorithms up to reaching a linear dependency in\nthe number of vertices (up to poly log terms).",
        "InterpretML is an open-source Python package which exposes machine learning\ninterpretability algorithms to practitioners and researchers. InterpretML\nexposes two types of interpretability - glassbox models, which are machine\nlearning models designed for interpretability (ex: linear models, rule lists,\ngeneralized additive models), and blackbox explainability techniques for\nexplaining existing systems (ex: Partial Dependence, LIME). The package enables\npractitioners to easily compare interpretability algorithms by exposing\nmultiple methods under a unified API, and by having a built-in, extensible\nvisualization platform. InterpretML also includes the first implementation of\nthe Explainable Boosting Machine, a powerful, interpretable, glassbox model\nthat can be as accurate as many blackbox models. The MIT licensed source code\ncan be downloaded from github.com/microsoft/interpret.",
        "Visual Dialogue task requires an agent to be engaged in a conversation with\nhuman about an image. The ability of generating detailed and non-repetitive\nresponses is crucial for the agent to achieve human-like conversation. In this\npaper, we propose a novel generative decoding architecture to generate\nhigh-quality responses, which moves away from decoding the whole encoded\nsemantics towards the design that advocates both transparency and flexibility.\nIn this architecture, word generation is decomposed into a series of\nattention-based information selection steps, performed by the novel recurrent\nDeliberation, Abandon and Memory (DAM) module. Each DAM module performs an\nadaptive combination of the response-level semantics captured from the encoder\nand the word-level semantics specifically selected for generating each word.\nTherefore, the responses contain more detailed and non-repetitive descriptions\nwhile maintaining the semantic accuracy. Furthermore, DAM is flexible to\ncooperate with existing visual dialogue encoders and adaptive to the encoder\nstructures by constraining the information selection mode in DAM. We apply DAM\nto three typical encoders and verify the performance on the VisDial v1.0\ndataset. Experimental results show that the proposed models achieve new\nstate-of-the-art performance with high-quality responses. The code is available\nat https://github.com/JXZe/DAM.",
        "In image captioning where fluency is an important factor in evaluation, e.g.,\n$n$-gram metrics, sequential models are commonly used; however, sequential\nmodels generally result in overgeneralized expressions that lack the details\nthat may be present in an input image. Inspired by the idea of the\ncompositional neural module networks in the visual question answering task, we\nintroduce a hierarchical framework for image captioning that explores both\ncompositionality and sequentiality of natural language. Our algorithm learns to\ncompose a detail-rich sentence by selectively attending to different modules\ncorresponding to unique aspects of each object detected in an input image to\ninclude specific descriptions such as counts and color. In a set of experiments\non the MSCOCO dataset, the proposed model outperforms a state-of-the art model\nacross multiple evaluation metrics, more importantly, presenting visually\ninterpretable results. Furthermore, the breakdown of subcategories $f$-scores\nof the SPICE metric and human evaluation on Amazon Mechanical Turk show that\nour compositional module networks effectively generate accurate and detailed\ncaptions.",
        "Recent advances in GPU accelerated global and detail placement have reduced\nthe time to solution by an order of magnitude. This advancement allows us to\nleverage data driven optimization (such as Reinforcement Learning) in an effort\nto improve the final quality of placement results. In this work we augment\nstate-of-the-art, force-based global placement solvers with a reinforcement\nlearning agent trained to improve the final detail placed Half Perimeter Wire\nLength (HPWL).\n  We propose novel control schemes with either global or localized control of\nthe placement process. We then train reinforcement learning agents to use these\ncontrols to guide placement to improved solutions. In both cases, the augmented\noptimizer finds improved placement solutions.\n  Our trained agents achieve an average 1% improvement in final detail place\nHPWL across a range of academic benchmarks and more than 1% in global place\nHPWL on real industry designs.",
        "A major goal of materials design is to find material structures with desired\nproperties and in a second step to find a processing path to reach one of these\nstructures. In this paper, we propose and investigate a deep reinforcement\nlearning approach for the optimization of processing paths. The goal is to find\noptimal processing paths in the material structure space that lead to\ntarget-structures, which have been identified beforehand to result in desired\nmaterial properties. There exists a target set containing one or multiple\ndifferent structures. Our proposed methods can find an optimal path from a\nstart structure to a single target structure, or optimize the processing paths\nto one of the equivalent target-structures in the set. In the latter case, the\nalgorithm learns during processing to simultaneously identify the best\nreachable target structure and the optimal path to it. The proposed methods\nbelong to the family of model-free deep reinforcement learning algorithms. They\nare guided by structure representations as features of the process state and by\na reward signal, which is formulated based on a distance function in the\nstructure space. Model-free reinforcement learning algorithms learn through\ntrial and error while interacting with the process. Thereby, they are not\nrestricted to information from a priori sampled processing data and are able to\nadapt to the specific process. The optimization itself is model-free and does\nnot require any prior knowledge about the process itself. We instantiate and\nevaluate the proposed methods by optimizing paths of a generic metal forming\nprocess. We show the ability of both methods to find processing paths leading\nclose to target structures and the ability of the extended method to identify\ntarget-structures that can be reached effectively and efficiently and to focus\non these targets for sample efficient processing path optimization.",
        "Recent advances of 3D acquisition devices have enabled large-scale\nacquisition of 3D scene data. Such data, if completely and well annotated, can\nserve as useful ingredients for a wide spectrum of computer vision and graphics\nworks such as data-driven modeling and scene understanding, object detection\nand recognition. However, annotating a vast amount of 3D scene data remains\nchallenging due to the lack of an effective tool and/or the complexity of 3D\nscenes (e.g. clutter, varying illumination conditions). This paper aims to\nbuild a robust annotation tool that effectively and conveniently enables the\nsegmentation and annotation of massive 3D data. Our tool works by coupling 2D\nand 3D information via an interactive framework, through which users can\nprovide high-level semantic annotation for objects. We have experimented our\ntool and found that a typical indoor scene could be well segmented and\nannotated in less than 30 minutes by using the tool, as opposed to a few hours\nif done manually. Along with the tool, we created a dataset of over a hundred\n3D scenes associated with complete annotations using our tool. The tool and\ndataset are available at www.scenenn.net.",
        "Since many real-world data can be described from multiple views, multi-view\nlearning has attracted considerable attention. Various methods have been\nproposed and successfully applied to multi-view learning, typically based on\nmatrix factorization models. Recently, it is extended to the deep structure to\nexploit the hierarchical information of multi-view data, but the view-specific\nfeatures and the label information are seldom considered. To address these\nconcerns, we present a partially shared semi-supervised deep matrix\nfactorization model (PSDMF). By integrating the partially shared deep\ndecomposition structure, graph regularization and the semi-supervised\nregression model, PSDMF can learn a compact and discriminative representation\nthrough eliminating the effects of uncorrelated information. In addition, we\ndevelop an efficient iterative updating algorithm for PSDMF. Extensive\nexperiments on five benchmark datasets demonstrate that PSDMF can achieve\nbetter performance than the state-of-the-art multi-view learning approaches.\nThe MATLAB source code is available at\nhttps://github.com/libertyhhn/PartiallySharedDMF.",
        "Prospection is an important part of how humans come up with new task plans,\nbut has not been explored in depth in robotics. Predicting multiple task-level\nis a challenging problem that involves capturing both task semantics and\ncontinuous variability over the state of the world. Ideally, we would combine\nthe ability of machine learning to leverage big data for learning the semantics\nof a task, while using techniques from task planning to reliably generalize to\nnew environment. In this work, we propose a method for learning a model\nencoding just such a representation for task planning. We learn a neural net\nthat encodes the $k$ most likely outcomes from high level actions from a given\nworld. Our approach creates comprehensible task plans that allow us to predict\nchanges to the environment many time steps into the future. We demonstrate this\napproach via application to a stacking task in a cluttered environment, where\nthe robot must select between different colored blocks while avoiding\nobstacles, in order to perform a task. We also show results on a simple\nnavigation task. Our algorithm generates realistic image and pose predictions\nat multiple points in a given task.",
        "Neurons in the brain communicate with each other through discrete action\nspikes as opposed to continuous signal transmission in artificial neural\nnetworks. Therefore, the traditional techniques for optimization of parameters\nin neural networks which rely on the assumption of differentiability of\nactivation functions are no longer applicable to modeling the learning\nprocesses in the brain. In this project, we propose biologically-plausible\nalternatives to backpropagation to facilitate the training of spiking neural\nnetworks. We primarily focus on investigating the candidacy of reinforcement\nlearning (RL) rules in solving the spatial and temporal credit assignment\nproblems to enable decision-making in complex tasks. In one approach, we\nconsider each neuron in a multi-layer neural network as an independent RL agent\nforming a different representation of the feature space while the network as a\nwhole forms the representation of the complex policy to solve the task at hand.\nIn other approach, we apply the reparameterization trick to enable\ndifferentiation through stochastic transformations in spiking neural networks.\nWe compare and contrast the two approaches by applying them to traditional RL\ndomains such as gridworld, cartpole and mountain car. Further we also suggest\nvariations and enhancements to enable future research in this area.",
        "It has been observed that deep learning architectures tend to make erroneous\ndecisions with high reliability for particularly designed adversarial\ninstances. In this work, we show that the perturbation analysis of these\narchitectures provides a framework for generating adversarial instances by\nconvex programming which, for classification tasks, is able to recover variants\nof existing non-adaptive adversarial methods. The proposed framework can be\nused for the design of adversarial noise under various desirable constraints\nand different types of networks. Moreover, this framework is capable of\nexplaining various existing adversarial methods and can be used to derive new\nalgorithms as well. We make use of these results to obtain novel algorithms.\nThe experiments show the competitive performance of the obtained solutions, in\nterms of fooling ratio, when benchmarked with well-known adversarial methods.",
        "We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.",
        "To determine the 3D orientation and 3D location of objects in the\nsurroundings of a camera mounted on a robot or mobile device, we developed two\npowerful algorithms in object detection and temporal tracking that are combined\nseamlessly for robotic perception and interaction as well as Augmented Reality\n(AR). A separate evaluation of, respectively, the object detection and the\ntemporal tracker demonstrates the important stride in research as well as the\nimpact on industrial robotic applications and AR. When evaluated on a standard\ndataset, the detector produced the highest f1-score with a large margin while\nthe tracker generated the best accuracy at a very low latency of approximately\n2 ms per frame with one CPU core: both algorithms outperforming the state of\nthe art. When combined, we achieve a powerful framework that is robust to\nhandle multiple instances of the same object under occlusion and clutter while\nattaining real-time performance. Aiming at stepping beyond the simple scenarios\nused by current systems, often constrained by having a single object in absence\nof clutter, averting to touch the object to prevent close-range partial\nocclusion, selecting brightly colored objects to easily segment them\nindividually or assuming that the object has simple geometric structure, we\ndemonstrate the capacity to handle challenging cases under clutter, partial\nocclusion and varying lighting conditions with objects of different shapes and\nsizes.",
        "Many theoretical results on estimation of high dimensional time series\nrequire specifying an underlying data generating model (DGM). Instead, along\nthe footsteps of~\\cite{wong2017lasso}, this paper relies only on (strict)\nstationarity and $ \\beta $-mixing condition to establish consistency of lasso\nwhen data comes from a $\\beta$-mixing process with marginals having subgaussian\ntails. Because of the general assumptions, the data can come from DGMs\ndifferent than standard time series models such as VAR or ARCH. When the true\nDGM is not VAR, the lasso estimates correspond to those of the best linear\npredictors using the past observations. We establish non-asymptotic\ninequalities for estimation and prediction errors of the lasso estimates.\nTogether with~\\cite{wong2017lasso}, we provide lasso guarantees that cover full\nspectrum of the parameters in specifications of $ \\beta $-mixing subgaussian\ntime series. Applications of these results potentially extend to non-Gaussian,\nnon-Markovian and non-linear times series models as the examples we provide\ndemonstrate. In order to prove our results, we derive a novel Hanson-Wright\ntype concentration inequality for $\\beta$-mixing subgaussian random vectors\nthat may be of independent interest.",
        "To date, research on sensor-equipped mobile devices has primarily focused on\nthe purely supervised task of human activity recognition (walking, running,\netc), demonstrating limited success in inferring high-level health outcomes\nfrom low-level signals, such as acceleration. Here, we present a novel\nself-supervised representation learning method using activity and heart rate\n(HR) signals without semantic labels. With a deep neural network, we set HR\nresponses as the supervisory signal for the activity data, leveraging their\nunderlying physiological relationship.\n  We evaluate our model in the largest free-living combined-sensing dataset\n(comprising more than 280,000 hours of wrist accelerometer & wearable ECG data)\nand show that the resulting embeddings can generalize in various downstream\ntasks through transfer learning with linear classifiers, capturing\nphysiologically meaningful, personalized information. For instance, they can be\nused to predict (higher than 70 AUC) variables associated with individuals'\nhealth, fitness and demographic characteristics, outperforming unsupervised\nautoencoders and common bio-markers. Overall, we propose the first multimodal\nself-supervised method for behavioral and physiological data with implications\nfor large-scale health and lifestyle monitoring.",
        "Deep Neural Networks have been very successfully used for many computer\nvision and pattern recognition applications. While Convolutional Neural\nNetworks(CNNs) have shown the path to state of art image classifications,\nGenerative Adversarial Networks or GANs have provided state of art capabilities\nin image generation. In this paper we extend the applications of CNNs and GANs\nto experiment with up-sampling techniques in the domains of security and\nsurveillance. Through this work we evaluate, compare and contrast the state of\nart techniques in both CNN and GAN based image and video up-sampling in the\nsurveillance domain. As a result of this study we also provide experimental\nevidence to establish DISTS as a stronger Image Quality Assessment(IQA) metric\nfor comparing GAN Based Image Up-sampling in the surveillance domain.",
        "In this paper, we propose a novel unsupervised color constancy method, called\nProbabilistic Color Constancy (PCC). We define a framework for estimating the\nillumination of a scene by weighting the contribution of different image\nregions using a graph-based representation of the image. To estimate the weight\nof each (super-)pixel, we rely on two assumptions: (Super-)pixels with similar\ncolors contribute similarly and darker (super-)pixels contribute less. The\nresulting system has one global optimum solution. The proposed method achieves\ncompetitive performance, compared to the state-of-the-art, on INTEL-TAU\ndataset.",
        "Deep Neural Networks - especially Convolutional Neural Network (ConvNet) has\nbecome the state-of-the-art for image classification, pattern recognition and\nvarious computer vision tasks. ConvNet has a huge potential in medical domain\nfor analyzing medical data to diagnose diseases in an efficient way. Based on\nextracted features by ConvNet model from MRI data, early diagnosis is very\ncrucial for preventing progress and treating the Alzheimer's disease. Despite\nhaving the ability to deliver great performance, absence of interpretability of\nthe model's decision can lead to misdiagnosis which can be life threatening. In\nthis thesis, learned shape features and abstractions by 3D ConvNets for\ndetecting Alzheimer's disease were investigated using various visualization\ntechniques. How changes in network structures, used filters sizes and filters\nshapes affects the overall performance and learned features of the model were\nalso inspected. LRP relevance map of different models revealed which parts of\nthe brain were more relevant for the classification decision. Comparing the\nlearned filters by Activation Maximization showed how patterns were encoded in\ndifferent layers of the network. Finally, transfer learning from a\nconvolutional autoencoder was implemented to check whether increasing the\nnumber of training samples with patches of input to extract the low-level\nfeatures improves learned features and the model performance.",
        "Deep learning-based recommendation models are used pervasively and broadly,\nfor example, to recommend movies, products, or other information most relevant\nto users, in order to enhance the user experience. Among various application\ndomains which have received significant industry and academia research\nattention, such as image classification, object detection, language and speech\ntranslation, the performance of deep learning-based recommendation models is\nless well explored, even though recommendation tasks unarguably represent\nsignificant AI inference cycles at large-scale datacenter fleets. To advance\nthe state of understanding and enable machine learning system development and\noptimization for the commerce domain, we aim to define an industry-relevant\nrecommendation benchmark for the MLPerf Training andInference Suites. The paper\nsynthesizes the desirable modeling strategies for personalized recommendation\nsystems. We lay out desirable characteristics of recommendation model\narchitectures and data sets. We then summarize the discussions and advice from\nthe MLPerf Recommendation Advisory Board.",
        "In this paper we propose an algorithm that builds sparse decision DAGs\n(directed acyclic graphs) from a list of base classifiers provided by an\nexternal learning method such as AdaBoost. The basic idea is to cast the DAG\ndesign task as a Markov decision process. Each instance can decide to use or to\nskip each base classifier, based on the current state of the classifier being\nbuilt. The result is a sparse decision DAG where the base classifiers are\nselected in a data-dependent way. The method has a single hyperparameter with a\nclear semantics of controlling the accuracy/speed trade-off. The algorithm is\ncompetitive with state-of-the-art cascade detectors on three object-detection\nbenchmarks, and it clearly outperforms them when there is a small number of\nbase classifiers. Unlike cascades, it is also readily applicable for\nmulti-class classification. Using the multi-class setup, we show on a benchmark\nweb page ranking data set that we can significantly improve the decision speed\nwithout harming the performance of the ranker.",
        "Comorbid diseases co-occur and progress via complex temporal patterns that\nvary among individuals. In electronic health records we can observe the\ndifferent diseases a patient has, but can only infer the temporal relationship\nbetween each co-morbid condition. Learning such temporal patterns from event\ndata is crucial for understanding disease pathology and predicting prognoses.\nTo this end, we develop deep diffusion processes (DDP) to model \"dynamic\ncomorbidity networks\", i.e., the temporal relationships between comorbid\ndisease onsets expressed through a dynamic graph. A DDP comprises events\nmodelled as a multi-dimensional point process, with an intensity function\nparameterized by the edges of a dynamic weighted graph. The graph structure is\nmodulated by a neural network that maps patient history to edge weights,\nenabling rich temporal representations for disease trajectories. The DDP\nparameters decouple into clinically meaningful components, which enables\nserving the dual purpose of accurate risk prediction and intelligible\nrepresentation of disease pathology. We illustrate these features in\nexperiments using cancer registry data.",
        "We presents in this paper a novel fish classification methodology based on a\ncombination between robust feature selection, image segmentation and\ngeometrical parameter techniques using Artificial Neural Network and Decision\nTree. Unlike existing works for fish classification, which propose descriptors\nand do not analyze their individual impacts in the whole classification task\nand do not make the combination between the feature selection, image\nsegmentation and geometrical parameter, we propose a general set of features\nextraction using robust feature selection, image segmentation and geometrical\nparameter and their correspondent weights that should be used as a priori\ninformation by the classifier. In this sense, instead of studying techniques\nfor improving the classifiers structure itself, we consider it as a black box\nand focus our research in the determination of which input information must\nbring a robust fish discrimination.The main contribution of this paper is\nenhancement recognize and classify fishes based on digital image and To develop\nand implement a novel fish recognition prototype using global feature\nextraction, image segmentation and geometrical parameters, it have the ability\nto Categorize the given fish into its cluster and Categorize the clustered fish\ninto poison or non-poison fish, and categorizes the non-poison fish into its\nfamily .",
        "Graph neural networks (GNN) has been successfully applied to operate on the\ngraph-structured data. Given a specific scenario, rich human expertise and\ntremendous laborious trials are usually required to identify a suitable GNN\narchitecture. It is because the performance of a GNN architecture is\nsignificantly affected by the choice of graph convolution components, such as\naggregate function and hidden dimension. Neural architecture search (NAS) has\nshown its potential in discovering effective deep architectures for learning\ntasks in image and language modeling. However, existing NAS algorithms cannot\nbe directly applied to the GNN search problem. First, the search space of GNN\nis different from the ones in existing NAS work. Second, the representation\nlearning capacity of GNN architecture changes obviously with slight\narchitecture modifications. It affects the search efficiency of traditional\nsearch methods. Third, widely used techniques in NAS such as parameter sharing\nmight become unstable in GNN.\n  To bridge the gap, we propose the automated graph neural networks (AGNN)\nframework, which aims to find an optimal GNN architecture within a predefined\nsearch space. A reinforcement learning based controller is designed to greedily\nvalidate architectures via small steps. AGNN has a novel parameter sharing\nstrategy that enables homogeneous architectures to share parameters, based on a\ncarefully-designed homogeneity definition. Experiments on real-world benchmark\ndatasets demonstrate that the GNN architecture identified by AGNN achieves the\nbest performance, comparing with existing handcrafted models and tradistional\nsearch methods.",
        "Region Proposal Network (RPN) provides strong support for handling the scale\nvariation of objects in two-stage object detection. For one-stage detectors\nwhich do not have RPN, it is more demanding to have powerful sub-networks\ncapable of directly capturing objects of unknown sizes. To enhance such\ncapability, we propose an extremely efficient neural architecture search\nmethod, named Fast And Diverse (FAD), to better explore the optimal\nconfiguration of receptive fields and convolution types in the sub-networks for\none-stage detectors. FAD consists of a designed search space and an efficient\narchitecture search algorithm. The search space contains a rich set of diverse\ntransformations designed specifically for object detection. To cope with the\ndesigned search space, a novel search algorithm termed Representation Sharing\n(RepShare) is proposed to effectively identify the best combinations of the\ndefined transformations. In our experiments, FAD obtains prominent improvements\non two types of one-stage detectors with various backbones. In particular, our\nFAD detector achieves 46.4 AP on MS-COCO (under single-scale testing),\noutperforming the state-of-the-art detectors, including the most recent\nNAS-based detectors, Auto-FPN (searched for 16 GPU-days) and NAS-FCOS (28\nGPU-days), while significantly reduces the search cost to 0.6 GPU-days. Beyond\nobject detection, we further demonstrate the generality of FAD on the more\nchallenging instance segmentation, and expect it to benefit more tasks.",
        "Recently, graph neural networks (GNNs) have proved to be suitable in tasks on\nunstructured data. Particularly in tasks as community detection, node\nclassification, and link prediction. However, most GNN models still operate\nwith static relationships. We propose the Graph Learning Network (GLN), a\nsimple yet effective process to learn node embeddings and structure prediction\nfunctions. Our model uses graph convolutions to propose expected node features,\nand predict the best structure based on them. We repeat these steps recursively\nto enhance the prediction and the embeddings.",
        "Surveillance video parsing, which segments the video frames into several\nlabels, e.g., face, pants, left-leg, has wide applications.\nHowever,pixel-wisely annotating all frames is tedious and inefficient. In this\npaper, we develop a Single frame Video Parsing (SVP) method which requires only\none labeled frame per video in training stage. To parse one particular frame,\nthe video segment preceding the frame is jointly considered. SVP (1) roughly\nparses the frames within the video segment, (2) estimates the optical flow\nbetween frames and (3) fuses the rough parsing results warped by optical flow\nto produce the refined parsing result. The three components of SVP, namely\nframe parsing, optical flow estimation and temporal fusion are integrated in an\nend-to-end manner. Experimental results on two surveillance video datasets show\nthe superiority of SVP over state-of-the-arts.",
        "Hybrid modeling, the combination of first principle and machine learning\nmodels, is an emerging research field that gathers more and more attention.\nEven if hybrid models produce formidable results for academic examples, there\nare still different technical challenges that hinder the use of hybrid modeling\nin real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a\nnumerical ODE solver and an ANN, we are paving the way for the use of a variety\nof first principle models from different modeling tools as parts of hybrid\nmodels. This contribution handles the hybrid modeling of a complex, real-world\nexample: Starting with a simplified 1D-fluid model of the human cardiovascular\nsystem (arterial side), the aim is to learn neglected physical effects like\narterial elasticity from data. We will show that the hybrid modeling process is\nmore comfortable, needs less system knowledge and is therefore less error-prone\ncompared to modeling solely based on first principle. Further, the resulting\nhybrid model has improved in computation performance, compared to a pure first\nprinciple white-box model, while still fulfilling the requirements regarding\naccuracy of the considered hemodynamic quantities. The use of the presented\ntechniques is explained in a general manner and the considered use-case can\nserve as example for other modeling and simulation applications in and beyond\nthe medical domain.",
        "The accurate and interpretable prediction of future events in time-series\ndata often requires the capturing of representative patterns (or referred to as\nstates) underpinning the observed data. To this end, most existing studies\nfocus on the representation and recognition of states, but ignore the changing\ntransitional relations among them. In this paper, we present evolutionary state\ngraph, a dynamic graph structure designed to systematically represent the\nevolving relations (edges) among states (nodes) along time. We conduct analysis\non the dynamic graphs constructed from the time-series data and show that\nchanges on the graph structures (e.g., edges connecting certain state nodes)\ncan inform the occurrences of events (i.e., time-series fluctuation). Inspired\nby this, we propose a novel graph neural network model, Evolutionary State\nGraph Network (EvoNet), to encode the evolutionary state graph for accurate and\ninterpretable time-series event prediction. Specifically, Evolutionary State\nGraph Network models both the node-level (state-to-state) and graph-level\n(segment-to-segment) propagation, and captures the node-graph\n(state-to-segment) interactions over time. Experimental results based on five\nreal-world datasets show that our approach not only achieves clear improvements\ncompared with 11 baselines, but also provides more insights towards explaining\nthe results of event predictions.",
        "Wide accessibility of imaging and profile sensors in modern industrial\nsystems created an abundance of high-dimensional sensing variables. This led to\na a growing interest in the research of high-dimensional process monitoring.\nHowever, most of the approaches in the literature assume the in-control\npopulation to lie on a linear manifold with a given basis (i.e., spline,\nwavelet, kernel, etc) or an unknown basis (i.e., principal component analysis\nand its variants), which cannot be used to efficiently model profiles with a\nnonlinear manifold which is common in many real-life cases. We propose deep\nprobabilistic autoencoders as a viable unsupervised learning approach to model\nsuch manifolds. To do so, we formulate nonlinear and probabilistic extensions\nof the monitoring statistics from classical approaches as the expected\nreconstruction error (ERE) and the KL-divergence (KLD) based monitoring\nstatistics. Through extensive simulation study, we provide insights on why\nlatent-space based statistics are unreliable and why residual-space based ones\ntypically perform much better for deep learning based approaches. Finally, we\ndemonstrate the superiority of deep probabilistic models via both simulation\nstudy and a real-life case study involving images of defects from a hot steel\nrolling process.",
        "Local feature frameworks are difficult to learn in an end-to-end fashion, due\nto the discreteness inherent to the selection and matching of sparse keypoints.\nWe introduce DISK (DIScrete Keypoints), a novel method that overcomes these\nobstacles by leveraging principles from Reinforcement Learning (RL), optimizing\nend-to-end for a high number of correct feature matches. Our simple yet\nexpressive probabilistic model lets us keep the training and inference regimes\nclose, while maintaining good enough convergence properties to reliably train\nfrom scratch. Our features can be extracted very densely while remaining\ndiscriminative, challenging commonly held assumptions about what constitutes a\ngood keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on\nthree public benchmarks.",
        "Image quality measurement is a critical problem for image super-resolution\n(SR) algorithms. Usually, they are evaluated by some well-known objective\nmetrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results\nin accordance with the perception of human being. Recently, a more reasonable\nperception measurement has been proposed in [1], which is also adopted by the\nPIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a\nhigh-quality SR result which balances between the two indices, i.e., the\nperception index and root-mean-square error (RMSE). To do so, we design a new\ndeep SR framework, dubbed Bi-GANs-ST, by integrating two complementary\ngenerative adversarial networks (GAN) branches. One is memory residual SRGAN\n(MR-SRGAN), which emphasizes on improving the objective performance, such as\nreducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which\nobtains the result that favors better subjective perception via a two-stage\nadversarial training mechanism. Then, to produce final result with excellent\nperception scores and RMSE, we use soft-thresholding method to merge the\nresults generated by the two GANs. Our method performs well on the perceptual\nimage super-resolution task of the PIRM 2018 challenge. Experimental results on\nfive benchmarks show that our proposal achieves highly competent performance\ncompared with other state-of-the-art methods.",
        "Large-scale distributed training of neural networks is often limited by\nnetwork bandwidth, wherein the communication time overwhelms the local\ncomputation time. Motivated by the success of sketching methods in\nsub-linear/streaming algorithms, we introduce Sketched SGD, an algorithm for\ncarrying out distributed SGD by communicating sketches instead of full\ngradients. We show that Sketched SGD has favorable convergence rates on several\nclasses of functions. When considering all communication -- both of gradients\nand of updated model weights -- Sketched SGD reduces the amount of\ncommunication required compared to other gradient compression methods from\n$\\mathcal{O}(d)$ or $\\mathcal{O}(W)$ to $\\mathcal{O}(\\log d)$, where $d$ is the\nnumber of model parameters and $W$ is the number of workers participating in\ntraining. We run experiments on a transformer model, an LSTM, and a residual\nnetwork, demonstrating up to a 40x reduction in total communication cost with\nno loss in final model performance. We also show experimentally that Sketched\nSGD scales to at least 256 workers without increasing communication cost or\ndegrading model performance.",
        "I consider unsupervised extensions of the fast stepwise linear regression\nalgorithm \\cite{efroymson1960multiple}. These extensions allow one to\nefficiently identify highly-representative feature variable subsets within a\ngiven set of jointly distributed variables. This in turn allows for the\nefficient dimensional reduction of large data sets via the removal of redundant\nfeatures. Fast search is effected here through the avoidance of repeat\ncomputations across trial fits, allowing for a full representative-importance\nranking of a set of feature variables to be carried out in $O(n^2 m)$ time,\nwhere $n$ is the number of variables and $m$ is the number of data samples\navailable. This runtime complexity matches that needed to carry out a single\nregression and is $O(n^2)$ faster than that of naive implementations. I present\npseudocode suitable for efficient forward, reverse, and forward-reverse\nunsupervised feature selection. To illustrate the algorithm's application, I\napply it to the problem of identifying representative stocks within a given\nfinancial market index -- a challenge relevant to the design of Exchange Traded\nFunds (ETFs). I also characterize the growth of numerical error with iteration\nstep in these algorithms, and finally demonstrate and rationalize the\nobservation that the forward and reverse algorithms return exactly inverted\nfeature orderings in the weakly-correlated feature set regime.",
        "Classification with abstention has gained a lot of attention in recent years\nas it allows to incorporate human decision-makers in the process. Yet,\nabstention can potentially amplify disparities and lead to discriminatory\npredictions. The goal of this work is to build a general purpose classification\nalgorithm, which is able to abstain from prediction, while avoiding disparate\nimpact. We formalize this problem as risk minimization under fairness and\nabstention constraints for which we derive the form of the optimal classifier.\nBuilding on this result, we propose a post-processing classification algorithm,\nwhich is able to modify any off-the-shelf score-based classifier using only\nunlabeled sample. We establish finite sample risk, fairness, and abstention\nguarantees for the proposed algorithm. In particular, it is shown that fairness\nand abstention constraints can be achieved independently from the initial\nclassifier as long as sufficiently many unlabeled data is available. The risk\nguarantee is established in terms of the quality of the initial classifier. Our\npost-processing scheme reduces to a sparse linear program allowing for an\nefficient implementation, which we provide. Finally, we validate our method\nempirically showing that moderate abstention rates allow to bypass the\nrisk-fairness trade-off.",
        "Pedestrian trajectory prediction is a critical to avoid autonomous driving\ncollision. But this prediction is a challenging problem due to social forces\nand cluttered scenes. Such human-human and human-space interactions lead to\nmany socially plausible trajectories. In this paper, we propose a novel\nLSTM-based algorithm. We tackle the problem by considering the static scene and\npedestrian which combine the Graph Convolutional Networks and Temporal\nConvolutional Networks to extract features from pedestrians. Each pedestrian in\nthe scene is regarded as a node, and we can obtain the relationship between\neach node and its neighborhoods by graph embedding. It is LSTM that encode the\nrelationship so that our model predicts nodes trajectories in crowd scenarios\nsimultaneously. To effectively predict multiple possible future trajectories,\nwe further introduce Spatio-Temporal Convolutional Block to make the network\nflexible. Experimental results on two public datasets, i.e. ETH and UCY,\ndemonstrate the effectiveness of our proposed ST-Block and we achieve\nstate-of-the-art approaches in human trajectory prediction.",
        "Current state-of-the-art methods for image segmentation form a dense image\nrepresentation where the color, shape and texture information are all processed\ntogether inside a deep CNN. This however may not be ideal as they contain very\ndifferent type of information relevant for recognition. Here, we propose a new\ntwo-stream CNN architecture for semantic segmentation that explicitly wires\nshape information as a separate processing branch, i.e. shape stream, that\nprocesses information in parallel to the classical stream. Key to this\narchitecture is a new type of gates that connect the intermediate layers of the\ntwo streams. Specifically, we use the higher-level activations in the classical\nstream to gate the lower-level activations in the shape stream, effectively\nremoving noise and helping the shape stream to only focus on processing the\nrelevant boundary-related information. This enables us to use a very shallow\narchitecture for the shape stream that operates on the image-level resolution.\nOur experiments show that this leads to a highly effective architecture that\nproduces sharper predictions around object boundaries and significantly boosts\nperformance on thinner and smaller objects. Our method achieves\nstate-of-the-art performance on the Cityscapes benchmark, in terms of both mask\n(mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong\nbaselines.",
        "Object detection remains as one of the most notorious open problems in\ncomputer vision. Despite large strides in accuracy in recent years, modern\nobject detectors have started to saturate on popular benchmarks raising the\nquestion of how far we can reach with deep learning tools and tricks. Here, by\nemploying 2 state-of-the-art object detection benchmarks, and analyzing more\nthan 15 models over 4 large scale datasets, we I) carefully determine the upper\nbound in AP, which is 91.6% on VOC (test2007), 78.2% on COCO (val2017), and\n58.9% on OpenImages V4 (validation), regardless of the IOU threshold. These\nnumbers are much better than the mAP of the best model (47.9% on VOC, and 46.9%\non COCO; IOUs=.5:.05:.95), II) characterize the sources of errors in object\ndetectors, in a novel and intuitive way, and find that classification error\n(confusion with other classes and misses) explains the largest fraction of\nerrors and weighs more than localization and duplicate errors, and III) analyze\nthe invariance properties of models when surrounding context of an object is\nremoved, when an object is placed in an incongruent background, and when images\nare blurred or flipped vertically. We find that models generate a lot of boxes\non empty regions and that context is more important for detecting small objects\nthan larger ones. Our work taps into the tight relationship between object\ndetection and object recognition and offers insights for building better\nmodels. Our code is publicly available at\nhttps://github.com/aliborji/Deetctionupper bound.git.",
        "In recent years, graph neural networks (GNNs) have gained increasing\npopularity and have shown very promising results for data that are represented\nby graphs. The majority of GNN architectures are designed based on developing\nnew convolutional and/or pooling layers that better extract the hidden and\ndeeper representations of the graphs to be used for different prediction tasks.\nThe inputs to these layers are mainly the three default descriptors of a graph,\nnode features $(X)$, adjacency matrix $(A)$, and edge features $(W)$ (if\navailable). To provide a more enriched input to the network, we propose a\nrandom walk data processing of the graphs based on three selected lengths.\nNamely, (regular) walks of length 1 and 2, and a fractional walk of length\n$\\gamma \\in (0,1)$, in order to capture the different local and global dynamics\non the graphs. We also calculate the stationary distribution of each random\nwalk, which is then used as a scaling factor for the initial node features\n($X$). This way, for each graph, the network receives multiple adjacency\nmatrices along with their individual weighting for the node features. We test\nour method on various molecular datasets by passing the processed node features\nto the network in order to perform several classification and regression tasks.\nInterestingly, our method, not using edge features which are heavily exploited\nin molecular graph learning, let a shallow network outperform well known deep\nGNNs.",
        "How to produce expressive molecular representations is a fundamental\nchallenge in AI-driven drug discovery. Graph neural network (GNN) has emerged\nas a powerful technique for modeling molecular data. However, previous\nsupervised approaches usually suffer from the scarcity of labeled data and have\npoor generalization capability. Here, we proposed a novel Molecular\nPre-training Graph-based deep learning framework, named MPG, that leans\nmolecular representations from large-scale unlabeled molecules. In MPG, we\nproposed a powerful MolGNet model and an effective self-supervised strategy for\npre-training the model at both the node and graph-level. After pre-training on\n11 million unlabeled molecules, we revealed that MolGNet can capture valuable\nchemistry insights to produce interpretable representation. The pre-trained\nMolGNet can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of drug discovery tasks, including\nmolecular properties prediction, drug-drug interaction, and drug-target\ninteraction, involving 13 benchmark datasets. Our work demonstrates that MPG is\npromising to become a novel approach in the drug discovery pipeline.",
        "In this work, we present a simple yet effective framework to address the\ndomain translation problem between different sensor modalities with unique data\nformats. By relying only on the semantics of the scene, our modular generative\nframework can, for the first time, synthesize a panoramic color image from a\ngiven full 3D LiDAR point cloud. The framework starts with semantic\nsegmentation of the point cloud, which is initially projected onto a spherical\nsurface. The same semantic segmentation is applied to the corresponding camera\nimage. Next, our new conditional generative model adversarially learns to\ntranslate the predicted LiDAR segment maps to the camera image counterparts.\nFinally, generated image segments are processed to render the panoramic scene\nimages. We provide a thorough quantitative evaluation on the SemanticKitti\ndataset and show that our proposed framework outperforms other strong baseline\nmodels.\n  Our source code is available at\nhttps://github.com/halmstad-University/TITAN-NET",
        "Visual place recognition is an important component of systems for camera\nlocalization and loop closure detection. It concerns the recognition of a\npreviously visited place based on visual cues only. Although it is a widely\nstudied problem for indoor and urban environments, the recent use of robots for\nautomation of agricultural and gardening tasks has created new problems, due to\nthe challenging appearance of garden-like environments. Garden scenes\npredominantly contain green colors, as well as repetitive patterns and\ntextures. The lack of available data recorded in gardens and natural\nenvironments makes the improvement of visual localization algorithms difficult.\nIn this paper we propose an extended version of the TB-Places data set, which\nis designed for testing algorithms for visual place recognition. It contains\nimages with ground truth camera pose recorded in real gardens in different\nseasons, with varying light conditions. We constructed and released a ground\ntruth for all possible pairs of images, indicating whether they depict the same\nplace or not. We present the results of a benchmark analysis of methods based\non convolutional neural networks for holistic image description and place\nrecognition. We train existing networks (i.e. ResNet, DenseNet and VGG NetVLAD)\nas backbone of a two-way architecture with a contrastive loss function. The\nresults that we obtained demonstrate that learning garden-tailored\nrepresentations contribute to an improvement of performance, although the\ngeneralization capabilities are limited.",
        "Learning from unordered sets is a fundamental learning setup, recently\nattracting increasing attention. Research in this area has focused on the case\nwhere elements of the set are represented by feature vectors, and far less\nemphasis has been given to the common case where set elements themselves adhere\nto their own symmetries. That case is relevant to numerous applications, from\ndeblurring image bursts to multi-view 3D shape recognition and reconstruction.\nIn this paper, we present a principled approach to learning sets of general\nsymmetric elements. We first characterize the space of linear layers that are\nequivariant both to element reordering and to the inherent symmetries of\nelements, like translation in the case of images. We further show that networks\nthat are composed of these layers, called Deep Sets for Symmetric Elements\n(DSS) layers, are universal approximators of both invariant and equivariant\nfunctions, and that these networks are strictly more expressive than Siamese\nnetworks. DSS layers are also straightforward to implement. Finally, we show\nthat they improve over existing set-learning architectures in a series of\nexperiments with images, graphs, and point-clouds.",
        "Using the raw data from consumer-level RGB-D cameras as input, we propose a\ndeep-learning based approach to efficiently generate RGB-D images with\ncompleted information in high resolution. To process the input images in low\nresolution with missing regions, new operators for adaptive convolution are\nintroduced in our deep-learning network that consists of three cascaded modules\n-- the completion module, the refinement module and the super-resolution\nmodule. The completion module is based on an architecture of encoder-decoder,\nwhere the features of input raw RGB-D will be automatically extracted by the\nencoding layers of a deep neural-network. The decoding layers are applied to\nreconstruct the completed depth map, which is followed by a refinement module\nto sharpen the boundary of different regions. For the super-resolution module,\nwe generate RGB-D images in high resolution by multiple layers for feature\nextraction and a layer for up-sampling. Benefited from the adaptive convolution\noperators newly proposed in this paper, our results outperform the existing\ndeep-learning based approaches for RGB-D image complete and super-resolution.\nAs an end-to-end approach, high fidelity RGB-D images can be generated\nefficiently at the rate of around 21 frames per second.",
        "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
        "'Sharing of statistical strength' is a phrase often employed in machine\nlearning and signal processing. In sensor networks, for example, missing\nsignals from certain sensors may be predicted by exploiting their correlation\nwith observed signals acquired from other sensors. For humans, our hands move\nsynchronously with our legs, and we can exploit these implicit correlations for\npredicting new poses and for generating new natural-looking walking sequences.\nWe can also go much further and exploit this form of transfer learning, to\ndevelop new control schemas for robust control of rehabilitation robots. In\nthis short paper we introduce coregionalised locomotion envelopes - a method\nfor multi-dimensional manifold regression, on human locomotion variates. Herein\nwe render a qualitative description of this method.",
        "Nowadays, modern Earth Observation systems continuously generate huge amounts\nof data. A notable example is represented by the Sentinel-2 mission, which\nprovides images at high spatial resolution (up to 10m) with high temporal\nrevisit period (every 5 days), which can be organized in Satellite Image Time\nSeries (SITS). While the use of SITS has been proved to be beneficial in the\ncontext of Land Use/Land Cover (LULC) map generation, unfortunately, machine\nlearning approaches commonly leveraged in remote sensing field fail to take\nadvantage of spatio-temporal dependencies present in such data.\n  Recently, new generation deep learning methods allowed to significantly\nadvance research in this field. These approaches have generally focused on a\nsingle type of neural network, i.e., Convolutional Neural Networks (CNNs) or\nRecurrent Neural Networks (RNNs), which model different but complementary\ninformation: spatial autocorrelation (CNNs) and temporal dependencies (RNNs).\nIn this work, we propose the first deep learning architecture for the analysis\nof SITS data, namely \\method{} (DUal view Point deep Learning architecture for\ntime series classificatiOn), that combines Convolutional and Recurrent neural\nnetworks to exploit their complementarity. Our hypothesis is that, since CNNs\nand RNNs capture different aspects of the data, a combination of both models\nwould produce a more diverse and complete representation of the information for\nthe underlying land cover classification task. Experiments carried out on two\nstudy sites characterized by different land cover characteristics (i.e., the\n\\textit{Gard} site in France and the \\textit{Reunion Island} in the Indian\nOcean), demonstrate the significance of our proposal.",
        "Learning to reach goal states and learning diverse skills through mutual\ninformation (MI) maximization have been proposed as principled frameworks for\nself-supervised reinforcement learning, allowing agents to acquire broadly\napplicable multitask policies with minimal reward engineering. Starting from a\nsimple observation that the standard goal-conditioned RL (GCRL) is encapsulated\nby the optimization objective of variational empowerment, we discuss how GCRL\nand MI-based RL can be generalized into a single family of methods, which we\nname variational GCRL (VGCRL), interpreting variational MI maximization, or\nvariational empowerment, as representation learning methods that acquire\nfunctionally-aware state representations for goal reaching. This novel\nperspective allows us to: (1) derive simple but unexplored variants of GCRL to\nstudy how adding small representation capacity can already expand its\ncapabilities; (2) investigate how discriminator function capacity and\nsmoothness determine the quality of discovered skills, or latent goals, through\nmodifying latent dimensionality and applying spectral normalization; (3) adapt\ntechniques such as hindsight experience replay (HER) from GCRL to MI-based RL;\nand lastly, (4) propose a novel evaluation metric, named latent goal reaching\n(LGR), for comparing empowerment algorithms with different choices of latent\ndimensionality and discriminator parameterization. Through principled\nmathematical derivations and careful experimental studies, our work lays a\nnovel foundation from which to evaluate, analyze, and develop representation\nlearning techniques in goal-based RL.",
        "It is not easy when evaluating 3D mapping performance because existing\nmetrics require ground truth data that can only be collected with special\ninstruments. In this paper, we propose a metric, dense map posterior (DMP), for\nthis evaluation. It can work without any ground truth data. Instead, it\ncalculates a comparable value, reflecting a map posterior probability, from\ndense point cloud observations. In our experiments, the proposed DMP is\nbenchmarked against ground truth-based metrics. Results show that DMP can\nprovide a similar evaluation capability. The proposed metric makes evaluating\ndifferent methods more flexible and opens many new possibilities, such as\nself-supervised methods and more available datasets.",
        "Cutting and pasting image segments feels intuitive: the choice of source\ntemplates gives artists flexibility in recombining existing source material.\nFormally, this process takes an image set as input and outputs a collage of the\nset elements. Such selection from sets of source templates does not fit easily\nin classical convolutional neural models requiring inputs of fixed size.\nInspired by advances in attention and set-input machine learning, we present a\nnovel architecture that can generate in one forward pass image collages of\nsource templates using set-structured representations. This paper has the\nfollowing contributions: (i) a novel framework for image generation called\nMemory Attentive Generation of Image Collages (MAGIC) which gives artists new\nways to create digital collages; (ii) from the machine-learning perspective, we\nshow a novel Generative Adversarial Networks (GAN) architecture that uses\nSet-Transformer layers and set-pooling to blend sets of random image samples -\na hybrid non-parametric approach.",
        "Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results.",
        "Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.",
        "In this paper we suggest NICE: a new algorithm to generate counterfactual\nexplanations for heterogeneous tabular data. The design of our algorithm\nspecifically takes into account algorithmic requirements that often emerge in\nreal-life deployments: the ability to provide an explanation for all\npredictions, being efficient in run-time, and being able to handle any\nclassification model (also non-differentiable ones). More specifically, our\napproach exploits information from a nearest instance tospeed up the search\nprocess. We propose four versions of NICE, where three of them optimize the\nexplanations for one of the following properties: sparsity, proximity or\nplausibility. An extensive empirical comparison on 10 datasets shows that our\nalgorithm performs better on all properties than the current state-of-the-art.\nThese analyses show a trade-off between on the one hand plausiblity and on the\nother hand proximity or sparsity, with our different optimization methods\noffering the choice to select the preferred trade-off. An open-source\nimplementation of NICE can be found at https://github.com/ADMAntwerp/NICE.",
        "In the last decade, crowd counting and localization attract much attention of\nresearchers due to its wide-spread applications, including crowd monitoring,\npublic safety, space design, etc. Many Convolutional Neural Networks (CNN) are\ndesigned for tackling this task. However, currently released datasets are so\nsmall-scale that they can not meet the needs of the supervised CNN-based\nalgorithms. To remedy this problem, we construct a large-scale congested crowd\ncounting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a\ntotal of 2,133,375 annotated heads with points and boxes. Compared with other\nreal-world datasets, it contains various illumination scenes and has the\nlargest density range (0~20,033). Besides, a benchmark website is developed for\nimpartially evaluating the different methods, which allows researchers to\nsubmit the results of the test set. Based on the proposed dataset, we further\ndescribe the data characteristics, evaluate the performance of some mainstream\nstate-of-the-art (SOTA) methods, and analyze the new problems that arise on the\nnew data. What's more, the benchmark is deployed at\n\\url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are\navailable at \\url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.",
        "Our research aims to propose a new performance-explainability analytical\nframework to assess and benchmark machine learning methods. The framework\ndetails a set of characteristics that systematize the\nperformance-explainability assessment of existing machine learning methods. In\norder to illustrate the use of the framework, we apply it to benchmark the\ncurrent state-of-the-art multivariate time series classifiers.",
        "As the class size grows, maintaining a balanced dataset across many classes\nis challenging because the data are long-tailed in nature; it is even\nimpossible when the sample-of-interest co-exists with each other in one\ncollectable unit, e.g., multiple visual instances in one image. Therefore,\nlong-tailed classification is the key to deep learning at scale. However,\nexisting methods are mainly based on re-weighting/re-sampling heuristics that\nlack a fundamental theory. In this paper, we establish a causal inference\nframework, which not only unravels the whys of previous methods, but also\nderives a new principled solution. Specifically, our theory shows that the SGD\nmomentum is essentially a confounder in long-tailed classification. On one\nhand, it has a harmful causal effect that misleads the tail prediction biased\ntowards the head. On the other hand, its induced mediation also benefits the\nrepresentation learning and head prediction. Our framework elegantly\ndisentangles the paradoxical effects of the momentum, by pursuing the direct\ncausal effect caused by an input sample. In particular, we use causal\nintervention in training, and counterfactual reasoning in inference, to remove\nthe \"bad\" while keep the \"good\". We achieve new state-of-the-arts on three\nlong-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100,\nImageNet-LT for image classification and LVIS for instance segmentation.",
        "The growth of autonomous vehicles, ridesharing systems, and self driving\ntechnology will bring a shift in the way ride hailing platforms plan out their\nservices. However, these advances in technology coupled with road congestion,\nenvironmental concerns, fuel usage, vehicles emissions, and the high cost of\nthe vehicle usage have brought more attention to better utilize the use of\nvehicles and their capacities. In this paper, we propose a novel multi-hop\nride-sharing (MHRS) algorithm that uses deep reinforcement learning to learn\noptimal vehicle dispatch and matching decisions by interacting with the\nexternal environment. By allowing customers to transfer between vehicles, i.e.,\nride with one vehicle for sometime and then transfer to another one, MHRS helps\nin attaining 30\\% lower cost and 20\\% more efficient utilization of fleets, as\ncompared to the ride-sharing algorithms. This flexibility of multi-hop feature\ngives a seamless experience to customers and ride-sharing companies, and thus\nimproves ride-sharing services.",
        "Recent work on predicting patient outcomes in the Intensive Care Unit (ICU)\nhas focused heavily on the physiological time series data, largely ignoring\nsparse data such as diagnoses and medications. When they are included, they are\nusually concatenated in the late stages of a model, which may struggle to learn\nfrom rarer disease patterns. Instead, we propose a strategy to exploit\ndiagnoses as relational information by connecting similar patients in a graph.\nTo this end, we propose LSTM-GNN for patient outcome prediction tasks: a hybrid\nmodel combining Long Short-Term Memory networks (LSTMs) for extracting temporal\nfeatures and Graph Neural Networks (GNNs) for extracting the patient\nneighbourhood information. We demonstrate that LSTM-GNNs outperform the\nLSTM-only baseline on length of stay prediction tasks on the eICU database.\nMore generally, our results indicate that exploiting information from\nneighbouring patient cases using graph neural networks is a promising research\ndirection, yielding tangible returns in supervised learning performance on\nElectronic Health Records.",
        "We present a new learning-based framework to recover vehicle pose in SO(3)\nfrom a single RGB image. In contrast to previous works that map from local\nappearance to observation angles, we explore a progressive approach by\nextracting meaningful Intermediate Geometrical Representations (IGRs) to\nestimate egocentric vehicle orientation. This approach features a deep model\nthat transforms perceived intensities to IGRs, which are mapped to a 3D\nrepresentation encoding object orientation in the camera coordinate system.\nCore problems are what IGRs to use and how to learn them more effectively. We\nanswer the former question by designing IGRs based on an interpolated cuboid\nthat derives from primitive 3D annotation readily. The latter question\nmotivates us to incorporate geometry knowledge with a new loss function based\non a projective invariant. This loss function allows unlabeled data to be used\nin the training stage to improve representation learning. Without additional\nlabels, our system outperforms previous monocular RGB-based methods for joint\nvehicle detection and pose estimation on the KITTI benchmark, achieving\nperformance even comparable to stereo methods. Code and pre-trained models are\navailable at this https URL.",
        "Low-rank approximation is an effective model compression technique to not\nonly reduce parameter storage requirements, but to also reduce computations.\nFor convolutional neural networks (CNNs), however, well-known low-rank\napproximation methods, such as Tucker or CP decomposition, result in degraded\nmodel accuracy because decomposed layers hinder training convergence. In this\npaper, we propose a new training technique that finds a flat minimum in the\nview of low-rank approximation without a decomposed structure during training.\nBy preserving the original model structure, 2-dimensional low-rank\napproximation demanding lowering (such as im2col) is available in our proposed\nscheme. We show that CNN models can be compressed by low-rank approximation\nwith much higher compression ratio than conventional training methods while\nmaintaining or even enhancing model accuracy. We also discuss various\n2-dimensional low-rank approximation techniques for CNNs.",
        "Transmission electron microscopy (TEM) is one of the primary tools to show\nmicrostructural characterization of materials as well as film thickness.\nHowever, manual determination of film thickness from TEM images is\ntime-consuming as well as subjective, especially when the films in question are\nvery thin and the need for measurement precision is very high. Such is the case\nfor head overcoat (HOC) thickness measurements in the magnetic hard disk drive\nindustry. It is therefore necessary to develop software to automatically\nmeasure HOC thickness. In this paper, for the first time, we propose a HOC\nlayer segmentation method using NASNet-Large as an encoder and then followed by\na decoder architecture, which is one of the most commonly used architectures in\ndeep learning for image segmentation. To further improve segmentation results,\nwe are the first to propose a post-processing layer to remove irrelevant\nportions in the segmentation result. To measure the thickness of the segmented\nHOC layer, we propose a regressive convolutional neural network (RCNN) model as\nwell as orthogonal thickness calculation methods. Experimental results\ndemonstrate a higher dice score for our model which has lower mean squared\nerror and outperforms current state-of-the-art manual measurement.",
        "Modern object detection methods based on convolutional neural network suffer\nfrom severe catastrophic forgetting in learning new classes without original\ndata. Due to time consumption, storage burden and privacy of old data, it is\ninadvisable to train the model from scratch with both old and new data when new\nobject classes emerge after the model trained. In this paper, we propose a\nnovel incremental object detector based on Faster R-CNN to continuously learn\nfrom new object classes without using old data. It is a triple network where an\nold model and a residual model as assistants for helping the incremental model\nlearning on new classes without forgetting the previous learned knowledge. To\nbetter maintain the discrimination of features between old and new classes, the\nresidual model is jointly trained on new classes in the incremental learning\nprocedure. In addition, a corresponding distillation scheme is designed to\nguide the training process, which consists of a two-level residual distillation\nloss and a joint classification distillation loss. Extensive experiments on\nVOC2007 and COCO are conducted, and the results demonstrate that the proposed\nmethod can effectively learn to incrementally detect objects of new classes,\nand the problem of catastrophic forgetting is mitigated in this context.",
        "Conditional Generative Adversarial Networks (cGANs) extend the standard\nunconditional GAN framework to learning joint data-label distributions from\nsamples, and have been established as powerful generative models capable of\ngenerating high-fidelity imagery. A challenge of training such a model lies in\nproperly infusing class information into its generator and discriminator. For\nthe discriminator, class conditioning can be achieved by either (1) directly\nincorporating labels as input or (2) involving labels in an auxiliary\nclassification loss. In this paper, we show that the former directly aligns the\nclass-conditioned fake-and-real data distributions\n$P(\\text{image}|\\text{class})$ ({\\em data matching}), while the latter aligns\ndata-conditioned class distributions $P(\\text{class}|\\text{image})$ ({\\em label\nmatching}). Although class separability does not directly translate to sample\nquality and becomes a burden if classification itself is intrinsically\ndifficult, the discriminator cannot provide useful guidance for the generator\nif features of distinct classes are mapped to the same point and thus become\ninseparable. Motivated by this intuition, we propose a Dual Projection GAN\n(P2GAN) model that learns to balance between {\\em data matching} and {\\em label\nmatching}. We then propose an improved cGAN model with Auxiliary Classification\nthat directly aligns the fake and real conditionals\n$P(\\text{class}|\\text{image})$ by minimizing their $f$-divergence. Experiments\non a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world\ndatasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of\nour proposed models.",
        "Referring expression comprehension aims to localize objects identified by\nnatural language descriptions. This is a challenging task as it requires\nunderstanding of both visual and language domains. One nature is that each\nobject can be described by synonymous sentences with paraphrases, and such\nvarieties in languages have critical impact on learning a comprehension model.\nWhile prior work usually treats each sentence and attends it to an object\nseparately, we focus on learning a referring expression comprehension model\nthat considers the property in synonymous sentences. To this end, we develop an\nend-to-end trainable framework to learn contrastive features on the image and\nobject instance levels, where features extracted from synonymous sentences to\ndescribe the same object should be closer to each other after mapping to the\nvisual domain. We conduct extensive experiments to evaluate the proposed\nalgorithm on several benchmark datasets, and demonstrate that our method\nperforms favorably against the state-of-the-art approaches. Furthermore, since\nthe varieties in expressions become larger across datasets when they describe\nobjects in different ways, we present the cross-dataset and transfer learning\nsettings to validate the ability of our learned transferable features.",
        "Unsupervised outlier detection, which predicts if a test sample is an outlier\nor not using only the information from unlabelled inlier data, is an important\nbut challenging task. Recently, methods based on the two-stage framework\nachieve state-of-the-art performance on this task. The framework leverages\nself-supervised representation learning algorithms to train a feature extractor\non inlier data, and applies a simple outlier detector in the feature space. In\nthis paper, we explore the possibility of avoiding the high cost of training a\ndistinct representation for each outlier detection task, and instead using a\nsingle pre-trained network as the universal feature extractor regardless of the\nsource of in-domain data. In particular, we replace the task-specific feature\nextractor by one network pre-trained on ImageNet with a self-supervised loss.\nIn experiments, we demonstrate competitive or better performance on a variety\nof outlier detection benchmarks compared with previous two-stage methods,\nsuggesting that learning representations from in-domain data may be unnecessary\nfor outlier detection.",
        "The end-to-end Human Mesh Recovery (HMR) approach has been successfully used\nfor 3D body reconstruction. However, most HMR-based frameworks reconstruct\nhuman body by directly learning mesh parameters from images or videos, while\nlacking explicit guidance of 3D human pose in visual data. As a result, the\ngenerated mesh often exhibits incorrect pose for complex activities. To tackle\nthis problem, we propose to exploit 3D pose to calibrate human mesh.\nSpecifically, we develop two novel Pose Calibration frameworks, i.e., Serial\nPC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in\na serial or parallel manner, these two frameworks can effectively correct human\nmesh with guidance of a concise pose calibration module. Furthermore, since the\ncalibration module is designed via non-rigid pose transformation, our PC-HMR\nframeworks can flexibly tackle bone length variations to alleviate misplacement\nin the calibrated mesh. Finally, our frameworks are based on generic and\ncomplementary integration of data-driven learning and geometrical modeling. Via\nplug-and-play modules, they can be efficiently adapted for both\nimage/video-based human mesh recovery. Additionally, they have no requirement\nof extra 3D pose annotations in the testing phase, which releases inference\ndifficulties in practice. We perform extensive experiments on the popular\nbench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks\nachieve the SOTA results.",
        "We present a novel framework to generate images of different age while\npreserving identity information, which is known as face aging. Different from\nmost recent popular face aging networks utilizing Generative Adversarial\nNetworks(GANs) application, our approach do not simply transfer a young face to\nan old one. Instead, we employ the edge map as intermediate representations,\nfirstly edge maps of young faces are extracted, a CycleGAN-based network is\nadopted to transfer them into edge maps of old faces, then another\npix2pixHD-based network is adopted to transfer the synthesized edge maps,\nconcatenated with identity information, into old faces. In this way, our method\ncan generate more realistic transfered images, simultaneously ensuring that\nface identity information be preserved well, and the apparent age of the\ngenerated image be accurately appropriate. Experimental results demonstrate\nthat our method is feasible for face age translation.",
        "Change captioning tasks aim to detect changes in image pairs observed before\nand after a scene change and generate a natural language description of the\nchanges. Existing change captioning studies have mainly focused on a single\nchange.However, detecting and describing multiple changed parts in image pairs\nis essential for enhancing adaptability to complex scenarios. We solve the\nabove issues from three aspects: (i) We propose a simulation-based multi-change\ncaptioning dataset; (ii) We benchmark existing state-of-the-art methods of\nsingle change captioning on multi-change captioning; (iii) We further propose\nMulti-Change Captioning transformers (MCCFormers) that identify change regions\nby densely correlating different regions in image pairs and dynamically\ndetermines the related change regions with words in sentences. The proposed\nmethod obtained the highest scores on four conventional change captioning\nevaluation metrics for multi-change captioning. Additionally, our proposed\nmethod can separate attention maps for each change and performs well with\nrespect to change localization. Moreover, the proposed framework outperformed\nthe previous state-of-the-art methods on an existing change captioning\nbenchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr\nscores), indicating its general ability in change captioning tasks.",
        "Learning a good representation is an essential component for deep\nreinforcement learning (RL). Representation learning is especially important in\nmultitask and partially observable settings where building a representation of\nthe unknown environment is crucial to solve the tasks. Here we introduce\nPrediction of Bootstrap Latents (PBL), a simple and flexible self-supervised\nrepresentation learning algorithm for multitask deep RL. PBL builds on\nmultistep predictive representations of future observations, and focuses on\ncapturing structured information about environment dynamics. Specifically, PBL\ntrains its representation by predicting latent embeddings of future\nobservations. These latent embeddings are themselves trained to be predictive\nof the aforementioned representations. These predictions form a bootstrapping\neffect, allowing the agent to learn more about the key aspects of the\nenvironment dynamics. In addition, by defining prediction tasks completely in\nlatent space, PBL provides the flexibility of using multimodal observations\ninvolving pixel images, language instructions, rewards and more. We show in our\nexperiments that PBL delivers across-the-board improved performance over state\nof the art deep RL agents in the DMLab-30 and Atari-57 multitask setting.",
        "We propose a novel transformer-based styled handwritten text image generation\napproach, HWT, that strives to learn both style-content entanglement as well as\nglobal and local writing style patterns. The proposed HWT captures the long and\nshort range relationships within the style examples through a self-attention\nmechanism, thereby encoding both global and local style patterns. Further, the\nproposed transformer-based HWT comprises an encoder-decoder attention that\nenables style-content entanglement by gathering the style representation of\neach query character. To the best of our knowledge, we are the first to\nintroduce a transformer-based generative network for styled handwritten text\ngeneration. Our proposed HWT generates realistic styled handwritten text images\nand significantly outperforms the state-of-the-art demonstrated through\nextensive qualitative, quantitative and human-based evaluations. The proposed\nHWT can handle arbitrary length of text and any desired writing style in a\nfew-shot setting. Further, our HWT generalizes well to the challenging scenario\nwhere both words and writing style are unseen during training, generating\nrealistic styled handwritten text images.",
        "Vehicle Re-identification is a challenging task due to intra-class\nvariability and inter-class similarity across non-overlapping cameras. To\ntackle these problems, recently proposed methods require additional annotation\nto extract more features for false positive image exclusion. In this paper, we\npropose a model powered by adaptive attention modules that requires fewer label\nannotations but still out-performs the previous models. We also include a\nre-ranking method that takes account of the importance of metadata feature\nembeddings in our paper. The proposed method is evaluated on CVPR AI City\nChallenge 2020 dataset and achieves mAP of 37.25% in Track 2.",
        "In spite of the recent progresses on classifying 3D point cloud with deep\nCNNs, large geometric transformations like rotation and translation remain\nchallenging problem and harm the final classification performance. To address\nthis challenge, we propose Geometry Sharing Network (GS-Net) which effectively\nlearns point descriptors with holistic context to enhance the robustness to\ngeometric transformations. Compared with previous 3D point CNNs which perform\nconvolution on nearby points, GS-Net can aggregate point features in a more\nglobal way. Specially, GS-Net consists of Geometry Similarity Connection (GSC)\nmodules which exploit Eigen-Graph to group distant points with similar and\nrelevant geometric information, and aggregate features from nearest neighbors\nin both Euclidean space and Eigenvalue space. This design allows GS-Net to\nefficiently capture both local and holistic geometric features such as\nsymmetry, curvature, convexity and connectivity. Theoretically, we show the\nnearest neighbors of each point in Eigenvalue space are invariant to rotation\nand translation. We conduct extensive experiments on public datasets,\nModelNet40, ShapeNet Part. Experiments demonstrate that GS-Net achieves the\nstate-of-the-art performances on major datasets, 93.3% on ModelNet40, and are\nmore robust to geometric transformations.",
        "Recent advances in deep learning for 3D point clouds have shown great\npromises in scene understanding tasks thanks to the introduction of convolution\noperators to consume 3D point clouds directly in a neural network. Point cloud\ndata, however, could have arbitrary rotations, especially those acquired from\n3D scanning. Recent works show that it is possible to design point cloud\nconvolutions with rotation invariance property, but such methods generally do\nnot perform as well as translation-invariant only convolution. We found that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a novel convolution operator that enhances feature\ndistinction by integrating global context information from the input point\ncloud to the convolution. To this end, a globally weighted local reference\nframe is constructed in each point neighborhood in which the local point set is\ndecomposed into bins. Anchor points are generated in each bin to represent\nglobal shape features. A convolution can then be performed to transform the\npoints and anchor features into final rotation-invariant features. We conduct\nseveral experiments on point cloud classification, part segmentation, shape\nretrieval, and normals estimation to evaluate our convolution, which achieves\nstate-of-the-art accuracy under challenging rotations.",
        "Although object detection has reached a milestone thanks to the great success\nof deep learning, the scale variation is still the key challenge. Integrating\nmulti-level features is presented to alleviate the problems, like the classic\nFeature Pyramid Network (FPN) and its improvements. However, the specifically\ndesigned feature integration modules of these methods may not have the optimal\narchitecture for feature fusion. Moreover, these models have fixed\narchitectures and data flow paths, when fed with various samples. They cannot\nadjust and be compatible with each kind of data. To overcome the above\nlimitations, we propose a Dynamic Sample-Individualized Connector (DSIC) for\nmulti-scale object detection. It dynamically adjusts network connections to fit\ndifferent samples. In particular, DSIC consists of two components: Intra-scale\nSelection Gate (ISG) and Cross-scale Selection Gate (CSG). ISG adaptively\nextracts multi-level features from backbone as the input of feature\nintegration. CSG automatically activate informative data flow paths based on\nthe multi-level features. Furthermore, these two components are both\nplug-and-play and can be embedded in any backbone. Experimental results\ndemonstrate that the proposed method outperforms the state-of-the-arts.",
        "The nonlocal-based blocks are designed for capturing long-range\nspatial-temporal dependencies in computer vision tasks. Although having shown\nexcellent performance, they still lack the mechanism to encode the rich,\nstructured information among elements in an image or video. In this paper, to\ntheoretically analyze the property of these nonlocal-based blocks, we provide a\nnew perspective to interpret them, where we view them as a set of graph filters\ngenerated on a fully-connected graph. Specifically, when choosing the Chebyshev\ngraph filter, a unified formulation can be derived for explaining and analyzing\nthe existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,\ndouble attention block). Furthermore, by concerning the property of spectral,\nwe propose an efficient and robust spectral nonlocal block, which can be more\nrobust and flexible to catch long-range dependencies when inserted into deep\nneural networks than the existing nonlocal blocks. Experimental results\ndemonstrate the clear-cut improvements and practical applicabilities of our\nmethod on image classification, action recognition, semantic segmentation, and\nperson re-identification tasks.",
        "We present an algorithm based on the \\emph{Optimism in the Face of\nUncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL)\nmodeled by Markov decision process (MDP) with finite state-action space\nefficiently. By evaluating the state-pair difference of the optimal bias\nfunction $h^{*}$, the proposed algorithm achieves a regret bound of\n$\\tilde{O}(\\sqrt{SAHT})$\\footnote{The symbol $\\tilde{O}$ means $O$ with log\nfactors ignored. } for MDP with $S$ states and $A$ actions, in the case that an\nupper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known. This result\noutperforms the best previous regret bounds $\\tilde{O}(S\\sqrt{AHT})\n$\\citep{fruit2019improved} by a factor of $\\sqrt{S}$. Furthermore, this regret\nbound matches the lower bound of $\\Omega(\\sqrt{SAHT}) $\\citep{jaksch2010near}\nup to a logarithmic factor. As a consequence, we show that there is a near\noptimal regret bound of $\\tilde{O}(\\sqrt{SADT})$ for MDPs with a finite\ndiameter $D$ compared to the lower bound of $\\Omega(\\sqrt{SADT})\n$\\citep{jaksch2010near}.",
        "How can neural networks trained by contrastive learning extract features from\nthe unlabeled data? Why does contrastive learning usually need much stronger\ndata augmentations than supervised learning to ensure good representations?\nThese questions involve both the optimization and statistical aspects of deep\nlearning, but can hardly be answered by analyzing supervised learning, where\nthe target functions are the highest pursuit. Indeed, in self-supervised\nlearning, it is inevitable to relate to the optimization/generalization of\nneural networks to how they can encode the latent structures in the data, which\nwe refer to as the feature learning process.\n  In this work, we formally study how contrastive learning learns the feature\nrepresentations for neural networks by analyzing its feature learning process.\nWe consider the case where our data are comprised of two types of features: the\nmore semantically aligned sparse features which we want to learn from, and the\nother dense features we want to avoid. Theoretically, we prove that contrastive\nlearning using $\\mathbf{ReLU}$ networks provably learns the desired sparse\nfeatures if proper augmentations are adopted. We present an underlying\nprinciple called $\\textbf{feature decoupling}$ to explain the effects of\naugmentations, where we theoretically characterize how augmentations can reduce\nthe correlations of dense features between positive samples while keeping the\ncorrelations of sparse features intact, thereby forcing the neural networks to\nlearn from the self-supervision of sparse features. Empirically, we verified\nthat the feature decoupling principle matches the underlying mechanism of\ncontrastive learning in practice.",
        "Designing missiles' autopilot controllers has been a complex task, given the\nextensive flight envelope and the nonlinear flight dynamics. A solution that\ncan excel both in nominal performance and in robustness to uncertainties is\nstill to be found. While Control Theory often debouches into parameters'\nscheduling procedures, Reinforcement Learning has presented interesting results\nin ever more complex tasks, going from videogames to robotic tasks with\ncontinuous action domains. However, it still lacks clearer insights on how to\nfind adequate reward functions and exploration strategies. To the best of our\nknowledge, this work is pioneer in proposing Reinforcement Learning as a\nframework for flight control. In fact, it aims at training a model-free agent\nthat can control the longitudinal flight of a missile, achieving optimal\nperformance and robustness to uncertainties. To that end, under TRPO's\nmethodology, the collected experience is augmented according to HER, stored in\na replay buffer and sampled according to its significance. Not only does this\nwork enhance the concept of prioritized experience replay into BPER, but it\nalso reformulates HER, activating them both only when the training progress\nconverges to suboptimal policies, in what is proposed as the SER methodology.\nBesides, the Reward Engineering process is carefully detailed. The results show\nthat it is possible both to achieve the optimal performance and to improve the\nagent's robustness to uncertainties (with low damage on nominal performance) by\nfurther training it in non-nominal environments, therefore validating the\nproposed approach and encouraging future research in this field.",
        "Instance segmentation is an important computer vision problem which remains\nchallenging despite impressive recent advances due to deep learning-based\nmethods. Given sufficient training data, fully supervised methods can yield\nexcellent performance, but annotation of ground-truth data remains a major\nbottleneck, especially for biomedical applications where it has to be performed\nby domain experts. The amount of labels required can be drastically reduced by\nusing rules derived from prior knowledge to guide the segmentation. However,\nthese rules are in general not differentiable and thus cannot be used with\nexisting methods. Here, we relax this requirement by using stateless actor\ncritic reinforcement learning, which enables non-differentiable rewards. We\nformulate the instance segmentation problem as graph partitioning and the actor\ncritic predicts the edge weights driven by the rewards, which are based on the\nconformity of segmented instances to high-level priors on object shape,\nposition or size. The experiments on toy and real datasets demonstrate that we\ncan achieve excellent performance without any direct supervision based only on\na rich set of priors.",
        "Style transfer usually refers to the task of applying color and texture\ninformation from a specific style image to a given content image while\npreserving the structure of the latter. Here we tackle the more generic problem\nof semantic style transfer: given two unpaired collections of images, we aim to\nlearn a mapping between the corpus-level style of each collection, while\npreserving semantic content shared across the two domains. We introduce XGAN\n(\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared\nrepresentation of the common domain semantic content in an unsupervised way,\nwhile jointly learning the domain-to-domain image translations in both\ndirections. We exploit ideas from the domain adaptation literature and define a\nsemantic consistency loss which encourages the model to preserve semantics in\nthe learned embedding space. We report promising qualitative results for the\ntask of face-to-cartoon translation. The cartoon dataset, CartoonSet, we\ncollected for this purpose is publicly available at\ngoogle.github.io/cartoonset/ as a new benchmark for semantic style transfer.",
        "Medical image segmentation demands an efficient and robust segmentation\nalgorithm against noise. The conventional fuzzy c-means algorithm is an\nefficient clustering algorithm that is used in medical image segmentation. But\nFCM is highly vulnerable to noise since it uses only intensity values for\nclustering the images. This paper aims to develop a novel and efficient fuzzy\nspatial c-means clustering algorithm which is robust to noise. The proposed\nclustering algorithm uses fuzzy spatial information to calculate membership\nvalue. The input image is clustered using proposed ISFCM algorithm. A\ncomparative study has been made between the conventional FCM and proposed\nISFCM. The proposed approach is found to be outperforming the conventional FCM.",
        "Automating molecular design using deep reinforcement learning (RL) has the\npotential to greatly accelerate the search for novel materials. Despite recent\nprogress on leveraging graph representations to design molecules, such methods\nare fundamentally limited by the lack of three-dimensional (3D) information. In\nlight of this, we propose a novel actor-critic architecture for 3D molecular\ndesign that can generate molecular structures unattainable with previous\napproaches. This is achieved by exploiting the symmetries of the design process\nthrough a rotationally covariant state-action representation based on a\nspherical harmonics series expansion. We demonstrate the benefits of our\napproach on several 3D molecular design tasks, where we find that building in\nsuch symmetries significantly improves generalization and the quality of\ngenerated molecules.",
        "Lesion segmentation from the surrounding skin is the first task for\ndeveloping automatic Computer-Aided Diagnosis of skin cancer. Variant features\nof lesion like uneven distribution of color, irregular shape, border and\ntexture make this task challenging. The contribution of this paper is to\npresent and compare two different approaches to skin lesion segmentation. The\nfirst approach uses watershed, while the second approach uses mean-shift.\nPre-processing steps were performed in both approaches for removing hair and\ndark borders of microscopic images. The Evaluation of the proposed approaches\nwas performed using Jaccard Index (Intersection over Union or IoU). An\nadditional contribution of this paper is to present pipelines for performing\npre-processing and segmentation applying existing segmentation and\nmorphological algorithms which led to promising results. On average, the first\napproach showed better performance than the second one with average Jaccard\nIndex over 200 ISIC-2017 challenge images are 89.16% and 76.94% respectively.",
        "Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease\nwhich may result in loss of vision. There is limited access to high-quality\nrelevant retinal images and poor understanding of the features defining\nsub-classes of this disease. Motivated by recent advances in machine learning\nwe specifically explore the potential of generative modeling, using Generative\nAdversarial Networks (GANs) and style transferring, to facilitate clinical\ndiagnosis and disease understanding by feature extraction. We design an\nanalytic pipeline which first generates synthetic retinal images from clinical\nimages; a subsequent verification step is applied. In the synthesizing step we\nmerge GANs (DCGANs and WGANs architectures) and style transferring for the\nimage generation, whereas the verified step controls the accuracy of the\ngenerated images. We find that the generated images contain sufficient\npathological details to facilitate ophthalmologists' task of disease\nclassification and in discovery of disease relevant features. In particular,\nour system predicts the drusen and geographic atrophy sub-classes of AMD.\nFurthermore, the performance using CFP images for GANs outperforms the\nclassification based on using only the original clinical dataset. Our results\nare evaluated using existing classifier of retinal diseases and class activated\nmaps, supporting the predictive power of the synthetic images and their utility\nfor feature extraction. Our code examples are available online.",
        "Omnidirectional images and spherical representations of $3D$ shapes cannot be\nprocessed with conventional 2D convolutional neural networks (CNNs) as the\nunwrapping leads to large distortion. Using fast implementations of spherical\nand $SO(3)$ convolutions, researchers have recently developed deep learning\nmethods better suited for classifying spherical images. These newly proposed\nconvolutional layers naturally extend the notion of convolution to functions on\nthe unit sphere $S^2$ and the group of rotations $SO(3)$ and these layers are\nequivariant to 3D rotations. In this paper, we consider the problem of\nunsupervised learning of rotation-invariant representations for spherical\nimages. In particular, we carefully design an autoencoder architecture\nconsisting of $S^2$ and $SO(3)$ convolutional layers. As 3D rotations are often\na nuisance factor, the latent space is constrained to be exactly invariant to\nthese input transformations. As the rotation information is discarded in the\nlatent space, we craft a novel rotation-invariant loss function for training\nthe network. Extensive experiments on multiple datasets demonstrate the\nusefulness of the learned representations on clustering, retrieval and\nclassification applications.",
        "VBM3D is an extension to video of the well known image denoising algorithm\nBM3D, which takes advantage of the sparse representation of stacks of similar\npatches in a transform domain. The extension is rather straightforward: the\nsimilar 2D patches are taken from a spatio-temporal neighborhood which includes\nneighboring frames. In spite of its simplicity, the algorithm offers a good\ntrade-off between denoising performance and computational complexity. In this\nwork we revisit this method, providing an open-source C++ implementation\nreproducing the results. A detailed description is given and the choice of\nparameters is thoroughly discussed. Furthermore, we discuss several extensions\nof the original algorithm: (1) a multi-scale implementation, (2) the use of 3D\npatches, (3) the use of optical flow to guide the patch search. These\nextensions allow to obtain results which are competitive with even the most\nrecent state of the art.",
        "Although Deep Neural Networks (DNNs) achieve excellent performance on many\nreal-world tasks, they are highly vulnerable to adversarial attacks. A leading\ndefense against such attacks is adversarial training, a technique in which a\nDNN is trained to be robust to adversarial attacks by introducing adversarial\nnoise to its input. This procedure is effective but must be done during the\ntraining phase. In this work, we propose a new simple and easy-to-use\ntechnique, KATANA, for robustifying an existing pretrained DNN without\nmodifying its weights. For every image, we generate N randomized Test Time\nAugmentations (TTAs) by applying diverse color, blur, noise, and geometric\ntransforms. Next, we utilize the DNN's logits output to train a simple random\nforest classifier to predict the real class label. Our strategy achieves\nstate-of-the-art adversarial robustness on diverse attacks with minimal\ncompromise on the natural images' classification. We test KATANA also against\ntwo adaptive white-box attacks and it shows excellent results when combined\nwith adversarial training. Code is available in\nhttps://github.com/giladcohen/KATANA.",
        "We adapt previous research on category theory and topological unsupervised\nlearning to develop a functorial perspective on manifold learning. We first\ncharacterize manifold learning algorithms as functors that map pseudometric\nspaces to optimization objectives and factor through hierachical clustering\nfunctors. We then use this characterization to prove refinement bounds on\nmanifold learning loss functions and construct a hierarchy of manifold learning\nalgorithms based on their invariants. We express several popular manifold\nlearning algorithms as functors at different levels of this hierarchy,\nincluding Metric Multidimensional Scaling, IsoMap, and UMAP. Next, we use\ninterleaving distance to study the stability of a broad class of manifold\nlearning algorithms. We present bounds on how closely the embeddings these\nalgorithms produce from noisy data approximate the embeddings they would learn\nfrom noiseless data. Finally, we use our framework to derive a set of novel\nmanifold learning algorithms, which we experimentally demonstrate are\ncompetitive with the state of the art.",
        "Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as the conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and target images to facilitate the\nfew-shot semantic segmentation task. We design a novel Cycle-Consistent\nTransformer (CyCTR) module to aggregate pixel-wise support features into query\nones. CyCTR performs cross-attention between features from different images,\ni.e. support and query images. We observe that there may exist unexpected\nirrelevant pixel-level support features. Directly performing cross-attention\nmay aggregate these features from support to query and bias the query features.\nThus, we propose using a novel cycle-consistent attention mechanism to filter\nout possible harmful support features and encourage query features to attend to\nthe most informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%\nrespectively.",
        "In this work, an automatic and simple framework for hockey ice-rink\nlocalization from broadcast videos is introduced. First, video is broken into\nvideo-shots by a hierarchical partitioning of the video frames, and\nthresholding based on their histograms. To localize the frames on the ice-rink\nmodel, a ResNet18-based regressor is implemented and trained, which regresses\nto four control points on the model in a frame-by-frame fashion. This leads to\nthe projection jittering problem in the video. To overcome this, in the\ninference phase, the trajectory of the control points on the ice-rink model are\nsmoothed, for all the consecutive frames of a given video-shot, by convolving a\nHann window with the achieved coordinates. Finally, the smoothed homography\nmatrix is computed by using the direct linear transform on the four pairs of\ncorresponding points. A hockey dataset for training and testing the regressor\nis gathered. The results show success of this simple and comprehensive\nprocedure for localizing the hockey ice-rink and addressing the problem of\njittering without affecting the accuracy of homography estimation.",
        "In computer vision, image segmentation is always selected as a major research\ntopic by researchers. Due to its vital rule in image processing, there always\narises the need of a better image segmentation method. Clustering is an\nunsupervised study with its application in almost every field of science and\nengineering. Many researchers used clustering in image segmentation process.\nBut still there requires improvement of such approaches. In this paper, a novel\napproach for clustering based image segmentation is proposed. Here, we give\nimportance on color space and choose lab for this task. The famous hard\nclustering algorithm K-means is used, but as its performance is dependent on\nchoosing a proper distance measure, so, we go for cosine distance measure. Then\nthe segmented image is filtered with sobel filter. The filtered image is\nanalyzed with marker watershed algorithm to have the final segmented result of\nour original image. The MSE and PSNR values are evaluated to observe the\nperformance.",
        "Relative position encoding (RPE) is important for transformer to capture\nsequence ordering of input tokens. General efficacy has been proven in natural\nlanguage processing. However, in computer vision, its efficacy is not well\nstudied and even remains controversial, e.g., whether relative position\nencoding can work equally well as absolute position? In order to clarify this,\nwe first review existing relative position encoding methods and analyze their\npros and cons when applied in vision transformers. We then propose new relative\nposition encoding methods dedicated to 2D images, called image RPE (iRPE). Our\nmethods consider directional relative distance modeling as well as the\ninteractions between queries and relative position embeddings in self-attention\nmechanism. The proposed iRPE methods are simple and lightweight. They can be\neasily plugged into transformer blocks. Experiments demonstrate that solely due\nto the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc)\nand 1.3% (mAP) stable improvements over their original versions on ImageNet and\nCOCO respectively, without tuning any extra hyperparameters such as learning\nrate and weight decay. Our ablation and analysis also yield interesting\nfindings, some of which run counter to previous understanding. Code and models\nare open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",
        "Recent work about synthetic indoor datasets from perspective views has shown\nsignificant improvements of object detection results with Convolutional Neural\nNetworks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale\nindoor dataset containing 100,000 high-resolution diversified fisheye images\nwith 14 classes. To this end, we create 3D virtual environments of living\nrooms, different human characters and interior textures. Beside capturing\nfisheye images from virtual environments we create annotations for semantic\nsegmentation, instance masks and bounding boxes for object detection tasks. We\ncompare our synthetic dataset to state of the art real-world datasets for\nomnidirectional images. Based on MS COCO weights, we show that our dataset is\nwell suited for fine-tuning CNNs for object detection. Through a high\ngeneralization of our models by means of image synthesis and domain\nrandomization, we reach an AP up to 0.84 for class person on High-Definition\nAnalytics dataset.",
        "3D panoramic multi-person localization and tracking are prominent in many\napplications, however, conventional methods using LiDAR equipment could be\neconomically expensive and also computationally inefficient due to the\nprocessing of point cloud data. In this work, we propose an effective and\nefficient approach at a low cost. First, we obtain panoramic videos with four\nnormal cameras. Then, we transform human locations from a 2D panoramic image\ncoordinate to a 3D panoramic camera coordinate using camera geometry and human\nbio-metric property (i.e., height). Finally, we generate 3D tracklets by\nassociating human appearance and 3D trajectory. We verify the effectiveness of\nour method on three datasets including a new one built by us, in terms of 3D\nsingle-view multi-person localization, 3D single-view multi-person tracking,\nand 3D panoramic multi-person localization and tracking. Our code and dataset\nare available at \\url{https://github.com/fandulu/MPLT}.",
        "Deep learning has recently attracted significant attention in the field of\nhyperspectral images (HSIs) classification. However, the construction of an\nefficient deep neural network (DNN) mostly relies on a large number of labeled\nsamples being available. To address this problem, this paper proposes a unified\ndeep network, combined with active transfer learning that can be well-trained\nfor HSIs classification using only minimally labeled training data. More\nspecifically, deep joint spectral-spatial feature is first extracted through\nhierarchical stacked sparse autoencoder (SSAE) networks. Active transfer\nlearning is then exploited to transfer the pre-trained SSAE network and the\nlimited training samples from the source domain to the target domain, where the\nSSAE network is subsequently fine-tuned using the limited labeled samples\nselected from both source and target domain by corresponding active learning\nstrategies. The advantages of our proposed method are threefold: 1) the network\ncan be effectively trained using only limited labeled samples with the help of\nnovel active learning strategies; 2) the network is flexible and scalable\nenough to function across various transfer situations, including cross-dataset\nand intra-image; 3) the learned deep joint spectral-spatial feature\nrepresentation is more generic and robust than many joint spectral-spatial\nfeature representation. Extensive comparative evaluations demonstrate that our\nproposed method significantly outperforms many state-of-the-art approaches,\nincluding both traditional and deep network-based methods, on three popular\ndatasets.",
        "The Encoder-Decoder architecture is a main stream deep learning model for\nbiomedical image segmentation. The encoder fully compresses the input and\ngenerates encoded features, and the decoder then produces dense predictions\nusing encoded features. However, decoders are still under-explored in such\narchitectures. In this paper, we comprehensively study the state-of-the-art\nEncoder-Decoder architectures, and propose a new universal decoder, called\ncascade decoder, to improve semantic segmentation accuracy. Our cascade decoder\ncan be embedded into existing networks and trained altogether in an end-to-end\nfashion. The cascade decoder structure aims to conduct more effective decoding\nof hierarchically encoded features and is more compatible with common encoders\nthan the known decoders. We replace the decoders of state-of-the-art models\nwith our cascade decoder for several challenging biomedical image segmentation\ntasks, and the considerable improvements achieved demonstrate the efficacy of\nour new decoding method.",
        "Greenhouse environment is the key to influence crops production. However, it\nis difficult for classical control methods to give precise environment\nsetpoints, such as temperature, humidity, light intensity and carbon dioxide\nconcentration for greenhouse because it is uncertain nonlinear system.\nTherefore, an intelligent close loop control framework based on model embedded\ndeep reinforcement learning (MEDRL) is designed for greenhouse environment\ncontrol. Specifically, computer vision algorithms are used to recognize growing\nperiods and sex of crops, followed by the crop growth models, which can be\ntrained with different growing periods and sex. These model outputs combined\nwith the cost factor provide the setpoints for greenhouse and feedback to the\ncontrol system in real-time. The whole MEDRL system has capability to conduct\noptimization control precisely and conveniently, and costs will be greatly\nreduced compared with traditional greenhouse control approaches.",
        "A key challenge in applying reinforcement learning to safety-critical domains\nis understanding how to balance exploration (needed to attain good performance\non the task) with safety (needed to avoid catastrophic failure). Although a\ngrowing line of work in reinforcement learning has investigated this area of\n\"safe exploration,\" most existing techniques either 1) do not guarantee safety\nduring the actual exploration process; and/or 2) limit the problem to a priori\nknown and/or deterministic transition dynamics with strong smoothness\nassumptions. Addressing this gap, we propose Analogous Safe-state Exploration\n(ASE), an algorithm for provably safe exploration in MDPs with unknown,\nstochastic dynamics. Our method exploits analogies between state-action pairs\nto safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE\nalso guides exploration towards the most task-relevant states, which\nempirically results in significant improvements in terms of sample efficiency,\nwhen compared to existing methods.",
        "Automated equipment health monitoring from streaming multisensor time-series\ndata can be used to enable condition-based maintenance, avoid sudden\ncatastrophic failures, and ensure high operational availability. We note that\nmost complex machinery has a well-documented and readily accessible underlying\nstructure capturing the inter-dependencies between sub-systems or modules. Deep\nlearning models such as those based on recurrent neural networks (RNNs) or\nconvolutional neural networks (CNNs) fail to explicitly leverage this\npotentially rich source of domain-knowledge into the learning procedure. In\nthis work, we propose to capture the structure of a complex equipment in the\nform of a graph, and use graph neural networks (GNNs) to model multi-sensor\ntime-series data. Using remaining useful life estimation as an application\ntask, we evaluate the advantage of incorporating the graph structure via GNNs\non the publicly available turbofan engine benchmark dataset. We observe that\nthe proposed GNN-based RUL estimation model compares favorably to several\nstrong baselines from literature such as those based on RNNs and CNNs.\nAdditionally, we observe that the learned network is able to focus on the\nmodule (node) with impending failure through a simple attention mechanism,\npotentially paving the way for actionable diagnosis.",
        "This work presents Kornia, an open source computer vision library built upon\na set of differentiable routines and modules that aims to solve generic\ncomputer vision problems. The package uses PyTorch as its main backend, not\nonly for efficiency but also to take advantage of the reverse\nauto-differentiation engine to define and compute the gradient of complex\nfunctions. Inspired by OpenCV, Kornia is composed of a set of modules\ncontaining operators that can be integrated into neural networks to train\nmodels to perform a wide range of operations including image\ntransformations,camera calibration, epipolar geometry, and low level image\nprocessing techniques, such as filtering and edge detection that operate\ndirectly on high dimensional tensor representations on graphical processing\nunits, generating faster systems. Examples of classical vision problems\nimplemented using our framework are provided including a benchmark comparing to\nexisting vision libraries.",
        "Mutual Information (MI) plays an important role in representation learning.\nHowever, MI is unfortunately intractable in continuous and high-dimensional\nsettings. Recent advances establish tractable and scalable MI estimators to\ndiscover useful representation. However, most of the existing methods are not\ncapable of providing an accurate estimation of MI with low-variance when the MI\nis large. We argue that directly estimating the gradients of MI is more\nappealing for representation learning than estimating MI in itself. To this\nend, we propose the Mutual Information Gradient Estimator (MIGE) for\nrepresentation learning based on the score estimation of implicit\ndistributions. MIGE exhibits a tight and smooth gradient estimation of MI in\nthe high-dimensional and large-MI settings. We expand the applications of MIGE\nin both unsupervised learning of deep representations based on InfoMax and the\nInformation Bottleneck method. Experimental results have indicated significant\nperformance improvement in learning useful representation.",
        "Modern reinforcement learning (RL) commonly engages practical problems with\nlarge state spaces, where function approximation must be deployed to\napproximate either the value function or the policy. While recent progresses in\nRL theory address a rich set of RL problems with general function\napproximation, such successes are mostly restricted to the single-agent\nsetting. It remains elusive how to extend these results to multi-agent RL,\nespecially due to the new challenges arising from its game-theoretical nature.\nThis paper considers two-player zero-sum Markov Games (MGs). We propose a new\nalgorithm that can provably find the Nash equilibrium policy using a polynomial\nnumber of samples, for any MG with low multi-agent Bellman-Eluder dimension --\na new complexity measure adapted from its single-agent version (Jin et al.,\n2021). A key component of our new algorithm is the exploiter, which facilitates\nthe learning of the main player by deliberately exploiting her weakness. Our\ntheoretical framework is generic, which applies to a wide range of models\nincluding but not limited to tabular MGs, MGs with linear or kernel function\napproximation, and MGs with rich observations.",
        "The compression of deep neural networks (DNNs) to reduce inference cost\nbecomes increasingly important to meet realistic deployment requirements of\nvarious applications. There have been a significant amount of work regarding\nnetwork compression, while most of them are heuristic rule-based or typically\nnot friendly to be incorporated into varying scenarios. On the other hand,\nsparse optimization yielding sparse solutions naturally fits the compression\nrequirement, but due to the limited study of sparse optimization in stochastic\nlearning, its extension and application onto model compression is rarely well\nexplored. In this work, we propose a model compression framework based on the\nrecent progress on sparse stochastic optimization. Compared to existing model\ncompression techniques, our method is effective and requires fewer extra\nengineering efforts to incorporate with varying applications, and has been\nnumerically demonstrated on benchmark compression tasks. Particularly, we\nachieve up to 7.2 and 2.9 times FLOPs reduction with the same level of\nevaluation accuracy on VGG16 for CIFAR10 and ResNet50 for ImageNet compared to\nthe baseline heavy models, respectively.",
        "We present a new generic method for shadow-aware multi-view satellite\nphotogrammetry of Earth Observation scenes. Our proposed method, the Shadow\nNeural Radiance Field (S-NeRF) follows recent advances in implicit volumetric\nrepresentation learning. For each scene, we train S-NeRF using very high\nspatial resolution optical images taken from known viewing angles. The learning\nrequires no labels or shape priors: it is self-supervised by an image\nreconstruction loss. To accommodate for changing light source conditions both\nfrom a directional light source (the Sun) and a diffuse light source (the sky),\nwe extend the NeRF approach in two ways. First, direct illumination from the\nSun is modeled via a local light source visibility field. Second, indirect\nillumination from a diffuse light source is learned as a non-local color field\nas a function of the position of the Sun. Quantitatively, the combination of\nthese factors reduces the altitude and color errors in shaded areas, compared\nto NeRF. The S-NeRF methodology not only performs novel view synthesis and full\n3D shape estimation, it also enables shadow detection, albedo synthesis, and\ntransient object filtering, without any explicit shape supervision.",
        "Gaussian Processes (GPs) can be used as flexible, non-parametric function\npriors. Inspired by the growing body of work on Normalizing Flows, we enlarge\nthis class of priors through a parametric invertible transformation that can be\nmade input-dependent. Doing so also allows us to encode interpretable prior\nknowledge (e.g., boundedness constraints). We derive a variational\napproximation to the resulting Bayesian inference problem, which is as fast as\nstochastic variational GP regression (Hensman et al., 2013; Dezfouli and\nBonilla,2015). This makes the model a computationally efficient alternative to\nother hierarchical extensions of GP priors (Lazaro-Gredilla,2012; Damianou and\nLawrence, 2013). The resulting algorithm's computational and inferential\nperformance is excellent, and we demonstrate this on a range of data sets. For\nexample, even with only 5 inducing points and an input-dependent flow, our\nmethod is consistently competitive with a standard sparse GP fitted using 100\ninducing points.",
        "Molecular machine learning bears promise for efficient molecule property\nprediction and drug discovery. However, due to the limited labeled data and the\ngiant chemical space, machine learning models trained via supervised learning\nperform poorly in generalization. This greatly limits the applications of\nmachine learning methods for molecular design and discovery. In this work, we\npresent MolCLR: Molecular Contrastive Learning of Representations via Graph\nNeural Networks (GNNs), a self-supervised learning framework for large\nunlabeled molecule datasets. Specifically, we first build a molecular graph,\nwhere each node represents an atom and each edge represents a chemical bond. A\nGNN is then used to encode the molecule graph. We propose three novel molecule\ngraph augmentations: atom masking, bond deletion, and subgraph removal. A\ncontrastive estimator is utilized to maximize the agreement of different graph\naugmentations from the same molecule. Experiments show that molecule\nrepresentations learned by MolCLR can be transferred to multiple downstream\nmolecular property prediction tasks. Our method thus achieves state-of-the-art\nperformance on many challenging datasets. We also prove the efficiency of our\nproposed molecule graph augmentations on supervised molecular classification\ntasks.",
        "Novel view synthesis is a challenging problem in computer vision and\nrobotics. Different from the existing works, which need the reference images or\n3D models of the scene to generate images under novel views, we propose a novel\nparadigm to this problem. That is, we synthesize the novel view from only a\n6-DoF camera pose directly. Although this setting is the most straightforward\nway, there are few works addressing it. While, our experiments demonstrate\nthat, with a concise CNN, we could get a meaningful parametric model that could\nreconstruct the correct scenery images only from the 6-DoF pose. To this end,\nwe propose a two-stage learning strategy, which consists of two consecutive\nCNNs: GenNet and RefineNet. GenNet generates a coarse image from a camera pose.\nRefineNet is a generative adversarial network that refines the coarse image. In\nthis way, we decouple the geometric relationship between mapping and texture\ndetail rendering. Extensive experiments conducted on the public datasets prove\nthe effectiveness of our method. We believe this paradigm is of high research\nand application value and could be an important direction in novel view\nsynthesis.",
        "Path signatures are powerful nonparametric tools for time series analysis,\nshown to form a universal and characteristic feature map for Euclidean valued\ntime series data. We lift the theory of path signatures to the setting of Lie\ngroup valued time series, adapting these tools for time series with underlying\ngeometric constraints. We prove that this generalized path signature is\nuniversal and characteristic. To demonstrate universality, we analyze the human\naction recognition problem in computer vision, using $SO(3)$ representations\nfor the time series, providing comparable performance to other shallow learning\napproaches, while offering an easily interpretable feature set. We also provide\na two-sample hypothesis test for Lie group-valued random walks to illustrate\nits characteristic property. Finally we provide algorithms and a Julia\nimplementation of these methods.",
        "Transfer learning --- transferring learned knowledge --- has brought a\nparadigm shift in the way models are trained. The lucrative benefits of\nimproved accuracy and reduced training time have shown promise in training\nmodels with constrained computational resources and fewer training samples.\nSpecifically, publicly available text-based models such as GloVe and BERT that\nare trained on large corpus of datasets have seen ubiquitous adoption in\npractice. In this paper, we ask, \"can transfer learning in text prediction\nmodels be exploited to perform misclassification attacks?\" As our main\ncontribution, we present novel attack techniques that utilize unintended\nfeatures learnt in the teacher (public) model to generate adversarial examples\nfor student (downstream) models. To the best of our knowledge, ours is the\nfirst work to show that transfer learning from state-of-the-art word-based and\nsentence-based teacher models increase the susceptibility of student models to\nmisclassification attacks. First, we propose a novel word-score based attack\nalgorithm for generating adversarial examples against student models trained\nusing context-free word-level embedding model. On binary classification tasks\ntrained using the GloVe teacher model, we achieve an average attack accuracy of\n97% for the IMDB Movie Reviews and 80% for the Fake News Detection. For\nmulti-class tasks, we divide the Newsgroup dataset into 6 and 20 classes and\nachieve an average attack accuracy of 75% and 41% respectively. Next, we\npresent length-based and sentence-based misclassification attacks for the Fake\nNews Detection task trained using a context-aware BERT model and achieve 78%\nand 39% attack accuracy respectively. Thus, our results motivate the need for\ndesigning training techniques that are robust to unintended feature learning,\nspecifically for transfer learned models.",
        "Recently, researchers have started applying convolutional neural networks\n(CNNs) with one-dimensional convolutions to clinical tasks involving\ntime-series data. This is due, in part, to their computational efficiency,\nrelative to recurrent neural networks and their ability to efficiently exploit\ncertain temporal invariances, (e.g., phase invariance). However, it is\nwell-established that clinical data may exhibit many other types of invariances\n(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)\nmay successfully transform and align inputs, their use often requires one to\nidentify the types of invariances in advance. In contrast, we propose the use\nof Sequence Transformer Networks, an end-to-end trainable architecture that\nlearns to identify and account for invariances in clinical time-series data.\nApplied to the task of predicting in-hospital mortality, our proposed approach\nachieves an improvement in the area under the receiver operating characteristic\ncurve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Our\nresults suggest that a variety of valuable invariances can be learned directly\nfrom the data.",
        "Recently, image-to-image translation has obtained significant attention.\nAmong many, those approaches based on an exemplar image that contains the\ntarget style information has been actively studied, due to its capability to\nhandle multimodality as well as its applicability in practical use. However,\ntwo intrinsic problems exist in the existing methods: what and where to\ntransfer. First, those methods extract style from an entire exemplar which\nincludes noisy information, which impedes a translation model from properly\nextracting the intended style of the exemplar. That is, we need to carefully\ndetermine what to transfer from the exemplar. Second, the extracted style is\napplied to the entire input image, which causes unnecessary distortion in\nirrelevant image regions. In response, we need to decide where to transfer the\nextracted style. In this paper, we propose a novel approach that extracts out a\nlocal mask from the exemplar that determines what style to transfer, and\nanother local mask from the input image that determines where to transfer the\nextracted style. The main novelty of this paper lies in (1) the highway\nadaptive instance normalization technique and (2) an end-to-end translation\nframework which achieves an outstanding performance in reflecting a style of an\nexemplar. We demonstrate the quantitative and qualitative evaluation results to\nconfirm the advantages of our proposed approach.",
        "Submodular functions have been studied extensively in machine learning and\ndata mining. In particular, the optimization of submodular functions over the\ninteger lattice (integer submodular functions) has recently attracted much\ninterest, because this domain relates naturally to many practical problem\nsettings, such as multilabel graph cut, budget allocation and revenue\nmaximization with discrete assignments. In contrast, the use of these functions\nfor probabilistic modeling has received surprisingly little attention so far.\nIn this work, we firstly propose the Generalized Multilinear Extension, a\ncontinuous DR-submodular extension for integer submodular functions. We study\ncentral properties of this extension and formulate a new probabilistic model\nwhich is defined through integer submodular functions. Then, we introduce a\nblock-coordinate ascent algorithm to perform approximate inference for those\nclass of models. Finally, we demonstrate its effectiveness and viability on\nseveral real-world social connection graph datasets with integer submodular\nobjectives.",
        "We consider the problem of visually explaining similarity models, i.e.,\nexplaining why a model predicts two images to be similar in addition to\nproducing a scalar score. While much recent work in visual model\ninterpretability has focused on gradient-based attention, these methods rely on\na classification module to generate visual explanations. Consequently, they\ncannot readily explain other kinds of models that do not use or need\nclassification-like loss functions (e.g., similarity models trained with a\nmetric learning loss). In this work, we bridge this crucial gap, presenting a\nmethod to generate gradient-based visual attention for image similarity\npredictors. By relying solely on the learned feature embedding, we show that\nour approach can be applied to any kind of CNN-based similarity architecture,\nan important step towards generic visual explainability. We show that our\nresulting attention maps serve more than just interpretability; they can be\ninfused into the model learning process itself with new trainable constraints.\nWe show that the resulting similarity models perform, and can be visually\nexplained, better than the corresponding baseline models trained without these\nconstraints. We demonstrate our approach using extensive experiments on three\ndifferent kinds of tasks: generic image retrieval, person re-identification,\nand low-shot semantic segmentation.",
        "Image recognition with prototypes is considered an interpretable alternative\nfor black box deep learning models. Classification depends on the extent to\nwhich a test image \"looks like\" a prototype. However, perceptual similarity for\nhumans can be different from the similarity learned by the classification\nmodel. Hence, only visualising prototypes can be insufficient for a user to\nunderstand what a prototype exactly represents, and why the model considers a\nprototype and an image to be similar. We address this ambiguity and argue that\nprototypes should be explained. We improve interpretability by automatically\nenhancing visual prototypes with textual quantitative information about visual\ncharacteristics deemed important by the classification model. Specifically, our\nmethod clarifies the meaning of a prototype by quantifying the influence of\ncolour hue, shape, texture, contrast and saturation and can generate both\nglobal and local explanations. Because of the generality of our approach, it\ncan improve the interpretability of any similarity-based method for\nprototypical image recognition. In our experiments, we apply our method to the\nexisting Prototypical Part Network (ProtoPNet). Our analysis confirms that the\nglobal explanations are generalisable, and often correspond to the visually\nperceptible properties of a prototype. Our explanations are especially relevant\nfor prototypes which might have been interpreted incorrectly otherwise. By\nexplaining such 'misleading' prototypes, we improve the interpretability and\nsimulatability of a prototype-based classification model. We also use our\nmethod to check whether visually similar prototypes have similar explanations,\nand are able to discover redundancy. Code is available at\nhttps://github.com/M-Nauta/Explaining_Prototypes .",
        "Deep neural networks have been the driving force behind the success in\nclassification tasks, e.g., object and audio recognition. Impressive results\nand generalization have been achieved by a variety of recently proposed\narchitectures, the majority of which are seemingly disconnected. In this work,\nwe cast the study of deep classifiers under a unifying framework. In\nparticular, we express state-of-the-art architectures (e.g., residual and\nnon-local networks) in the form of different degree polynomials of the input.\nOur framework provides insights on the inductive biases of each model and\nenables natural extensions building upon their polynomial nature. The efficacy\nof the proposed models is evaluated on standard image and audio classification\nbenchmarks. The expressivity of the proposed models is highlighted both in\nterms of increased model performance as well as model compression. Lastly, the\nextensions allowed by this taxonomy showcase benefits in the presence of\nlimited data and long-tailed data distributions. We expect this taxonomy to\nprovide links between existing domain-specific architectures.",
        "3D engineering of matter has opened up new avenues for designing systems that\ncan perform various computational tasks through light-matter interaction. Here,\nwe demonstrate the design of optical networks in the form of multiple\ndiffractive layers that are trained using deep learning to transform and encode\nthe spatial information of objects into the power spectrum of the diffracted\nlight, which are used to perform optical classification of objects with a\nsingle-pixel spectroscopic detector. Using a time-domain spectroscopy setup\nwith a plasmonic nanoantenna-based detector, we experimentally validated this\nmachine vision framework at terahertz spectrum to optically classify the images\nof handwritten digits by detecting the spectral power of the diffracted light\nat ten distinct wavelengths, each representing one class/digit. We also report\nthe coupling of this spectral encoding achieved through a diffractive optical\nnetwork with a shallow electronic neural network, separately trained to\nreconstruct the images of handwritten digits based on solely the spectral\ninformation encoded in these ten distinct wavelengths within the diffracted\nlight. These reconstructed images demonstrate task-specific image decompression\nand can also be cycled back as new inputs to the same diffractive network to\nimprove its optical object classification. This unique machine vision framework\nmerges the power of deep learning with the spatial and spectral processing\ncapabilities of diffractive networks, and can also be extended to other\nspectral-domain measurement systems to enable new 3D imaging and sensing\nmodalities integrated with spectrally encoded classification tasks performed\nthrough diffractive optical networks.",
        "We consider the large-scale query-document retrieval problem: given a query\n(e.g., a question), return the set of relevant documents (e.g., paragraphs\ncontaining the answer) from a large document corpus. This problem is often\nsolved in two steps. The retrieval phase first reduces the solution space,\nreturning a subset of candidate documents. The scoring phase then re-ranks the\ndocuments. Critically, the retrieval algorithm not only desires high recall but\nalso requires to be highly efficient, returning candidates in time sublinear to\nthe number of documents. Unlike the scoring phase witnessing significant\nadvances recently due to the BERT-style pre-training tasks on cross-attention\nmodels, the retrieval phase remains less well studied. Most previous works rely\non classic Information Retrieval (IR) methods such as BM-25 (token matching +\nTF-IDF weights). These models only accept sparse handcrafted features and can\nnot be optimized for different downstream tasks of interest. In this paper, we\nconduct a comprehensive study on the embedding-based retrieval models. We show\nthat the key ingredient of learning a strong embedding-based Transformer model\nis the set of pre-training tasks. With adequately designed paragraph-level\npre-training tasks, the Transformer models can remarkably improve over the\nwidely-used BM-25 as well as embedding models without Transformers. The\nparagraph-level pre-training tasks we studied are Inverse Cloze Task (ICT),\nBody First Selection (BFS), Wiki Link Prediction (WLP), and the combination of\nall three.",
        "The pre-training on the graph neural network model can learn the general\nfeatures of large-scale networks or networks of the same type by\nself-supervised methods, which allows the model to work even when node labels\nare missing. However, the existing pre-training methods do not take network\nevolution into consideration. This paper proposes a pre-training method on\ndynamic graph neural networks (PT-DGNN), which uses dynamic attributed graph\ngeneration tasks to simultaneously learn the structure, semantics, and\nevolution features of the graph. The method includes two steps: 1) dynamic\nsub-graph sampling, and 2) pre-training with dynamic attributed graph\ngeneration task. Comparative experiments on three realistic dynamic network\ndatasets show that the proposed method achieves the best results on the link\nprediction fine-tuning task.",
        "In this paper, we investigate algorithms for anomaly detection. Previous\nanomaly detection methods focus on modeling the distribution of non-anomalous\ndata provided during training. However, this does not necessarily ensure the\ncorrect detection of anomalous data. We propose a new Regularized Cycle\nConsistent Generative Adversarial Network (RCGAN) in which deep neural networks\nare adversarially trained to better recognize anomalous samples. This approach\nis based on leveraging a penalty distribution with a new definition of the loss\nfunction and novel use of discriminator networks. It is based on a solid\nmathematical foundation, and proofs show that our approach has stronger\nguarantees for detecting anomalous examples compared to the current\nstate-of-the-art. Experimental results on both real-world and synthetic data\nshow that our model leads to significant and consistent improvements on\nprevious anomaly detection benchmarks. Notably, RCGAN improves on the\nstate-of-the-art on the KDDCUP, Arrhythmia, Thyroid, Musk and CIFAR10 datasets.",
        "Many automotive applications, such as Advanced Driver Assistance Systems\n(ADAS) for collision avoidance and warnings, require estimating the future\nautomotive risk of a driving scene. We present a low-cost system that predicts\nthe collision risk over an intermediate time horizon from a monocular video\nsource, such as a dashboard-mounted camera. The modular system includes\ncomponents for object detection, object tracking, and state estimation. We\nintroduce solutions to the object tracking and distance estimation problems.\nAdvanced approaches to the other tasks are used to produce real-time\npredictions of the automotive risk for the next 10 s at over 5 Hz. The system\nis designed such that alternative components can be substituted with minimal\neffort. It is demonstrated on common physical hardware, specifically an\noff-the-shelf gaming laptop and a webcam. We extend the framework to support\nabsolute speed estimation and more advanced risk estimation techniques.",
        "Recent progress in advanced driver assistance systems and the race towards\nautonomous vehicles is mainly driven by two factors: (1) increasingly\nsophisticated algorithms that interpret the environment around the vehicle and\nreact accordingly, and (2) the continuous improvements of sensor technology\nitself. In terms of cameras, these improvements typically include higher\nspatial resolution, which as a consequence requires more data to be processed.\nThe trend to add multiple cameras to cover the entire surrounding of the\nvehicle is not conducive in that matter. At the same time, an increasing number\nof special purpose algorithms need access to the sensor input data to correctly\ninterpret the various complex situations that can occur, particularly in urban\ntraffic.\n  By observing those trends, it becomes clear that a key challenge for vision\narchitectures in intelligent vehicles is to share computational resources. We\nbelieve this challenge should be faced by introducing a representation of the\nsensory data that provides compressed and structured access to all relevant\nvisual content of the scene. The Stixel World discussed in this paper is such a\nrepresentation. It is a medium-level model of the environment that is\nspecifically designed to compress information about obstacles by leveraging the\ntypical layout of outdoor traffic scenes. It has proven useful for a multitude\nof automotive vision applications, including object detection, tracking,\nsegmentation, and mapping.\n  In this paper, we summarize the ideas behind the model and generalize it to\ntake into account multiple dense input streams: the image itself, stereo depth\nmaps, and semantic class probability maps that can be generated, e.g., by CNNs.\nOur generalization is embedded into a novel mathematical formulation for the\nStixel model. We further sketch how the free parameters of the model can be\nlearned using structured SVMs.",
        "The success of reinforcement learning in typical settings is, in part,\npredicated on underlying Markovian assumptions on the reward signal by which an\nagent learns optimal policies. In recent years, the use of reward machines has\nrelaxed this assumption by enabling a structured representation of\nnon-Markovian rewards. In particular, such representations can be used to\naugment the state space of the underlying decision process, thereby\nfacilitating non-Markovian reinforcement learning. However, these reward\nmachines cannot capture the semantics of stochastic reward signals. In this\npaper, we make progress on this front by introducing probabilistic reward\nmachines (PRMs) as a representation of non-Markovian stochastic rewards. We\npresent an algorithm to learn PRMs from the underlying decision process as well\nas to learn the PRM representation of a given decision-making policy.",
        "Value functions derived from Markov decision processes arise as a central\ncomponent of algorithms as well as performance metrics in many statistics and\nengineering applications of machine learning techniques. Computation of the\nsolution to the associated Bellman equations is challenging in most practical\ncases of interest. A popular class of approximation techniques, known as\nTemporal Difference (TD) learning algorithms, are an important sub-class of\ngeneral reinforcement learning methods. The algorithms introduced in this paper\nare intended to resolve two well-known difficulties of TD-learning approaches:\nTheir slow convergence due to very high variance, and the fact that, for the\nproblem of computing the relative value function, consistent algorithms exist\nonly in special cases. First we show that the gradients of these value\nfunctions admit a representation that lends itself to algorithm design. Based\non this result, a new class of differential TD-learning algorithms is\nintroduced. For Markovian models on Euclidean space with smooth dynamics, the\nalgorithms are shown to be consistent under general conditions. Numerical\nresults show dramatic variance reduction when compared to standard methods.",
        "With the rising interest in graph representation learning, a variety of\napproaches have been proposed to effectively capture a graph's properties.\nWhile these approaches have improved performance in graph machine learning\ntasks compared to traditional graph techniques, they are still perceived as\ntechniques with limited insight into the information encoded in these\nrepresentations. In this work, we explore methods to interpret node embeddings\nand propose the creation of a robust evaluation framework for comparing graph\nrepresentation learning algorithms and hyperparameters. We test our methods on\ngraphs with different properties and investigate the relationship between\nembedding training parameters and the ability of the produced embedding to\nrecover the structure of the original graph in a downstream task.",
        "IoT systems have been facing increasingly sophisticated technical problems\ndue to the growing complexity of these systems and their fast deployment\npractices. Consequently, IoT managers have to judiciously detect failures\n(anomalies) in order to reduce their cyber risk and operational cost. While\nthere is a rich literature on anomaly detection in many IoT-based systems,\nthere is no existing work that documents the use of ML models for anomaly\ndetection in digital agriculture and in smart manufacturing systems. These two\napplication domains pose certain salient technical challenges. In agriculture\nthe data is often sparse, due to the vast areas of farms and the requirement to\nkeep the cost of monitoring low. Second, in both domains, there are multiple\ntypes of sensors with varying capabilities and costs. The sensor data\ncharacteristics change with the operating point of the environment or machines,\nsuch as, the RPM of the motor. The inferencing and the anomaly detection\nprocesses therefore have to be calibrated for the operating point.\n  In this paper, we analyze data from sensors deployed in an agricultural farm\nwith data from seven different kinds of sensors, and from an advanced\nmanufacturing testbed with vibration sensors. We evaluate the performance of\nARIMA and LSTM models for predicting the time series of sensor data. Then,\nconsidering the sparse data from one kind of sensor, we perform transfer\nlearning from a high data rate sensor. We then perform anomaly detection using\nthe predicted sensor data. Taken together, we show how in these two application\ndomains, predictive failure classification can be achieved, thus paving the way\nfor predictive maintenance.",
        "We propose a Graph Neural Network with greater expressive power than commonly\nused GNNs - not constrained to only differentiate between graphs that\nWeisfeiler-Lehman test recognizes to be non-isomorphic. We use a graph\nattention network with expanding attention window that aggregates information\nfrom nodes exponentially far away. We also use partially random initial\nembeddings, allowing differentiation between nodes that would otherwise look\nthe same. This could cause problem with a traditional dropout mechanism,\ntherefore we use a \"head dropout\", randomly ignoring some attention heads\nrather than some dimensions of the embedding.",
        "Learning maps between data samples is fundamental. Applications range from\nrepresentation learning, image translation and generative modeling, to the\nestimation of spatial deformations. Such maps relate feature vectors, or map\nbetween feature spaces. Well-behaved maps should be regular, which can be\nimposed explicitly or may emanate from the data itself. We explore what induces\nregularity for spatial transformations, e.g., when computing image\nregistrations. Classical optimization-based models compute maps between pairs\nof samples and rely on an appropriate regularizer for well-posedness. Recent\ndeep learning approaches have attempted to avoid using such regularizers\naltogether by relying on the sample population instead. We explore if it is\npossible to obtain spatial regularity using an inverse consistency loss only\nand elucidate what explains map regularity in such a context. We find that deep\nnetworks combined with an inverse consistency loss and randomized off-grid\ninterpolation yield well behaved, approximately diffeomorphic, spatial\ntransformations. Despite the simplicity of this approach, our experiments\npresent compelling evidence, on both synthetic and real data, that regular maps\ncan be obtained without carefully tuned explicit regularizers, while achieving\ncompetitive registration performance.",
        "We propose a new perspective on representation learning in reinforcement\nlearning based on geometric properties of the space of value functions. We\nleverage this perspective to provide formal evidence regarding the usefulness\nof value functions as auxiliary tasks. Our formulation considers adapting the\nrepresentation to minimize the (linear) approximation of the value function of\nall stationary policies for a given environment. We show that this optimization\nreduces to making accurate predictions regarding a special class of value\nfunctions which we call adversarial value functions (AVFs). We demonstrate that\nusing value functions as auxiliary tasks corresponds to an expected-error\nrelaxation of our formulation, with AVFs a natural candidate, and identify a\nclose relationship with proto-value functions (Mahadevan, 2005). We highlight\ncharacteristics of AVFs and their usefulness as auxiliary tasks in a series of\nexperiments on the four-room domain.",
        "Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.",
        "Secure multi-party machine learning allows several parties to build a model\non their pooled data to increase utility while not explicitly sharing data with\neach other. We show that such multi-party computation can cause leakage of\nglobal dataset properties between the parties even when parties obtain only\nblack-box access to the final model. In particular, a ``curious'' party can\ninfer the distribution of sensitive attributes in other parties' data with high\naccuracy. This raises concerns regarding the confidentiality of properties\npertaining to the whole dataset as opposed to individual data records. We show\nthat our attack can leak population-level properties in datasets of different\ntypes, including tabular, text, and graph data. To understand and measure the\nsource of leakage, we consider several models of correlation between a\nsensitive attribute and the rest of the data. Using multiple machine learning\nmodels, we show that leakage occurs even if the sensitive attribute is not\nincluded in the training data and has a low correlation with other attributes\nor the target variable.",
        "Sequence data is challenging for machine learning approaches, because the\nlengths of the sequences may vary between samples. In this paper, we present an\nunsupervised learning model for sequence data, called the Integrated Sequence\nAutoencoder (ISA), to learn a fixed-length vectorial representation by\nminimizing the reconstruction error. Specifically, we propose to integrate two\nclassical mechanisms for sequence reconstruction which takes into account both\nthe global silhouette information and the local temporal dependencies.\nFurthermore, we propose a stop feature that serves as a temporal stamp to guide\nthe reconstruction process, which results in a higher-quality representation.\nThe learned representation is able to effectively summarize not only the\napparent features, but also the underlying and high-level style information.\nTake for example a speech sequence sample: our ISA model can not only recognize\nthe spoken text (apparent feature), but can also discriminate the speaker who\nutters the audio (more high-level style). One promising application of the ISA\nmodel is that it can be readily used in the semi-supervised learning scenario,\nin which a large amount of unlabeled data is leveraged to extract high-quality\nsequence representations and thus to improve the performance of the subsequent\nsupervised learning tasks on limited labeled data.",
        "In this paper, we introduce a new vision-language pre-trained model --\nImageBERT -- for image-text joint embedding. Our model is a Transformer-based\nmodel, which takes different modalities as input and models the relationship\nbetween them. The model is pre-trained on four tasks simultaneously: Masked\nLanguage Modeling (MLM), Masked Object Classification (MOC), Masked Region\nFeature Regression (MRFR), and Image Text Matching (ITM). To further enhance\nthe pre-training quality, we have collected a Large-scale weAk-supervised\nImage-Text (LAIT) dataset from Web. We first pre-train the model on this\ndataset, then conduct a second stage pre-training on Conceptual Captions and\nSBU Captions. Our experiments show that multi-stage pre-training strategy\noutperforms single-stage pre-training. We also fine-tune and evaluate our\npre-trained ImageBERT model on image retrieval and text retrieval tasks, and\nachieve new state-of-the-art results on both MSCOCO and Flickr30k datasets.",
        "We are witnessing a modeling shift from CNN to Transformers in computer\nvision. In this work, we present a self-supervised learning approach called\nMoBY, with Vision Transformers as its backbone architecture. The approach\nbasically has no new inventions, which is combined from MoCo v2 and BYOL and\ntuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation:\n72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by\n300-epoch training. The performance is slightly better than recent works of\nMoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter\ntricks.\n  More importantly, the general-purpose Swin Transformer backbone enables us to\nalso evaluate the learnt representations on downstream tasks such as object\ndetection and semantic segmentation, in contrast to a few recent approaches\nbuilt on ViT/DeiT which only report linear evaluation results on ImageNet-1K\ndue to ViT/DeiT not tamed for these dense prediction tasks. We hope our results\ncan facilitate more comprehensive evaluation of self-supervised learning\nmethods designed for Transformer architectures. Our code and models are\navailable at https://github.com/SwinTransformer/Transformer-SSL, which will be\ncontinually enriched.",
        "The study of provable adversarial robustness for deep neural network (DNN)\nmodels has mainly focused on static supervised learning tasks such as image\nclassification. However, DNNs have been used extensively in real-world adaptive\ntasks such as reinforcement learning (RL), making RL systems vulnerable to\nadversarial attacks. The key challenge in adversarial RL is that the attacker\ncan adapt itself to the defense strategy used by the agent in previous\ntime-steps to strengthen its attack in future steps. In this work, we study the\nprovable robustness of RL against norm-bounded adversarial perturbations of the\ninputs. We focus on smoothing-based provable defenses and propose policy\nsmoothing where the agent adds a Gaussian noise to its observation at each\ntime-step before applying the policy network to make itself less sensitive to\nadversarial perturbations of its inputs. Our main theoretical contribution is\nto prove an adaptive version of the Neyman-Pearson Lemma where the adversarial\nperturbation at a particular time can be a stochastic function of current and\nprevious observations and states as well as previously observed actions. Using\nthis lemma, we adapt the robustness certificates produced by randomized\nsmoothing in the static setting of image classification to the dynamic setting\nof RL. We generate certificates that guarantee that the total reward obtained\nby the smoothed policy will not fall below a certain threshold under a\nnorm-bounded adversarial perturbation of the input. We show that our\ncertificates are tight by constructing a worst-case setting that achieves the\nbounds derived in our analysis. In our experiments, we show that this method\ncan yield meaningful certificates in complex environments demonstrating its\neffectiveness against adversarial attacks.",
        "While learning based depth estimation from images/videos has achieved\nsubstantial progress, there still exist intrinsic limitations. Supervised\nmethods are limited by a small amount of ground truth or labeled data and\nunsupervised methods for monocular videos are mostly based on the static scene\nassumption, not performing well on real world scenarios with the presence of\ndynamic objects. In this paper, we propose a new learning based method\nconsisting of DepthNet, PoseNet and Region Deformer Networks (RDN) to estimate\ndepth from unconstrained monocular videos without ground truth supervision. The\ncore contribution lies in RDN for proper handling of rigid and non-rigid\nmotions of various objects such as rigidly moving cars and deformable humans.\nIn particular, a deformation based motion representation is proposed to model\nindividual object motion on 2D images. This representation enables our method\nto be applicable to diverse unconstrained monocular videos. Our method can not\nonly achieve the state-of-the-art results on standard benchmarks KITTI and\nCityscapes, but also show promising results on a crowded pedestrian tracking\ndataset, which demonstrates the effectiveness of the deformation based motion\nrepresentation. Code and trained models are available at\nhttps://github.com/haofeixu/rdn4depth.",
        "Transformer is the backbone of modern NLP models. In this paper, we propose\nRealFormer, a simple and generic technique to create Residual Attention Layer\nTransformer networks that significantly outperform the canonical Transformer\nand its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked\nLanguage Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA,\nNatural Questions, and OpenKP. We also observe empirically that RealFormer\nstabilizes training and leads to models with sparser attention. Source code and\npre-trained checkpoints for RealFormer can be found at\nhttps://github.com/google-research/google-research/tree/master/realformer.",
        "In this paper, we propose a multi-object detection and tracking method using\ndepth cameras. Depth maps are very noisy and obscure in object detection. We\nfirst propose a region-based method to suppress high magnitude noise which\ncannot be filtered using spatial filters. Second, the proposed method detect\nRegion of Interests by temporal learning which are then tracked using weighted\ngraph-based approach. We demonstrate the performance of the proposed method on\nstandard depth camera datasets with and without object occlusions. Experimental\nresults show that the proposed method is able to suppress high magnitude noise\nin depth maps and detect/track the objects (with and without occlusion).",
        "We present a filter correlation based model compression approach for deep\nconvolutional neural networks. Our approach iteratively identifies pairs of\nfilters with the largest pairwise correlations and drops one of the filters\nfrom each such pair. However, instead of discarding one of the filters from\neach such pair na\\\"{i}vely, the model is re-optimized to make the filters in\nthese pairs maximally correlated, so that discarding one of the filters from\nthe pair results in minimal information loss. Moreover, after discarding the\nfilters in each round, we further finetune the model to recover from the\npotential small loss incurred by the compression. We evaluate our proposed\napproach using a comprehensive set of experiments and ablation studies. Our\ncompression method yields state-of-the-art FLOPs compression rates on various\nbenchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving\nexcellent predictive performance for tasks such as object detection on\nbenchmark datasets.",
        "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear\ncomplexity with respect to the number of nodes when applied to sparse graphs,\nthey have been widely implemented and still raise a lot of interest even though\ntheir theoretical expressive power is limited to the first order\nWeisfeiler-Lehman test (1-WL). In this paper, we show that if the graph\nconvolution supports are designed in spectral-domain by a non-linear custom\nfunction of eigenvalues and masked with an arbitrary large receptive field, the\nMPNN is theoretically more powerful than the 1-WL test and experimentally as\npowerful as a 3-WL existing models, while remaining spatially localized.\nMoreover, by designing custom filter functions, outputs can have various\nfrequency components that allow the convolution process to learn different\nrelationships between a given input graph signal and its associated properties.\nSo far, the best 3-WL equivalent graph neural networks have a computational\ncomplexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$,\nconsider non-local update mechanism and do not provide the spectral richness of\noutput profile. The proposed method overcomes all these aforementioned problems\nand reaches state-of-the-art results in many downstream tasks.",
        "This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.",
        "EEG-based emotion recognition often requires sufficient labeled training\nsamples to build an effective computational model. Labeling EEG data, on the\nother hand, is often expensive and time-consuming. To tackle this problem and\nreduce the need for output labels in the context of EEG-based emotion\nrecognition, we propose a semi-supervised pipeline to jointly exploit both\nunlabeled and labeled data for learning EEG representations. Our\nsemi-supervised framework consists of both unsupervised and supervised\ncomponents. The unsupervised part maximizes the consistency between original\nand reconstructed input data using an autoencoder, while simultaneously the\nsupervised part minimizes the cross-entropy between the input and output\nlabels. We evaluate our framework using both a stacked autoencoder and an\nattention-based recurrent autoencoder. We test our framework on the large-scale\nSEED EEG dataset and compare our results with several other popular\nsemi-supervised methods. Our semi-supervised framework with a deep\nattention-based recurrent autoencoder consistently outperforms the benchmark\nmethods, even when small sub-sets (3\\%, 5\\% and 10\\%) of the output labels are\navailable during training, achieving a new state-of-the-art semi-supervised\nperformance.",
        "We propose a novel and unsupervised representation learning model, i.e.,\nRobust Block-Diagonal Adaptive Locality-constrained Latent Representation\n(rBDLR). rBDLR is able to recover multi-subspace structures and extract the\nadaptive locality-preserving salient features jointly. Leveraging on the\nFrobenius-norm based latent low-rank representation model, rBDLR jointly learns\nthe coding coefficients and salient features, and improves the results by\nenhancing the robustness to outliers and errors in given data, preserving local\ninformation of salient features adaptively and ensuring the block-diagonal\nstructures of the coefficients. To improve the robustness, we perform the\nlatent representation and adaptive weighting in a recovered clean data space.\nTo force the coefficients to be block-diagonal, we perform auto-weighting by\nminimizing the reconstruction error based on salient features, constrained\nusing a block-diagonal regularizer. This ensures that a strict block-diagonal\nweight matrix can be obtained and salient features will possess the adaptive\nlocality preserving ability. By minimizing the difference between the\ncoefficient and weights matrices, we can obtain a block-diagonal coefficients\nmatrix and it can also propagate and exchange useful information between\nsalient features and coefficients. Extensive results demonstrate the\nsuperiority of rBDLR over other state-of-the-art methods.",
        "Multiview representation learning is very popular for latent factor analysis.\nIt naturally arises in many data analysis, machine learning, and information\nretrieval applications to model dependent structures among multiple data\nsources. For computational convenience, existing approaches usually formulate\nthe multiview representation learning as convex optimization problems, where\nglobal optima can be obtained by certain algorithms in polynomial time.\nHowever, many pieces of evidence have corroborated that heuristic nonconvex\napproaches also have good empirical computational performance and convergence\nto the global optima, although there is a lack of theoretical justification.\nSuch a gap between theory and practice motivates us to study a nonconvex\nformulation for multiview representation learning, which can be efficiently\nsolved by a simple stochastic gradient descent (SGD) algorithm. We first\nillustrate the geometry of the nonconvex formulation; Then, we establish\nasymptotic global rates of convergence to the global optima by diffusion\napproximations. Numerical experiments are provided to support our theory.",
        "Over the last few decades, many architectures have been developed that\nharness the power of neural networks to detect objects in near real-time.\nTraining such systems requires substantial time across multiple GPUs and\nmassive labeled training datasets. Although the goal of these systems is\ngeneralizability, they are often impractical in real-life applications due to\nflexibility, robustness, or speed issues. This paper proposes RMOPP: A robust\nmulti-objective post-processing algorithm to boost the performance of fast\npre-trained object detectors with a negligible impact on their speed.\nSpecifically, RMOPP is a statistically driven, post-processing algorithm that\nallows for simultaneous optimization of precision and recall. A unique feature\nof RMOPP is the Pareto frontier that identifies dominant possible\npost-processed detectors to optimize for both precision and recall. RMOPP\nexplores the full potential of a pre-trained object detector and is deployable\nfor near real-time predictions. We also provide a compelling test case on\nYOLOv2 using the MS-COCO dataset.",
        "Social Media Platforms (SMPs) like Facebook, Twitter, Instagram etc. have\nlarge user base all around the world that generates huge amount of data every\nsecond. This includes a lot of posts by fake and spam users, typically used by\nmany organisations around the globe to have competitive edge over others. In\nthis work, we aim at detecting such user accounts in Twitter using a novel\napproach. We show how to distinguish between Genuine and Spam accounts in\nTwitter using a combination of Graph Representation Learning and Natural\nLanguage Processing techniques.",
        "Weaknesses in computer systems such as faults, bugs and errors in the\narchitecture, design or implementation of software provide vulnerabilities that\ncan be exploited by attackers to compromise the security of a system. Common\nWeakness Enumerations (CWE) are a hierarchically designed dictionary of\nsoftware weaknesses that provide a means to understand software flaws,\npotential impact of their exploitation, and means to mitigate these flaws.\nCommon Vulnerabilities and Exposures (CVE) are brief low-level descriptions\nthat uniquely identify vulnerabilities in a specific product or protocol.\nClassifying or mapping of CVEs to CWEs provides a means to understand the\nimpact and mitigate the vulnerabilities. Since manual mapping of CVEs is not a\nviable option, automated approaches are desirable but challenging.\n  We present a novel Transformer-based learning framework (V2W-BERT) in this\npaper. By using ideas from natural language processing, link prediction and\ntransfer learning, our method outperforms previous approaches not only for CWE\ninstances with abundant data to train, but also rare CWE classes with little or\nno data to train. Our approach also shows significant improvements in using\nhistorical data to predict links for future instances of CVEs, and therefore,\nprovides a viable approach for practical applications. Using data from MITRE\nand National Vulnerability Database, we achieve up to 97% prediction accuracy\nfor randomly partitioned data and up to 94% prediction accuracy in temporally\npartitioned data. We believe that our work will influence the design of better\nmethods and training models, as well as applications to solve increasingly\nharder problems in cybersecurity.",
        "Color-coded aperture (CCA) methods can physically measure the depth of a\nscene given by physical cues from a single-shot image of a monocular camera.\nHowever, they are vulnerable to actual lens aberrations in real scenes because\nthey assume an ideal lens for simplifying algorithms. In this paper, we propose\nphysical cue-based deep learning for CCA photography. To address actual lens\naberrations, we developed a deep deaberration network (DDN) that is\nadditionally equipped with a self-attention mechanism of position and color\nchannels to efficiently learn the lens aberration. Furthermore, a new Bayes L1\nloss function based on Bayesian deep learning enables to handle the uncertainty\nof depth estimation more accurately. Quantitative and qualitative comparisons\ndemonstrate that our method is superior to conventional methods including real\noutdoor scenes. Furthermore, compared to a long-baseline stereo camera, the\nproposed method provides an error-free depth map at close range, as there is no\nblind spot between the left and right cameras.",
        "In this paper, we present a deep learning architecture which addresses the\nproblem of 3D semantic segmentation of unstructured point clouds. Compared to\nprevious work, we introduce grouping techniques which define point\nneighborhoods in the initial world space and the learned feature space.\nNeighborhoods are important as they allow to compute local or global point\nfeatures depending on the spatial extend of the neighborhood. Additionally, we\nincorporate dedicated loss functions to further structure the learned point\nfeature space: the pairwise distance loss and the centroid loss. We show how to\napply these mechanisms to the task of 3D semantic segmentation of point clouds\nand report state-of-the-art performance on indoor and outdoor datasets.",
        "Timely prediction of clinically critical events in Intensive Care Unit (ICU)\nis important for improving care and survival rate. Most of the existing\napproaches are based on the application of various classification methods on\nexplicitly extracted statistical features from vital signals. In this work, we\npropose to eliminate the high cost of engineering hand-crafted features from\nmultivariate time-series of physiologic signals by learning their\nrepresentation with a sequence-to-sequence auto-encoder. We then propose to\nhash the learned representations to enable signal similarity assessment for the\nprediction of critical events. We apply this methodological framework to\npredict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of\nvital signal recordings. Experiments demonstrate the ability of the presented\nframework in accurately predicting an upcoming AHE.",
        "Medical image segmentation - the prerequisite of numerous clinical needs -\nhas been significantly prospered by recent advances in convolutional neural\nnetworks (CNNs). However, it exhibits general limitations on modeling explicit\nlong-range relation, and existing cures, resorting to building deep encoders\nalong with aggressive downsampling operations, leads to redundant deepened\nnetworks and loss of localized details. Hence, the segmentation task awaits a\nbetter solution to improve the efficiency of modeling global contexts while\nmaintaining a strong grasp of low-level details. In this paper, we propose a\nnovel parallel-in-branch architecture, TransFuse, to address this challenge.\nTransFuse combines Transformers and CNNs in a parallel style, where both global\ndependency and low-level spatial details can be efficiently captured in a much\nshallower manner. Besides, a novel fusion technique - BiFusion module is\ncreated to efficiently fuse the multi-level features from both branches.\nExtensive experiments demonstrate that TransFuse achieves the newest\nstate-of-the-art results on both 2D and 3D medical image sets including polyp,\nskin lesion, hip, and prostate segmentation, with significant parameter\ndecrease and inference speed improvement.",
        "Reusable model design becomes desirable with the rapid expansion of computer\nvision and machine learning applications. In this paper, we focus on the\nreusability of pre-trained deep convolutional models. Specifically, different\nfrom treating pre-trained models as feature extractors, we reveal more\ntreasures beneath convolutional layers, i.e., the convolutional activations\ncould act as a detector for the common object in the image co-localization\nproblem. We propose a simple yet effective method, termed Deep Descriptor\nTransforming (DDT), for evaluating the correlations of descriptors and then\nobtaining the category-consistent regions, which can accurately locate the\ncommon object in a set of unlabeled images, i.e., unsupervised object\ndiscovery. Empirical studies validate the effectiveness of the proposed DDT\nmethod. On benchmark image co-localization datasets, DDT consistently\noutperforms existing state-of-the-art methods by a large margin. Moreover, DDT\nalso demonstrates good generalization ability for unseen categories and\nrobustness for dealing with noisy data. Beyond those, DDT can be also employed\nfor harvesting web images into valid external data sources for improving\nperformance of both image recognition and object detection.",
        "Games such as go, chess and checkers have multiple equivalent game states,\ni.e. multiple board positions where symmetrical and opposite moves should be\nmade. These equivalences are not exploited by current state of the art neural\nagents which instead must relearn similar information, thereby wasting\ncomputing time. Group equivariant CNNs in existing work create networks which\ncan exploit symmetries to improve learning, however, they lack the\nexpressiveness to correctly reflect the move embeddings necessary for games. We\nintroduce Finite Group Neural Networks (FGNNs), a method for creating agents\nwith an innate understanding of these board positions. FGNNs are shown to\nimprove the performance of networks playing checkers (draughts), and can be\neasily adapted to other games and learning problems. Additionally, FGNNs can be\ncreated from existing network architectures. These include, for the first time,\nthose with skip connections and arbitrary layer types. We demonstrate that an\nequivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network in\nimage segmentation.",
        "Understanding how \"black-box\" models arrive at their predictions has sparked\nsignificant interest from both within and outside the AI community. Our work\nfocuses on doing this by generating local explanations about individual\npredictions for tree-based ensembles, specifically Gradient Boosting Decision\nTrees (GBDTs). Given a correctly predicted instance in the training set, we\nwish to generate a counterfactual explanation for this instance, that is, the\nminimal perturbation of this instance such that the prediction flips to the\nopposite class. Most existing methods for counterfactual explanations are (1)\nmodel-agnostic, so they do not take into account the structure of the original\nmodel, and/or (2) involve building a surrogate model on top of the original\nmodel, which is not guaranteed to represent the original model accurately.\nThere exists a method specifically for random forests; we wish to extend this\nmethod for GBDTs. This involves accounting for (1) the sequential dependency\nbetween trees and (2) training on the negative gradients instead of the\noriginal labels.",
        "Frequency control is an important problem in modern recommender systems. It\ndictates the delivery frequency of recommendations to maintain product quality\nand efficiency. For example, the frequency of delivering promotional\nnotifications impacts daily metrics as well as the infrastructure resource\nconsumption (e.g. CPU and memory usage). There remain open questions on what\nobjective we should optimize to represent business values in the long term\nbest, and how we should balance between daily metrics and resource consumption\nin a dynamically fluctuating environment. We propose a personalized methodology\nfor the frequency control problem, which combines long-term value optimization\nusing reinforcement learning (RL) with a robust volume control technique we\ntermed \"Effective Factor\". We demonstrate statistically significant improvement\nin daily metrics and resource efficiency by our method in several notification\napplications at a scale of billions of users. To our best knowledge, our study\nrepresents the first deep RL application on the frequency control problem at\nsuch an industrial scale.",
        "The effective representation, processing, analysis, and visualization of\nlarge-scale structured data, especially those related to complex domains such\nas networks and graphs, are one of the key questions in modern machine\nlearning. Graph signal processing (GSP), a vibrant branch of signal processing\nmodels and algorithms that aims at handling data supported on graphs, opens new\npaths of research to address this challenge. In this article, we review a few\nimportant contributions made by GSP concepts and tools, such as graph filters\nand transforms, to the development of novel machine learning algorithms. In\nparticular, our discussion focuses on the following three aspects: exploiting\ndata structure and relational priors, improving data and computational\nefficiency, and enhancing model interpretability. Furthermore, we provide new\nperspectives on future development of GSP techniques that may serve as a bridge\nbetween applied mathematics and signal processing on one side, and machine\nlearning and network science on the other. Cross-fertilization across these\ndifferent disciplines may help unlock the numerous challenges of complex data\nanalysis in the modern age.",
        "We study the fundamental question of the sample complexity of learning a good\npolicy in finite Markov decision processes (MDPs) when the data available for\nlearning is obtained by following a logging policy that must be chosen without\nknowledge of the underlying MDP. Our main results show that the sample\ncomplexity, the minimum number of transitions necessary and sufficient to\nobtain a good policy, is an exponential function of the relevant quantities\nwhen the planning horizon $H$ is finite. In particular, we prove that the\nsample complexity of obtaining $\\epsilon$-optimal policies is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H+1)})$ for $\\gamma$-discounted\nproblems, where $\\mathrm{S}$ is the number of states, $\\mathrm{A}$ is the\nnumber of actions, and $H$ is the effective horizon defined as $H=\\lfloor\n\\tfrac{\\ln(1/\\epsilon)}{\\ln(1/\\gamma)} \\rfloor$; and it is at least\n$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$ for finite horizon\nproblems, where $H$ is the planning horizon of the problem. This lower bound is\nessentially matched by an upper bound. For the average-reward setting we show\nthat there is no algorithm finding $\\epsilon$-optimal policies with a finite\namount of data.",
        "Graph convolutional networks have achieved great success on graph-structured\ndata. Many graph convolutional networks can be regarded as low-pass filters for\ngraph signals. In this paper, we propose a new model, BiGCN, which represents a\ngraph neural network as a bi-directional low-pass filter. Specifically, we not\nonly consider the original graph structure information but also the latent\ncorrelation between features, thus BiGCN can filter the signals along with both\nthe original graph and a latent feature-connection graph. Our model outperforms\nprevious graph neural networks in the tasks of node classification and link\nprediction on most of the benchmark datasets, especially when we add noise to\nthe node features.",
        "Exploiting convolutional neural networks for point cloud processing is quite\nchallenging, due to the inherent irregular distribution and discrete shape\nrepresentation of point clouds. To address these problems, many handcrafted\nconvolution variants have sprung up in recent years. Though with elaborate\ndesign, these variants could be far from optimal in sufficiently capturing\ndiverse shapes formed by discrete points. In this paper, we propose\nPointSeaConv, i.e., a novel differential convolution search paradigm on point\nclouds. It can work in a purely data-driven manner and thus is capable of\nauto-creating a group of suitable convolutions for geometric shape modeling. We\nalso propose a joint optimization framework for simultaneous search of internal\nconvolution and external architecture, and introduce epsilon-greedy algorithm\nto alleviate the effect of discretization error. As a result, PointSeaNet, a\ndeep network that is sufficient to capture geometric shapes at both convolution\nlevel and architecture level, can be searched out for point cloud processing.\nExtensive experiments strongly evidence that our proposed PointSeaNet surpasses\ncurrent handcrafted deep models on challenging benchmarks across multiple tasks\nwith remarkable margins.",
        "In this paper, we propose an approach that spatially localizes the activities\nin a video frame where each person can perform multiple activities at the same\ntime. Our approach takes the temporal scene context as well as the relations of\nthe actions of detected persons into account. While the temporal context is\nmodeled by a temporal recurrent neural network (RNN), the relations of the\nactions are modeled by a graph RNN. Both networks are trained together and the\nproposed approach achieves state of the art results on the AVA dataset.",
        "Unsupervised domain transfer is the task of transferring or translating\nsamples from a source distribution to a different target distribution. Current\nsolutions unsupervised domain transfer often operate on data on which the modes\nof the distribution are well-matched, for instance have the same frequencies of\nclasses between source and target distributions. However, these models do not\nperform well when the modes are not well-matched, as would be the case when\nsamples are drawn independently from two different, but related, domains. This\nmode imbalance is problematic as generative adversarial networks (GANs), a\nsuccessful approach in this setting, are sensitive to mode frequency, which\nresults in a mismatch of semantics between source samples and generated samples\nof the target distribution. We propose a principled method of re-weighting\ntraining samples to correct for such mass shift between the transferred\ndistributions, which we call batch-weight. We also provide rigorous\nprobabilistic setting for domain transfer and new simplified objective for\ntraining transfer networks, an alternative to complex, multi-component loss\nfunctions used in the current state-of-the art image-to-image translation\nmodels. The new objective stems from the discrimination of joint distributions\nand enforces cycle-consistency in an abstract, high-level, rather than\npixel-wise, sense. Lastly, we experimentally show the effectiveness of the\nproposed methods in several image-to-image translation tasks.",
        "In this paper, we propose a new data poisoning attack and apply it to deep\nreinforcement learning agents. Our attack centers on what we call\nin-distribution triggers, which are triggers native to the data distributions\nthe model will be trained on and deployed in. We outline a simple procedure for\nembedding these, and other, triggers in deep reinforcement learning agents\nfollowing a multi-task learning paradigm, and demonstrate in three common\nreinforcement learning environments. We believe that this work has important\nimplications for the security of deep learning models.",
        "Classifying time series data using neural networks is a challenging problem\nwhen the length of the data varies. Video object trajectories, which are key to\nmany of the visual surveillance applications, are often found to be of varying\nlength. If such trajectories are used to understand the behavior (normal or\nanomalous) of moving objects, they need to be represented correctly. In this\npaper, we propose video object trajectory classification and anomaly detection\nusing a hybrid Convolutional Neural Network (CNN) and Variational Autoencoder\n(VAE) architecture. First, we introduce a high level representation of object\ntrajectories using color gradient form. In the next stage, a semi-supervised\nway to annotate moving object trajectories extracted using Temporal Unknown\nIncremental Clustering (TUIC), has been applied for trajectory class labeling.\nAnomalous trajectories are separated using t-Distributed Stochastic Neighbor\nEmbedding (t-SNE). Finally, a hybrid CNN-VAE architecture has been used for\ntrajectory classification and anomaly detection. The results obtained using\npublicly available surveillance video datasets reveal that the proposed method\ncan successfully identify some of the important traffic anomalies such as\nvehicles not following lane driving, sudden speed variations, abrupt\ntermination of vehicle movement, and vehicles moving in wrong directions. The\nproposed method is able to detect above anomalies at higher accuracy as\ncompared to existing anomaly detection methods.",
        "The accurate and interpretable prediction of future events in time-series\ndata often requires the capturing of representative patterns (or referred to as\nstates) underpinning the observed data. To this end, most existing studies\nfocus on the representation and recognition of states, but ignore the changing\ntransitional relations among them. In this paper, we present evolutionary state\ngraph, a dynamic graph structure designed to systematically represent the\nevolving relations (edges) among states (nodes) along time. We conduct analysis\non the dynamic graphs constructed from the time-series data and show that\nchanges on the graph structures (e.g., edges connecting certain state nodes)\ncan inform the occurrences of events (i.e., time-series fluctuation). Inspired\nby this, we propose a novel graph neural network model, Evolutionary State\nGraph Network (EvoNet), to encode the evolutionary state graph for accurate and\ninterpretable time-series event prediction. Specifically, Evolutionary State\nGraph Network models both the node-level (state-to-state) and graph-level\n(segment-to-segment) propagation, and captures the node-graph\n(state-to-segment) interactions over time. Experimental results based on five\nreal-world datasets show that our approach not only achieves clear improvements\ncompared with 11 baselines, but also provides more insights towards explaining\nthe results of event predictions.",
        "Segmenting unseen object instances in cluttered environments is an important\ncapability that robots need when functioning in unstructured environments.\nWhile previous methods have exhibited promising results, they still tend to\nprovide incorrect results in highly cluttered scenes. We postulate that a\nnetwork architecture that encodes relations between objects at a high-level can\nbe beneficial. Thus, in this work, we propose a novel framework that refines\nthe output of such methods by utilizing a graph-based representation of\ninstance masks. We train deep networks capable of sampling smart perturbations\nto the segmentations, and a graph neural network, which can encode relations\nbetween objects, to evaluate the perturbed segmentations. Our proposed method\nis orthogonal to previous works and achieves state-of-the-art performance when\ncombined with them. We demonstrate an application that uses uncertainty\nestimates generated by our method to guide a manipulator, leading to efficient\nunderstanding of cluttered scenes. Code, models, and video can be found at\nhttps://github.com/chrisdxie/rice .",
        "This work introduces a highly scalable spectral graph densification framework\nfor learning resistor networks with linear measurements, such as node voltages\nand currents. We prove that given $O(\\log N)$ pairs of voltage and current\nmeasurements, it is possible to recover ultra-sparse $N$-node resistor networks\nwhich can well preserve the effective resistance distances on the graph. Also,\nthe learned graphs preserve the structural (spectral) properties of the\noriginal graph, which can potentially be leveraged in many circuit design and\noptimization tasks. We show that the proposed graph learning approach is\nequivalent to solving the classical graphical Lasso problems with\nLaplacian-like precision matrices. Through extensive experiments for a variety\nof real-world test cases, we show that the proposed approach is highly scalable\nfor learning ultra-sparse resistor networks without sacrificing solution\nquality.",
        "Deep neural nets have caused a revolution in many classification tasks. A\nrelated ongoing revolution -- also theoretically not understood -- concerns\ntheir ability to serve as generative models for complicated types of data such\nas images and texts. These models are trained using ideas like variational\nautoencoders and Generative Adversarial Networks.\n  We take a first cut at explaining the expressivity of multilayer nets by\ngiving a sufficient criterion for a function to be approximable by a neural\nnetwork with $n$ hidden layers. A key ingredient is Barron's Theorem\n\\cite{Barron1993}, which gives a Fourier criterion for approximability of a\nfunction by a neural network with 1 hidden layer. We show that a composition of\n$n$ functions which satisfy certain Fourier conditions (\"Barron functions\") can\nbe approximated by a $n+1$-layer neural network.\n  For probability distributions, this translates into a criterion for a\nprobability distribution to be approximable in Wasserstein distance -- a\nnatural metric on probability distributions -- by a neural network applied to a\nfixed base distribution (e.g., multivariate gaussian).\n  Building up recent lower bound work, we also give an example function that\nshows that composition of Barron functions is more expressive than Barron\nfunctions alone.",
        "Due to the difficulty in generating the effective descriptors which are\nrobust to occlusion and viewpoint changes, place recognition for 3D point cloud\nremains an open issue. Unlike most of the existing methods that focus on\nextracting local, global, and statistical features of raw point clouds, our\nmethod aims at the semantic level that can be superior in terms of robustness\nto environmental changes. Inspired by the perspective of humans, who recognize\nscenes through identifying semantic objects and capturing their relations, this\npaper presents a novel semantic graph based approach for place recognition.\nFirst, we propose a novel semantic graph representation for the point cloud\nscenes by reserving the semantic and topological information of the raw point\ncloud. Thus, place recognition is modeled as a graph matching problem. Then we\ndesign a fast and effective graph similarity network to compute the similarity.\nExhaustive evaluations on the KITTI dataset show that our approach is robust to\nthe occlusion as well as viewpoint changes and outperforms the state-of-the-art\nmethods with a large margin. Our code is available at:\n\\url{https://github.com/kxhit/SG_PR}.",
        "In this paper, we propose a general dual convolutional neural network\n(DualCNN) for low-level vision problems, e.g., super-resolution,\nedge-preserving filtering, deraining and dehazing. These problems usually\ninvolve the estimation of two components of the target signals: structures and\ndetails. Motivated by this, our proposed DualCNN consists of two parallel\nbranches, which respectively recovers the structures and details in an\nend-to-end manner. The recovered structures and details can generate the target\nsignals according to the formation model for each particular application. The\nDualCNN is a flexible framework for low-level vision tasks and can be easily\nincorporated into existing CNNs. Experimental results show that the DualCNN can\nbe effectively applied to numerous low-level vision tasks with favorable\nperformance against the state-of-the-art methods.",
        "The Lucid methods described by Olah et al. (2018) provide a way to inspect\nthe inner workings of neural networks trained on image classification tasks\nusing feature visualization. Such methods have generally been applied to\nnetworks trained on visually rich, large-scale image datasets like ImageNet,\nwhich enables them to produce enticing feature visualizations. To investigate\nthese methods further, we applied them to classifiers trained to perform the\nmuch simpler (in terms of dataset size and visual richness), yet challenging\ntask of distinguishing between different kinds of white blood cell from\nmicroscope images. Such a task makes generating useful feature visualizations\ndifficult, as the discriminative features are inherently hard to identify and\ninterpret. We address this by presenting the \"Illuminated Decision Tree\"\napproach, in which we use a neural network trained on the task as a feature\nextractor, then learn a decision tree based on these features, and provide\nLucid visualizations for each node in the tree. We demonstrate our approach\nwith several examples, showing how this approach could be useful both in model\ndevelopment and debugging, and when explaining model outputs to non-experts.",
        "The success of deep neural networks relies on significant architecture\nengineering. Recently neural architecture search (NAS) has emerged as a promise\nto greatly reduce manual effort in network design by automatically searching\nfor optimal architectures, although typically such algorithms need an excessive\namount of computational resources, e.g., a few thousand GPU-days. To date, on\nchallenging vision tasks such as object detection, NAS, especially fast\nversions of NAS, is less studied. Here we propose to search for the decoder\nstructure of object detectors with search efficiency being taken into\nconsideration. To be more specific, we aim to efficiently search for the\nfeature pyramid network (FPN) as well as the prediction head of a simple\nanchor-free object detector, namely FCOS, using a tailored reinforcement\nlearning paradigm. With carefully designed search space, search algorithms and\nstrategies for evaluating network quality, we are able to efficiently search a\ntop-performing detection architecture within 4 days using 8 V100 GPUs. The\ndiscovered architecture surpasses state-of-the-art object detection models\n(such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points in AP on the\nCOCO dataset, with comparable computation complexity and memory footprint,\ndemonstrating the efficacy of the proposed NAS for object detection.",
        "Although numerous improvements have been made in the field of image\nsegmentation using convolutional neural networks, the majority of these\nimprovements rely on training with larger datasets, model architecture\nmodifications, novel loss functions, and better optimizers. In this paper, we\npropose a new segmentation performance boosting paradigm that relies on\noptimally modifying the network's input instead of the network itself. In\nparticular, we leverage the gradients of a trained segmentation network with\nrespect to the input to transfer it to a space where the segmentation accuracy\nimproves. We test the proposed method on three publicly available medical image\nsegmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, the\nShenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our method\nachieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores,\nrespectively.",
        "We present DeepPerimeter, a deep learning based pipeline for inferring a full\nindoor perimeter (i.e. exterior boundary map) from a sequence of posed RGB\nimages. Our method relies on robust deep methods for depth estimation and wall\nsegmentation to generate an exterior boundary point cloud, and then uses deep\nunsupervised clustering to fit wall planes to obtain a final boundary map of\nthe room. We demonstrate that DeepPerimeter results in excellent visual and\nquantitative performance on the popular ScanNet and FloorNet datasets and works\nfor room shapes of various complexities as well as in multiroom scenarios. We\nalso establish important baselines for future work on indoor perimeter\nestimation, topics which will become increasingly prevalent as application\nareas like augmented reality and robotics become more significant.",
        "Commonly, machine learning models minimize an empirical expectation. As a\nresult, the trained models typically perform well for the majority of the data\nbut the performance may deteriorate in less dense regions of the dataset. This\nissue also arises in generative modeling. A generative model may overlook\nunderrepresented modes that are less frequent in the empirical data\ndistribution. This problem is known as complete mode coverage. We propose a\nsampling procedure based on ridge leverage scores which significantly improves\nmode coverage when compared to standard methods and can easily be combined with\nany GAN. Ridge leverage scores are computed by using an explicit feature map,\nassociated with the next-to-last layer of a GAN discriminator or of a\npre-trained network, or by using an implicit feature map corresponding to a\nGaussian kernel. Multiple evaluations against recent approaches of complete\nmode coverage show a clear improvement when using the proposed sampling\nstrategy.",
        "We propose a method for efficient training of Q-functions for\ncontinuous-state Markov Decision Processes (MDPs) such that the traces of the\nresulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a\nmodal logic, can express a wide range of time-dependent logical properties\n(including \"safety\") that are quite similar to patterns in natural language. We\nconvert the LTL property into a limit deterministic Buchi automaton and\nconstruct an on-the-fly synchronised product MDP. The control policy is then\nsynthesised by defining an adaptive reward function and by applying a modified\nneural fitted Q-iteration algorithm to the synchronised structure, assuming\nthat no prior knowledge is available from the original MDP. The proposed method\nis evaluated in a numerical study to test the quality of the generated control\npolicy and is compared with conventional methods for policy synthesis such as\nMDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted\nvalue iteration).",
        "Despite ample motivation from costly exploration and limited trajectory data,\nrapidly adapting to new environments with few-shot reinforcement learning (RL)\ncan remain a challenging task, especially with respect to personalized\nsettings. Here, we consider the problem of recommending optimal policies to a\nset of multiple entities each with potentially different characteristics, such\nthat individual entities may parameterize distinct environments with unique\ntransition dynamics. Inspired by existing literature in meta-learning, we\nextend previous work by focusing on the notion that certain environments are\nmore similar to each other than others in personalized settings, and propose a\nmodel-free meta-learning algorithm that prioritizes past experiences by\nrelevance during gradient-based adaptation. Our algorithm involves\ncharacterizing past policy divergence through methods in inverse reinforcement\nlearning, and we illustrate how such metrics are able to effectively\ndistinguish past policy parameters by the environment they were deployed in,\nleading to more effective fast adaptation during test time. To study\npersonalization more effectively we introduce a navigation testbed to\nspecifically incorporate environment diversity across training episodes, and\ndemonstrate that our approach outperforms meta-learning alternatives with\nrespect to few-shot reinforcement learning in personalized settings.",
        "We introduce the novel problem of localizing all the instances of an object\n(seen or unseen during training) in a natural image via sketch query. We refer\nto this problem as sketch-guided object localization. This problem is\ndistinctively different from the traditional sketch-based image retrieval task\nwhere the gallery set often contains images with only one object. The\nsketch-guided object localization proves to be more challenging when we\nconsider the following: (i) the sketches used as queries are abstract\nrepresentations with little information on the shape and salient attributes of\nthe object, (ii) the sketches have significant variability as they are\nhand-drawn by a diverse set of untrained human subjects, and (iii) there exists\na domain gap between sketch queries and target natural images as these are\nsampled from very different data distributions. To address the problem of\nsketch-guided object localization, we propose a novel cross-modal attention\nscheme that guides the region proposal network (RPN) to generate object\nproposals relevant to the sketch query. These object proposals are later scored\nagainst the query to obtain final localization. Our method is effective with as\nlittle as a single sketch query. Moreover, it also generalizes well to object\ncategories not seen during training and is effective in localizing multiple\nobject instances present in the image. Furthermore, we extend our framework to\na multi-query setting using novel feature fusion and attention fusion\nstrategies introduced in this paper. The localization performance is evaluated\non publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC,\nwith sketch queries obtained from `Quick, Draw!'. The proposed method\nsignificantly outperforms related baselines on both single-query and\nmulti-query localization tasks.",
        "Offline reinforcement learning proposes to learn policies from large\ncollected datasets without interacting with the physical environment. These\nalgorithms have made it possible to learn useful skills from data that can then\nbe deployed in the environment in real-world settings where interactions may be\ncostly or dangerous, such as autonomous driving or factories. However, current\nalgorithms overfit to the dataset they are trained on and exhibit poor\nout-of-distribution generalization to the environment when deployed. In this\npaper, we study the effectiveness of performing data augmentations on the state\nspace, and study 7 different augmentation schemes and how they behave with\nexisting offline RL algorithms. We then combine the best data performing\naugmentation scheme with a state-of-the-art Q-learning technique, and improve\nthe function approximation of the Q-networks by smoothening out the learned\nstate-action space. We experimentally show that using this Surprisingly Simple\nSelf-Supervision technique in RL (S4RL), we significantly improve over the\ncurrent state-of-the-art algorithms on offline robot learning environments such\nas MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].",
        "Human explanations of high-level decisions are often expressed in terms of\nkey concepts the decisions are based on. In this paper, we study such\nconcept-based explainability for Deep Neural Networks (DNNs). First, we define\nthe notion of completeness, which quantifies how sufficient a particular set of\nconcepts is in explaining a model's prediction behavior based on the assumption\nthat complete concept scores are sufficient statistics of the model prediction.\nNext, we propose a concept discovery method that aims to infer a complete set\nof concepts that are additionally encouraged to be interpretable, which\naddresses the limitations of existing methods on concept explanations. To\ndefine an importance score for each discovered concept, we adapt game-theoretic\nnotions to aggregate over sets and propose ConceptSHAP. Via proposed metrics\nand user studies, on a synthetic dataset with apriori-known concept\nexplanations, as well as on real-world image and language datasets, we validate\nthe effectiveness of our method in finding concepts that are both complete in\nexplaining the decisions and interpretable. (The code is released at\nhttps://github.com/chihkuanyeh/concept_exp)",
        "We consider the problem of representation learning for temporal interaction\ngraphs where a network of entities with complex interactions over an extended\nperiod of time is modeled as a graph with a rich set of node and edge\nattributes. In particular, an edge between a node-pair within the graph\ncorresponds to a multi-dimensional time-series. To fully capture and model the\ndynamics of the network, we propose GTEA, a framework of representation\nlearning for temporal interaction graphs with per-edge time-based aggregation.\nUnder GTEA, a Graph Neural Network (GNN) is integrated with a state-of-the-art\nsequence model, such as LSTM, Transformer and their time-aware variants. The\nsequence model generates edge embeddings to encode temporal interaction\npatterns between each pair of nodes, while the GNN-based backbone learns the\ntopological dependencies and relationships among different nodes. GTEA also\nincorporates a sparsity-inducing self-attention mechanism to distinguish and\nfocus on the more important neighbors of each node during the aggregation\nprocess. By capturing temporal interactive dynamics together with\nmulti-dimensional node and edge attributes in a network, GTEA can learn\nfine-grained representations for a temporal interaction graph to enable or\nfacilitate other downstream data analytic tasks. Experimental results show that\nGTEA outperforms state-of-the-art schemes including GraphSAGE, APPNP, and TGAT\nby delivering higher accuracy (100.00%, 98.51%, 98.05% ,79.90%) and macro-F1\nscore (100.00%, 98.51%, 96.68% ,79.90%) over four large-scale real-world\ndatasets for binary/ multi-class node classification.",
        "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
        "Graph Neural Networks (GNNs) have demonstrated superior performance in\nlearning node representations for various graph inference tasks. However,\nlearning over graph data can raise privacy concerns when nodes represent people\nor human-related variables that involve sensitive or personal information.\nWhile numerous techniques have been proposed for privacy-preserving deep\nlearning over non-relational data, there is less work addressing the privacy\nissues pertained to applying deep learning algorithms on graphs. In this paper,\nwe study the problem of node data privacy, where graph nodes have potentially\nsensitive data that is kept private, but they could be beneficial for a central\nserver for training a GNN over the graph. To address this problem, we develop a\nprivacy-preserving, architecture-agnostic GNN learning algorithm with formal\nprivacy guarantees based on Local Differential Privacy (LDP). Specifically, we\npropose an LDP encoder and an unbiased rectifier, by which the server can\ncommunicate with the graph nodes to privately collect their data and\napproximate the GNN's first layer. To further reduce the effect of the injected\nnoise, we propose to prepend a simple graph convolution layer, called KProp,\nwhich is based on the multi-hop aggregation of the nodes' features acting as a\ndenoising mechanism. Finally, we propose a robust training framework, in which\nwe benefit from KProp's denoising capability to increase the accuracy of\ninference in the presence of noisy labels. Extensive experiments conducted over\nreal-world datasets demonstrate that our method can maintain a satisfying level\nof accuracy with low privacy loss.",
        "In machine learning, a nonparametric forecasting algorithm for time series\ndata has been proposed, called the kernel spectral hidden Markov model (KSHMM).\nIn this paper, we propose a technique for short-term wind-speed prediction\nbased on KSHMM. We numerically compared the performance of our KSHMM-based\nforecasting technique to other techniques with machine learning, using\nwind-speed data offered by the National Renewable Energy Laboratory. Our\nresults demonstrate that, compared to these methods, the proposed technique\noffers comparable or better performance.",
        "With widespread applications of artificial intelligence (AI), the\ncapabilities of the perception, understanding, decision-making and control for\nautonomous systems have improved significantly in the past years. When\nautonomous systems consider the performance of accuracy and transferability,\nseveral AI methods, like adversarial learning, reinforcement learning (RL) and\nmeta-learning, show their powerful performance. Here, we review the\nlearning-based approaches in autonomous systems from the perspectives of\naccuracy and transferability. Accuracy means that a well-trained model shows\ngood results during the testing phase, in which the testing set shares a same\ntask or a data distribution with the training set. Transferability means that\nwhen a well-trained model is transferred to other testing domains, the accuracy\nis still good. Firstly, we introduce some basic concepts of transfer learning\nand then present some preliminaries of adversarial learning, RL and\nmeta-learning. Secondly, we focus on reviewing the accuracy or transferability\nor both of them to show the advantages of adversarial learning, like generative\nadversarial networks (GANs), in typical computer vision tasks in autonomous\nsystems, including image style transfer, image superresolution, image\ndeblurring/dehazing/rain removal, semantic segmentation, depth estimation,\npedestrian detection and person re-identification (re-ID). Then, we further\nreview the performance of RL and meta-learning from the aspects of accuracy or\ntransferability or both of them in autonomous systems, involving pedestrian\ntracking, robot navigation and robotic manipulation. Finally, we discuss\nseveral challenges and future topics for using adversarial learning, RL and\nmeta-learning in autonomous systems.",
        "Constraining linear layers in neural networks to respect symmetry\ntransformations from a group $G$ is a common design principle for invariant\nnetworks that has found many applications in machine learning.\n  In this paper, we consider a fundamental question that has received little\nattention to date: Can these networks approximate any (continuous) invariant\nfunction?\n  We tackle the rather general case where $G\\leq S_n$ (an arbitrary subgroup of\nthe symmetric group) that acts on $\\mathbb{R}^n$ by permuting coordinates. This\nsetting includes several recent popular invariant networks. We present two main\nresults: First, $G$-invariant networks are universal if high-order tensors are\nallowed. Second, there are groups $G$ for which higher-order tensors are\nunavoidable for obtaining universality.\n  $G$-invariant networks consisting of only first-order tensors are of special\ninterest due to their practical value. We conclude the paper by proving a\nnecessary condition for the universality of $G$-invariant networks that\nincorporate only first-order tensors.",
        "Colorization of grayscale images has been a hot topic in computer vision.\nPrevious research mainly focuses on producing a colored image to match the\noriginal one. However, since many colors share the same gray value, an input\ngrayscale image could be diversely colored while maintaining its reality. In\nthis paper, we design a novel solution for unsupervised diverse colorization.\nSpecifically, we leverage conditional generative adversarial networks to model\nthe distribution of real-world item colors, in which we develop a fully\nconvolutional generator with multi-layer noise to enhance diversity, with\nmulti-layer condition concatenation to maintain reality, and with stride 1 to\nkeep spatial information. With such a novel network architecture, the model\nyields highly competitive performance on the open LSUN bedroom dataset. The\nTuring test of 80 humans further indicates our generated color schemes are\nhighly convincible.",
        "Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation.",
        "Existing reference (RF)-based super-resolution (SR) models try to improve\nperceptual quality in SR under the assumption of the availability of\nhigh-resolution RF images paired with low-resolution (LR) inputs at testing. As\nthe RF images should be similar in terms of content, colors, contrast, etc. to\nthe test image, this hinders the applicability in a real scenario. Other\napproaches to increase the perceptual quality of images, including perceptual\nloss and adversarial losses, tend to dramatically decrease fidelity to the\nground-truth through significant decreases in PSNR/SSIM. Addressing both\nissues, we propose a simple yet universal approach to improve the perceptual\nquality of the HR prediction from a pre-trained SR network on a given LR input\nby further fine-tuning the SR network on a subset of images from the training\ndataset with similar patterns of activation as the initial HR prediction, with\nrespect to the filters of a feature extractor. In particular, we show the\neffects of fine-tuning on these images in terms of the perceptual quality and\nPSNR/SSIM values. Contrary to perceptually driven approaches, we demonstrate\nthat the fine-tuned network produces a HR prediction with both greater\nperceptual quality and minimal changes to the PSNR/SSIM with respect to the\ninitial HR prediction. Further, we present novel numerical experiments\nconcerning the filters of SR networks, where we show through filter\ncorrelation, that the filters of the fine-tuned network from our method are\ncloser to \"ideal\" filters, than those of the baseline network or a network\nfine-tuned on random images.",
        "Image-Text Matching is one major task in cross-modal information processing.\nThe main challenge is to learn the unified visual and textual representations.\nPrevious methods that perform well on this task primarily focus on not only the\nalignment between region features in images and the corresponding words in\nsentences, but also the alignment between relations of regions and relational\nwords. However, the lack of joint learning of regional features and global\nfeatures will cause the regional features to lose contact with the global\ncontext, leading to the mismatch with those non-object words which have global\nmeanings in some sentences. In this work, in order to alleviate this issue, it\nis necessary to enhance the relations between regions and the relations between\nregional and global concepts to obtain a more accurate visual representation so\nas to be better correlated to the corresponding text. Thus, a novel multi-level\nsemantic relations enhancement approach named Dual Semantic Relations Attention\nNetwork(DSRAN) is proposed which mainly consists of two modules, separate\nsemantic relations module and the joint semantic relations module. DSRAN\nperforms graph attention in both modules respectively for region-level\nrelations enhancement and regional-global relations enhancement at the same\ntime. With these two modules, different hierarchies of semantic relations are\nlearned simultaneously, thus promoting the image-text matching process by\nproviding more information for the final visual representation. Quantitative\nexperimental results have been performed on MS-COCO and Flickr30K and our\nmethod outperforms previous approaches by a large margin due to the\neffectiveness of the dual semantic relations learning scheme. Codes are\navailable at https://github.com/kywen1119/DSRAN.",
        "Siamese networks have drawn great attention in visual tracking because of\ntheir balanced accuracy and speed. However, the backbone networks used in\nSiamese trackers are relatively shallow, such as AlexNet [18], which does not\nfully take advantage of the capability of modern deep neural networks. In this\npaper, we investigate how to leverage deeper and wider convolutional neural\nnetworks to enhance tracking robustness and accuracy. We observe that direct\nreplacement of backbones with existing powerful architectures, such as ResNet\n[14] and Inception [33], does not bring improvements. The main reasons are that\n1)large increases in the receptive field of neurons lead to reduced feature\ndiscriminability and localization precision; and 2) the network padding for\nconvolutions induces a positional bias in learning. To address these issues, we\npropose new residual modules to eliminate the negative impact of padding, and\nfurther design new architectures using these modules with controlled receptive\nfield size and network stride. The designed architectures are lightweight and\nguarantee real-time tracking speed when applied to SiamFC [2] and SiamRPN [20].\nExperiments show that solely due to the proposed network architectures, our\nSiamFC+ and SiamRPN+ obtain up to 9.8%/5.7% (AUC), 23.3%/8.8% (EAO) and\n24.4%/25.0% (EAO) relative improvements over the original versions [2, 20] on\nthe OTB-15, VOT-16 and VOT-17 datasets, respectively.",
        "Objective: This study aims to establish a generalized transfer-learning\nframework for boosting the performance of steady-state visual evoked potential\n(SSVEP)-based brain-computer interfaces (BCIs) by leveraging cross-domain data\ntransferring. Approach: We enhanced the state-of-the-art template-based SSVEP\ndecoding through incorporating a least-squares transformation (LST)-based\ntransfer learning to leverage calibration data across multiple domains\n(sessions, subjects, and EEG montages). Main results: Study results verified\nthe efficacy of LST in obviating the variability of SSVEPs when transferring\nexisting data across domains. Furthermore, the LST-based method achieved\nsignificantly higher SSVEP-decoding accuracy than the standard task-related\ncomponent analysis (TRCA)-based method and the non-LST naive transfer-learning\nmethod. Significance: This study demonstrated the capability of the LST-based\ntransfer learning to leverage existing data across subjects and/or devices with\nan in-depth investigation of its rationale and behavior in various\ncircumstances. The proposed framework significantly improved the SSVEP decoding\naccuracy over the standard TRCA approach when calibration data are limited. Its\nperformance in calibration reduction could facilitate plug-and-play SSVEP-based\nBCIs and further practical applications.",
        "Initial DR studies mainly adopt model predictive control and thus require\naccurate models of the control problem (e.g., a customer behavior model), which\nare to a large extent uncertain for the EV scenario. Hence, model-free\napproaches, especially based on reinforcement learning (RL) are an attractive\nalternative. In this paper, we propose a new Markov decision process (MDP)\nformulation in the RL framework, to jointly coordinate a set of EV charging\nstations. State-of-the-art algorithms either focus on a single EV, or perform\nthe control of an aggregate of EVs in multiple steps (e.g., aggregate load\ndecisions in one step, then a step translating the aggregate decision to\nindividual connected EVs). On the contrary, we propose an RL approach to\njointly control the whole set of EVs at once. We contribute a new MDP\nformulation, with a scalable state representation that is independent of the\nnumber of EV charging stations. Further, we use a batch reinforcement learning\nalgorithm, i.e., an instance of fitted Q-iteration, to learn the optimal\ncharging policy. We analyze its performance using simulation experiments based\non a real-world EV charging data. More specifically, we (i) explore the various\nsettings in training the RL policy (e.g., duration of the period with training\ndata), (ii) compare its performance to an oracle all-knowing benchmark (which\nprovides an upper bound for performance, relying on information that is not\navailable or at least imperfect in practice), (iii) analyze performance over\ntime, over the course of a full year to evaluate possible performance\nfluctuations (e.g, across different seasons), and (iv) demonstrate the\ngeneralization capacity of a learned control policy to larger sets of charging\nstations.",
        "End-to-end learning refers to training a possibly complex learning system by\napplying gradient-based learning to the system as a whole. End-to-end learning\nsystem is specifically designed so that all modules are differentiable. In\neffect, not only a central learning machine, but also all \"peripheral\" modules\nlike representation learning and memory formation are covered by a holistic\nlearning process. The power of end-to-end learning has been demonstrated on\nmany tasks, like playing a whole array of Atari video games with a single\narchitecture. While pushing for solutions to more challenging tasks, network\narchitectures keep growing more and more complex.\n  In this paper we ask the question whether and to what extent end-to-end\nlearning is a future-proof technique in the sense of scaling to complex and\ndiverse data processing architectures. We point out potential inefficiencies,\nand we argue in particular that end-to-end learning does not make optimal use\nof the modular design of present neural networks. Our surprisingly simple\nexperiments demonstrate these inefficiencies, up to the complete breakdown of\nlearning.",
        "In this paper, we will investigate the contribution of color names for the\ntask of salient object detection. An input image is first converted to color\nname space, which is consisted of 11 probabilistic channels. By exploiting a\nsurroundedness cue, we obtain a saliency map through a linear combination of a\nset of sequential attention maps. To overcome the limitation of only using the\nsurroundedness cue, two global cues with respect to color names are invoked to\nguide the computation of a weighted saliency map. Finally, we integrate the\nabove two saliency maps into a unified framework to generate the final result.\nIn addition, an improved post-processing procedure is introduced to effectively\nsuppress image backgrounds while uniformly highlight salient objects.\nExperimental results show that the proposed model produces more accurate\nsaliency maps and performs well against twenty-one saliency models in terms of\nthree evaluation metrics on three public data sets.",
        "As an effective data preprocessing step, feature selection has shown its\neffectiveness to prepare high-dimensional data for many machine learning tasks.\nThe proliferation of high di-mension and huge volume big data, however, has\nbrought major challenges, e.g. computation complexity and stability on noisy\ndata, upon existing feature-selection techniques. This paper introduces a novel\nneural network-based feature selection architecture, dubbed Attention-based\nFeature Selec-tion (AFS). AFS consists of two detachable modules: an at-tention\nmodule for feature weight generation and a learning module for the problem\nmodeling. The attention module for-mulates correlation problem among features\nand supervision target into a binary classification problem, supported by a\nshallow attention net for each feature. Feature weights are generated based on\nthe distribution of respective feature se-lection patterns adjusted by\nbackpropagation during the train-ing process. The detachable structure allows\nexisting off-the-shelf models to be directly reused, which allows for much less\ntraining time, demands for the training data and requirements for expertise. A\nhybrid initialization method is also intro-duced to boost the selection\naccuracy for datasets without enough samples for feature weight generation.\nExperimental results show that AFS achieves the best accuracy and stability in\ncomparison to several state-of-art feature selection algo-rithms upon both\nMNIST, noisy MNIST and several datasets with small samples.",
        "Model compression and knowledge distillation have been successfully applied\nfor cross-architecture and cross-domain transfer learning. However, a key\nrequirement is that training examples are in correspondence across the domains.\nWe show that in many scenarios of practical importance such aligned data can be\nsynthetically generated using computer graphics pipelines allowing domain\nadaptation through distillation. We apply this technique to learn models for\nrecognizing low-resolution images using labeled high-resolution images,\nnon-localized objects using labeled localized objects, line-drawings using\nlabeled color images, etc. Experiments on various fine-grained recognition\ndatasets demonstrate that the technique improves recognition performance on the\nlow-quality data and beats strong baselines for domain adaptation. Finally, we\npresent insights into workings of the technique through visualizations and\nrelating it to existing literature.",
        "Point cloud semantic segmentation is a crucial task in 3D scene\nunderstanding. Existing methods mainly focus on employing a large number of\nannotated labels for supervised semantic segmentation. Nonetheless, manually\nlabeling such large point clouds for the supervised segmentation task is\ntime-consuming. In order to reduce the number of annotated labels, we propose a\nsemi-supervised semantic point cloud segmentation network, named SSPC-Net,\nwhere we train the semantic segmentation network by inferring the labels of\nunlabeled points from the few annotated 3D points. In our method, we first\npartition the whole point cloud into superpoints and build superpoint graphs to\nmine the long-range dependencies in point clouds. Based on the constructed\nsuperpoint graph, we then develop a dynamic label propagation method to\ngenerate the pseudo labels for the unsupervised superpoints. Particularly, we\nadopt a superpoint dropout strategy to dynamically select the generated pseudo\nlabels. In order to fully exploit the generated pseudo labels of the\nunsupervised superpoints, we furthermore propose a coupled attention mechanism\nfor superpoint feature embedding. Finally, we employ the cross-entropy loss to\ntrain the semantic segmentation network with the labels of the supervised\nsuperpoints and the pseudo labels of the unsupervised superpoints. Experiments\non various datasets demonstrate that our semi-supervised segmentation method\ncan achieve better performance than the current semi-supervised segmentation\nmethod with fewer annotated 3D points. Our code is available at\nhttps://github.com/MMCheng/SSPC-Net.",
        "Prior Networks are a recently developed class of models which yield\ninterpretable measures of uncertainty and have been shown to outperform\nstate-of-the-art ensemble approaches on a range of tasks. They can also be used\nto distill an ensemble of models via Ensemble Distribution Distillation\n(EnD$^2$), such that its accuracy, calibration and uncertainty estimates are\nretained within a single model. However, Prior Networks have so far been\ndeveloped only for classification tasks. This work extends Prior Networks and\nEnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The\nproperties of Regression Prior Networks are demonstrated on synthetic data,\nselected UCI datasets and a monocular depth estimation task, where they yield\nperformance competitive with ensemble approaches.",
        "Federated Learning (FL) is a framework which enables distributed model\ntraining using a large corpus of decentralized training data. Existing methods\naggregate models disregarding their internal representations, which are crucial\nfor training models in vision tasks. System and statistical heterogeneity\n(e.g., highly imbalanced and non-i.i.d. data) further harm model training. To\nthis end, we introduce a method, called FedProto, which computes client\ndeviations using margins of prototypical representations learned on distributed\ndata, and applies them to drive federated optimization via an attention\nmechanism. In addition, we propose three methods to analyse statistical\nproperties of feature representations learned in FL, in order to elucidate the\nrelationship between accuracy, margins and feature discrepancy of FL models. In\nexperimental analyses, FedProto demonstrates state-of-the-art accuracy and\nconvergence rate across image classification and semantic segmentation\nbenchmarks by enabling maximum margin training of FL models. Moreover, FedProto\nreduces uncertainty of predictions of FL models compared to the baseline. To\nour knowledge, this is the first work evaluating FL models in dense prediction\ntasks, such as semantic segmentation.",
        "To make efficient use of limited spectral resources, we in this work propose\na deep actor-critic reinforcement learning based framework for dynamic\nmultichannel access. We consider both a single-user case and a scenario in\nwhich multiple users attempt to access channels simultaneously. We employ the\nproposed framework as a single agent in the single-user case, and extend it to\na decentralized multi-agent framework in the multi-user scenario. In both\ncases, we develop algorithms for the actor-critic deep reinforcement learning\nand evaluate the proposed learning policies via experiments and numerical\nresults. In the single-user model, in order to evaluate the performance of the\nproposed channel access policy and the framework's tolerance against\nuncertainty, we explore different channel switching patterns and different\nswitching probabilities. In the case of multiple users, we analyze the\nprobabilities of each user accessing channels with favorable channel conditions\nand the probability of collision. We also address a time-varying environment to\nidentify the adaptive ability of the proposed framework. Additionally, we\nprovide comparisons (in terms of both the average reward and time efficiency)\nbetween the proposed actor-critic deep reinforcement learning framework, Deep-Q\nnetwork (DQN) based approach, random access, and the optimal policy when the\nchannel dynamics are known.",
        "Precise calibration is a must for high reliance 3D computer vision\nalgorithms. A challenging case is when the camera is behind a protective glass\nor transparent object: due to refraction, the image is heavily distorted; the\npinhole camera model alone can not be used and a distortion correction step is\nrequired. By directly modeling the geometry of the refractive media, we build\nthe image generation process by tracing individual light rays from the camera\nto a target. Comparing the generated images to their distorted - observed -\ncounterparts, we estimate the geometry parameters of the refractive surface via\nmodel inversion by employing an RBF neural network. We present an image\ncollection methodology that produces data suited for finding the distortion\nparameters and test our algorithm on synthetic and real-world data. We analyze\nthe results of the algorithm.",
        "Over the last decades, most approaches proposed for handwritten digit string\nrecognition (HDSR) have resorted to digit segmentation, which is dominated by\nheuristics, thereby imposing substantial constraints on the final performance.\nFew of them have been based on segmentation-free strategies where each pixel\ncolumn has a potential cut location. Recently, segmentation-free strategies has\nadded another perspective to the problem, leading to promising results.\nHowever, these strategies still show some limitations when dealing with a large\nnumber of touching digits. To bridge the resulting gap, in this paper, we\nhypothesize that a string of digits can be approached as a sequence of objects.\nWe thus evaluate different end-to-end approaches to solve the HDSR problem,\nparticularly in two verticals: those based on object-detection (e.g., Yolo and\nRetinaNet) and those based on sequence-to-sequence representation (CRNN). The\nmain contribution of this work lies in its provision of a comprehensive\ncomparison with a critical analysis of the above mentioned strategies on five\nbenchmarks commonly used to assess HDSR, including the challenging Touching\nPair dataset, NIST SD19, and two real-world datasets (CAR and CVL) proposed for\nthe ICFHR 2014 competition on HDSR. Our results show that the Yolo model\ncompares favorably against segmentation-free models with the advantage of\nhaving a shorter pipeline that minimizes the presence of heuristics-based\nmodels. It achieved a 97%, 96%, and 84% recognition rate on the NIST-SD19, CAR,\nand CVL datasets, respectively.",
        "The instance discrimination paradigm has become dominant in unsupervised\nlearning. It always adopts a teacher-student framework, in which the teacher\nprovides embedded knowledge as a supervision signal for the student. The\nstudent learns meaningful representations by enforcing instance spatial\nconsistency with the views from the teacher. However, the outputs of the\nteacher can vary dramatically on the same instance during different training\nstages, introducing unexpected noise and leading to catastrophic forgetting\ncaused by inconsistent objectives. In this paper, we first integrate instance\ntemporal consistency into current instance discrimination paradigms, and\npropose a novel and strong algorithm named Temporal Knowledge Consistency\n(TKC). Specifically, our TKC dynamically ensembles the knowledge of temporal\nteachers and adaptively selects useful information according to its importance\nto learning instance temporal consistency. Experimental result shows that TKC\ncan learn better visual representations on both ResNet and AlexNet on linear\nevaluation protocol while transfer well to downstream tasks. All experiments\nsuggest the good effectiveness and generalization of our method.",
        "Deep neural network models used for medical image segmentation are large\nbecause they are trained with high-resolution three-dimensional (3D) images.\nGraphics processing units (GPUs) are widely used to accelerate the trainings.\nHowever, the memory on a GPU is not large enough to train the models. A popular\napproach to tackling this problem is patch-based method, which divides a large\nimage into small patches and trains the models with these small patches.\nHowever, this method would degrade the segmentation quality if a target object\nspans multiple patches. In this paper, we propose a novel approach for 3D\nmedical image segmentation that utilizes the data-swapping, which swaps out\nintermediate data from GPU memory to CPU memory to enlarge the effective GPU\nmemory size, for training high-resolution 3D medical images without patching.\nWe carefully tuned parameters in the data-swapping method to obtain the best\ntraining performance for 3D U-Net, a widely used deep neural network model for\nmedical image segmentation. We applied our tuning to train 3D U-Net with\nfull-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,\ncommunication overhead, which is the most important issue, was reduced by\n17.1%. Compared with the patch-based method for patches of 128 x 128 x 128\nvoxels, our training for full-size images achieved improvement on the mean Dice\nscore by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core\nsub-region, respectively. The total training time was reduced from 164 hours to\n47 hours, resulting in 3.53 times of acceleration.",
        "The recent empirical success of unsupervised cross-domain mapping algorithms,\nbetween two domains that share common characteristics, is not well-supported by\ntheoretical justifications. This lacuna is especially troubling, given the\nclear ambiguity in such mappings.\n  We work with adversarial training methods based on IPMs and derive a novel\nrisk bound, which upper bounds the risk between the learned mapping $h$ and the\ntarget mapping $y$, by a sum of three terms: (i) the risk between $h$ and the\nmost distant alternative mapping that was learned by the same cross-domain\nmapping algorithm, (ii) the minimal discrepancy between the target domain and\nthe domain obtained by applying a hypothesis $h^*$ on the samples of the source\ndomain, where $h^*$ is a hypothesis selectable by the same algorithm. The bound\nis directly related to Occam's razor and encourages the selection of the\nminimal architecture that supports a small mapping discrepancy and (iii) an\napproximation error term that decreases as the complexity of the class of\ndiscriminators increases and is empirically shown to be small.\n  The bound leads to multiple algorithmic consequences, including a method for\nhyperparameters selection and for early stopping in cross-domain mapping GANs.\nWe also demonstrate a novel capability for unsupervised learning of estimating\nconfidence in the mapping of every specific sample.",
        "Conventionally, model-based reinforcement learning (MBRL) aims to learn a\nglobal model for the dynamics of the environment. A good model can potentially\nenable planning algorithms to generate a large variety of behaviors and solve\ndiverse tasks. However, learning an accurate model for complex dynamical\nsystems is difficult, and even then, the model might not generalize well\noutside the distribution of states on which it was trained. In this work, we\ncombine model-based learning with model-free learning of primitives that make\nmodel-based planning easy. To that end, we aim to answer the question: how can\nwe discover skills whose outcomes are easy to predict? We propose an\nunsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS),\nwhich simultaneously discovers predictable behaviors and learns their dynamics.\nOur method can leverage continuous skill spaces, theoretically, allowing us to\nlearn infinitely many behaviors even for high-dimensional state-spaces. We\ndemonstrate that zero-shot planning in the learned latent space significantly\noutperforms standard MBRL and model-free goal-conditioned RL, can handle\nsparse-reward tasks, and substantially improves over prior hierarchical RL\nmethods for unsupervised skill discovery.",
        "Portfolio management is the decision-making process of allocating an amount\nof fund into different financial investment products. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. This paper presents a\nmodel-less convolutional neural network with historic prices of a set of\nfinancial assets as its input, outputting portfolio weights of the set. The\nnetwork is trained with 0.7 years' price data from a cryptocurrency exchange.\nThe training is done in a reinforcement manner, maximizing the accumulative\nreturn, which is regarded as the reward function of the network. Backtest\ntrading experiments with trading period of 30 minutes is conducted in the same\nmarket, achieving 10-fold returns in 1.8 months' periods. Some recently\npublished portfolio selection strategies are also used to perform the same\nback-tests, whose results are compared with the neural network. The network is\nnot limited to cryptocurrency, but can be applied to any other financial\nmarkets.",
        "Panoptic segmentation has recently unified semantic and instance\nsegmentation, previously addressed separately, thus taking a step further\ntowards creating more comprehensive and efficient perception systems. In this\npaper, we present Panoster, a novel proposal-free panoptic segmentation method\nfor LiDAR point clouds. Unlike previous approaches relying on several steps to\ngroup pixels or points into objects, Panoster proposes a simplified framework\nincorporating a learning-based clustering solution to identify instances. At\ninference time, this acts as a class-agnostic segmentation, allowing Panoster\nto be fast, while outperforming prior methods in terms of accuracy. Without any\npost-processing, Panoster reached state-of-the-art results among published\napproaches on the challenging SemanticKITTI benchmark, and further increased\nits lead by exploiting heuristic techniques. Additionally, we showcase how our\nmethod can be flexibly and effectively applied on diverse existing semantic\narchitectures to deliver panoptic predictions.",
        "Potential-based reward shaping provides an approach for designing good reward\nfunctions, with the purpose of speeding up learning. However, automatically\nfinding potential functions for complex environments is a difficult problem (in\nfact, of the same difficulty as learning a value function from scratch). We\npropose a new framework for learning potential functions by leveraging ideas\nfrom graph representation learning. Our approach relies on Graph Convolutional\nNetworks which we use as a key ingredient in combination with the probabilistic\ninference view of reinforcement learning. More precisely, we leverage Graph\nConvolutional Networks to perform message passing from rewarding states. The\npropagated messages can then be used as potential functions for reward shaping\nto accelerate learning. We verify empirically that our approach can achieve\nconsiderable improvements in both small and high-dimensional control problems.",
        "In the deployment of deep neural models, how to effectively and automatically\nfind feasible deep models under diverse design objectives is fundamental. Most\nexisting neural architecture search (NAS) methods utilize surrogates to predict\nthe detailed performance (e.g., accuracy and model size) of a candidate\narchitecture during the search, which however is complicated and inefficient.\nIn contrast, we aim to learn an efficient Pareto classifier to simplify the\nsearch process of NAS by transforming the complex multi-objective NAS task into\na simple Pareto-dominance classification task. To this end, we propose a\nclassification-wise Pareto evolution approach for one-shot NAS, where an online\nclassifier is trained to predict the dominance relationship between the\ncandidate and constructed reference architectures, instead of using surrogates\nto fit the objective functions. The main contribution of this study is to\nchange supernet adaption into a Pareto classifier. Besides, we design two\nadaptive schemes to select the reference set of architectures for constructing\nclassification boundary and regulate the rate of positive samples over negative\nones, respectively. We compare the proposed evolution approach with\nstate-of-the-art approaches on widely-used benchmark datasets, and experimental\nresults indicate that the proposed approach outperforms other approaches and\nhave found a number of neural architectures with different model sizes ranging\nfrom 2M to 6M under diverse objectives and constraints.",
        "We present a novel group collaborative learning framework (GCoNet) capable of\ndetecting co-salient objects in real time (16ms), by simultaneously mining\nconsensus representations at group level based on the two necessary criteria:\n1) intra-group compactness to better formulate the consistency among co-salient\nobjects by capturing their inherent shared attributes using our novel group\naffinity module; 2) inter-group separability to effectively suppress the\ninfluence of noisy objects on the output by introducing our new group\ncollaborating module conditioning the inconsistent consensus. To learn a better\nembedding space without extra computational overhead, we explicitly employ\nauxiliary classification supervision. Extensive experiments on three\nchallenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate that\nour simple GCoNet outperforms 10 cutting-edge models and achieves the new\nstate-of-the-art. We demonstrate this paper's new technical contributions on a\nnumber of important downstream computer vision applications including content\naware co-segmentation, co-localization based automatic thumbnails, etc.",
        "Point cloud registration is a fundamental problem in 3D computer vision.\nOutdoor LiDAR point clouds are typically large-scale and complexly distributed,\nwhich makes the registration challenging. In this paper, we propose an\nefficient hierarchical network named HRegNet for large-scale outdoor LiDAR\npoint cloud registration. Instead of using all points in the point clouds,\nHRegNet performs registration on hierarchically extracted keypoints and\ndescriptors. The overall framework combines the reliable features in deeper\nlayer and the precise position information in shallower layers to achieve\nrobust and precise registration. We present a correspondence network to\ngenerate correct and accurate keypoints correspondences. Moreover, bilateral\nconsensus and neighborhood consensus are introduced for keypoints matching and\nnovel similarity features are designed to incorporate them into the\ncorrespondence network, which significantly improves the registration\nperformance. Besides, the whole network is also highly efficient since only a\nsmall number of keypoints are used for registration. Extensive experiments are\nconducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate\nthe high accuracy and efficiency of the proposed HRegNet. The project website\nis https://ispc-group.github.io/hregnet.",
        "Classical anomaly detection is principally concerned with point-based\nanomalies, those anomalies that occur at a single point in time. Yet, many\nreal-world anomalies are range-based, meaning they occur over a period of time.\nMotivated by this observation, we present a new mathematical model to evaluate\nthe accuracy of time series classification algorithms. Our model expands the\nwell-known Precision and Recall metrics to measure ranges, while simultaneously\nenabling customization support for domain-specific preferences.",
        "It is important, but challenging, for the forest industry to accurately map\nroads which are used for timber transport by trucks. In this work, we propose a\nDense Dilated Convolutions Merging Network (DDCM-Net) to detect these roads in\nlidar images. The DDCM-Net can effectively recognize multi-scale and complex\nshaped roads with similar texture and colors, and also is shown to have\nsuperior performance over existing methods. To further improve its ability to\naccurately infer categories of roads, we propose the use of a joint-task\nlearning strategy that utilizes two auxiliary output branches, i.e, multi-class\nclassification and binary segmentation, joined with the main output of\nfull-class segmentation. This pushes the network towards learning more robust\nrepresentations that are expected to boost the ultimate performance of the main\ntask. In addition, we introduce an iterative-random-weighting method to\nautomatically weigh the joint losses for auxiliary tasks. This can avoid the\ndifficult and expensive process of tuning the weights of each task's loss by\nhand. The experiments demonstrate that our proposed joint-task DDCM-Net can\nachieve better performance with fewer parameters and higher computational\nefficiency than previous state-of-the-art approaches.",
        "Despite the recent progress of generative adversarial networks (GANs) at\nsynthesizing photo-realistic images, producing complex urban scenes remains a\nchallenging problem. Previous works break down scene generation into two\nconsecutive phases: unconditional semantic layout synthesis and image synthesis\nconditioned on layouts. In this work, we propose to condition layout generation\nas well for higher semantic control: given a vector of class proportions, we\ngenerate layouts with matching composition. To this end, we introduce a\nconditional framework with novel architecture designs and learning objectives,\nwhich effectively accommodates class proportions to guide the scene generation\nprocess. The proposed architecture also allows partial layout editing with\ninteresting applications. Thanks to the semantic control, we can produce\nlayouts close to the real distribution, helping enhance the whole scene\ngeneration process. On different metrics and urban scene benchmarks, our models\noutperform existing baselines. Moreover, we demonstrate the merit of our\napproach for data augmentation: semantic segmenters trained on real\nlayout-image pairs along with additional ones generated by our approach\noutperform models only trained on real pairs.",
        "Combining Natural Language with Vision represents a unique and interesting\nchallenge in the domain of Artificial Intelligence. The AI City Challenge Track\n5 for Natural Language-Based Vehicle Retrieval focuses on the problem of\ncombining visual and textual information, applied to a smart-city use case. In\nthis paper, we present All You Can Embed (AYCE), a modular solution to\ncorrelate single-vehicle tracking sequences with natural language. The main\nbuilding blocks of the proposed architecture are (i) BERT to provide an\nembedding of the textual descriptions, (ii) a convolutional backbone along with\na Transformer model to embed the visual information. For the training of the\nretrieval model, a variation of the Triplet Margin Loss is proposed to learn a\ndistance measure between the visual and language embeddings. The code is\npublicly available at https://github.com/cscribano/AYCE_2021.",
        "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
        "Advances in Artificial Intelligence and Image Processing are changing the way\npeople interacts with digital images and video. Widespread mobile apps like\nFACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\nproduce extreme transformations on human face photos such gender swap, aging,\netc. The results are utterly realistic and extremely easy to be exploited even\nfor non-experienced users. This kind of media object took the name of Deepfake\nand raised a new challenge in the multimedia forensics field: the Deepfake\ndetection challenge. Indeed, discriminating a Deepfake from a real image could\nbe a difficult task even for human eyes but recent works are trying to apply\nthe same technology used for generating images for discriminating them with\npreliminary good results but with many limitations: employed Convolutional\nNeural Networks are not so robust, demonstrate to be specific to the context\nand tend to extract semantics from images. In this paper, a new approach aimed\nto extract a Deepfake fingerprint from images is proposed. The method is based\non the Expectation-Maximization algorithm trained to detect and extract a\nfingerprint that represents the Convolutional Traces (CT) left by GANs during\nimage generation. The CT demonstrates to have high discriminative power\nachieving better results than state-of-the-art in the Deepfake detection task\nalso proving to be robust to different attacks. Achieving an overall\nclassification accuracy of over 98%, considering Deepfakes from 10 different\nGAN architectures not only involved in images of faces, the CT demonstrates to\nbe reliable and without any dependence on image semantic. Finally, tests\ncarried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\nfake detection task, demonstrated the effectiveness of the proposed technique\non a real-case scenario.",
        "Bird's-eye-view (BEV) is a powerful and widely adopted representation for\nroad scenes that captures surrounding objects and their spatial locations,\nalong with overall context in the scene. In this work, we focus on bird's eye\nsemantic segmentation, a task that predicts pixel-wise semantic segmentation in\nBEV from side RGB images. This task is made possible by simulators such as\nCarla, which allow for cheap data collection, arbitrary camera placements, and\nsupervision in ways otherwise not possible in the real world. There are two\nmain challenges to this task: the view transformation from side view to bird's\neye view, as well as transfer learning to unseen domains. Existing work\ntransforms between views through fully connected layers and transfer learns via\nGANs. This suffers from a lack of depth reasoning and performance degradation\nacross domains. Our novel 2-staged perception pipeline explicitly predicts\npixel depths and combines them with pixel semantics in an efficient manner,\nallowing the model to leverage depth information to infer objects' spatial\nlocations in the BEV. In addition, we transfer learning by abstracting\nhigh-level geometric features and predicting an intermediate representation\nthat is common across different domains. We publish a new dataset called\nBEVSEG-Carla and show that our approach improves state-of-the-art by 24% mIoU\nand performs well when transferred to a new domain.",
        "Normal map is an important and efficient way to represent complex 3D models.\nA designer may benefit from the auto-generation of high quality and accurate\nnormal maps from freehand sketches in 3D content creation. This paper proposes\na deep generative model for generating normal maps from users sketch with\ngeometric sampling. Our generative model is based on Conditional Generative\nAdversarial Network with the curvature-sensitive points sampling of conditional\nmasks. This sampling process can help eliminate the ambiguity of generation\nresults as network input. In addition, we adopted a U-Net structure\ndiscriminator to help the generator be better trained. It is verified that the\nproposed framework can generate more accurate normal maps.",
        "How to build a good model for image generation given an abstract concept is a\nfundamental problem in computer vision. In this paper, we explore a generative\nmodel for the task of generating unseen images with desired features. We\npropose the Generative Cooperative Net (GCN) for image generation. The idea is\nsimilar to generative adversarial networks except that the generators and\ndiscriminators are trained to work accordingly. Our experiments on hand-written\ndigit generation and facial expression generation show that GCN's two\ncooperative counterparts (the generator and the classifier) can work together\nnicely and achieve promising results. We also discovered a usage of such\ngenerative model as an data-augmentation tool. Our experiment of applying this\nmethod on a recognition task shows that it is very effective comparing to other\nexisting methods. It is easy to set up and could help generate a very large\nsynthesized dataset.",
        "This thesis introduces a new unsupervised learning framework, called\nAlignment-Based Learning, which is based on the alignment of sentences and\nHarris's (1951) notion of substitutability. Instances of the framework can be\napplied to an untagged, unstructured corpus of natural language sentences,\nresulting in a labelled, bracketed version of that corpus.\n  Firstly, the framework aligns all sentences in the corpus in pairs, resulting\nin a partition of the sentences consisting of parts of the sentences that are\nequal in both sentences and parts that are unequal. Unequal parts of sentences\ncan be seen as being substitutable for each other, since substituting one\nunequal part for the other results in another valid sentence. The unequal parts\nof the sentences are thus considered to be possible (possibly overlapping)\nconstituents, called hypotheses.\n  Secondly, the selection learning phase considers all hypotheses found by the\nalignment learning phase and selects the best of these. The hypotheses are\nselected based on the order in which they were found, or based on a\nprobabilistic function.\n  The framework can be extended with a grammar extraction phase. This extended\nframework is called parseABL. Instead of returning a structured version of the\nunstructured input corpus, like the ABL system, this system also returns a\nstochastic context-free or tree substitution grammar.\n  Different instances of the framework have been tested on the English ATIS\ncorpus, the Dutch OVIS corpus and the Wall Street Journal corpus. One of the\ninteresting results, apart from the encouraging numerical results, is that all\ninstances can (and do) learn recursive structures.",
        "We propose a stereo vision-based approach for tracking the camera ego-motion\nand 3D semantic objects in dynamic autonomous driving scenarios. Instead of\ndirectly regressing the 3D bounding box using end-to-end approaches, we propose\nto use the easy-to-labeled 2D detection and discrete viewpoint classification\ntogether with a light-weight semantic inference method to obtain rough 3D\nobject measurements. Based on the object-aware-aided camera pose tracking which\nis robust in dynamic environments, in combination with our novel dynamic object\nbundle adjustment (BA) approach to fuse temporal sparse feature correspondences\nand the semantic 3D measurement model, we obtain 3D object pose, velocity and\nanchored dynamic point cloud estimation with instance accuracy and temporal\nconsistency. The performance of our proposed method is demonstrated in diverse\nscenarios. Both the ego-motion estimation and object localization are compared\nwith the state-of-of-the-art solutions.",
        "Nowadays, deep neural networks (DNNs) have become the main instrument for\nmachine learning tasks within a wide range of domains, including vision, NLP,\nand speech. Meanwhile, in an important case of heterogenous tabular data, the\nadvantage of DNNs over shallow counterparts remains questionable. In\nparticular, there is no sufficient evidence that deep learning machinery allows\nconstructing methods that outperform gradient boosting decision trees (GBDT),\nwhich are often the top choice for tabular problems. In this paper, we\nintroduce Neural Oblivious Decision Ensembles (NODE), a new deep learning\narchitecture, designed to work with any tabular data. In a nutshell, the\nproposed NODE architecture generalizes ensembles of oblivious decision trees,\nbut benefits from both end-to-end gradient-based optimization and the power of\nmulti-layer hierarchical representation learning. With an extensive\nexperimental comparison to the leading GBDT packages on a large number of\ntabular datasets, we demonstrate the advantage of the proposed NODE\narchitecture, which outperforms the competitors on most of the tasks. We\nopen-source the PyTorch implementation of NODE and believe that it will become\na universal framework for machine learning on tabular data.",
        "This paper proposes a simple, accurate, and robust approach to single image\nnonparametric blind Super-Resolution (SR). This task is formulated as a\nfunctional to be minimized with respect to both an intermediate super-resolved\nimage and a nonparametric blur-kernel. The proposed approach includes a\nconvolution consistency constraint which uses a non-blind learning-based SR\nresult to better guide the estimation process. Another key component is the\nunnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharp\nimage and the blur-kernel, which is shown to be quite beneficial for estimating\nthe blur-kernel accurately. The numerical optimization is implemented by\ncoupling the splitting augmented Lagrangian and the conjugate gradient (CG).\nUsing the pre-estimated blur-kernel, we finally reconstruct the SR image by a\nvery simple non-blind SR method that uses a natural image prior. The proposed\napproach is demonstrated to achieve better performance than the recent method\nby Michaeli and Irani [2] in both terms of the kernel estimation accuracy and\nimage SR quality.",
        "Point cloud analysis is still a challenging task due to the disorder and\nsparsity of samplings of their geometric structures from 3D sensors. In this\npaper, we introduce the homotopy equivalence relation (HER) to make the neural\nnetworks learn the data distribution from a high-dimension manifold. A shuffle\noperation is adopted to construct HER for its randomness and zero-parameter. In\naddition, inspired by prior works, we propose a local mutual information\nregularizer (LMIR) to cut off the trivial path that leads to a classification\nerror from HER. LMIR utilizes mutual information to measure the distance\nbetween the original feature and HER transformed feature and learns common\nfeatures in a contrastive learning scheme. Thus, we combine HER and LMIR to\ngive our model the ability to learn non-Euclidean features from a\nhigh-dimension manifold. This is named the non-Euclidean feature learner.\nFurthermore, we propose a new heuristics and efficiency point sampling\nalgorithm named ClusterFPS to obtain approximate uniform sampling but at faster\nspeed. ClusterFPS uses a cluster algorithm to divide a point cloud into several\nclusters and deploy the farthest point sampling algorithm on each cluster in\nparallel. By combining the above methods, we propose a novel point cloud\nanalysis neural network called PointShuffleNet (PSN), which shows great promise\nin point cloud classification and segmentation. Extensive experiments show that\nour PSN achieves state-of-the-art results on ModelNet40, ShapeNet and S3DIS\nwith high efficiency. Theoretically, we provide mathematical analysis toward\nunderstanding of what the data distribution HER has developed and why LMIR can\ndrop the trivial path by maximizing mutual information implicitly.",
        "A probability density function (pdf) encodes the entire stochastic knowledge\nabout data distribution, where data may represent stochastic observations in\nrobotics, transition state pairs in reinforcement learning or any other\nempirically acquired modality. Inferring data pdf is of prime importance,\nallowing to analyze various model hypotheses and perform smart decision making.\nHowever, most density estimation techniques are limited in their representation\nexpressiveness to specific kernel type or predetermined distribution family,\nand have other restrictions. For example, kernel density estimation (KDE)\nmethods require meticulous parameter search and are extremely slow at querying\nnew points. In this paper we present a novel non-parametric density estimation\napproach, DeepPDF, that uses a neural network to approximate a target pdf given\nsamples from thereof. Such a representation provides high inference accuracy\nfor a wide range of target pdfs using a relatively simple network structure,\nmaking our method highly statistically robust. This is done via a new\nstochastic optimization algorithm, \\emph{Probabilistic Surface Optimization}\n(PSO), that turns to advantage the stochastic nature of sample points in order\nto force network output to be identical to the output of a target pdf. Once\ntrained, query point evaluation can be efficiently done in DeepPDF by a simple\nnetwork forward pass, with linear complexity in the number of query points.\nMoreover, the PSO algorithm is capable of inferring the frequency of data\nsamples and may also be used in other statistical tasks such as conditional\nestimation and distribution transformation. We compare the derived approach\nwith KDE methods showing its superior performance and accuracy.",
        "Although the notion of task similarity is potentially interesting in a wide\nrange of areas such as curriculum learning or automated planning, it has mostly\nbeen tied to transfer learning. Transfer is based on the idea of reusing the\nknowledge acquired in the learning of a set of source tasks to a new learning\nprocess in a target task, assuming that the target and source tasks are close\nenough. In recent years, transfer learning has succeeded in making\nReinforcement Learning (RL) algorithms more efficient (e.g., by reducing the\nnumber of samples needed to achieve the (near-)optimal performance). Transfer\nin RL is based on the core concept of similarity: whenever the tasks are\nsimilar, the transferred knowledge can be reused to solve the target task and\nsignificantly improve the learning performance. Therefore, the selection of\ngood metrics to measure these similarities is a critical aspect when building\ntransfer RL algorithms, especially when this knowledge is transferred from\nsimulation to the real world. In the literature, there are many metrics to\nmeasure the similarity between MDPs, hence, many definitions of similarity or\nits complement distance have been considered. In this paper, we propose a\ncategorization of these metrics and analyze the definitions of similarity\nproposed so far, taking into account such categorization. We also follow this\ntaxonomy to survey the existing literature, as well as suggesting future\ndirections for the construction of new metrics.",
        "The current research focus on Content-Based Video Retrieval requires\nhigher-level video representation describing the long-range semantic\ndependencies of relevant incidents, events, etc. However, existing methods\ncommonly process the frames of a video as individual images or short clips,\nmaking the modeling of long-range semantic dependencies difficult. In this\npaper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a\nvideo representation learning framework that incorporates long-range temporal\ninformation between frame-level features using the self-attention mechanism. To\ntrain it on video retrieval datasets, we propose a supervised contrastive\nlearning method that performs automatic hard negative mining and utilizes the\nmemory bank mechanism to increase the capacity of negative samples. Extensive\nexperiments are conducted on multiple video retrieval tasks, such as\nCC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant\nperformance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods\nwith video-level features, and deliver competitive results with 22x faster\ninference time comparing with frame-level features.",
        "Predicting a scene graph that captures visual entities and their interactions\nin an image has been considered a crucial step towards full scene\ncomprehension. Recent scene graph generation (SGG) models have shown their\ncapability of capturing the most frequent relations among visual entities.\nHowever, the state-of-the-art results are still far from satisfactory, e.g.\nmodels can obtain 31% in overall recall R@100, whereas the likewise important\nmean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The\ndiscrepancy between R and mR results urges to shift the focus from pursuing a\nhigh R to a high mR with a still competitive R. We suspect that the observed\ndiscrepancy stems from both the annotation bias and sparse annotations in VG,\nin which many visual entity pairs are either not annotated at all or only with\na single relation when multiple ones could be valid. To address this particular\nissue, we propose a novel SGG training scheme that capitalizes on self-learned\nknowledge. It involves two relation classifiers, one offering a less biased\nsetting for the other to base on. The proposed scheme can be applied to most of\nthe existing SGG models and is straightforward to implement. We observe\nsignificant relative improvements in mR (between +6.6% and +20.4%) and\ncompetitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.",
        "Current methods for single-image depth estimation use training datasets with\nreal image-depth pairs or stereo pairs, which are not easy to acquire. We\npropose a framework, trained on synthetic image-depth pairs and unpaired real\nimages, that comprises an image translation network for enhancing realism of\ninput images, followed by a depth prediction network. A key idea is having the\nfirst network act as a wide-spectrum input translator, taking in either\nsynthetic or real images, and ideally producing minimally modified realistic\nimages. This is done via a reconstruction loss when the training input is real,\nand GAN loss when synthetic, removing the need for heuristic\nself-regularization. The second network is trained on a task loss for synthetic\nimage-depth pairs, with extra GAN loss to unify real and synthetic feature\ndistributions. Importantly, the framework can be trained end-to-end, leading to\ngood results, even surpassing early deep-learning methods that use real paired\ndata.",
        "Lane marker extraction is a basic yet necessary task for autonomous driving.\nAlthough past years have witnessed major advances in lane marker extraction\nwith deep learning models, they all aim at ordinary RGB images generated by\nframe-based cameras, which limits their performance in extreme cases, like huge\nillumination change. To tackle this problem, we introduce Dynamic Vision Sensor\n(DVS), a type of event-based sensor to lane marker extraction task and build a\nhigh-resolution DVS dataset for lane marker extraction. We collect the raw\nevent data and generate 5,424 DVS images with a resolution of 1280$\\times$800\npixels, the highest one among all DVS datasets available now. All images are\nannotated with multi-class semantic segmentation format. We then propose a\nstructure-aware network for lane marker extraction in DVS images. It can\ncapture directional information comprehensively with multidirectional slice\nconvolution. We evaluate our proposed network with other state-of-the-art lane\nmarker extraction models on this dataset. Experimental results demonstrate that\nour method outperforms other competitors. The dataset is made publicly\navailable, including the raw event data, accumulated images and labels.",
        "There have been a few recent methods proposed in text to video moment\nretrieval using natural language queries, but requiring full supervision during\ntraining. However, acquiring a large number of training videos with temporal\nboundary annotations for each text description is extremely time-consuming and\noften not scalable. In order to cope with this issue, in this work, we\nintroduce the problem of learning from weak labels for the task of text to\nvideo moment retrieval. The weak nature of the supervision is because, during\ntraining, we only have access to the video-text pairs rather than the temporal\nextent of the video to which different text descriptions relate. We propose a\njoint visual-semantic embedding based framework that learns the notion of\nrelevant segments from video using only video-level sentence descriptions.\nSpecifically, our main idea is to utilize latent alignment between video frames\nand sentence descriptions using Text-Guided Attention (TGA). TGA is then used\nduring the test phase to retrieve relevant moments. Experiments on two\nbenchmark datasets demonstrate that our method achieves comparable performance\nto state-of-the-art fully supervised approaches.",
        "Over the last several years, research on facial recognition based on Deep\nNeural Network has evolved with approaches like task-specific loss functions,\nimage normalization and augmentation, network architectures, etc. However,\nthere have been few approaches with attention to how human faces differ from\nperson to person. Premising that inter-personal differences are found both\ngenerally and locally on the human face, I propose FusiformNet, a novel\nframework for feature extraction that leverages the nature of discriminative\nfacial features. Tested on Image-Unrestricted setting of Labeled Faces in the\nWild benchmark, this method achieved a state-of-the-art accuracy of 96.67%\nwithout labeled outside data, image augmentation, normalization, or special\nloss functions. Likewise, the method also performed on a par with previous\nstate-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its\nability to extract both general and local facial features, the utility of\nFusiformNet may not be limited to facial recognition but also extend to other\nDNN-based tasks.",
        "Mutual information maximization has emerged as a powerful learning objective\nfor unsupervised representation learning obtaining state-of-the-art performance\nin applications such as object recognition, speech recognition, and\nreinforcement learning. However, such approaches are fundamentally limited\nsince a tight lower bound of mutual information requires sample size\nexponential in the mutual information. This limits the applicability of these\napproaches for prediction tasks with high mutual information, such as in video\nunderstanding or reinforcement learning. In these settings, such techniques are\nprone to overfit, both in theory and in practice, and capture only a few of the\nrelevant factors of variation. This leads to incomplete representations that\nare not optimal for downstream tasks. In this work, we empirically demonstrate\nthat mutual information-based representation learning approaches do fail to\nlearn complete representations on a number of designed and real-world tasks. To\nmitigate these problems we introduce the Wasserstein dependency measure, which\nlearns more complete representations by using the Wasserstein distance instead\nof the KL divergence in the mutual information estimator. We show that a\npractical approximation to this theoretically motivated solution, constructed\nusing Lipschitz constraint techniques from the GAN literature, achieves\nsubstantially improved results on tasks where incomplete representations are a\nmajor challenge.",
        "With the advance of deep learning technology, automatic video generation from\naudio or text has become an emerging and promising research topic. In this\npaper, we present a novel approach to synthesize video from the text. The\nmethod builds a phoneme-pose dictionary and trains a generative adversarial\nnetwork (GAN) to generate video from interpolated phoneme poses. Compared to\naudio-driven video generation algorithms, our approach has a number of\nadvantages: 1) It only needs a fraction of the training data used by an\naudio-driven approach; 2) It is more flexible and not subject to vulnerability\ndue to speaker variation; 3) It significantly reduces the preprocessing,\ntraining and inference time. We perform extensive experiments to compare the\nproposed method with state-of-the-art talking face generation methods on a\nbenchmark dataset and datasets of our own. The results demonstrate the\neffectiveness and superiority of our approach.",
        "Three-dimensional Ultrasound image segmentation methods are surveyed in this\npaper. The focus of this report is to investigate applications of these\ntechniques and a review of the original ideas and concepts. Although many\ntwo-dimensional image segmentation in the literature have been considered as a\nthree-dimensional approach by mistake but we review them as a three-dimensional\ntechnique. We select the studies that have addressed the problem of medical\nthree-dimensional Ultrasound image segmentation utilizing their proposed\ntechniques. The evaluation methods and comparison between them are presented\nand tabulated in terms of evaluation techniques, interactivity, and robustness.",
        "Machine learning, especially deep neural networks, has been rapidly developed\nin fields including computer vision, speech recognition and reinforcement\nlearning. Although Mini-batch SGD is one of the most popular stochastic\noptimization methods in training deep networks, it shows a slow convergence\nrate due to the large noise in gradient approximation. In this paper, we\nattempt to remedy this problem by building more efficient batch selection\nmethod based on typicality sampling, which reduces the error of gradient\nestimation in conventional Minibatch SGD. We analyze the convergence rate of\nthe resulting typical batch SGD algorithm and compare convergence properties\nbetween Minibatch SGD and the algorithm. Experimental results demonstrate that\nour batch selection scheme works well and more complex Minibatch SGD variants\ncan benefit from the proposed batch selection strategy.",
        "What is the best way to learn a universal face representation? Recent work on\nDeep Learning in the area of face analysis has focused on supervised learning\nfor specific tasks of interest (e.g. face recognition, facial landmark\nlocalization etc.) but has overlooked the overarching question of how to find a\nfacial representation that can be readily adapted to several facial analysis\ntasks and datasets. To this end, we make the following 4 contributions: (a) we\nintroduce, for the first time, a comprehensive evaluation benchmark for facial\nrepresentation learning consisting of 5 important face analysis tasks. (b) We\nsystematically investigate two ways of large-scale representation learning\napplied to faces: supervised and unsupervised pre-training. Importantly, we\nfocus our evaluations on the case of few-shot facial learning. (c) We\ninvestigate important properties of the training datasets including their size\nand quality (labelled, unlabelled or even uncurated). (d) To draw our\nconclusions, we conducted a very large number of experiments. Our main two\nfindings are: (1) Unsupervised pre-training on completely in-the-wild,\nuncurated data provides consistent and, in some cases, significant accuracy\nimprovements for all facial tasks considered. (2) Many existing facial video\ndatasets seem to have a large amount of redundancy. We will release code,\npre-trained models and data to facilitate future research.",
        "Object detection has achieved remarkable progress in the past decade.\nHowever, the detection of oriented and densely packed objects remains\nchallenging because of following inherent reasons: (1) receptive fields of\nneurons are all axis-aligned and of the same shape, whereas objects are usually\nof diverse shapes and align along various directions; (2) detection models are\ntypically trained with generic knowledge and may not generalize well to handle\nspecific objects at test time; (3) the limited dataset hinders the development\non this task. To resolve the first two issues, we present a dynamic refinement\nnetwork that consists of two novel components, i.e., a feature selection module\n(FSM) and a dynamic refinement head (DRH). Our FSM enables neurons to adjust\nreceptive fields in accordance with the shapes and orientations of target\nobjects, whereas the DRH empowers our model to refine the prediction\ndynamically in an object-aware manner. To address the limited availability of\nrelated benchmarks, we collect an extensive and fully annotated dataset,\nnamely, SKU110K-R, which is relabeled with oriented bounding boxes based on\nSKU110K. We perform quantitative evaluations on several publicly available\nbenchmarks including DOTA, HRSC2016, SKU110K, and our own SKU110K-R dataset.\nExperimental results show that our method achieves consistent and substantial\ngains compared with baseline approaches. The code and dataset are available at\nhttps://github.com/Anymake/DRN_CVPR2020.",
        "The scale invariant feature transform (SIFT) algorithm is considered a\nclassical feature extraction algorithm within the field of computer vision.\nSIFT keypoint descriptor matching is a computationally intensive process due to\nthe amount of data consumed. In this work, we designed a novel fully pipelined\nhardware accelerator architecture for SIFT keypoint descriptor matching. The\naccelerator core was implemented and tested on a field programmable gate array\n(FPGA). The proposed hardware architecture is able to properly handle the\nmemory bandwidth necessary for a fully-pipelined implementation and hits the\nroofline performance model, achieving the potential maximum throughput. The\nfully pipelined matching architecture was designed based on the consine angle\ndistance method. Our architecture was optimized for 16-bit fixed-point\noperations and implemented on hardware using a Xilinx Zynq-based FPGA\ndevelopment board. Our proposed architecture shows a noticeable reduction of\narea resources compared with its counterparts in literature, while maintaining\nhigh throughput by alleviating memory bandwidth restrictions. The results show\na reduction in consumed device resources of up to 91 percent in LUTs and 79\npercent of BRAMs. Our hardware implementation is 15.7 times faster than the\ncomparable software approach.",
        "In batch reinforcement learning (RL), one often constrains a learned policy\nto be close to the behavior (data-generating) policy, e.g., by constraining the\nlearned action distribution to differ from the behavior policy by some maximum\ndegree that is the same at each state. This can cause batch RL to be overly\nconservative, unable to exploit large policy changes at frequently-visited,\nhigh-confidence states without risking poor performance at sparsely-visited\nstates. To remedy this, we propose residual policies, where the allowable\ndeviation of the learned policy is state-action-dependent. We derive a new for\nRL method, BRPO, which learns both the policy and allowable deviation that\njointly maximize a lower bound on policy performance. We show that BRPO\nachieves the state-of-the-art performance in a number of tasks.",
        "Although various distributed machine learning schemes have been proposed\nrecently for pure linear models and fully nonparametric models, little\nattention has been paid on distributed optimization for semi-paramemetric\nmodels with multiple-level structures (e.g. sparsity, linearity and\nnonlinearity). To address these issues, the current paper proposes a new\ncommunication-efficient distributed learning algorithm for partially sparse\nlinear models with an increasing number of features. The proposed method is\nbased on the classical divide and conquer strategy for handing big data and\neach sub-method defined on each subsample consists of a debiased estimation of\nthe double-regularized least squares approach. With the proposed method, we\ntheoretically prove that our global parametric estimator can achieve optimal\nparametric rate in our semi-parametric model given an appropriate partition on\nthe total data. Specially, the choice of data partition relies on the\nunderlying smoothness of the nonparametric component, but it is adaptive to the\nsparsity parameter. Even under the non-distributed setting, we develop a new\nand easily-read proof for optimal estimation of the parametric error in high\ndimensional partial linear model. Finally, several simulated experiments are\nimplemented to indicate comparable empirical performance of our debiased\ntechnique under the distributed setting.",
        "Many tasks in graph machine learning, such as link prediction and node\nclassification, are typically solved by using representation learning, in which\neach node or edge in the network is encoded via an embedding. Though there\nexists a lot of network embeddings for static graphs, the task becomes much\nmore complicated when the dynamic (i.e. temporal) network is analyzed. In this\npaper, we propose a novel approach for dynamic network representation learning\nbased on Temporal Graph Network by using a highly custom message generating\nfunction by extracting Causal Anonymous Walks. For evaluation, we provide a\nbenchmark pipeline for the evaluation of temporal network embeddings. This work\nprovides the first comprehensive comparison framework for temporal network\nrepresentation learning in every available setting for graph machine learning\nproblems involving node classification and link prediction. The proposed model\noutperforms state-of-the-art baseline models. The work also justifies the\ndifference between them based on evaluation in various transductive/inductive\nedge/node classification tasks. In addition, we show the applicability and\nsuperior performance of our model in the real-world downstream graph machine\nlearning task provided by one of the top European banks, involving credit\nscoring based on transaction data.",
        "Learning depth and ego-motion from unlabeled videos via self-supervision from\nepipolar projection can improve the robustness and accuracy of the 3D\nperception and localization of vision-based robots. However, the rigid\nprojection computed by ego-motion cannot represent all scene points, such as\npoints on moving objects, leading to false guidance in these regions. To\naddress this problem, we propose an Attentional Separation-and-Aggregation\nNetwork (ASANet), which can learn to distinguish and extract the scene's static\nand dynamic characteristics via the attention mechanism. We further propose a\nnovel MotionNet with an ASANet as the encoder, followed by two separate\ndecoders, to estimate the camera's ego-motion and the scene's dynamic motion\nfield. Then, we introduce an auto-selecting approach to detect the moving\nobjects for dynamic-aware learning automatically. Empirical experiments\ndemonstrate that our method can achieve the state-of-the-art performance on the\nKITTI benchmark.",
        "Deep learning has significantly improved the precision of instance\nsegmentation with abundant labeled data. However, in many areas like medical\nand manufacturing, collecting sufficient data is extremely hard and labeling\nthis data requires high professional skills. We follow this motivation and\npropose a new task set named zero-shot instance segmentation (ZSI). In the\ntraining phase of ZSI, the model is trained with seen data, while in the\ntesting phase, it is used to segment all seen and unseen instances. We first\nformulate the ZSI task and propose a method to tackle the challenge, which\nconsists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and\nSynchronized Background Strategy. We present a new benchmark for zero-shot\ninstance segmentation based on the MS-COCO dataset. The extensive empirical\nresults in this benchmark show that our method not only surpasses the\nstate-of-the-art results in zero-shot object detection task but also achieves\npromising performance on ZSI. Our approach will serve as a solid baseline and\nfacilitate future research in zero-shot instance segmentation.",
        "In this paper, we propose an improved quantitative evaluation framework for\nGenerative Adversarial Networks (GANs) on generating domain-specific images,\nwhere we improve conventional evaluation methods on two levels: the feature\nrepresentation and the evaluation metric. Unlike most existing evaluation\nframeworks which transfer the representation of ImageNet inception model to map\nimages onto the feature space, our framework uses a specialized encoder to\nacquire fine-grained domain-specific representation. Moreover, for datasets\nwith multiple classes, we propose Class-Aware Frechet Distance (CAFD), which\nemploys a Gaussian mixture model on the feature space to better fit the\nmulti-manifold feature distribution. Experiments and analysis on both the\nfeature level and the image level were conducted to demonstrate improvements of\nour proposed framework over the recently proposed state-of-the-art FID method.\nTo our best knowledge, we are the first to provide counter examples where FID\ngives inconsistent results with human judgments. It is shown in the experiments\nthat our framework is able to overcome the shortness of FID and improves\nrobustness. Code will be made available.",
        "Graph are a ubiquitous data representation, as they represent a flexible and\ncompact representation. For instance, the 3D structure of RNA can be\nefficiently represented as $\\textit{2.5D graphs}$, graphs whose nodes are\nnucleotides and edges represent chemical interactions. In this setting, we have\nbiological evidence of the similarity between the edge types, as some chemical\ninteractions are more similar than others.\n  Machine learning on graphs have recently experienced a breakthrough with the\nintroduction of Graph Neural Networks. This algorithm can be framed as a\nmessage passing algorithm between graph nodes over graph edges. These messages\ncan depend on the edge type they are transmitted through, but no method\ncurrently constrains how a message is altered when the edge type changes.\n  Motivated by the RNA use case, in this project we introduce a graph neural\nnetwork layer which can leverage prior information about similarities between\nedges. We show that despite the theoretical appeal of including this similarity\nprior, the empirical performance is not enhanced on the tasks and datasets we\ninclude here.",
        "Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),\nrequires an alignment of semantic concepts across domains. Despite the\nwidespread success of end-to-end learning, today's multimodal pipelines by and\nlarge leverage pre-extracted, fixed features from object detectors, typically\nFaster R-CNN, as representations of the visual world. The obvious downside is\nthat the visual representation is not specifically tuned to the multimodal task\nat hand. At the same time, while transformer-based object detectors have gained\npopularity, they have not been employed in today's multimodal pipelines. We\naddress both shortcomings with TxT, a transformer-based crossmodal pipeline\nthat enables fine-tuning both language and visual components on the downstream\ntask in a fully end-to-end manner. We overcome existing limitations of\ntransformer-based detectors for multimodal reasoning regarding the integration\nof global context and their scalability. Our transformer-based multimodal model\nachieves considerable gains from end-to-end learning for multimodal question\nanswering.",
        "Graph neural networks (GNNs) have shown great prowess in learning\nrepresentations suitable for numerous graph-based machine learning tasks. When\napplied to semi-supervised node classification, GNNs are widely believed to\nwork well due to the homophily assumption (\"like attracts like\"), and fail to\ngeneralize to heterophilous graphs where dissimilar nodes connect. Recent works\ndesign new architectures to overcome such heterophily-related limitations,\nciting poor baseline performance and new architecture improvements on a few\nheterophilous graph benchmark datasets as evidence for this notion. In our\nexperiments, we empirically find that standard graph convolutional networks\n(GCNs) can actually achieve better performance than such carefully designed\nmethods on some commonly used heterophilous graphs. This motivates us to\nreconsider whether homophily is truly necessary for good GNN performance. We\nfind that this claim is not quite true, and in fact, GCNs can achieve strong\nperformance on heterophilous graphs under certain conditions. Our work\ncarefully characterizes these conditions, and provides supporting theoretical\nunderstanding and empirical observations. Finally, we examine existing\nheterophilous graphs benchmarks and reconcile how the GCN (under)performs on\nthem based on this understanding.",
        "Learning the representation of data with hierarchical structures in the\nhyperbolic space attracts increasing attention in recent years. Due to the\nconstant negative curvature, the hyperbolic space resembles tree metrics and\ncaptures the tree-like properties naturally, which enables the hyperbolic\nembeddings to improve over traditional Euclidean models. However, many\nreal-world hierarchically structured data such as taxonomies and multitree\nnetworks have varying local structures and they are not trees, thus they do not\nubiquitously match the constant curvature property of the hyperbolic space. To\naddress this limitation of hyperbolic embeddings, we explore the complex\nhyperbolic space, which has the variable negative curvature, for representation\nlearning. Specifically, we propose to learn the embeddings of hierarchically\nstructured data in the unit ball model of the complex hyperbolic space. The\nunit ball model based embeddings have a more powerful representation capacity\nto capture a variety of hierarchical structures. Through experiments on\nsynthetic and real-world data, we show that our approach improves over the\nhyperbolic embedding models significantly.",
        "Convolutional neural network has made remarkable achievements in\nclassification of idealized point cloud, however, non-idealized point cloud\nclassification is still a challenging task. In this paper, DNDFN, namely,\nDual-Neighborhood Deep Fusion Network, is proposed to deal with this problem.\nDNDFN has two key points. One is combination of local neighborhood and global\nneigh-borhood. nearest neighbor (kNN) or ball query can capture the local\nneighborhood but ignores long-distance dependencies. A trainable neighborhood\nlearning meth-od called TN-Learning is proposed, which can capture the global\nneighborhood. TN-Learning is combined with them to obtain richer neighborhood\ninformation. The other is information transfer convolution (IT-Conv) which can\nlearn the structural information between two points and transfer features\nthrough it. Extensive exper-iments on idealized and non-idealized benchmarks\nacross four tasks verify DNDFN achieves the state of the arts.",
        "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
        "We propose an active learning approach to image segmentation that exploits\ngeometric priors to speed up and streamline the annotation process. It can be\napplied for both background-foreground and multi-class segmentation tasks in 2D\nimages and 3D image volumes. Our approach combines geometric smoothness priors\nin the image space with more traditional uncertainty measures to estimate which\npixels or voxels are the most informative, and thus should to be annotated\nnext. For multi-class settings, we additionally introduce two novel criteria\nfor uncertainty. In the 3D case, we use the resulting uncertainty measure to\nselect voxels lying on a planar patch, which makes batch annotation much more\nconvenient for the end user compared to the setting where voxels are randomly\ndistributed in a volume. The planar patch is found using a branch-and-bound\nalgorithm that looks for a 2D patch in a 3D volume where the most informative\ninstances are located. We evaluate our approach on Electron Microscopy and\nMagnetic Resonance image volumes, as well as on regular images of horses and\nfaces. We demonstrate a substantial performance increase over other approaches\nthanks to the use of geometric priors.",
        "A variety of cooperative multi-agent control problems require agents to\nachieve individual goals while contributing to collective success. This\nmulti-goal multi-agent setting poses difficulties for recent algorithms, which\nprimarily target settings with a single global reward, due to two new\nchallenges: efficient exploration for learning both individual goal attainment\nand cooperation for others' success, and credit-assignment for interactions\nbetween actions and goals of different agents. To address both challenges, we\nrestructure the problem into a novel two-stage curriculum, in which\nsingle-agent goal attainment is learned prior to learning multi-agent\ncooperation, and we derive a new multi-goal multi-agent policy gradient with a\ncredit function for localized credit assignment. We use a function augmentation\nscheme to bridge value and policy functions across the curriculum. The complete\narchitecture, called CM3, learns significantly faster than direct adaptations\nof existing algorithms on three challenging multi-goal multi-agent problems:\ncooperative navigation in difficult formations, negotiating multi-vehicle lane\nchanges in the SUMO traffic simulator, and strategic cooperation in a Checkers\nenvironment.",
        "Artificial Neural Networks form the basis of very powerful learning methods.\nIt has been observed that a naive application of fully connected neural\nnetworks to data with many irrelevant variables often leads to overfitting. In\nan attempt to circumvent this issue, a prior knowledge pertaining to what\nfeatures are relevant and their possible feature interactions can be encoded\ninto these networks. In this work, we use decision trees to capture such\nrelevant features and their interactions and define a mapping to encode\nextracted relationships into a neural network. This addresses the\ninitialization related concern of fully connected neural networks. At the same\ntime through feature selection it enables learning of compact representations\ncompared to state of the art tree-based approaches. Empirical evaluations and\nsimulation studies show the superiority of such an approach over fully\nconnected neural networks and tree-based approaches",
        "3D multi-object tracking (MOT) is an essential component for many\napplications such as autonomous driving and assistive robotics. Recent work on\n3D MOT focuses on developing accurate systems giving less attention to\npractical considerations such as computational cost and system complexity. In\ncontrast, this work proposes a simple real-time 3D MOT system. Our system first\nobtains 3D detections from a LiDAR point cloud. Then, a straightforward\ncombination of a 3D Kalman filter and the Hungarian algorithm is used for state\nestimation and data association. Additionally, 3D MOT datasets such as KITTI\nevaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools\nare missing for a fair comparison of 3D MOT methods. Therefore, we propose a\nnew 3D MOT evaluation tool along with three new metrics to comprehensively\nevaluate 3D MOT methods. We show that, although our system employs a\ncombination of classical MOT modules, we achieve state-of-the-art 3D MOT\nperformance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly,\nalthough our system does not use any 2D data as inputs, we achieve competitive\nperformance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate\nof $207.4$ FPS on the KITTI dataset, achieving the fastest speed among all\nmodern MOT systems. To encourage standardized 3D MOT evaluation, our system and\nevaluation code are made publicly available at\nhttps://github.com/xinshuoweng/AB3DMOT.",
        "Learning from small amounts of labeled data is a challenge in the area of\ndeep learning. This is currently addressed by Transfer Learning where one\nlearns the small data set as a transfer task from a larger source dataset.\nTransfer Learning can deliver higher accuracy if the hyperparameters and source\ndataset are chosen well. One of the important parameters is the learning rate\nfor the layers of the neural network. We show through experiments on the\nImageNet22k and Oxford Flowers datasets that improvements in accuracy in range\nof 127% can be obtained by proper choice of learning rates. We also show that\nthe images/label parameter for a dataset can potentially be used to determine\noptimal learning rates for the layers to get the best overall accuracy. We\nadditionally validate this method on a sample of real-world image\nclassification tasks from a public visual recognition API.",
        "We propose a multi-resolution convolutional autoencoder (MrCAE) architecture\nthat integrates and leverages three highly successful mathematical\narchitectures: (i) multigrid methods, (ii) convolutional autoencoders and (iii)\ntransfer learning. The method provides an adaptive, hierarchical architecture\nthat capitalizes on a progressive training approach for multiscale\nspatio-temporal data. This framework allows for inputs across multiple scales:\nstarting from a compact (small number of weights) network architecture and\nlow-resolution data, our network progressively deepens and widens itself in a\nprincipled manner to encode new information in the higher resolution data based\non its current performance of reconstruction. Basic transfer learning\ntechniques are applied to ensure information learned from previous training\nsteps can be rapidly transferred to the larger network. As a result, the\nnetwork can dynamically capture different scaled features at different depths\nof the network. The performance gains of this adaptive multiscale architecture\nare illustrated through a sequence of numerical experiments on synthetic\nexamples and real-world spatial-temporal data.",
        "Policy gradient methods have achieved remarkable successes in solving\nchallenging reinforcement learning problems. However, it still often suffers\nfrom the large variance issue on policy gradient estimation, which leads to\npoor sample efficiency during training. In this work, we propose a control\nvariate method to effectively reduce variance for policy gradient methods.\nMotivated by the Stein's identity, our method extends the previous control\nvariate methods used in REINFORCE and advantage actor-critic by introducing\nmore general action-dependent baseline functions. Empirical studies show that\nour method significantly improves the sample efficiency of the state-of-the-art\npolicy gradient approaches.",
        "Automated data-driven decision-making systems are ubiquitous across a wide\nspread of online as well as offline services. These systems, depend on\nsophisticated learning algorithms and available data, to optimize the service\nfunction for decision support assistance. However, there is a growing concern\nabout the accountability and fairness of the employed models by the fact that\noften the available historic data is intrinsically discriminatory, i.e., the\nproportion of members sharing one or more sensitive attributes is higher than\nthe proportion in the population as a whole when receiving positive\nclassification, which leads to a lack of fairness in decision support system. A\nnumber of fairness-aware learning methods have been proposed to handle this\nconcern. However, these methods tackle fairness as a static problem and do not\ntake the evolution of the underlying stream population into consideration. In\nthis paper, we introduce a learning mechanism to design a fair classifier for\nonline stream based decision-making. Our learning model, FAHT (Fairness-Aware\nHoeffding Tree), is an extension of the well-known Hoeffding Tree algorithm for\ndecision tree induction over streams, that also accounts for fairness. Our\nexperiments show that our algorithm is able to deal with discrimination in\nstreaming environments, while maintaining a moderate predictive performance\nover the stream.",
        "Autoregressive models use chain rule to define a joint probability\ndistribution as a product of conditionals. These conditionals need to be\nnormalized, imposing constraints on the functional families that can be used.\nTo increase flexibility, we propose autoregressive conditional score models\n(AR-CSM) where we parameterize the joint distribution in terms of the\nderivatives of univariate log-conditionals (scores), which need not be\nnormalized. To train AR-CSM, we introduce a new divergence between\ndistributions named Composite Score Matching (CSM). For AR-CSM models, this\ndivergence between data and model distributions can be computed and optimized\nefficiently, requiring no expensive sampling or adversarial training. Compared\nto previous score matching algorithms, our method is more scalable to high\ndimensional data and more stable to optimize. We show with extensive\nexperimental results that it can be applied to density estimation on synthetic\ndata, image generation, image denoising, and training latent variable models\nwith implicit encoders.",
        "Generative adversarial networks (GANs) are a learning framework that rely on\ntraining a discriminator to estimate a measure of difference between a target\nand generated distributions. GANs, as normally formulated, rely on the\ngenerated samples being completely differentiable w.r.t. the generative\nparameters, and thus do not work for discrete data. We introduce a method for\ntraining GANs with discrete data that uses the estimated difference measure\nfrom the discriminator to compute importance weights for generated samples,\nthus providing a policy gradient for training the generator. The importance\nweights have a strong connection to the decision boundary of the discriminator,\nand we call our method boundary-seeking GANs (BGANs). We demonstrate the\neffectiveness of the proposed algorithm with discrete image and character-based\nnatural language generation. In addition, the boundary-seeking objective\nextends to continuous data, which can be used to improve stability of training,\nand we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN)\nbedrooms, and Imagenet without conditioning.",
        "Previous video object segmentation approaches mainly focus on using simplex\nsolutions between appearance and motion, limiting feature collaboration\nefficiency among and across these two cues. In this work, we study a novel and\nefficient full-duplex strategy network (FSNet) to address this issue, by\nconsidering a better mutual restraint scheme between motion and appearance in\nexploiting the cross-modal features from the fusion and decoding stage.\nSpecifically, we introduce the relational cross-attention module (RCAM) to\nachieve bidirectional message propagation across embedding sub-spaces. To\nimprove the model's robustness and update the inconsistent features from the\nspatial-temporal embeddings, we adopt the bidirectional purification module\n(BPM) after the RCAM. Extensive experiments on five popular benchmarks show\nthat our FSNet is robust to various challenging scenarios (e.g., motion blur,\nocclusion) and achieves favourable performance against existing cutting-edges\nboth in the video object segmentation and video salient object detection tasks.\nThe project is publicly available at: https://dpfan.net/FSNet.",
        "Generative Adversarial Networks (GANs) are typically trained to synthesize\ndata, from images and more recently tabular data, under the assumption of\ndirectly accessible training data. Recently, federated learning (FL) is an\nemerging paradigm that features decentralized learning on client's local data\nwith a privacy-preserving capability. And, while learning GANs to synthesize\nimages on FL systems has just been demonstrated, it is unknown if GANs for\ntabular data can be learned from decentralized data sources. Moreover, it\nremains unclear which distributed architecture suits them best. Different from\nimage GANs, state-of-the-art tabular GANs require prior knowledge on the data\ndistribution of each (discrete and continuous) column to agree on a common\nencoding -- risking privacy guarantees. In this paper, we propose Fed-TGAN, the\nfirst Federated learning framework for Tabular GANs. To effectively learn a\ncomplex tabular GAN on non-identical participants, Fed-TGAN designs two novel\nfeatures: (i) a privacy-preserving multi-source feature encoding for model\ninitialization; and (ii) table similarity aware weighting strategies to\naggregate local models for countering data skew. We extensively evaluate the\nproposed Fed-TGAN against variants of decentralized learning architectures on\nfour widely used datasets. Results show that Fed-TGAN accelerates training time\nper epoch up to 200% compared to the alternative architectures, for both IID\nand Non-IID data. Overall, Fed-TGAN not only stabilizes the training loss, but\nalso achieves better similarity between generated and original data.",
        "Various methods for solving the inverse reinforcement learning (IRL) problem\nhave been developed independently in machine learning and economics. In\nparticular, the method of Maximum Causal Entropy IRL is based on the\nperspective of entropy maximization, while related advances in the field of\neconomics instead assume the existence of unobserved action shocks to explain\nexpert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability\nmethod, Nested Pseudo-Likelihood Algorithm). In this work, we make previously\nunknown connections between these related methods from both fields. We achieve\nthis by showing that they all belong to a class of optimization problems,\ncharacterized by a common form of the objective, the associated policy and the\nobjective gradient. We demonstrate key computational and algorithmic\ndifferences which arise between the methods due to an approximation of the\noptimal soft value function, and describe how this leads to more efficient\nalgorithms. Using insights which emerge from our study of this class of\noptimization problems, we identify various problem scenarios and investigate\neach method's suitability for these problems.",
        "Polarimetric synthetic aperture radar (PolSAR) image segmentation is\ncurrently of great importance in image processing for remote sensing\napplications. However, it is a challenging task due to two main reasons.\nFirstly, the label information is difficult to acquire due to high annotation\ncosts. Secondly, the speckle effect embedded in the PolSAR imaging process\nremarkably degrades the segmentation performance. To address these two issues,\nwe present a contextual PolSAR image semantic segmentation method in this\npaper.With a newly defined channelwise consistent feature set as input, the\nthree-dimensional discrete wavelet transform (3D-DWT) technique is employed to\nextract discriminative multi-scale features that are robust to speckle noise.\nThen Markov random field (MRF) is further applied to enforce label smoothness\nspatially during segmentation. By simultaneously utilizing 3D-DWT features and\nMRF priors for the first time, contextual information is fully integrated\nduring the segmentation to ensure accurate and smooth segmentation. To\ndemonstrate the effectiveness of the proposed method, we conduct extensive\nexperiments on three real benchmark PolSAR image data sets. Experimental\nresults indicate that the proposed method achieves promising segmentation\naccuracy and preferable spatial consistency using a minimal number of labeled\npixels.",
        "Situational awareness as a necessity in the connected and autonomous vehicles\n(CAV) domain is the subject of a significant number of researches in recent\nyears. The driver's safety is directly dependent on the robustness,\nreliability, and scalability of such systems. Cooperative mechanisms have\nprovided a solution to improve situational awareness by utilizing high speed\nwireless vehicular networks. These mechanisms mitigate problems such as\nocclusion and sensor range limitation. However, the network capacity is a\nfactor determining the maximum amount of information being shared among\ncooperative entities. The notion of feature sharing, proposed in our previous\nwork, aims to address these challenges by maintaining a balance between\ncomputation and communication load. In this work, we propose a mechanism to add\nflexibility in adapting to communication channel capacity and a novel\ndecentralized shared data alignment method to further improve cooperative\nobject detection performance. The performance of the proposed framework is\nverified through experiments on Volony dataset. The results confirm that our\nproposed framework outperforms our previous cooperative object detection method\n(FS-COD) in terms of average precision.",
        "Two key challenges within Reinforcement Learning involve improving (a) agent\nlearning within environments with sparse extrinsic rewards and (b) the\nexplainability of agent actions. We describe a curious subgoal focused agent to\naddress both these challenges. We use a novel method for curiosity produced\nfrom a Generative Adversarial Network (GAN) based model of environment\ntransitions that is robust to stochastic environment transitions. Additionally,\nwe use a subgoal generating network to guide navigation. The explainability of\nthe agent's behavior is increased by decomposing complex tasks into a sequence\nof interpretable subgoals that do not require any manual design. We show that\nthis method also enables the agent to solve challenging procedurally-generated\ntasks that contain stochastic transitions above other state-of-the-art methods.",
        "The process of selecting points for training a machine learning model is\noften a challenging task. Many times, we will have a lot of data, but for\ntraining, we require the labels and labeling is often costly. So we need to\nselect the points for training in an efficient manner so that the model trained\non the points selected will be better than the ones trained on any other\ntraining set. We propose a novel method to select the nodes in graph datasets\nusing the concept of graph centrality. Two methods are proposed - one using a\nsmart selection strategy, where the model is required to be trained only once\nand another using active learning method. We have tested this idea on three\npopular graph datasets - Cora, Citeseer and Pubmed- and the results are found\nto be encouraging.",
        "State-of-the-art image segmentation algorithms generally consist of at least\ntwo successive and distinct computations: a boundary detection process that\nuses local image information to classify image locations as boundaries between\nobjects, followed by a pixel grouping step such as watershed or connected\ncomponents that clusters pixels into segments. Prior work has varied the\ncomplexity and approach employed in these two steps, including the\nincorporation of multi-layer neural networks to perform boundary prediction,\nand the use of global optimizations during pixel clustering. We propose a\nunified and end-to-end trainable machine learning approach, flood-filling\nnetworks, in which a recurrent 3d convolutional network directly produces\nindividual segments from a raw image. The proposed approach robustly segments\nimages with an unknown and variable number of objects as well as highly\nvariable object sizes. We demonstrate the approach on a challenging 3d image\nsegmentation task, connectomic reconstruction from volume electron microscopy\ndata, on which flood-filling neural networks substantially improve accuracy\nover other state-of-the-art methods. The proposed approach can replace complex\nmulti-step segmentation pipelines with a single neural network that is learned\nend-to-end.",
        "In this paper we propose a method for estimating depth from a single image\nusing a coarse to fine approach. We argue that modeling the fine depth details\nis easier after a coarse depth map has been computed. We express a global\n(coarse) depth map of an image as a linear combination of a depth basis learned\nfrom training examples. The depth basis captures spatial and statistical\nregularities and reduces the problem of global depth estimation to the task of\npredicting the input-specific coefficients in the linear combination. This is\nformulated as a regression problem from a holistic representation of the image.\nCrucially, the depth basis and the regression function are {\\bf coupled} and\njointly optimized by our learning scheme. We demonstrate that this results in a\nsignificant improvement in accuracy compared to direct regression of depth\npixel values or approaches learning the depth basis disjointly from the\nregression function. The global depth estimate is then used as a guidance by a\nlocal refinement method that introduces depth details that were not captured at\nthe global level. Experiments on the NYUv2 and KITTI datasets show that our\nmethod outperforms the existing state-of-the-art at a considerably lower\ncomputational cost for both training and testing.",
        "Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM. Existing deep learning-based approaches formulate the problem\nby either recovering absolute pose scales from two consecutive frames or\npredicting a depth map from a single image, both of which are ill-posed\nproblems. In contrast, we propose to revisit the problem of deep two-view SfM\nby leveraging the well-posedness of the classic pipeline. Our method consists\nof 1) an optical flow estimation network that predicts dense correspondences\nbetween two frames; 2) a normalized pose estimation module that computes\nrelative camera poses from the 2D optical flow correspondences, and 3) a\nscale-invariant depth estimation network that leverages epipolar geometry to\nreduce the search space, refine the dense correspondences, and estimate\nrelative depth maps. Extensive experiments show that our method outperforms all\nstate-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI\nVO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth\nestimation.",
        "Salient object detection (SOD) is a fundamental computer vision task.\nRecently, with the revival of deep neural networks, SOD has made great\nprogresses. However, there still exist two thorny issues that cannot be well\naddressed by existing methods, indistinguishable regions and complex\nstructures. To address these two issues, in this paper we propose a novel deep\nnetwork for accurate SOD, named CLASS. First, in order to leverage the\ndifferent advantages of low-level and high-level features, we propose a novel\nnon-local cross-level attention (CLA), which can capture the long-range feature\ndependencies to enhance the distinction of complete salient object. Second, a\nnovel cross-level supervision (CLS) is designed to learn complementary context\nfor complex structures through pixel-level, region-level and object-level. Then\nthe fine structures and boundaries of salient objects can be well restored. In\nexperiments, with the proposed CLA and CLS, our CLASS net. consistently\noutperforms 13 state-of-the-art methods on five datasets.",
        "Text to speech (TTS) is a crucial task for user interaction, but TTS model\ntraining relies on a sizable set of high-quality original datasets. Due to\nprivacy and security issues, the original datasets are usually unavailable\ndirectly. Recently, federated learning proposes a popular distributed machine\nlearning paradigm with an enhanced privacy protection mechanism. It offers a\npractical and secure framework for data owners to collaborate with others, thus\nobtaining a better global model trained on the larger dataset. However, due to\nthe high complexity of transformer models, the convergence process becomes slow\nand unstable in the federated learning setting. Besides, the transformer model\ntrained in federated learning is costly communication and limited computational\nspeed on clients, impeding its popularity. To deal with these challenges, we\npropose the federated dynamic transformer. On the one hand, the performance is\ngreatly improved comparing with the federated transformer, approaching\ncentralize-trained Transformer-TTS when increasing clients number. On the other\nhand, it achieves faster and more stable convergence in the training phase and\nsignificantly reduces communication time. Experiments on the LJSpeech dataset\nalso strongly prove our method's advantage.",
        "This paper presents a study on power grid disturbance classification by Deep\nLearning (DL). A real synchrophasor set composing of three different types of\ndisturbance events from the Frequency Monitoring Network (FNET) is used. An\nimage embedding technique called Gramian Angular Field is applied to transform\neach time series of event data to a two-dimensional image for learning. Two\nmain DL algorithms, i.e. CNN (Convolutional Neural Network) and RNN (Recurrent\nNeural Network) are tested and compared with two widely used data mining tools,\nthe Support Vector Machine and Decision Tree. The test results demonstrate the\nsuperiority of the both DL algorithms over other methods in the application of\npower system transient disturbance classification.",
        "As one of the basic tasks of computer vision, object detection has been\nwidely used in many intelligent applications. However, object detection\nalgorithms are usually heavyweight in computation, hindering their\nimplementations on resource-constrained edge devices. Current edge-cloud\ncollaboration methods, such as CNN partition over Edge-cloud devices, are not\nsuitable for object detection since the huge data size of the intermediate\nresults will introduce extravagant communication costs. To address this\nchallenge, we propose a small-big model framework that deploys a big model in\nthe cloud and a small model on the edge devices. Upon receiving data, the edge\ndevice operates a difficult-case discriminator to classify the images into easy\ncases and difficult cases according to the specific semantics of the images.\nThe easy cases will be processed locally at the edge, and the difficult cases\nwill be uploaded to the cloud. Experimental results on the VOC, COCO, HELMET\ndatasets using two different object detection algorithms demonstrate that the\nsmall-big model system can detect 94.01%-97.84% of objects with only about 50%\nimages uploaded to the cloud when using SSD. In addition, the small-big model\naveragely reaches 91.22%- 92.52% end-to-end mAP of the scheme that uploading\nall images to the cloud.",
        "We present a generic framework for parallel coordinate descent (CD)\nalgorithms that includes, as special cases, the original sequential algorithms\nCyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.\nWe introduce two novel parallel algorithms that are also special\ncases---Thread-Greedy CD and Coloring-Based CD---and give performance\nmeasurements for an OpenMP implementation of these.",
        "Image segmentation refers to the separation of objects from the background,\nand has been one of the most challenging aspects of digital image processing.\nPractically it is impossible to design a segmentation algorithm which has 100%\naccuracy, and therefore numerous segmentation techniques have been proposed in\nthe literature, each with certain limitations. In this paper, a novel\nFalling-Ball algorithm is presented, which is a region-based segmentation\nalgorithm, and an alternative to watershed transform (based on waterfall\nmodel). The proposed algorithm detects the catchment basins by assuming that a\nball falling from hilly terrains will stop in a catchment basin. Once catchment\nbasins are identified, the association of each pixel with one of the catchment\nbasin is obtained using multi-criterion fuzzy logic. Edges are constructed by\ndividing image into different catchment basins with the help of a membership\nfunction. Finally closed contour algorithm is applied to find closed regions\nand objects within closed regions are segmented using intensity information.\nThe performance of the proposed algorithm is evaluated both objectively as well\nas subjectively. Simulation results show that the proposed algorithms gives\nsuperior performance over conventional Sobel edge detection methods and the\nwatershed segmentation algorithm. For comparative analysis, various comparison\nmethods are used for demonstrating the superiority of proposed methods over\nexisting segmentation methods.",
        "Methods for Visual Question Anwering (VQA) are notorious for leveraging\ndataset biases rather than performing reasoning, hindering generalization. It\nhas been recently shown that better reasoning patterns emerge in attention\nlayers of a state-of-the-art VQA model when they are trained on perfect\n(oracle) visual inputs. This provides evidence that deep neural networks can\nlearn to reason when training conditions are favorable enough. However,\ntransferring this learned knowledge to deployable models is a challenge, as\nmuch of it is lost during the transfer. We propose a method for knowledge\ntransfer based on a regularization term in our loss function, supervising the\nsequence of required reasoning operations. We provide a theoretical analysis\nbased on PAC-learning, showing that such program prediction can lead to\ndecreased sample complexity under mild hypotheses. We also demonstrate the\neffectiveness of this approach experimentally on the GQA dataset and show its\ncomplementarity to BERT-like self-supervised pre-training.",
        "Modern data analytics take advantage of ensemble learning and transfer\nlearning approaches to tackle some of the most relevant issues in data\nanalysis, such as lack of labeled data to use to train the analysis models,\nsparsity of the information, and unbalanced distributions of the records.\nNonetheless, when applied to multimodal datasets (i.e., datasets acquired by\nmeans of multiple sensing techniques or strategies), the state-of-theart\nmethods for ensemble learning and transfer learning might show some\nlimitations. In fact, in multimodal data analysis, not all observations would\nshow the same level of reliability or information quality, nor an homogeneous\ndistribution of errors and uncertainties. This condition might undermine the\nclassic assumptions ensemble learning and transfer learning methods rely on. In\nthis work, we propose an adaptive approach for dimensionality reduction to\novercome this issue. By means of a graph theory-based approach, the most\nrelevant features across variable size subsets of the considered datasets are\nidentified. This information is then used to set-up ensemble learning and\ntransfer learning architectures. We test our approach on multimodal datasets\nacquired in diverse research fields (remote sensing, brain-computer interfaces,\nphotovoltaic energy). Experimental results show the validity and the robustness\nof our approach, able to outperform state-of-the-art techniques.",
        "The internal workings of modern deep learning models stay often unclear to an\nexternal observer, although spatial attention mechanisms are involved. The idea\nof this work is to translate these spatial attentions into natural language to\nprovide a simpler access to the model's function. Thus, I took a neural image\ncaptioning model and measured the reactions to external modification in its\nspatial attention for three different interface methods: a fixation over the\nwhole generation process, a fixation for the first time-steps and an addition\nto the generator's attention. The experimental results for bounding box based\nspatial attention vectors have shown that the captioning model reacts to method\ndependent changes in up to 52.65% and includes in 9.00% of the cases object\ncategories, which were otherwise unmentioned. Afterwards, I established such a\nlink to a hierarchical co-attention network for visual question answering by\nextraction of its word, phrase and question level spatial attentions. Here,\ngenerated captions for the word level included details of the question-answer\npairs in up to 55.20% of the cases. This work indicates that spatial attention\nseen as an external interface for image caption generators is an useful method\nto access visual functions in natural language.",
        "Although machine learning has become a powerful tool to augment doctors in\nclinical analysis, the immense amount of labeled data that is necessary to\ntrain supervised learning approaches burdens each development task as time and\nresource intensive. The vast majority of dense clinical information is stored\nin written reports, detailing pertinent patient information. The challenge with\nutilizing natural language data for standard model development is due to the\ncomplex nature of the modality. In this research, a model pipeline was\ndeveloped to utilize an unsupervised approach to train an encoder-language\nmodel, a recurrent network, to generate document encodings; which then can be\nused as features passed into a decoder-classifier model that requires\nmagnitudes less labeled data than previous approaches to differentiate between\nfine-grained disease classes accurately. The language model was trained on\nunlabeled radiology reports from the Massachusetts General Hospital Radiology\nDepartment (n=218,159) and terminated with a loss of 1.62. The classification\nmodels were trained on three labeled datasets of head CT studies of reported\npatients, presenting large vessel occlusion (n=1403), acute ischemic strokes\n(n=331), and intracranial hemorrhage (n=4350), to identify a variety of\ndifferent findings directly from the radiology report data; resulting in AUCs\nof 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute\nischemic stroke, and intracranial hemorrhage datasets. The output encodings are\nable to be used in conjunction with imaging data, to create models that can\nprocess a multitude of different modalities. The ability to automatically\nextract relevant features from textual data allows for faster model development\nand integration of textual modality, overall, allowing clinical reports to\nbecome a more viable input for more encompassing and accurate deep learning\nmodels.",
        "Attention is a powerful component of modern neural networks across a wide\nvariety of domains. However, despite its ubiquity in machine learning, there is\na gap in our understanding of attention from a theoretical point of view. We\npropose a framework to fill this gap by building a mathematically equivalent\nmodel of attention using measure theory. With this model, we are able to\ninterpret self-attention as a system of self-interacting particles, we shed\nlight on self-attention from a maximum entropy perspective, and we show that\nattention is actually Lipschitz-continuous (with an appropriate metric) under\nsuitable assumptions. We then apply these insights to the problem of\nmis-specified input data; infinitely-deep, weight-sharing self-attention\nnetworks; and more general Lipschitz estimates for a specific type of attention\nstudied in concurrent work.",
        "Tracking has traditionally been the art of following interest points through\nspace and time. This changed with the rise of powerful deep networks. Nowadays,\ntracking is dominated by pipelines that perform object detection followed by\ntemporal association, also known as tracking-by-detection. In this paper, we\npresent a simultaneous detection and tracking algorithm that is simpler,\nfaster, and more accurate than the state of the art. Our tracker, CenterTrack,\napplies a detection model to a pair of images and detections from the prior\nframe. Given this minimal input, CenterTrack localizes objects and predicts\ntheir associations with the previous frame. That's it. CenterTrack is simple,\nonline (no peeking into the future), and real-time. It achieves 67.3% MOTA on\nthe MOT17 challenge at 22 FPS and 89.4% MOTA on the KITTI tracking benchmark at\n15 FPS, setting a new state of the art on both datasets. CenterTrack is easily\nextended to monocular 3D tracking by regressing additional 3D attributes. Using\nmonocular video input, it achieves 28.3% AMOTA@0.2 on the newly released\nnuScenes 3D tracking benchmark, substantially outperforming the monocular\nbaseline on this benchmark while running at 28 FPS.",
        "By taking into account the properties and limitations of the human visual\nsystem, images can be more efficiently compressed, colors more accurately\nreproduced, prints better rendered. To show all these advantages in this paper\nnew adapted color charts have been created based on technical and visual image\ncategory analysis. A number of tests have been carried out using extreme images\nwith their key information strictly in dark and light areas. It was shown that\nthe image categorization using the adapted color charts improves the analysis\nof relevant image information with regard to both the image gradation and the\ndetail reproduction. The images with key information in hi-key areas were also\ntest printed using the adapted color charts.",
        "Real-world data with underlying structure, such as pictures of faces, are\nhypothesized to lie on a low-dimensional manifold. This manifold hypothesis has\nmotivated state-of-the-art generative algorithms that learn low-dimensional\ndata representations. Unfortunately, a popular generative model, normalizing\nflows, cannot take advantage of this. Normalizing flows are based on successive\nvariable transformations that are, by design, incapable of learning\nlower-dimensional representations. In this paper we introduce noisy injective\nflows (NIF), a generalization of normalizing flows that can go across\ndimensions. NIF explicitly map the latent space to a learnable manifold in a\nhigh-dimensional data space using injective transformations. We further employ\nan additive noise model to account for deviations from the manifold and\nidentify a stochastic inverse of the generative process. Empirically, we\ndemonstrate that a simple application of our method to existing flow\narchitectures can significantly improve sample quality and yield separable data\nembeddings.",
        "The preceding three decades have seen the emergence, rise, and proliferation\nof machine learning (ML). From half-recognised beginnings in perceptrons,\nneural nets, and decision trees, algorithms that extract correlations (that is,\npatterns) from a set of data points have broken free from their origin in\ncomputational cognition to embrace all forms of problem solving, from voice\nrecognition to medical diagnosis to automated scientific research and\ndriverless cars, and it is now widely opined that the real industrial\nrevolution lies less in mobile phone and similar than in the maturation and\nuniversal application of ML. Among the consequences just might be the triumph\nof anti-realism over realism.",
        "In recent years, the interest in leveraging quantum effects for enhancing\nmachine learning tasks has significantly increased. Many algorithms speeding up\nsupervised and unsupervised learning were established. The first framework in\nwhich ways to exploit quantum resources specifically for the broader context of\nreinforcement learning were found is projective simulation. Projective\nsimulation presents an agent-based reinforcement learning approach designed in\na manner which may support quantum walk-based speed-ups. Although classical\nvariants of projective simulation have been benchmarked against common\nreinforcement learning algorithms, very few formal theoretical analyses have\nbeen provided for its performance in standard learning scenarios. In this\npaper, we provide a detailed formal discussion of the properties of this model.\nSpecifically, we prove that one version of the projective simulation model,\nunderstood as a reinforcement learning approach, converges to optimal behavior\nin a large class of Markov decision processes. This proof shows that a\nphysically-inspired approach to reinforcement learning can guarantee to\nconverge.",
        "This work analyses the impact of self-supervised pre-training on document\nimages in the context of document image classification. While previous\napproaches explore the effect of self-supervision on natural images, we show\nthat patch-based pre-training performs poorly on document images because of\ntheir different structural properties and poor intra-sample semantic\ninformation. We propose two context-aware alternatives to improve performance\non the Tobacco-3482 image classification task. We also propose a novel method\nfor self-supervision, which makes use of the inherent multi-modality of\ndocuments (image and text), which performs better than other popular\nself-supervised methods, including supervised ImageNet pre-training, on\ndocument image classification scenarios with a limited amount of data.",
        "Deep fakes became extremely popular in the last years, also thanks to their\nincreasing realism. Therefore, there is the need to measures human's ability to\ndistinguish between real and synthetic face images when confronted with\ncutting-edge creation technologies. We describe the design and results of a\nperceptual experiment we have conducted, where a wide and diverse group of\nvolunteers has been exposed to synthetic face images produced by\nstate-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\nStyleGAN2). The experiment outcomes reveal how strongly we should call into\nquestion our human ability to discriminate real faces from synthetic ones\ngenerated through modern AI.",
        "Deep Neural Networks are often though to lack interpretability due to the\ndistributed nature of their internal representations. In contrast, humans can\ngenerally justify, in natural language, for their answer to a visual question\nwith simple common sense reasoning. However, human introspection abilities have\ntheir own limits as one often struggles to justify for the recognition process\nbehind our lowest level feature recognition ability: for instance, it is\ndifficult to precisely explain why a given texture seems more characteristic of\nthe surface of a finger nail rather than a plastic bottle. In this paper, we\nshowcase an application in which deep learning models can actually help human\nexperts justify for their own low-level visual recognition process: We study\nthe problem of assessing the adhesive potency of copper sheets from microscopic\npictures of their surface. Although highly trained material experts are able to\nqualitatively assess the surface adhesive potency, they are often unable to\nprecisely justify for their decision process. We present a model that, under\ncareful design considerations, is able to provide visual clues for human\nexperts to understand and justify for their own recognition process. Not only\ncan our model assist human experts in their interpretation of the surface\ncharacteristics, we show how this model can be used to test different\nhypothesis of the copper surface response to different manufacturing processes.",
        "\\textit{Attention} computes the dependency between representations, and it\nencourages the model to focus on the important selective features.\nAttention-based models, such as Transformer and graph attention network (GAT),\nare widely utilized for sequential data and graph-structured data. This paper\nsuggests a new interpretation and generalized structure of the attention in\nTransformer and GAT. For the attention in Transformer and GAT, we derive that\nthe attention is a product of two parts: 1) the RBF kernel to measure the\nsimilarity of two instances and 2) the exponential of $L^{2}$ norm to compute\nthe importance of individual instances. From this decomposition, we generalize\nthe attention in three ways. First, we propose implicit kernel attention with\nan implicit kernel function instead of manual kernel selection. Second, we\ngeneralize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to\nstructured multi-head attention. Our generalized attention shows better\nperformance on classification, translation, and regression tasks.",
        "The highest strength-to-weight ratio criterion has fascinated curiosity\nincreasingly in virtually all areas where heft reduction is indispensable.\nLightweight materials and their joining processes are also a recent point of\nresearch demands in the manufacturing industries. Friction Stir Welding (FSW)\nis one of the recent advancements for joining materials without adding any\nthird material (filler rod) and joining below the melting point of the parent\nmaterial. The process is widely used for joining similar and dissimilar metals,\nespecially lightweight non-ferrous materials like aluminum, copper, and\nmagnesium alloys. This paper presents verdicts of optimum process parameters on\nattaining enhanced mechanical properties of the weld joint. The experiment was\nconducted on a 5 mm 6061 aluminum alloy sheet. Process parameters; tool\nmaterial, rotational speed, traverse speed, and axial forces were utilized.\nMechanical properties of the weld joint are examined employing a tensile test,\nand the maximum joint strength efficiency was reached 94.2%. Supervised Machine\nLearning based Regression algorithms such as Decision Trees, Random Forest, and\nGradient Boosting Algorithm were used. The results showed that the Random\nForest algorithm yielded highest coefficient of determination value of 0.926\nwhich means it gives a best fit in comparison to other algorithms.",
        "Graph neural networks have become one of the most important techniques to\nsolve machine learning problems on graph-structured data. Recent work on vertex\nclassification proposed deep and distributed learning models to achieve high\nperformance and scalability. However, we find that the feature vectors of\nbenchmark datasets are already quite informative for the classification task,\nand the graph structure only provides a means to denoise the data. In this\npaper, we develop a theoretical framework based on graph signal processing for\nanalyzing graph neural networks. Our results indicate that graph neural\nnetworks only perform low-pass filtering on feature vectors and do not have the\nnon-linear manifold learning property. We further investigate their resilience\nto feature noise and propose some insights on GCN-based graph neural network\ndesign.",
        "Cascaded architectures have brought significant performance improvement in\nobject detection and instance segmentation. However, there are lingering issues\nregarding the disparity in the Intersection-over-Union (IoU) distribution of\nthe samples between training and inference. This disparity can potentially\nexacerbate detection accuracy. This paper proposes an architecture referred to\nas Sample Consistency Network (SCNet) to ensure that the IoU distribution of\nthe samples at training time is close to that at inference time. Furthermore,\nSCNet incorporates feature relay and utilizes global contextual information to\nfurther reinforce the reciprocal relationships among classifying, detecting,\nand segmenting sub-tasks. Extensive experiments on the standard COCO dataset\nreveal the effectiveness of the proposed method over multiple evaluation\nmetrics, including box AP, mask AP, and inference speed. In particular, while\nrunning 38\\% faster, the proposed SCNet improves the AP of the box and mask\npredictions by respectively 1.3 and 2.3 points compared to the strong Cascade\nMask R-CNN baseline. Code is available at\n\\url{https://github.com/thangvubk/SCNet}.",
        "Graph classification is an important problem with applications across many\ndomains, like chemistry and bioinformatics, for which graph neural networks\n(GNNs) have been state-of-the-art (SOTA) methods. GNNs are designed to learn\nnode-level representation based on neighborhood aggregation schemes, and to\nobtain graph-level representation, pooling methods are applied after the\naggregation operation in existing GNN models to generate coarse-grained graphs.\nHowever,due to highly diverse applications of graph classification, and the\nperformance of existing pooling methods vary on different graphs. In other\nwords, it is a challenging problem to design a universal pooling architecture\nto perform well in most cases, leading to a demand for data-specific pooling\nmethods in real-world applications. To address this problem, we propose to use\nneural architecture search (NAS) to search for adaptive pooling architectures\nfor graph classification. Firstly we designed a unified framework consisting of\nfour modules: Aggregation, Pooling, Readout, and Merge, which can cover\nexisting human-designed pooling methods for graph classification. Based on this\nframework, a novel search space is designed by incorporating popular operations\nin human-designed architectures. Then to enable efficient search, a coarsening\nstrategy is proposed to continuously relax the search space, thus a\ndifferentiable search method can be adopted. Extensive experiments on six\nreal-world datasets from three domains are conducted, and the results\ndemonstrate the effectiveness and efficiency of the proposed framework.",
        "Forward-looking sonar can capture high resolution images of underwater\nscenes, but their interpretation is complex. Generic object detection in such\nimages has not been solved, specially in cases of small and unknown objects. In\ncomparison, detection proposal algorithms have produced top performing object\ndetectors in real-world color images. In this work we develop a Convolutional\nNeural Network that can reliably score objectness of image windows in\nforward-looking sonar images and by thresholding objectness, we generate\ndetection proposals. In our dataset of marine garbage objects, we obtain 94%\nrecall, generating around 60 proposals per image. The biggest strength of our\nmethod is that it can generalize to previously unseen objects. We show this by\ndetecting chain links, walls and a wrench without previous training in such\nobjects. We strongly believe our method can be used for class-independent\nobject detection, with many real-world applications such as chain following and\nmine detection.",
        "We introduce a novel approach to feed-forward neural network interpretation\nbased on partitioning the space of sequences of neuron activations. In line\nwith this approach, we propose a model-specific interpretation method, called\nYASENN. Our method inherits many advantages of model-agnostic distillation,\nsuch as an ability to focus on the particular input region and to express an\nexplanation in terms of features different from those observed by a neural\nnetwork. Moreover, examination of distillation error makes the method\napplicable to the problems with low tolerance to interpretation mistakes.\nTechnically, YASENN distills the network with an ensemble of layer-wise\ngradient boosting decision trees and encodes the sequences of neuron\nactivations with leaf indices. The finite number of unique codes induces a\npartitioning of the input space. Each partition may be described in a variety\nof ways, including examination of an interpretable model (e.g. a logistic\nregression or a decision tree) trained to discriminate between objects of those\npartitions. Our experiments provide an intuition behind the method and\ndemonstrate revealed artifacts in neural network decision making.",
        "Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets.",
        "Magnetic resonance imaging (MRI) enables plant scientists to non-invasively\nstudy root system development and root-soil interaction. Challenging recording\nconditions, such as low resolution and a high level of noise hamper the\nperformance of traditional root extraction algorithms, though. We propose to\nincrease signal-to-noise ratio and resolution by segmenting the scanned volumes\ninto root and soil in super-resolution using a 3D U-Net. Tests on real data\nshow that the trained network is capable to detect most roots successfully and\neven finds roots that were missed by human annotators. Our experiments show\nthat the segmentation performance can be further improved with modifications of\nthe loss function.",
        "Gaussian processes are the leading class of distributions on random\nfunctions, but they suffer from well known issues including difficulty scaling\nand inflexibility with respect to certain shape constraints (such as\nnonnegativity). Here we propose Deep Random Splines, a flexible class of random\nfunctions obtained by transforming Gaussian noise through a deep neural network\nwhose output are the parameters of a spline. Unlike Gaussian processes, Deep\nRandom Splines allow us to readily enforce shape constraints while inheriting\nthe richness and tractability of deep generative models. We also present an\nobservational model for point process data which uses Deep Random Splines to\nmodel the intensity function of each point process and apply it to neural\npopulation data to obtain a low-dimensional representation of spiking activity.\nInference is performed via a variational autoencoder that uses a novel\nrecurrent encoder architecture that can handle multiple point processes as\ninput. We use a newly collected dataset where a primate completes a pedaling\ntask, and observe better dimensionality reduction with our model than with\ncompeting alternatives.",
        "With the rapid emergence of graph representation learning, the construction\nof new large-scale datasets are necessary to distinguish model capabilities and\naccurately assess the strengths and weaknesses of each technique. By carefully\nanalyzing existing graph databases, we identify 3 critical components important\nfor advancing the field of graph representation learning: (1) large graphs, (2)\nmany graphs, and (3) class diversity. To date, no single graph database offers\nall of these desired properties. We introduce MalNet, the largest public graph\ndatabase ever constructed, representing a large-scale ontology of software\nfunction call graphs. MalNet contains over 1.2 million graphs, averaging over\n17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696\nfamilies. Compared to the popular REDDIT-12K database, MalNet offers 105x more\ngraphs, 44x larger graphs on average, and 63x the classes. We provide a\ndetailed analysis of MalNet, discussing its properties and provenance. The\nunprecedented scale and diversity of MalNet offers exciting opportunities to\nadvance the frontiers of graph representation learning---enabling new\ndiscoveries and research into imbalanced classification, explainability and the\nimpact of class hardness. The database is publically available at\nwww.mal-net.org.",
        "Explainability and interpretability are two critical aspects of decision\nsupport systems. Within computer vision, they are critical in certain tasks\nrelated to human behavior analysis such as in health care applications. Despite\ntheir importance, it is only recently that researchers are starting to explore\nthese aspects. This paper provides an introduction to explainability and\ninterpretability in the context of computer vision with an emphasis on looking\nat people tasks. Specifically, we review and study those mechanisms in the\ncontext of first impressions analysis. To the best of our knowledge, this is\nthe first effort in this direction. Additionally, we describe a challenge we\norganized on explainability in first impressions analysis from video. We\nanalyze in detail the newly introduced data set, the evaluation protocol, and\nsummarize the results of the challenge. Finally, derived from our study, we\noutline research opportunities that we foresee will be decisive in the near\nfuture for the development of the explainable computer vision field.",
        "Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.",
        "Recent years have witnessed the tremendous research interests in network\nembedding. Extant works have taken the neighborhood formation as the critical\ninformation to reveal the inherent dynamics of network structures, and\nsuggested encoding temporal edge formation sequences to capture the historical\ninfluences of neighbors. In this paper, however, we argue that the edge\nformation can be attributed to a variety of driving factors including the\ntemporal influence, which is better referred to as multiple aspects. As a\nmatter of fact, different node aspects can drive the formation of distinctive\nneighbors, giving birth to the multi-aspect embedding that relates to but goes\nbeyond a temporal scope. Along this vein, we propose a Mixture of Hawkes-based\nTemporal Network Embeddings (MHNE) model to capture the aspect-driven\nneighborhood formation of networks. In MHNE, we encode the multi-aspect\nembeddings into the mixture of Hawkes processes to gain the advantages in\nmodeling the excitation effects and the latent aspects. Specifically, a graph\nattention mechanism is used to assign different weights to account for the\nexcitation effects of history events, while a Gumbel-Softmax is plugged in to\nderive the distribution over the aspects. Extensive experiments on 8 different\ntemporal networks have demonstrated the great performance of the multi-aspect\nembeddings obtained by MHNE in comparison with the state-of-the-art methods.",
        "Policy optimization methods are popular reinforcement learning algorithms,\nbecause their incremental and on-policy nature makes them more stable than the\nvalue-based counterparts. However, the same properties also make them slow to\nconverge and sample inefficient, as the on-policy requirement precludes data\nreuse and the incremental updates couple large iteration complexity into the\nsample complexity. These characteristics have been observed in experiments as\nwell as in theory in the recent work of~\\citet{agarwal2020pc}, which provides a\npolicy optimization method PCPG that can robustly find near optimal polices for\napproximately linear Markov decision processes but suffers from an extremely\npoor sample complexity compared with value-based techniques.\n  In this paper, we propose a new algorithm, COPOE, that overcomes the sample\ncomplexity issue of PCPG while retaining its robustness to model\nmisspecification. Compared with PCPG, COPOE makes several important algorithmic\nenhancements, such as enabling data reuse, and uses more refined analysis\ntechniques, which we expect to be more broadly applicable to designing new\nreinforcement learning algorithms. The result is an improvement in sample\ncomplexity from $\\widetilde{O}(1/\\epsilon^{11})$ for PCPG to\n$\\widetilde{O}(1/\\epsilon^3)$ for PCPG, nearly bridging the gap with\nvalue-based techniques.",
        "Graph Convolutional Network (GCN) is an emerging technique for information\nretrieval (IR) applications. While GCN assumes the homophily property of a\ngraph, real-world graphs are never perfect: the local structure of a node may\ncontain discrepancy, e.g., the labels of a node's neighbors could vary. This\npushes us to consider the discrepancy of local structure in GCN modeling.\nExisting work approaches this issue by introducing an additional module such as\ngraph attention, which is expected to learn the contribution of each neighbor.\nHowever, such module may not work reliably as expected, especially when there\nlacks supervision signal, e.g., when the labeled data is small. Moreover,\nexisting methods focus on modeling the nodes in the training data, and never\nconsider the local structure discrepancy of testing nodes.\n  This work focuses on the local structure discrepancy issue for testing nodes,\nwhich has received little scrutiny. From a novel perspective of causality, we\ninvestigate whether a GCN should trust the local structure of a testing node\nwhen predicting its label. To this end, we analyze the working mechanism of GCN\nwith causal graph, estimating the causal effect of a node's local structure for\nthe prediction. The idea is simple yet effective: given a trained GCN model, we\nfirst intervene the prediction by blocking the graph structure; we then compare\nthe original prediction with the intervened prediction to assess the causal\neffect of the local structure on the prediction. Through this way, we can\neliminate the impact of local structure discrepancy and make more accurate\nprediction. Extensive experiments on seven node classification datasets show\nthat our method effectively enhances the inference stage of GCN.",
        "The integrated positron emission tomography/magnetic resonance imaging\n(PET/MRI) scanner facilitates the simultaneous acquisition of metabolic\ninformation via PET and morphological information with high soft-tissue\ncontrast using MRI. Although PET/MRI facilitates the capture of high-accuracy\nfusion images, its major drawback can be attributed to the difficulty\nencountered when performing attenuation correction, which is necessary for\nquantitative PET evaluation. The combined PET/MRI scanning requires the\ngeneration of attenuation-correction maps from MRI owing to no direct\nrelationship between the gamma-ray attenuation information and MRIs. While\nMRI-based bone-tissue segmentation can be readily performed for the head and\npelvis regions, the realization of accurate bone segmentation via chest CT\ngeneration remains a challenging task. This can be attributed to the\nrespiratory and cardiac motions occurring in the chest as well as its\nanatomically complicated structure and relatively thin bone cortex. This paper\npresents a means to minimise the anatomical structural changes without human\nannotation by adding structural constraints using a modality-independent\nneighbourhood descriptor (MIND) to a generative adversarial network (GAN) that\ncan transform unpaired images. The results obtained in this study revealed the\nproposed U-GAT-IT + MIND approach to outperform all other competing approaches.\nThe findings of this study hint towards possibility of synthesising clinically\nacceptable CT images from chest MRI without human annotation, thereby\nminimising the changes in the anatomical structure.",
        "The convolution operation suffers from a limited receptive filed, while\nglobal modeling is fundamental to dense prediction tasks, such as semantic\nsegmentation. In this paper, we apply graph convolution into the semantic\nsegmentation task and propose an improved Laplacian. The graph reasoning is\ndirectly performed in the original feature space organized as a spatial\npyramid. Different from existing methods, our Laplacian is data-dependent and\nwe introduce an attention diagonal matrix to learn a better distance metric. It\ngets rid of projecting and re-projecting processes, which makes our proposed\nmethod a light-weight module that can be easily plugged into current computer\nvision architectures. More importantly, performing graph reasoning directly in\nthe feature space retains spatial relationships and makes spatial pyramid\npossible to explore multiple long-range contextual patterns from different\nscales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC\ndemonstrate the effectiveness of our proposed methods on semantic segmentation.\nWe achieve comparable performance with advantages in computational and memory\noverhead.",
        "We study the implicit bias of gradient flow (i.e., gradient descent with\ninfinitesimal step size) on linear neural network training. We propose a tensor\nformulation of neural networks that includes fully-connected, diagonal, and\nconvolutional networks as special cases, and investigate the linear version of\nthe formulation called linear tensor networks. With this formulation, we can\ncharacterize the convergence direction of the network parameters as singular\nvectors of a tensor defined by the network. For $L$-layer linear tensor\nnetworks that are orthogonally decomposable, we show that gradient flow on\nseparable classification finds a stationary point of the $\\ell_{2/L}$\nmax-margin problem in a \"transformed\" input space defined by the network. For\nunderdetermined regression, we prove that gradient flow finds a global minimum\nwhich minimizes a norm-like function that interpolates between weighted\n$\\ell_1$ and $\\ell_2$ norms in the transformed input space. Our theorems\nsubsume existing results in the literature while removing standard convergence\nassumptions. We also provide experiments that corroborate our analysis.",
        "With the widespread success of deep learning in biomedical image\nsegmentation, domain shift becomes a critical and challenging problem, as the\ngap between two domains can severely affect model performance when deployed to\nunseen data with heterogeneous features. To alleviate this problem, we present\na novel unsupervised domain adaptation network, for generalizing models learned\nfrom the labeled source domain to the unlabeled target domain for\ncross-modality biomedical image segmentation. Specifically, our approach\nconsists of two key modules, a conditional domain discriminator~(CDD) and a\ncategory-centric prototype aligner~(CCPA). The CDD, extended from conditional\ndomain adversarial networks in classifier tasks, is effective and robust in\nhandling complex cross-modality biomedical images. The CCPA, improved from the\ngraph-induced prototype alignment mechanism in cross-domain object detection,\ncan exploit precise instance-level features through an elaborate prototype\nrepresentation. In addition, it can address the negative effect of class\nimbalance via entropy-based loss. Extensive experiments on a public benchmark\nfor the cardiac substructure segmentation task demonstrate that our method\nsignificantly improves performance on the target domain.",
        "Learning latent representations of nodes in graphs is an important and\nubiquitous task with widespread applications such as link prediction, node\nclassification, and graph visualization. Previous methods on graph\nrepresentation learning mainly focus on static graphs, however, many real-world\ngraphs are dynamic and evolve over time. In this paper, we present Dynamic\nSelf-Attention Network (DySAT), a novel neural architecture that operates on\ndynamic graphs and learns node representations that capture both structural\nproperties and temporal evolutionary patterns. Specifically, DySAT computes\nnode representations by jointly employing self-attention layers along two\ndimensions: structural neighborhood and temporal dynamics. We conduct link\nprediction experiments on two classes of graphs: communication networks and\nbipartite rating networks. Our experimental results show that DySAT has a\nsignificant performance gain over several different state-of-the-art graph\nembedding baselines.",
        "Embedding static graphs in low-dimensional vector spaces plays a key role in\nnetwork analytics and inference, supporting applications like node\nclassification, link prediction, and graph visualization. However, many\nreal-world networks present dynamic behavior, including topological evolution,\nfeature evolution, and diffusion. Therefore, several methods for embedding\ndynamic graphs have been proposed to learn network representations over time,\nfacing novel challenges, such as time-domain modeling, temporal features to be\ncaptured, and the temporal granularity to be embedded. In this survey, we\noverview dynamic graph embedding, discussing its fundamentals and the recent\nadvances developed so far. We introduce the formal definition of dynamic graph\nembedding, focusing on the problem setting and introducing a novel taxonomy for\ndynamic graph embedding input and output. We further explore different dynamic\nbehaviors that may be encompassed by embeddings, classifying by topological\nevolution, feature evolution, and processes on networks. Afterward, we describe\nexisting techniques and propose a taxonomy for dynamic graph embedding\ntechniques based on algorithmic approaches, from matrix and tensor\nfactorization to deep learning, random walks, and temporal point processes. We\nalso elucidate main applications, including dynamic link prediction, anomaly\ndetection, and diffusion prediction, and we further state some promising\nresearch directions in the area.",
        "Benefiting from the advancement of computer vision, natural language\nprocessing and information retrieval techniques, visual question answering\n(VQA), which aims to answer questions about an image or a video, has received\nlots of attentions over the past few years. Although some progress has been\nachieved so far, several studies have pointed out that current VQA models are\nheavily affected by the \\emph{language prior problem}, which means they tend to\nanswer questions based on the co-occurrence patterns of question keywords\n(e.g., how many) and answers (e.g., 2) instead of understanding images and\nquestions. Existing methods attempt to solve this problem by either balancing\nthe biased datasets or forcing models to better understand images. However,\nonly marginal effects and even performance deterioration are observed for the\nfirst and second solution, respectively. In addition, another important issue\nis the lack of measurement to quantitatively measure the extent of the language\nprior effect, which severely hinders the advancement of related techniques.\n  In this paper, we make contributions to solve the above problems from two\nperspectives. Firstly, we design a metric to quantitatively measure the\nlanguage prior effect of VQA models. The proposed metric has been demonstrated\nto be effective in our empirical studies. Secondly, we propose a regularization\nmethod (i.e., score regularization module) to enhance current VQA models by\nalleviating the language prior problem as well as boosting the backbone model\nperformance. The proposed score regularization module adopts a pair-wise\nlearning strategy, which makes the VQA models answer the question based on the\nreasoning of the image (upon this question) instead of basing on\nquestion-answer patterns observed in the biased training set. The score\nregularization module is flexible to be integrated into various VQA models.",
        "Flame spray pyrolysis (FSP) is a process used to synthesize nanoparticles\nthrough the combustion of an atomized precursor solution; this process has\napplications in catalysts, battery materials, and pigments. Current limitations\nrevolve around understanding how to consistently achieve a stable flame and the\nreliable production of nanoparticles. Machine learning and artificial\nintelligence algorithms that detect unstable flame conditions in real time may\nbe a means of streamlining the synthesis process and improving FSP efficiency.\nIn this study, the FSP flame stability is first quantified by analyzing the\nbrightness of the flame's anchor point. This analysis is then used to label\ndata for both unsupervised and supervised machine learning approaches. The\nunsupervised learning approach allows for autonomous labelling and\nclassification of new data by representing data in a reduced dimensional space\nand identifying combinations of features that most effectively cluster it. The\nsupervised learning approach, on the other hand, requires human labeling of\ntraining and test data, but is able to classify multiple objects of interest\n(such as the burner and pilot flames) within the video feed. The accuracy of\neach of these techniques is compared against the evaluations of human experts.\nBoth the unsupervised and supervised approaches can track and classify FSP\nflame conditions in real time to alert users of unstable flame conditions. This\nresearch has the potential to autonomously track and manage flame spray\npyrolysis as well as other flame technologies by monitoring and classifying the\nflame stability.",
        "Deep Reinforcement Learning has shown great success in a variety of control\ntasks. However, it is unclear how close we are to the vision of putting Deep RL\ninto practice to solve real world problems. In particular, common practice in\nthe field is to train policies on largely deterministic simulators and to\nevaluate algorithms through training performance alone, without a train/test\ndistinction to ensure models generalise and are not overfitted. Moreover, it is\nnot standard practice to check for generalisation under domain shift, although\nrobustness to such system change between training and testing would be\nnecessary for real-world Deep RL control, for example, in robotics. In this\npaper we study these issues by first characterising the sources of uncertainty\nthat provide generalisation challenges in Deep RL. We then provide a new\nbenchmark and thorough empirical evaluation of generalisation challenges for\nstate of the art Deep RL methods. In particular, we show that, if\ngeneralisation is the goal, then common practice of evaluating algorithms based\non their training performance leads to the wrong conclusions about algorithm\nchoice. Finally, we evaluate several techniques for improving generalisation\nand draw conclusions about the most robust techniques to date.",
        "Translating machine learning (ML) models effectively to clinical practice\nrequires establishing clinicians' trust. Explainability, or the ability of an\nML model to justify its outcomes and assist clinicians in rationalizing the\nmodel prediction, has been generally understood to be critical to establishing\ntrust. However, the field suffers from the lack of concrete definitions for\nusable explanations in different settings. To identify specific aspects of\nexplainability that may catalyze building trust in ML models, we surveyed\nclinicians from two distinct acute care specialties (Intenstive Care Unit and\nEmergency Department). We use their feedback to characterize when\nexplainability helps to improve clinicians' trust in ML models. We further\nidentify the classes of explanations that clinicians identified as most\nrelevant and crucial for effective translation to clinical practice. Finally,\nwe discern concrete metrics for rigorous evaluation of clinical explainability\nmethods. By integrating perceptions of explainability between clinicians and ML\nresearchers we hope to facilitate the endorsement and broader adoption and\nsustained use of ML systems in healthcare.",
        "Local and global patterns of an object are closely related. Although each\npart of an object is incomplete, the underlying attributes about the object are\nshared among all parts, which makes reasoning the whole object from a single\npart possible. We hypothesize that a powerful representation of a 3D object\nshould model the attributes that are shared between parts and the whole object,\nand distinguishable from other objects. Based on this hypothesis, we propose to\nlearn point cloud representation by bidirectional reasoning between the local\nstructures at different abstraction hierarchies and the global shape without\nhuman supervision. Experimental results on various benchmark datasets\ndemonstrate the unsupervisedly learned representation is even better than\nsupervised representation in discriminative power, generalization ability, and\nrobustness. We show that unsupervisedly trained point cloud models can\noutperform their supervised counterparts on downstream classification tasks.\nMost notably, by simply increasing the channel width of an SSG PointNet++, our\nunsupervised model surpasses the state-of-the-art supervised methods on both\nsynthetic and real-world 3D object classification datasets. We expect our\nobservations to offer a new perspective on learning better representation from\ndata structures instead of human annotations for point cloud understanding.",
        "Batch normalization (BN) is a fundamental unit in modern deep networks, in\nwhich a linear transformation module was designed for improving BN's\nflexibility of fitting complex data distributions. In this paper, we\ndemonstrate properly enhancing this linear transformation module can\neffectively improve the ability of BN. Specifically, rather than using a single\nneuron, we propose to additionally consider each neuron's neighborhood for\ncalculating the outputs of the linear transformation. Our method, named BNET,\ncan be implemented with 2-3 lines of code in most deep learning libraries.\nDespite the simplicity, BNET brings consistent performance gains over a wide\nrange of backbones and visual benchmarks. Moreover, we verify that BNET\naccelerates the convergence of network training and enhances spatial\ninformation by assigning the important neurons with larger weights accordingly.\nThe code is available at https://github.com/yuhuixu1993/BNET.",
        "Neural Architecture Search (NAS) has shown great potentials in automatically\ndesigning scalable network architectures for dense image predictions. However,\nexisting NAS algorithms usually compromise on restricted search space and\nsearch on proxy task to meet the achievable computational demands. To allow as\nwide as possible network architectures and avoid the gap between target and\nproxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which\ndirectly searches the optimal network structures for the multi-scale\nrepresentations of visual information, over a large-scale target dataset.\nSpecifically, by connecting cells with each other using learnable weights, we\nintroduce a densely connected search space to cover an abundance of mainstream\nnetwork designs. Moreover, by combining both path-level and channel-level\nsampling strategies, we design a fusion module to reduce the memory consumption\nof ample search space. We demonstrate that the architecture obtained from our\nDCNAS algorithm achieves state-of-the-art performances on public semantic image\nsegmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC\n2012. We also retain leading performances when evaluating the architecture on\nthe more challenging ADE20K and Pascal Context dataset.",
        "Image recognition is an important topic in computer vision and image\nprocessing, and has been mainly addressed by supervised deep learning methods,\nwhich need a large set of labeled images to achieve promising performance.\nHowever, in most cases, labeled data are expensive or even impossible to\nobtain, while unlabeled data are readily available from numerous free on-line\nresources and have been exploited to improve the performance of deep neural\nnetworks. To better exploit the power of unlabeled data for image recognition,\nin this paper, we propose a semi-supervised and generative approach, namely the\nsemi-supervised self-growing generative adversarial network (SGGAN). Label\ninference is a key step for the success of semi-supervised learning approaches.\nThere are two main problems in label inference: how to measure the confidence\nof the unlabeled data and how to generalize the classifier. We address these\ntwo problems via the generative framework and a novel\nconvolution-block-transformation technique, respectively. To stabilize and\nspeed up the training process of SGGAN, we employ the metric Maximum Mean\nDiscrepancy as the feature matching objective function and achieve larger gain\nthan the standard semi-supervised GANs (SSGANs), narrowing the gap to the\nsupervised methods. Experiments on several benchmark datasets show the\neffectiveness of the proposed SGGAN on image recognition and facial attribute\nrecognition tasks. By using the training data with only 4% labeled facial\nattributes, the SGGAN approach can achieve comparable accuracy with leading\nsupervised deep learning methods with all labeled facial attributes.",
        "Video recognition has been advanced in recent years by benchmarks with rich\nannotations. However, research is still mainly limited to human action or\nsports recognition - focusing on a highly specific video understanding task and\nthus leaving a significant gap towards describing the overall content of a\nvideo. We fill this gap by presenting a large-scale \"Holistic Video\nUnderstanding Dataset\"~(HVU). HVU is organized hierarchically in a semantic\ntaxonomy that focuses on multi-label and multi-task video understanding as a\ncomprehensive problem that encompasses the recognition of multiple semantic\naspects in the dynamic scene. HVU contains approx.~572k videos in total with 9\nmillion annotations for training, validation, and test set spanning over 3142\nlabels. HVU encompasses semantic aspects defined on categories of scenes,\nobjects, actions, events, attributes, and concepts which naturally captures the\nreal-world scenarios.\n  We demonstrate the generalization capability of HVU on three challenging\ntasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering\ntasks. In particular for video classification, we introduce a new\nspatio-temporal deep neural network architecture called \"Holistic Appearance\nand Temporal Network\"~(HATNet) that builds on fusing 2D and 3D architectures\ninto one by combining intermediate representations of appearance and temporal\ncues. HATNet focuses on the multi-label and multi-task learning problem and is\ntrained in an end-to-end manner. Via our experiments, we validate the idea that\nholistic representation learning is complementary, and can play a key role in\nenabling many real-world applications.",
        "Visual Question Answering (VQA) models should have both high robustness and\naccuracy. Unfortunately, most of the current VQA research only focuses on\naccuracy because there is a lack of proper methods to measure the robustness of\nVQA models. There are two main modules in our algorithm. Given a natural\nlanguage question about an image, the first module takes the question as input\nand then outputs the ranked basic questions, with similarity scores, of the\nmain given question. The second module takes the main question, image and these\nbasic questions as input and then outputs the text-based answer of the main\nquestion about the given image. We claim that a robust VQA model is one, whose\nperformance is not changed much when related basic questions as also made\navailable to it as input. We formulate the basic questions generation problem\nas a LASSO optimization, and also propose a large scale Basic Question Dataset\n(BQD) and Rscore (novel robustness measure), for analyzing the robustness of\nVQA models. We hope our BQD will be used as a benchmark for to evaluate the\nrobustness of VQA models, so as to help the community build more robust and\naccurate VQA models.",
        "We present a hierarchical neural message passing architecture for learning on\nmolecular graphs. Our model takes in two complementary graph representations:\nthe raw molecular graph representation and its associated junction tree, where\nnodes represent meaningful clusters in the original graph, e.g., rings or\nbridged compounds. We then proceed to learn a molecule's representation by\npassing messages inside each graph, and exchange messages between the two\nrepresentations using a coarse-to-fine and fine-to-coarse information flow. Our\nmethod is able to overcome some of the restrictions known from classical GNNs,\nlike detecting cycles, while still being very efficient to train. We validate\nits performance on the ZINC dataset and datasets stemming from the MoleculeNet\nbenchmark collection.",
        "The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.",
        "K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning\nalgorithms that solve the well-known clustering problem. The procedure follows\na simple and easy way to classify a given data set to a predefined, say K\nnumber of clusters. Determination of K is a difficult job and it is not known\nthat which value of K can partition the objects as per our intuition. To\novercome this problem we proposed K+ Means algorithm. This algorithm is an\nenhancement over K-Means algorithm.",
        "There are quite a number of photographs captured under undesirable conditions\nin the last century. Thus, they are often noisy, regionally incomplete, and\ngrayscale formatted. Conventional approaches mainly focus on one point so that\nthose restoration results are not perceptually sharp or clean enough. To solve\nthese problems, we propose a noise prior learner NEGAN to simulate the noise\ndistribution of real legacy photos using unpaired images. It mainly focuses on\nmatching high-frequency parts of noisy images through discrete wavelet\ntransform (DWT) since they include most of noise statistics. We also create a\nlarge legacy photo dataset for learning noise prior. Using learned noise prior,\nwe can easily build valid training pairs by degrading clean images. Then, we\npropose an IEGAN framework performing image editing including joint denoising,\ninpainting and colorization based on the estimated noise prior. We evaluate the\nproposed system and compare it with state-of-the-art image enhancement methods.\nThe experimental results demonstrate that it achieves the best perceptual\nquality.\nhttps://github.com/zhaoyuzhi/Legacy-Photo-Editing-with-Learned-Noise-Prior for\nthe codes and the proposed LP dataset.",
        "Transfer learning is a widely used strategy in medical image analysis.\nInstead of only training a network with a limited amount of data from the\ntarget task of interest, we can first train the network with other, potentially\nlarger source datasets, creating a more robust model. The source datasets do\nnot have to be related to the target task. For a classification task in lung CT\nimages, we could use both head CT images, or images of cats, as the source.\nWhile head CT images appear more similar to lung CT images, the number and\ndiversity of cat images might lead to a better model overall. In this survey we\nreview a number of papers that have performed similar comparisons. Although the\nanswer to which strategy is best seems to be \"it depends\", we discuss a number\nof research directions we need to take as a community, to gain more\nunderstanding of this topic.",
        "Natural language processing has improved tremendously after the success of\nword embedding techniques such as word2vec. Recently, the same idea has been\napplied on source code with encouraging results. In this survey, we aim to\ncollect and discuss the usage of word embedding techniques on programs and\nsource code. The articles in this survey have been collected by asking authors\nof related work and with an extensive search on Google Scholar. Each article is\ncategorized into five categories: 1. embedding of tokens 2. embedding of\nfunctions or methods 3. embedding of sequences or sets of method calls 4.\nembedding of binary code 5. other embeddings. We also provide links to\nexperimental data and show some remarkable visualization of code embeddings. In\nsummary, word embedding has been successfully applied on different\ngranularities of source code. With access to countless open-source\nrepositories, we see a great potential of applying other data-driven natural\nlanguage processing techniques on source code in the future.",
        "We present and evaluate a new deep neural network architecture for automatic\nthoracic disease detection on chest X-rays. Deep neural networks have shown\ngreat success in a plethora of visual recognition tasks such as image\nclassification and object detection by stacking multiple layers of\nconvolutional neural networks (CNN) in a feed-forward manner. However, the\nperformance gain by going deeper has reached bottlenecks as a result of the\ntrade-off between model complexity and discrimination power. We address this\nproblem by utilizing the recently developed routing-by agreement mechanism in\nour architecture. A novel characteristic of our network structure is that it\nextends routing to two types of layer connections (1) connection between\nfeature maps in dense layers, (2) connection between primary capsules and\nprediction capsules in final classification layer. We show that our networks\nachieve comparable results with much fewer layers in the measurement of AUC\nscore. We further show the combined benefits of model interpretability by\ngenerating Gradient-weighted Class Activation Mapping (Grad-CAM) for\nlocalization. We demonstrate our results on the NIH chestX-ray14 dataset that\nconsists of 112,120 images on 30,805 unique patients including 14 kinds of lung\ndiseases.",
        "Adversarial examples have gained tons of attention in recent years. Many\nadversarial attacks have been proposed to attack image classifiers, but few\nwork shift attention to object detectors. In this paper, we propose Sparse\nAdversarial Attack (SAA) which enables adversaries to perform effective evasion\nattack on detectors with bounded \\emph{l$_{0}$} norm perturbation. We select\nthe fragile position of the image and designed evasion loss function for the\ntask. Experiment results on YOLOv4 and FasterRCNN reveal the effectiveness of\nour method. In addition, our SAA shows great transferability across different\ndetectors in the black-box attack setting. Codes are available at\n\\emph{https://github.com/THUrssq/Tianchi04}.",
        "We infer and generate three-dimensional (3D) scene information from a single\ninput image and without supervision. This problem is under-explored, with most\nprior work relying on supervision from, e.g., 3D ground-truth, multiple images\nof a scene, image silhouettes or key-points. We propose Pix2Shape, an approach\nto solve this problem with four components: (i) an encoder that infers the\nlatent 3D representation from an image, (ii) a decoder that generates an\nexplicit 2.5D surfel-based reconstruction of a scene from the latent code (iii)\na differentiable renderer that synthesizes a 2D image from the surfel\nrepresentation, and (iv) a critic network trained to discriminate between\nimages generated by the decoder-renderer and those from a training\ndistribution. Pix2Shape can generate complex 3D scenes that scale with the\nview-dependent on-screen resolution, unlike representations that capture\nworld-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a\nconsistent scene representation in its encoded latent space and that the\ndecoder can then be applied to this latent representation in order to\nsynthesize the scene from a novel viewpoint. We evaluate Pix2Shape with\nexperiments on the ShapeNet dataset as well as on a novel benchmark we\ndeveloped, called 3D-IQTT, to evaluate models based on their ability to enable\n3d spatial reasoning. Qualitative and quantitative evaluation demonstrate\nPix2Shape's ability to solve scene reconstruction, generation, and\nunderstanding tasks.",
        "Egocentric activity recognition in first-person videos has an increasing\nimportance with a variety of applications such as lifelogging, summarization,\nassisted-living and activity tracking. Existing methods for this task are based\non interpretation of various sensor information using pre-determined weights\nfor each feature. In this work, we propose a new framework for egocentric\nactivity recognition problem based on combining audio-visual features with\nmulti-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that\npurpose, firstly grid optical-flow, virtual-inertia feature, log-covariance,\ncuboid are extracted from the video. The audio signal is characterized using a\n\"supervector\", obtained based on Gaussian mixture modelling of frame-level\nfeatures, followed by a maximum a-posteriori adaptation. Then, the extracted\nmulti-modal features are adaptively fused by MKL classifiers in which both the\nfeature and kernel selection/weighing and recognition tasks are performed\ntogether. The proposed framework was evaluated on a number of egocentric\ndatasets. The results showed that using multi-modal features with MKL\noutperforms the existing methods.",
        "Causal reasoning is the main learning and explanation tool used by humans. AI\nsystems should possess causal reasoning capabilities to be deployed in the real\nworld with trust and reliability. Introducing the ideas of causality to machine\nlearning helps in providing better learning and explainable models.\nExplainability, causal disentanglement are some important aspects of any\nmachine learning model. Causal explanations are required to believe in a\nmodel's decision and causal disentanglement learning is important for transfer\nlearning applications. We exploit the ideas of causality to be used in deep\nlearning models to achieve better and causally explainable models that are\nuseful in fairness, disentangled representation, etc.",
        "High dynamic range (HDR) imaging has recently drawn much attention in\nmultimedia community. In this paper, we proposed a HDR image forensics method\nbased on convolutional neural network (CNN).To our best knowledge, this is the\nfirst time to apply deep learning method on HDR image forensics. The proposed\nalgorithm uses CNN to distinguish HDR images generated by multiple low dynamic\nrange (LDR) images from that expanded by single LDR image using inverse tone\nmapping (iTM). To do this, we learn the change of statistical characteristics\nextracted by the proposed CNN architectures and classify two kinds of HDR\nimages. Comparision results with some traditional statistical characteristics\nshows efficiency of the proposed method in HDR image source identification.",
        "This paper addresses the challenging unsupervised scene flow estimation\nproblem by jointly learning four low-level vision sub-tasks: optical flow\n$\\textbf{F}$, stereo-depth $\\textbf{D}$, camera pose $\\textbf{P}$ and motion\nsegmentation $\\textbf{S}$. Our key insight is that the rigidity of the scene\nshares the same inherent geometrical structure with object movements and scene\ndepth. Hence, rigidity from $\\textbf{S}$ can be inferred by jointly coupling\n$\\textbf{F}$, $\\textbf{D}$ and $\\textbf{P}$ to achieve more robust estimation.\nTo this end, we propose a novel scene flow framework named EffiScene with\nefficient joint rigidity learning, going beyond the existing pipeline with\nindependent auxiliary structures. In EffiScene, we first estimate optical flow\nand depth at the coarse level and then compute camera pose by\nPerspective-$n$-Points method. To jointly learn local rigidity, we design a\nnovel Rigidity From Motion (RfM) layer with three principal components:\n\\emph{}{(i)} correlation extraction; \\emph{}{(ii)} boundary learning; and\n\\emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid\nmap $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new\nlosses $\\mathcal{L}_{bnd}$ and $\\mathcal{L}_{unc}$ are designed to prevent\ntrivial solutions and to regularize the flow boundary discontinuity. Extensive\nexperiments on scene flow benchmark KITTI show that our method is effective and\nsignificantly improves the state-of-the-art approaches for all sub-tasks, i.e.\noptical flow ($5.19 \\rightarrow 4.20$), depth estimation ($3.78 \\rightarrow\n3.46$), visual odometry ($0.012 \\rightarrow 0.011$) and motion segmentation\n($0.57 \\rightarrow 0.62$).",
        "Deep multi-task learning attracts much attention in recent years as it\nachieves good performance in many applications. Feature learning is important\nto deep multi-task learning for sharing common information among tasks. In this\npaper, we propose a Hierarchical Graph Neural Network (HGNN) to learn augmented\nfeatures for deep multi-task learning. The HGNN consists of two-level graph\nneural networks. In the low level, an intra-task graph neural network is\nresponsible of learning a powerful representation for each data point in a task\nby aggregating its neighbors. Based on the learned representation, a task\nembedding can be generated for each task in a similar way to max pooling. In\nthe second level, an inter-task graph neural network updates task embeddings of\nall the tasks based on the attention mechanism to model task relations. Then\nthe task embedding of one task is used to augment the feature representation of\ndata points in this task. Moreover, for classification tasks, an inter-class\ngraph neural network is introduced to conduct similar operations on a finer\ngranularity, i.e., the class level, to generate class embeddings for each class\nin all the tasks use class embeddings to augment the feature representation.\nThe proposed feature augmentation strategy can be used in many deep multi-task\nlearning models. we analyze the HGNN in terms of training and generalization\nlosses. Experiments on real-world datastes show the significant performance\nimprovement when using this strategy.",
        "In recent years, post-hoc local instance-level and global dataset-level\nexplainability of black-box models has received a lot of attention. Much less\nattention has been given to obtaining insights at intermediate or group levels,\nwhich is a need outlined in recent works that study the challenges in realizing\nthe guidelines in the General Data Protection Regulation (GDPR). In this paper,\nwe propose a meta-method that, given a typical local explainability method, can\nbuild a multilevel explanation tree. The leaves of this tree correspond to the\nlocal explanations, the root corresponds to the global explanation, and\nintermediate levels correspond to explanations for groups of data points that\nit automatically clusters. The method can also leverage side information, where\nusers can specify points for which they may want the explanations to be\nsimilar. We argue that such a multilevel structure can also be an effective\nform of communication, where one could obtain few explanations that\ncharacterize the entire dataset by considering an appropriate level in our\nexplanation tree. Explanations for novel test points can be cost-efficiently\nobtained by associating them with the closest training points. When the local\nexplainability technique is generalized additive (viz. LIME, GAMs), we develop\na fast approximate algorithm for building the multilevel tree and study its\nconvergence behavior. We validate the effectiveness of the proposed technique\nbased on two human studies -- one with experts and the other with non-expert\nusers -- on real world datasets, and show that we produce high fidelity sparse\nexplanations on several other public datasets.",
        "Reinforcement learning algorithms usually assume that all actions are always\navailable to an agent. However, both people and animals understand the general\nlink between the features of their environment and the actions that are\nfeasible. Gibson (1977) coined the term \"affordances\" to describe the fact that\ncertain states enable an agent to do certain actions, in the context of\nembodied agents. In this paper, we develop a theory of affordances for agents\nwho learn and plan in Markov Decision Processes. Affordances play a dual role\nin this case. On one hand, they allow faster planning, by reducing the number\nof actions available in any given situation. On the other hand, they facilitate\nmore efficient and precise learning of transition models from data, especially\nwhen such models require function approximation. We establish these properties\nthrough theoretical results as well as illustrative examples. We also propose\nan approach to learn affordances and use it to estimate transition models that\nare simpler and generalize better.",
        "Generative Adversarial Networks (GANs) have the capability of synthesizing\nimages, which have been successfully applied to medical image synthesis tasks.\nHowever, most of existing methods merely consider the global contextual\ninformation and ignore the fine foreground structures, e.g., vessel, skeleton,\nwhich may contain diagnostic indicators for medical image analysis. Inspired by\nhuman painting procedure, which is composed of stroking and color rendering\nsteps, we propose a Sketching-rendering Unconditional Generative Adversarial\nNetwork (SkrGAN) to introduce a sketch prior constraint to guide the medical\nimage generation. In our SkrGAN, a sketch guidance module is utilized to\ngenerate a high quality structural sketch from random noise, then a color\nrender mapping is used to embed the sketch-based representations and resemble\nthe background appearances. Experimental results show that the proposed SkrGAN\nachieves the state-of-the-art results in synthesizing images for various image\nmodalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and\nMagnetic Resonance Imaging (MRI). In addition, we also show that the\nperformances of medical image segmentation method have been improved by using\nour synthesized images as data augmentation.",
        "In this paper, we present a generative adversarial network framework that\ngenerates compressed images instead of synthesizing raw RGB images and\ncompressing them separately. In the real world, most images and videos are\nstored and transferred in a compressed format to save storage capacity and data\ntransfer bandwidth. However, since typical generative adversarial networks\ngenerate raw RGB images, those generated images need to be compressed by a\npost-processing stage to reduce the data size. Among image compression methods,\nJPEG has been one of the most commonly used lossy compression methods for still\nimages. Hence, we propose a novel framework that generates JPEG compressed\nimages using generative adversarial networks. The novel generator consists of\nthe proposed locally connected layers, chroma subsampling layers, quantization\nlayers, residual blocks, and convolution layers. The locally connected layer is\nproposed to enable block-based operations. We also discuss training strategies\nfor the proposed architecture including the loss function and the\ntransformation between its generator and its discriminator. The proposed method\nis evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom\ndataset. The results demonstrate that the proposed method is able to generate\ncompressed data with competitive qualities. The proposed method is a promising\nbaseline method for joint image generation and compression using generative\nadversarial networks.",
        "This technical report presents panda-gym, a set Reinforcement Learning (RL)\nenvironments for the Franka Emika Panda robot integrated with OpenAI Gym. Five\ntasks are included: reach, push, slide, pick & place and stack. They all follow\na Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To\nfoster open-research, we chose to use the open-source physics engine PyBullet.\nThe implementation chosen for this package allows to define very easily new\ntasks or new robots. This report also presents a baseline of results obtained\nwith state-of-the-art model-free off-policy algorithms. panda-gym is\nopen-source at https://github.com/qgallouedec/panda-gym.",
        "We propose an algorithm for separating the foreground (mainly text and line\ngraphics) from the smoothly varying background in screen content images. The\nproposed method is designed based on the assumption that the background part of\nthe image is smoothly varying and can be represented by a linear combination of\na few smoothly varying basis functions, while the foreground text and graphics\ncreate sharp discontinuity and cannot be modeled by this smooth representation.\nThe algorithm separates the background and foreground using a least absolute\ndeviation method to fit the smooth model to the image pixels. This algorithm\nhas been tested on several images from HEVC standard test sequences for screen\ncontent coding, and is shown to have superior performance over other popular\nmethods, such as k-means clustering based segmentation in DjVu and shape\nprimitive extraction and coding (SPEC) algorithm. Such background/foreground\nsegmentation are important pre-processing steps for text extraction and\nseparate coding of background and foreground for compression of screen content\nimages.",
        "In this work, we address time-series forecasting as a computer vision task.\nWe capture input data as an image and train a model to produce the subsequent\nimage. This approach results in predicting distributions as opposed to\npointwise values. To assess the robustness and quality of our approach, we\nexamine various datasets and multiple evaluation metrics. Our experiments show\nthat our forecasting tool is effective for cyclic data but somewhat less for\nirregular data such as stock prices. Importantly, when using image-based\nevaluation metrics, we find our method to outperform various baselines,\nincluding ARIMA, and a numerical variation of our deep learning approach.",
        "A new automotive radar data set with measurements and point-wise annotations\nfrom more than four hours of driving is presented. Data provided by four series\nradar sensors mounted on one test vehicle were recorded and the individual\ndetections of dynamic objects were manually grouped to clusters and labeled\nafterwards. The purpose of this data set is to enable the development of novel\n(machine learning-based) radar perception algorithms with the focus on moving\nroad users. Images of the recorded sequences were captured using a documentary\ncamera. For the evaluation of future object detection and classification\nalgorithms, proposals for score calculation are made so that researchers can\nevaluate their algorithms on a common basis. Additional information as well as\ndownload instructions can be found on the website of the data set:\nwww.radar-scenes.com.",
        "Motivated by the vast success of deep convolutional networks, there is a\ngreat interest in generalizing convolutions to non-Euclidean manifolds. A major\ncomplication in comparison to flat spaces is that it is unclear in which\nalignment a convolution kernel should be applied on a manifold. The underlying\nreason for this ambiguity is that general manifolds do not come with a\ncanonical choice of reference frames (gauge). Kernels and features therefore\nhave to be expressed relative to arbitrary coordinates. We argue that the\nparticular choice of coordinatization should not affect a network's inference\n-- it should be coordinate independent. A simultaneous demand for coordinate\nindependence and weight sharing is shown to result in a requirement on the\nnetwork to be equivariant under local gauge transformations (changes of local\nreference frames). The ambiguity of reference frames depends thereby on the\nG-structure of the manifold, such that the necessary level of gauge\nequivariance is prescribed by the corresponding structure group G. Coordinate\nindependent convolutions are proven to be equivariant w.r.t. those isometries\nthat are symmetries of the G-structure. The resulting theory is formulated in a\ncoordinate free fashion in terms of fiber bundles. To exemplify the design of\ncoordinate independent convolutions, we implement a convolutional network on\nthe M\\\"obius strip. The generality of our differential geometric formulation of\nconvolutional networks is demonstrated by an extensive literature review which\nexplains a large number of Euclidean CNNs, spherical CNNs and CNNs on general\nsurfaces as specific instances of coordinate independent convolutions.",
        "Reinforcement learning offers the promise of automating the acquisition of\ncomplex behavioral skills. However, compared to commonly used and\nwell-understood supervised learning methods, reinforcement learning algorithms\ncan be brittle, difficult to use and tune, and sensitive to seemingly innocuous\nimplementation decisions. In contrast, imitation learning utilizes standard and\nwell-understood supervised learning methods, but requires near-optimal expert\ndata. Can we learn effective policies via supervised learning without\ndemonstrations? The main idea that we explore in this work is that non-expert\ntrajectories collected from sub-optimal policies can be viewed as optimal\nsupervision, not for maximizing the reward, but for matching the reward of the\ngiven trajectory. By then conditioning the policy on the numerical value of the\nreward, we can obtain a policy that generalizes to larger returns. We show how\nsuch an approach can be derived as a principled method for policy search,\ndiscuss several variants, and compare the method experimentally to a variety of\ncurrent reinforcement learning methods on standard benchmarks.",
        "Existing object detection-based text detectors mainly concentrate on\ndetecting horizontal and multioriented text. However, they do not pay enough\nattention to complex-shape text (curved or other irregularly shaped text).\nRecently, segmentation-based text detection methods have been introduced to\ndeal with the complex-shape text; however, the pixel level processing increases\nthe computational cost significantly. To further improve the accuracy and\nefficiency, we propose a novel detection framework for arbitrary-shape text\ndetection, termed as RayNet. RayNet uses Center Point Set (CPS) and Ray\nDistance (RD) to fit text, where CPS is used to determine the text general\nposition and the RD is combined with CPS to compute Ray Points (RP) to localize\nthe text accurate shape. Since RP are disordered, we develop the Ray Points\nConnection (RPC) algorithm to reorder RP, which significantly improves the\ndetection performance of complex-shape text. RayNet achieves impressive\nperformance on existing curved text dataset (CTW1500) and quadrangle text\ndataset (ICDAR2015), which demonstrate its superiority against several\nstate-of-the-art methods.",
        "Detecting novel objects without class information is not trivial, as it is\ndifficult to generalize from a small training set. This is an interesting\nproblem for underwater robotics, as modeling marine objects is inherently more\ndifficult in sonar images, and training data might not be available apriori.\nDetection proposals algorithms can be used for this purpose but usually\nrequires a large amount of output bounding boxes. In this paper we propose the\nuse of a fully convolutional neural network that regresses an objectness value\ndirectly from a Forward-Looking sonar image. By ranking objectness, we can\nproduce high recall (96 %) with only 100 proposals per image. In comparison,\nEdgeBoxes requires 5000 proposals to achieve a slightly better recall of 97 %,\nwhile Selective Search requires 2000 proposals to achieve 95 % recall. We also\nshow that our method outperforms a template matching baseline by a considerable\nmargin, and is able to generalize to completely new objects. We expect that\nthis kind of technique can be used in the field to find lost objects under the\nsea.",
        "Maintaining the long-term exploration capability of the agent remains one of\nthe critical challenges in deep reinforcement learning. A representative\nsolution is to leverage reward shaping to provide intrinsic rewards for the\nagent to encourage exploration. However, most existing methods suffer from\nvanishing intrinsic rewards, which cannot provide sustainable exploration\nincentives. Moreover, they rely heavily on complex models and additional memory\nto record learning procedures, resulting in high computational complexity and\nlow robustness. To tackle this problem, entropy-based methods are proposed to\nevaluate the global exploration performance, encouraging the agent to visit the\nstate space more equitably. However, the sample complexity of estimating the\nstate visitation entropy is prohibitive when handling environments with\nhigh-dimensional observations. In this paper, we introduce a novel metric\nentitled Jain's fairness index (JFI) to replace the entropy regularizer, which\nsolves the exploration problem from a brand new perspective. In sharp contrast\nto the entropy regularizer, JFI is more computable and robust and can be easily\napplied generalized into arbitrary tasks. Furthermore, we leverage a\nvariational auto-encoder (VAE) model to capture the life-long novelty of\nstates, which is combined with the global JFI score to form multimodal\nintrinsic rewards. Finally, extensive simulation results demonstrate that our\nmultimodal reward shaping (MMRS) method can achieve higher performance than\nother benchmark schemes.",
        "Unintended biases in machine learning (ML) models are among the major\nconcerns that must be addressed to maintain public trust in ML. In this paper,\nwe address process fairness of ML models that consists in reducing the\ndependence of models on sensitive features, without compromising their\nperformance. We revisit the framework FixOut that is inspired in the approach\n\"fairness through unawareness\" to build fairer models. We introduce several\nimprovements such as automating the choice of FixOut's parameters. Also, FixOut\nwas originally proposed to improve fairness of ML models on tabular data. We\nalso demonstrate the feasibility of FixOut's workflow for models on textual\ndata. We present several experimental results that illustrate the fact that\nFixOut improves process fairness on different classification settings.",
        "Safe reinforcement learning has been a promising approach for optimizing the\npolicy of an agent that operates in safety-critical applications. In this\npaper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov\ndecision processes under unknown safety constraints. Specifically, we take a\nstepwise approach for optimizing safety and cumulative reward. In our method,\nthe agent first learns safety constraints by expanding the safe region, and\nthen optimizes the cumulative reward in the certified safe region. We provide\ntheoretical guarantees on both the satisfaction of the safety constraint and\nthe near-optimality of the cumulative reward under proper regularity\nassumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP\nthrough two experiments: one uses a synthetic data in a new, openly-available\nenvironment named GP-SAFETY-GYM, and the other simulates Mars surface\nexploration by using real observation data.",
        "Prediction of spatio-temporal chaotic systems is important in various fields,\nsuch as Numerical Weather Prediction (NWP). While data assimilation methods\nhave been applied in NWP, machine learning techniques, such as Reservoir\nComputing (RC), are recently recognized as promising tools to predict\nspatio-temporal chaotic systems. However, the sensitivity of the skill of the\nmachine learning based prediction to the imperfectness of observations is\nunclear. In this study, we evaluate the skill of RC with noisy and sparsely\ndistributed observations. We intensively compare the performances of RC and\nLocal Ensemble Transform Kalman Filter (LETKF) by applying them to the\nprediction of the Lorenz 96 system. Although RC can successfully predict the\nLorenz 96 system if the system is perfectly observed, we find that RC is\nvulnerable to observation sparsity compared with LETKF. To overcome this\nlimitation of RC, we propose to combine LETKF and RC. In our proposed method,\nthe system is predicted by RC that learned the analysis time series estimated\nby LETKF. Our proposed method can successfully predict the Lorenz 96 system\nusing noisy and sparsely distributed observations. Most importantly, our method\ncan predict better than LETKF when the process-based model is imperfect.",
        "We propose a simple model selection approach for algorithms in stochastic\nbandit and reinforcement learning problems. As opposed to prior work that\n(implicitly) assumes knowledge of the optimal regret, we only require that each\nbase algorithm comes with a candidate regret bound that may or may not hold\nduring all rounds. In each round, our approach plays a base algorithm to keep\nthe candidate regret bounds of all remaining base algorithms balanced, and\neliminates algorithms that violate their candidate bound. We prove that the\ntotal regret of this approach is bounded by the best valid candidate regret\nbound times a multiplicative factor. This factor is reasonably small in several\napplications, including linear bandits and MDPs with nested function classes,\nlinear bandits with unknown misspecification, and LinUCB applied to linear\nbandits with different confidence parameters. We further show that, under a\nsuitable gap-assumption, this factor only scales with the number of base\nalgorithms and not their complexity when the number of rounds is large enough.\nFinally, unlike recent efforts in model selection for linear stochastic\nbandits, our approach is versatile enough to also cover cases where the context\ninformation is generated by an adversarial environment, rather than a\nstochastic one.",
        "Modern CNN-based object detectors assign anchors for ground-truth objects\nunder the restriction of object-anchor Intersection-over-Unit (IoU). In this\nstudy, we propose a learning-to-match approach to break IoU restriction,\nallowing objects to match anchors in a flexible manner. Our approach, referred\nto as FreeAnchor, updates hand-crafted anchor assignment to \"free\" anchor\nmatching by formulating detector training as a maximum likelihood estimation\n(MLE) procedure. FreeAnchor targets at learning features which best explain a\nclass of objects in terms of both classification and localization. FreeAnchor\nis implemented by optimizing detection customized likelihood and can be fused\nwith CNN-based detectors in a plug-and-play manner. Experiments on COCO\ndemonstrate that FreeAnchor consistently outperforms their counterparts with\nsignificant margins.",
        "In this paper, we address the problem of estimating dense depth from a\nsequence of images using deep neural networks. Specifically, we employ a\ndense-optical-flow network to compute correspondences and then triangulate the\npoint cloud to obtain an initial depth map.Parts of the point cloud, however,\nmay be less accurate than others due to lack of common observations or small\nparallax. To further increase the triangulation accuracy, we introduce a\ndepth-refinement network (DRN) that optimizes the initial depth map based on\nthe image's contextual cues. In particular, the DRN contains an iterative\nrefinement module (IRM) that improves the depth accuracy over iterations by\nrefining the deep features. Lastly, the DRN also predicts the uncertainty in\nthe refined depths, which is desirable in applications such as measurement\nselection for scene reconstruction. We show experimentally that our algorithm\noutperforms state-of-the-art approaches in terms of depth accuracy, and verify\nthat our predicted uncertainty is highly correlated to the actual depth error.",
        "Shapelets are phase independent subsequences designed for time series\nclassification. We propose three adaptations to the Shapelet Transform (ST) to\ncapture multivariate features in multivariate time series classification. We\ncreate a unified set of data to benchmark our work on, and compare with three\nother algorithms. We demonstrate that multivariate shapelets are not\nsignificantly worse than other state-of-the-art algorithms.",
        "In recent years, estimating the 6D pose of object instances with\nconvolutional neural network (CNN) has received considerable attention.\nDepending on whether intermediate cues are used, the relevant literature can be\nroughly divided into two broad categories: direct methods and two stage\npipelines. For the latter, intermediate cues, such as 3D object coordinates,\nsemantic keypoints, or virtual control points instead of pose parameters are\nregressed by CNN in the first stage. Object pose can then be solved by\ncorrespondence constraints constructed with these intermediate cues. In this\npaper, we focus on the postprocessing of a two-stage pipeline and propose to\ncombine two learning concepts for estimating object pose under challenging\nscenes: projection grouping on one side, and correspondence learning on the\nother. We firstly employ a local patch based method to predict projection\nheatmaps which denote the confidence distribution of projection of 3D bounding\nbox's corners. A projection grouping module is then proposed to remove\nredundant local maxima from each layer of heatmaps. Instead of directly feeding\n2D-3D correspondences to the perspective-n-point (PnP) algorithm, multiple\ncorrespondence hypotheses are sampled from local maxima and its corresponding\nneighborhood and ranked by a correspondence-evaluation network. Finally,\ncorrespondences with higher confidence are selected to determine object pose.\nExtensive experiments on three public datasets demonstrate that the proposed\nframework outperforms several state of the art methods.",
        "Graph Neural Networks (GNNs) are the subject of intense focus by the machine\nlearning community for problems involving relational reasoning. GNNs can be\nbroadly divided into spatial and spectral approaches. Spatial approaches use a\nform of learned message-passing, in which interactions among vertices are\ncomputed locally, and information propagates over longer distances on the graph\nwith greater numbers of message-passing steps. Spectral approaches use\neigendecompositions of the graph Laplacian to produce a generalization of\nspatial convolutions to graph structured data which access information over\nshort and long time scales simultaneously. Here we introduce the Spectral Graph\nNetwork, which applies message passing to both the spatial and spectral\ndomains. Our model projects vertices of the spatial graph onto the Laplacian\neigenvectors, which are each represented as vertices in a fully connected\n\"spectral graph\", and then applies learned message passing to them. We apply\nthis model to various benchmark tasks including a graph-based variant of MNIST\nclassification, molecular property prediction on MoleculeNet and QM9, and\nshortest path problems on random graphs. Our results show that the Spectral GN\npromotes efficient training, reaching high performance with fewer training\niterations despite having more parameters. The model also provides robustness\nto edge dropout and outperforms baselines for the classification tasks. We also\nexplore how these performance benefits depend on properties of the dataset.",
        "The increasing of pre-trained models has significantly facilitated the\nperformance on limited data tasks with transfer learning. However, progress on\ntransfer learning mainly focuses on optimizing the weights of pre-trained\nmodels, which ignores the structure mismatch between the model and the target\ntask. This paper aims to improve the transfer performance from another angle -\nin addition to tuning the weights, we tune the structure of pre-trained models,\nin order to better match the target task. To this end, we propose TransTailor,\ntargeting at pruning the pre-trained model for improved transfer learning.\nDifferent from traditional pruning pipelines, we prune and fine-tune the\npre-trained model according to the target-aware weight importance, generating\nan optimal sub-model tailored for a specific target task. In this way, we\ntransfer a more suitable sub-structure that can be applied during fine-tuning\nto benefit the final performance. Extensive experiments on multiple pre-trained\nmodels and datasets demonstrate that TransTailor outperforms the traditional\npruning methods and achieves competitive or even better performance than other\nstate-of-the-art transfer learning methods while using a smaller model.\nNotably, on the Stanford Dogs dataset, TransTailor can achieve 2.7% accuracy\nimprovement over other transfer methods with 20% fewer FLOPs.",
        "We propose a novel framework for cross-modal zero-shot learning (ZSL) in the\ncontext of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema\nmainly considers simultaneous mappings among the two image views and the\nsemantic side information. Therefore, it is desirable to consider fine-grained\nclasses mainly in the sketch domain using highly discriminative and\nsemantically rich feature space. However, the existing deep generative\nmodeling-based SBIR approaches majorly focus on bridging the gaps between the\nseen and unseen classes by generating pseudo-unseen-class samples. Besides,\nviolating the ZSL protocol by not utilizing any unseen-class information during\ntraining, such techniques do not pay explicit attention to modeling the\ndiscriminative nature of the shared space. Also, we note that learning a\nunified feature space for both the multi-view visual data is a tedious task\nconsidering the significant domain difference between sketches and color\nimages. In this respect, as a remedy, we introduce a novel framework for\nzero-shot SBIR. While we define a cross-modal triplet loss to ensure the\ndiscriminative nature of the shared space, an innovative cross-modal attention\nlearning strategy is also proposed to guide feature extraction from the image\ndomain exploiting information from the respective sketch counterpart. In order\nto preserve the semantic consistency of the shared space, we consider a graph\nCNN-based module that propagates the semantic class topology to the shared\nspace. To ensure an improved response time during inference, we further explore\nthe possibility of representing the shared space in terms of hash codes.\nExperimental results obtained on the benchmark TU-Berlin and the Sketchy\ndatasets confirm the superiority of CrossATNet in yielding state-of-the-art\nresults.",
        "Convolutional neural networks (CNN) have achieved great success in analyzing\ntropical cyclones (TC) with satellite images in several tasks, such as TC\nintensity estimation. In contrast, TC structure, which is conventionally\ndescribed by a few parameters estimated subjectively by meteorology\nspecialists, is still hard to be profiled objectively and routinely. This study\napplies CNN on satellite images to create the entire TC structure profiles,\ncovering all the structural parameters. By utilizing the meteorological domain\nknowledge to construct TC wind profiles based on historical structure\nparameters, we provide valuable labels for training in our newly released\nbenchmark dataset. With such a dataset, we hope to attract more attention to\nthis crucial issue among data scientists. Meanwhile, a baseline is established\nwith a specialized convolutional model operating on polar-coordinates. We\ndiscovered that it is more feasible and physically reasonable to extract\nstructural information on polar-coordinates, instead of Cartesian coordinates,\naccording to a TC's rotational and spiral natures. Experimental results on the\nreleased benchmark dataset verified the robustness of the proposed model and\ndemonstrated the potential for applying deep learning techniques for this\nbarely developed yet important topic.",
        "Probabilistic time-series models become popular in the forecasting field as\nthey help to make optimal decisions under uncertainty. Despite the growing\ninterest, a lack of thorough analysis hinders choosing what is worth applying\nfor the desired task. In this paper, we analyze the performance of three\nprominent probabilistic time-series models for sales forecasting. To remove the\nrole of random chance in architecture's performance, we make two experimental\nprinciples; 1) Large-scale dataset with various cross-validation sets. 2) A\nstandardized training and hyperparameter selection. The experimental results\nshow that a simple Multi-layer Perceptron and Linear Regression outperform the\nprobabilistic models on RMSE without any feature engineering. Overall, the\nprobabilistic models fail to achieve better performance on point estimation,\nsuch as RMSE and MAPE, than comparably simple baselines. We analyze and discuss\nthe performances of probabilistic time-series models.",
        "Recent deep networks have achieved good performance on a variety of 3d points\nclassification tasks. However, these models often face challenges in \"wild\ntasks\".There are considerable differences between the labeled training/source\ndata collected by one Lidar and unseen test/target data collected by a\ndifferent Lidar. Unsupervised domain adaptation (UDA) seeks to overcome such a\nproblem without target domain labels.Instead of aligning features between\nsource data and target data,we propose a method that use a Generative\nadversarial network to generate synthetic data from the source domain so that\nthe output is close to the target domain.Experiments show that our approach\nperforms better than other state-of-the-art UDA methods in three popular 3D\nobject/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain\n3D objects classification.",
        "The problem of linking functional connectomics to behavior is extremely\nchallenging due to the complex interactions between the two distinct, but\nrelated, data domains. We propose a coupled manifold optimization framework\nwhich projects fMRI data onto a low dimensional matrix manifold common to the\ncohort. The patient specific loadings simultaneously map onto a behavioral\nmeasure of interest via a second, non-linear, manifold. By leveraging the\nkernel trick, we can optimize over a potentially infinite dimensional space\nwithout explicitly computing the embeddings. As opposed to conventional\nmanifold learning, which assumes a fixed input representation, our framework\ndirectly optimizes for embedding directions that predict behavior. Our\noptimization algorithm combines proximal gradient descent with the trust region\nmethod, which has good convergence guarantees. We validate our framework on\nresting state fMRI from fifty-eight patients with Autism Spectrum Disorder\nusing three distinct measures of clinical severity. Our method outperforms\ntraditional representation learning techniques in a cross validated setting,\nthus demonstrating the predictive power of our coupled objective.",
        "Traditional deep generative models of images and other spatial modalities can\nonly generate fixed sized outputs. The generated images have exactly the same\nresolution as the training images, which is dictated by the number of layers in\nthe underlying neural network. Recent work has shown, however, that feeding\nspatial noise vectors into a fully convolutional neural network enables both\ngeneration of arbitrary resolution output images as well as training on\narbitrary resolution training images. While this work has provided impressive\nempirical results, little theoretical interpretation was provided to explain\nthe underlying generative process. In this paper we provide a firm theoretical\ninterpretation for infinite spatial generation, by drawing connections to\nspatial stochastic processes. We use the resulting intuition to improve upon\nexisting spatially infinite generative models to enable more efficient training\nthrough a model that we call an infinite generative adversarial network, or\n$\\infty$-GAN. Experiments on world map generation, panoramic images and texture\nsynthesis verify the ability of $\\infty$-GAN to efficiently generate images of\narbitrary size.",
        "Transfer learning research attempts to make model induction transferable\nacross different domains. This method assumes that specific information\nregarding to which domain each instance belongs is known. This paper helps to\nextend the capability of transfer learning for linear regression problems to\nsituations where the domain information is uncertain or unknown; in fact, the\nframework can be extended to classification problems. For normal datasets, we\nassume that some latent domain information is available for transfer learning.\nThe instances in each domain can be inferred by different parameters. We obtain\nthis domain information from the distribution of the regression coefficients\ncorresponding to the explanatory variable $x$ as well as the response variable\n$y$ based on a Dirichlet process, which is more reasonable. As a result, we\ntransfer not only variable $x$ as usual but also variable $y$, which is\nchallenging since the testing data have no response value. Previous work mainly\novercomes the problem via pseudo-labelling based on transductive learning,\nwhich introduces serious bias. We provide a novel framework for analysing the\nproblem and considering this general situation: the joint distribution of\nvariable $x$ and variable $y$. Furthermore, our method controls the bias well\ncompared with previous work. We perform linear regression on the new feature\nspace that consists of different latent domains and the target domain, which is\nfrom the testing data. The experimental results show that the proposed model\nperforms well on real datasets.",
        "The existence of redundancy in Convolutional Neural Networks (CNNs) enables\nus to remove some filters/channels with acceptable performance drops. However,\nthe training objective of CNNs usually tends to minimize an accuracy-related\nloss function without any attention paid to the redundancy, making the\nredundancy distribute randomly on all the filters, such that removing any of\nthem may trigger information loss and accuracy drop, necessitating a following\nfinetuning step for recovery. In this paper, we propose to manipulate the\nredundancy during training to facilitate network pruning. To this end, we\npropose a novel Centripetal SGD (C-SGD) to make some filters identical,\nresulting in ideal redundancy patterns, as such filters become purely redundant\ndue to their duplicates; hence removing them does not harm the network. As\nshown on CIFAR and ImageNet, C-SGD delivers better performance because the\nredundancy is better organized, compared to the existing methods. The\nefficiency also characterizes C-SGD because it is as fast as regular SGD,\nrequires no finetuning, and can be conducted simultaneously on all the layers\neven in very deep CNNs. Besides, C-SGD can improve the accuracy of CNNs by\nfirst training a model with the same architecture but wider layers then\nsqueezing it into the original width.",
        "We introduce a deep generative network for 3D shape detailization, akin to\nstylization with the style being geometric details. We address the challenge of\ncreating large varieties of high-resolution and detailed 3D geometry from a\nsmall set of exemplars by treating the problem as that of geometric detail\ntransfer. Given a low-resolution coarse voxel shape, our network refines it,\nvia voxel upsampling, into a higher-resolution shape enriched with geometric\ndetails. The output shape preserves the overall structure (or content) of the\ninput, while its detail generation is conditioned on an input \"style code\"\ncorresponding to a detailed exemplar. Our 3D detailization via conditional\nrefinement is realized by a generative adversarial network, coined DECOR-GAN.\nThe network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D\nPatchGAN discriminator to enforce local patches of the generated model to be\nsimilar to those in the training detailed shapes. During testing, a style code\nis fed into the generator to condition the refinement. We demonstrate that our\nmethod can refine a coarse shape into a variety of detailed shapes with\ndifferent styles. The generated results are evaluated in terms of content\npreservation, plausibility, and diversity. Comprehensive ablation studies are\nconducted to validate our network designs. Code is available at\nhttps://github.com/czq142857/DECOR-GAN.",
        "Customer behavior is often assumed to follow weak rationality, which implies\nthat adding a product to an assortment will not increase the choice probability\nof another product in that assortment. However, an increasing amount of\nresearch has revealed that customers are not necessarily rational when making\ndecisions. In this paper, we propose a new nonparametric choice model that\nrelaxes this assumption and can model a wider range of customer behavior, such\nas decoy effects between products. In this model, each customer type is\nassociated with a binary decision tree, which represents a decision process for\nmaking a purchase based on checking for the existence of specific products in\nthe assortment. Together with a probability distribution over customer types,\nwe show that the resulting model -- a decision forest -- is able to represent\nany customer choice model, including models that are inconsistent with weak\nrationality. We theoretically characterize the depth of the forest needed to\nfit a data set of historical assortments and prove that with high probability,\na forest whose depth scales logarithmically in the number of assortments is\nsufficient to fit most data sets. We also propose two practical algorithms --\none based on column generation and one based on random sampling -- for\nestimating such models from data. Using synthetic data and real transaction\ndata exhibiting non-rational behavior, we show that the model outperforms both\nrational and non-rational benchmark models in out-of-sample predictive ability.",
        "Transfer learning can significantly improve the sample efficiency of neural\nnetworks, by exploiting the relatedness between a data-scarce target task and a\ndata-abundant source task. Despite years of successful applications, transfer\nlearning practice often relies on ad-hoc solutions, while theoretical\nunderstanding of these procedures is still limited. In the present work, we\nre-think a solvable model of synthetic data as a framework for modeling\ncorrelation between data-sets. This setup allows for an analytic\ncharacterization of the generalization performance obtained when transferring\nthe learned feature map from the source to the target task. Focusing on the\nproblem of training two-layer networks in a binary classification setting, we\nshow that our model can capture a range of salient features of transfer\nlearning with real data. Moreover, by exploiting parametric control over the\ncorrelation between the two data-sets, we systematically investigate under\nwhich conditions the transfer of features is beneficial for generalization.",
        "Inspired by the adaptation phenomenon of neuronal firing, we propose the\nregularity normalization (RN) as an unsupervised attention mechanism (UAM)\nwhich computes the statistical regularity in the implicit space of neural\nnetworks under the Minimum Description Length (MDL) principle. Treating the\nneural network optimization process as a partially observable model selection\nproblem, UAM constrains the implicit space by a normalization factor, the\nuniversal code length. We compute this universal code incrementally across\nneural network layers and demonstrated the flexibility to include data priors\nsuch as top-down attention and other oracle information. Empirically, our\napproach outperforms existing normalization methods in tackling limited,\nimbalanced and non-stationary input distribution in image classification,\nclassic control, procedurally-generated reinforcement learning, generative\nmodeling, handwriting generation and question answering tasks with various\nneural network architectures. Lastly, UAM tracks dependency and critical\nlearning stages across layers and recurrent time steps of deep networks.",
        "Graph convolutional network (GCN) based approaches have achieved significant\nprogress for solving complex, graph-structured problems. GCNs incorporate the\ngraph structure information and the node (or edge) features through message\npassing and computes 'deep' node representations. Despite significant progress\nin the field, designing GCN architectures for heterogeneous graphs still\nremains an open challenge. Due to the schema of a heterogeneous graph, useful\ninformation may reside multiple hops away. A key question is how to perform\nmessage passing to incorporate information of neighbors multiple hops away\nwhile avoiding the well-known over-smoothing problem in GCNs. To address this\nquestion, we propose our GCN framework 'Deep Heterogeneous Graph Convolutional\nNetwork (DHGCN)', which takes advantage of the schema of a heterogeneous graph\nand uses a hierarchical approach to effectively utilize information many hops\naway. It first computes representations of the target nodes based on their\n'schema-derived ego-network' (SEN). It then links the nodes of the same type\nwith various pre-defined metapaths and performs message passing along these\nlinks to compute final node representations. Our design choices naturally\ncapture the way a heterogeneous graph is generated from the schema. The\nexperimental results on real and synthetic datasets corroborate the design\nchoice and illustrate the performance gains relative to competing alternatives.",
        "Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with\nnetworked sensors and actuators that are targets for cyber-attacks.\nConventional detection techniques are unable to deal with the increasingly\ndynamic and complex nature of the CPSs. On the other hand, the networked\nsensors and actuators generate large amounts of data streams that can be\ncontinuously monitored for intrusion events. Unsupervised machine learning\ntechniques can be used to model the system behaviour and classify deviant\nbehaviours as possible attacks. In this work, we proposed a novel Generative\nAdversarial Networks-based Anomaly Detection (GAN-AD) method for such complex\nnetworked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the\nmultivariate time series of the sensors and actuators under normal working\nconditions of a CPS. Instead of treating each sensor's and actuator's time\nseries independently, we model the time series of multiple sensors and\nactuators in the CPS concurrently to take into account of potential latent\ninteractions between them. To exploit both the generator and the discriminator\nof our GAN, we deployed the GAN-trained discriminator together with the\nresiduals between generator-reconstructed data and the actual samples to detect\npossible anomalies in the complex CPS. We used our GAN-AD to distinguish\nabnormal attacked situations from normal working conditions for a complex\nsix-stage Secure Water Treatment (SWaT) system. Experimental results showed\nthat the proposed strategy is effective in identifying anomalies caused by\nvarious attacks with high detection rate and low false positive rate as\ncompared to existing methods.",
        "Blind image deblurring is a fundamental and challenging computer vision\nproblem, which aims to recover both the blur kernel and the latent sharp image\nfrom only a blurry observation. Despite the superiority of deep learning\nmethods in image deblurring have displayed, there still exists major challenge\nwith various non-uniform motion blur. Previous methods simply take all the\nimage features as the input to the decoder, which handles different degrees\n(e.g. large blur, small blur) simultaneously, leading to challenges for sharp\nimage generation. To tackle the above problems, we present a deep two-branch\nnetwork to deal with blurry images via a component divided module, which\ndivides an image into two components based on the representation of blurry\ndegree. Specifically, two component attentive blocks are employed to learn\nattention maps to exploit useful deblurring feature representations on both\nlarge and small blurry regions. Then, the blur-aware features are fed into\ntwo-branch reconstruction decoders respectively. In addition, a new feature\nfusion mechanism, orientation-based feature fusion, is proposed to merge sharp\nfeatures of the two branches. Both qualitative and quantitative experimental\nresults show that our method performs favorably against the state-of-the-art\napproaches.",
        "Depth estimation is a fundamental issue in 4-D light field processing and\nanalysis. Although recent supervised learning-based light field depth\nestimation methods have significantly improved the accuracy and efficiency of\ntraditional optimization-based ones, these methods rely on the training over\nlight field data with ground-truth depth maps which are challenging to obtain\nor even unavailable for real-world light field data. Besides, due to the\ninevitable gap (or domain difference) between real-world and synthetic data,\nthey may suffer from serious performance degradation when generalizing the\nmodels trained with synthetic data to real-world data. By contrast, we propose\nan unsupervised learning-based method, which does not require ground-truth\ndepth as supervision during training. Specifically, based on the basic\nknowledge of the unique geometry structure of light field data, we present an\nocclusion-aware strategy to improve the accuracy on occlusion areas, in which\nwe explore the angular coherence among subsets of the light field views to\nestimate initial depth maps, and utilize a constrained unsupervised loss to\nlearn their corresponding reliability for final depth prediction. Additionally,\nwe adopt a multi-scale network with a weighted smoothness loss to handle the\ntextureless areas. Experimental results on synthetic data show that our method\ncan significantly shrink the performance gap between the previous unsupervised\nmethod and supervised ones, and produce depth maps with comparable accuracy to\ntraditional methods with obviously reduced computational cost. Moreover,\nexperiments on real-world datasets show that our method can avoid the domain\nshift problem presented in supervised methods, demonstrating the great\npotential of our method.",
        "Medical image datasets are usually imbalanced, due to the high costs of\nobtaining the data and time-consuming annotations. Training deep neural network\nmodels on such datasets to accurately classify the medical condition does not\nyield desired results and often over-fits the data on majority class samples.\nIn order to address this issue, data augmentation is often performed on\ntraining data by position augmentation techniques such as scaling, cropping,\nflipping, padding, rotation, translation, affine transformation, and color\naugmentation techniques such as brightness, contrast, saturation, and hue to\nincrease the dataset sizes. These augmentation techniques are not guaranteed to\nbe advantageous in domains with limited data, especially medical image data,\nand could lead to further overfitting. In this work, we performed data\naugmentation on the Chest X-rays dataset through generative modeling (deep\nconvolutional generative adversarial network) which creates artificial\ninstances retaining similar characteristics to the original data and evaluation\nof the model resulted in Fr\\'echet Distance of Inception (FID) score of 1.289.",
        "Histopathological prognostication of neoplasia including most tumor grading\nsystems are based upon a number of criteria. Probably the most important is the\nnumber of mitotic figures which are most commonly determined as the mitotic\ncount (MC), i.e. number of mitotic figures within 10 consecutive high power\nfields. Often the area with the highest mitotic activity is to be selected for\nthe MC. However, since mitotic activity is not known in advance, an arbitrary\nchoice of this region is considered one important cause for high variability in\nthe prognostication and grading.\n  In this work, we present an algorithmic approach that first calculates a\nmitotic cell map based upon a deep convolutional network. This map is in a\nsecond step used to construct a mitotic activity estimate. Lastly, we select\nthe image segment representing the size of ten high power fields with the\noverall highest mitotic activity as a region proposal for an expert MC\ndetermination. We evaluate the approach using a dataset of 32 completely\nannotated whole slide images, where 22 were used for training of the network\nand 10 for test. We find a correlation of r=0.936 in mitotic count estimate.",
        "Pre-trained word embeddings encode general word semantics and lexical\nregularities of natural language, and have proven useful across many NLP tasks,\nincluding word sense disambiguation, machine translation, and sentiment\nanalysis, to name a few. In supervised tasks such as multiclass text\nclassification (the focus of this article) it seems appealing to enhance word\nrepresentations with ad-hoc embeddings that encode task-specific information.\nWe propose (supervised) word-class embeddings (WCEs), and show that, when\nconcatenated to (unsupervised) pre-trained word embeddings, they substantially\nfacilitate the training of deep-learning models in multiclass classification by\ntopic. We show empirical evidence that WCEs yield a consistent improvement in\nmulticlass classification accuracy, using four popular neural architectures and\nsix widely used and publicly available datasets for multiclass text\nclassification. Our code that implements WCEs is publicly available at\nhttps://github.com/AlexMoreo/word-class-embeddings",
        "Segmentation is the process of partitioning a digital image into multiple\nsegments (sets of pixels). Such common segmentation tasks including segmenting\nwritten text or segmenting tumors from healthy brain tissue in an MRI image,\netc. Chan-Vese model for active contours is a powerful and flexible method\nwhich is able to segment many types of images, including some that would be\nquite difficult to segment in means of \"classical\" segmentation - i.e., using\nthresholding or gradient based methods. This model is based on the Mumford-Shah\nfunctional for segmentation, and is used widely in the medical imaging field,\nespecially for the segmentation of the brain, heart and trachea. The model is\nbased on an energy minimization problem, which can be reformulated in the level\nset formulation, leading to an easier way to solve the problem. In this\nproject, the model will be presented (there is an extension to color\n(vector-valued) images, but it will not be considered here), and Matlab code\nthat implements it will be introduced.",
        "The work investigates deep generative models, which allow us to use training\ndata from one domain to build a model for another domain. We propose the\nVariational Bi-domain Triplet Autoencoder (VBTA) that learns a joint\ndistribution of objects from different domains. We extend the VBTAs objective\nfunction by the relative constraints or triplets that sampled from the shared\nlatent space across domains. In other words, we combine the deep generative\nmodels with a metric learning ideas in order to improve the final objective\nwith the triplets information. The performance of the VBTA model is\ndemonstrated on different tasks: image-to-image translation, bi-directional\nimage generation and cross-lingual document classification.",
        "While Semi-supervised learning has gained much attention in computer vision\non image data, yet limited research exists on its applicability in the time\nseries domain. In this work, we investigate the transferability of\nstate-of-the-art deep semi-supervised models from image to time series\nclassification. We discuss the necessary model adaptations, in particular an\nappropriate model backbone architecture and the use of tailored data\naugmentation strategies. Based on these adaptations, we explore the potential\nof deep semi-supervised learning in the context of time series classification\nby evaluating our methods on large public time series classification problems\nwith varying amounts of labelled samples. We perform extensive comparisons\nunder a decidedly realistic and appropriate evaluation scheme with a unified\nreimplementation of all algorithms considered, which is yet lacking in the\nfield. We find that these transferred semi-supervised models show significant\nperformance gains over strong supervised, semi-supervised and self-supervised\nalternatives, especially for scenarios with very few labelled samples.",
        "The parameters of a neural network are naturally organized in groups, some of\nwhich might not contribute to its overall performance. To prune out unimportant\ngroups of parameters, we can include some non-differentiable penalty to the\nobjective function, and minimize it using proximal gradient methods. In this\npaper, we derive the weighted proximal operator, which is a necessary component\nof these proximal methods, of two structured sparsity inducing penalties.\nMoreover, they can be approximated efficiently with a numerical solver, and\ndespite this approximation, we prove that existing convergence guarantees are\npreserved when these operators are integrated as part of a generic adaptive\nproximal method. Finally, we show that this adaptive method, together with the\nweighted proximal operators derived here, is indeed capable of finding\nsolutions with structure in their sparsity patterns, on representative examples\nfrom computer vision and natural language processing.",
        "We explore the feasibility of combining Graph Neural Network-based policy\narchitectures with Deep Reinforcement Learning as an approach to problems in\nsystems. This fits particularly well with operations on networks, which\nnaturally take the form of graphs. As a case study, we take the idea of\ndata-driven routing in intradomain traffic engineering, whereby the routing of\ndata in a network can be managed taking into account the data itself. The\nparticular subproblem which we examine is minimising link congestion in\nnetworks using knowledge of historic traffic flows. We show through experiments\nthat an approach using Graph Neural Networks (GNNs) performs at least as well\nas previous work using Multilayer Perceptron architectures. GNNs have the added\nbenefit that they allow for the generalisation of trained agents to different\nnetwork topologies with no extra work. Furthermore, we believe that this\ntechnique is applicable to a far wider selection of problems in systems\nresearch.",
        "Accurate demand forecasts can help on-line retail organizations better plan\ntheir supply-chain processes. The challenge, however, is the large number of\nassociative factors that result in large, non-stationary shifts in demand,\nwhich traditional time series and regression approaches fail to model. In this\npaper, we propose a Neural Network architecture called AR-MDN, that\nsimultaneously models associative factors, time-series trends and the variance\nin the demand. We first identify several causal features and use a combination\nof feature embeddings, MLP and LSTM to represent them. We then model the output\ndensity as a learned mixture of Gaussian distributions. The AR-MDN can be\ntrained end-to-end without the need for additional supervision. We experiment\non a dataset of an year's worth of data over tens-of-thousands of products from\nFlipkart. The proposed architecture yields a significant improvement in\nforecasting accuracy when compared with existing alternatives.",
        "We consider learning based methods for visual localization that do not\nrequire the construction of explicit maps in the form of point clouds or\nvoxels. The goal is to learn an implicit representation of the environment at a\nhigher, more abstract level. We propose to use a generative approach based on\nGenerative Query Networks (GQNs, Eslami et al. 2018), asking the following\nquestions: 1) Can GQN capture more complex scenes than those it was originally\ndemonstrated on? 2) Can GQN be used for localization in those scenes? To study\nthis approach we consider procedurally generated Minecraft worlds, for which we\ncan generate images of complex 3D scenes along with camera pose coordinates. We\nfirst show that GQNs, enhanced with a novel attention mechanism can capture the\nstructure of 3D scenes in Minecraft, as evidenced by their samples. We then\napply the models to the localization problem, comparing the results to a\ndiscriminative baseline, and comparing the ways each approach captures the task\nuncertainty.",
        "Connecting multiple machine learning models into a pipeline is effective for\nhandling complex problems. By breaking down the problem into steps, each\ntackled by a specific component model of the pipeline, the overall solution can\nbe made accurate and explainable. This paper describes an enhancement of object\ndetection based on this multi-step concept, where a post-processing step called\nthe calibration model is introduced. The calibration model consists of a\nconvolutional neural network, and utilizes rich contextual information based on\nthe domain knowledge of the input. Improvements of object detection performance\nby 0.8-1.9 in average precision metric over existing object detectors have been\nobserved using the new model.",
        "Deep neural networks (DNNs) often suffer from \"catastrophic forgetting\"\nduring incremental learning (IL) --- an abrupt degradation of performance on\nthe original set of classes when the training objective is adapted to a newly\nadded set of classes. Existing IL approaches tend to produce a model that is\nbiased towards either the old classes or new classes, unless with the help of\nexemplars of the old data. To address this issue, we propose a\nclass-incremental learning paradigm called Deep Model Consolidation (DMC),\nwhich works well even when the original training data is not available. The\nidea is to first train a separate model only for the new classes, and then\ncombine the two individual models trained on data of two distinct set of\nclasses (old classes and new classes) via a novel double distillation training\nobjective. The two existing models are consolidated by exploiting publicly\navailable unlabeled auxiliary data. This overcomes the potential difficulties\ndue to the unavailability of original training data. Compared to the\nstate-of-the-art techniques, DMC demonstrates significantly better performance\nin image classification (CIFAR-100 and CUB-200) and object detection (PASCAL\nVOC 2007) in the single-headed IL setting.",
        "As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a post\nhoc manner. In this work, we analyze two popular post hoc interpretation\ntechniques: SmoothGrad which is a gradient based method, and a variant of LIME\nwhich is a perturbation based method. More specifically, we derive explicit\nclosed form expressions for the explanations output by these two methods and\nshow that they both converge to the same explanation in expectation, i.e., when\nthe number of perturbed samples used by these methods is large. We then\nleverage this connection to establish other desirable properties, such as\nrobustness, for these techniques. We also derive finite sample complexity\nbounds for the number of perturbations required for these methods to converge\nto their expected explanation. Finally, we empirically validate our theory\nusing extensive experimentation on both synthetic and real world datasets.",
        "The goal of our work is to use visual attention to enhance autonomous driving\nperformance. We present two methods of predicting visual attention maps. The\nfirst method is a supervised learning approach in which we collect eye-gaze\ndata for the task of driving and use this to train a model for predicting the\nattention map. The second method is a novel unsupervised approach where we\ntrain a model to learn to predict attention as it learns to drive a car.\nFinally, we present a comparative study of our results and show that the\nsupervised approach for predicting attention when incorporated performs better\nthan other approaches.",
        "Recent work has proven the effectiveness of transformers in many computer\nvision tasks. However, the performance of transformers in gaze estimation is\nstill unexplored. In this paper, we employ transformers and assess their\neffectiveness for gaze estimation. We consider two forms of vision transformer\nwhich are pure transformers and hybrid transformers. We first follow the\npopular ViT and employ a pure transformer to estimate gaze from images. On the\nother hand, we preserve the convolutional layers and integrate CNNs as well as\ntransformers. The transformer serves as a component to complement CNNs. We\ncompare the performance of the two transformers in gaze estimation. The Hybrid\ntransformer significantly outperforms the pure transformer in all evaluation\ndatasets with less parameters. We further conduct experiments to assess the\neffectiveness of the hybrid transformer and explore the advantage of\nself-attention mechanism. Experiments show the hybrid transformer can achieve\nstate-of-the-art performance in all benchmarks with pre-training.To facilitate\nfurther research, we release codes and models in\nhttps://github.com/yihuacheng/GazeTR.",
        "In this paper, we provide a novel dataset designed for camera invariant color\nconstancy research. Camera invariance corresponds to the robustness of an\nalgorithm's performance when run on images of the same scene taken by different\ncameras. Accordingly, images in the database correspond to several lab and\nfield scenes each of which are captured by three different cameras with minimal\nregistration errors. The lab scenes are also captured under five different\nilluminations. The spectral responses of cameras and the spectral power\ndistributions of the lab light sources are also provided, as they may prove\nbeneficial for training future algorithms to achieve color constancy. For a\nfair evaluation of future methods, we provide guidelines for supervised methods\nwith indicated training, validation and testing partitions. Accordingly, we\nevaluate a recently proposed convolutional neural network based color constancy\nalgorithm as a baseline for future research. As a side contribution, this\ndataset also includes images taken by a mobile camera with color shading\ncorrected and uncorrected results. This allows research on the effect of color\nshading as well.",
        "Given a textual description of an image, phrase grounding localizes objects\nin the image referred by query phrases in the description. State-of-the-art\nmethods address the problem by ranking a set of proposals based on the\nrelevance to each query, which are limited by the performance of independent\nproposal generation systems and ignore useful cues from context in the\ndescription. In this paper, we adopt a spatial regression method to break the\nperformance limit, and introduce reinforcement learning techniques to further\nleverage semantic context information. We propose a novel Query-guided\nRegression network with Context policy (QRC Net) which jointly learns a\nProposal Generation Network (PGN), a Query-guided Regression Network (QRN) and\na Context Policy Network (CPN). Experiments show QRC Net provides a significant\nimprovement in accuracy on two popular datasets: Flickr30K Entities and Referit\nGame, with 14.25% and 17.14% increase over the state-of-the-arts respectively.",
        "Instance discriminative self-supervised representation learning has been\nattracted attention thanks to its unsupervised nature and informative feature\nrepresentation for downstream tasks. In practice, it commonly uses a larger\nnumber of negative samples than the number of supervised classes. However,\nthere is an inconsistency in the existing analysis; theoretically, a large\nnumber of negative samples degrade classification performance on a downstream\nsupervised task, while empirically, they improve the performance. We provide a\nnovel framework to analyze this empirical result regarding negative samples\nusing the coupon collector's problem. Our bound can implicitly incorporate the\nsupervised loss of the downstream task in the self-supervised loss by\nincreasing the number of negative samples. We confirm that our proposed\nanalysis holds on real-world benchmark datasets.",
        "Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions. To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance.",
        "We present SPSG, a novel approach to generate high-quality, colored 3D models\nof scenes from RGB-D scan observations by learning to infer unobserved scene\ngeometry and color in a self-supervised fashion. Our self-supervised approach\nlearns to jointly inpaint geometry and color by correlating an incomplete RGB-D\nscan with a more complete version of that scan. Notably, rather than relying on\n3D reconstruction losses to inform our 3D geometry and color reconstruction, we\npropose adversarial and perceptual losses operating on 2D renderings in order\nto achieve high-resolution, high-quality colored reconstructions of scenes.\nThis exploits the high-resolution, self-consistent signal from individual raw\nRGB-D frames, in contrast to fused 3D reconstructions of the frames which\nexhibit inconsistencies from view-dependent effects, such as color balancing or\npose inconsistencies. Thus, by informing our 3D scene generation directly\nthrough 2D signal, we produce high-quality colored reconstructions of 3D\nscenes, outperforming state of the art on both synthetic and real data.",
        "The teacher-student (T/S) learning has been shown to be effective for a\nvariety of problems such as domain adaptation and model compression. One\nshortcoming of the T/S learning is that a teacher model, not always perfect,\nsporadically produces wrong guidance in form of posterior probabilities that\nmisleads the student model towards a suboptimal performance. To overcome this\nproblem, we propose a conditional T/S learning scheme, in which a \"smart\"\nstudent model selectively chooses to learn from either the teacher model or the\nground truth labels conditioned on whether the teacher can correctly predict\nthe ground truth. Unlike a naive linear combination of the two knowledge\nsources, the conditional learning is exclusively engaged with the teacher model\nwhen the teacher model's prediction is correct, and otherwise backs off to the\nground truth. Thus, the student model is able to learn effectively from the\nteacher and even potentially surpass the teacher. We examine the proposed\nlearning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker\nadaptation on Microsoft short message dictation dataset. The proposed method\nachieves 9.8% and 12.8% relative word error rate reductions, respectively, over\nT/S learning for environment adaptation and speaker-independent model for\nspeaker adaptation.",
        "World models improve a learning agent's ability to efficiently operate in\ninteractive and situated environments. This work focuses on the task of\nbuilding world models of text-based game environments. Text-based games, or\ninteractive narratives, are reinforcement learning environments in which agents\nperceive and interact with the world using textual natural language. These\nenvironments contain long, multi-step puzzles or quests woven through a world\nthat is filled with hundreds of characters, locations, and objects. Our world\nmodel learns to simultaneously: (1) predict changes in the world caused by an\nagent's actions when representing the world as a knowledge graph; and (2)\ngenerate the set of contextually relevant natural language actions required to\noperate in the world. We frame this task as a Set of Sequences generation\nproblem by exploiting the inherent structure of knowledge graphs and actions\nand introduce both a transformer-based multi-task architecture and a loss\nfunction to train it. A zero-shot ablation study on never-before-seen textual\nworlds shows that our methodology significantly outperforms existing textual\nworld modeling techniques as well as the importance of each of our\ncontributions.",
        "In this paper, we present a new reinforcement learning (RL) algorithm called\nDistributional Soft Actor Critic (DSAC), which exploits the distributional\ninformation of accumulated rewards to achieve better performance. Seamlessly\nintegrating SAC (which uses entropy to encourage exploration) with a principled\ndistributional view of the underlying objective, DSAC takes into consideration\nthe randomness in both action and rewards, and beats the state-of-the-art\nbaselines in several continuous control benchmarks. Moreover, with the\ndistributional information of rewards, we propose a unified framework for\nrisk-sensitive learning, one that goes beyond maximizing only expected\naccumulated rewards. Under this framework we discuss three specific\nrisk-related metrics: percentile, mean-variance and distorted expectation. Our\nextensive experiments demonstrate that with distribution modeling in RL, the\nagent performs better for both risk-averse and risk-seeking control tasks.",
        "Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.",
        "In this paper, we present a novel approach for incorporating external\nknowledge in Recurrent Neural Networks (RNNs). We propose the integration of\nlexicon features into the self-attention mechanism of RNN-based architectures.\nThis form of conditioning on the attention distribution, enforces the\ncontribution of the most salient words for the task at hand. We introduce three\nmethods, namely attentional concatenation, feature-based gating and affine\ntransformation. Experiments on six benchmark datasets show the effectiveness of\nour methods. Attentional feature-based gating yields consistent performance\nimprovement across tasks. Our approach is implemented as a simple add-on module\nfor RNN-based models with minimal computational overhead and can be adapted to\nany deep neural architecture.",
        "Forecasting multivariate time series is challenging as the variables are\nintertwined in time and space, like in the case of traffic signals. Defining\nsignals on graphs relaxes such complexities by representing the evolution of\nsignals over a space using relevant graph kernels such as the heat diffusion\nkernel. However, this kernel alone does not fully capture the actual dynamics\nof the data as it only relies on the graph structure. The gap can be filled by\ncombining the graph kernel representation with data-driven models that utilize\nhistorical data. This paper proposes a traffic propagation model that merges\nmultiple heat diffusion kernels into a data-driven prediction model to forecast\ntraffic signals. We optimize the model parameters using Bayesian inference to\nminimize the prediction errors and, consequently, determine the mixing ratio of\nthe two approaches. Such mixing ratio strongly depends on training data size\nand data anomalies, which typically correspond to the peak hours for traffic\ndata. The proposed model demonstrates prediction accuracy comparable to that of\nthe state-of-the-art deep neural networks with lower computational effort. It\nparticularly shows excellent performance for long-term prediction since it\ninherits the data-driven models' periodicity modeling.",
        "We introduce LEAF-QA, a comprehensive dataset of $250,000$ densely annotated\nfigures/charts, constructed from real-world open data sources, along with ~2\nmillion question-answer (QA) pairs querying the structure and semantics of\nthese charts. LEAF-QA highlights the problem of multimodal QA, which is notably\ndifferent from conventional visual QA (VQA), and has recently gained interest\nin the community. Furthermore, LEAF-QA is significantly more complex than\nprevious attempts at chart QA, viz. FigureQA and DVQA, which present only\nlimited variations in chart data. LEAF-QA being constructed from real-world\nsources, requires a novel architecture to enable question answering. To this\nend, LEAF-Net, a deep architecture involving chart element localization,\nquestion and answer encoding in terms of chart elements, and an attention\nnetwork is proposed. Different experiments are conducted to demonstrate the\nchallenges of QA on LEAF-QA. The proposed architecture, LEAF-Net also\nconsiderably advances the current state-of-the-art on FigureQA and DVQA.",
        "Object detection is a trendy branch of computer vision, especially on human\nrecognition and pedestrian detection. Recognizing the complete body of a person\nhas always been a difficult problem. Over the years, researchers proposed\nvarious methods, and recently, Mask R-CNN has made a breakthrough for instance\nsegmentation. Based on Faster R-CNN, Mask R-CNN has been able to generate a\nsegmentation mask for each instance. We propose an application to extracts\nmultiple persons from images and videos for pleasant life scenes to grouping\nhappy moments of people such as family or friends and a community for QOL\n(Quality Of Life). We likewise propose a methodology to put extracted images of\npersons into the new background. This enables a user to make a pleasant\ncollection of happy facial expressions and actions of his/her family and\nfriends in his/her life. Mask R-CNN detects all types of object masks from\nimages. Then our algorithm considers only the target person and extracts a\nperson only without obstacles, such as dogs in front of the person, and the\nuser also can select multiple persons as their expectations. Our algorithm is\neffective for both an image and a video irrespective of the length of it. Our\nalgorithm does not add any overhead to Mask R-CNN, running at 5 fps. We show\nexamples of yoga-person in an image and a dancer in a dance-video frame. We\nhope our simple and effective approach would serve as a baseline for replacing\nthe image background and help ease future research.",
        "Modern deep learning methods provide effective means to learn good\nrepresentations. However, is a good representation itself sufficient for sample\nefficient reinforcement learning? This question has largely been studied only\nwith respect to (worst-case) approximation error, in the more classical\napproximate dynamic programming literature. With regards to the statistical\nviewpoint, this question is largely unexplored, and the extant body of\nliterature mainly focuses on conditions which permit sample efficient\nreinforcement learning with little understanding of what are necessary\nconditions for efficient reinforcement learning.\n  This work shows that, from the statistical viewpoint, the situation is far\nsubtler than suggested by the more traditional approximation viewpoint, where\nthe requirements on the representation that suffice for sample efficient RL are\neven more stringent. Our main results provide sharp thresholds for\nreinforcement learning methods, showing that there are hard limitations on what\nconstitutes good function approximation (in terms of the dimensionality of the\nrepresentation), where we focus on natural representational conditions relevant\nto value-based, model-based, and policy-based learning. These lower bounds\nhighlight that having a good (value-based, model-based, or policy-based)\nrepresentation in and of itself is insufficient for efficient reinforcement\nlearning, unless the quality of this approximation passes certain hard\nthresholds. Furthermore, our lower bounds also imply exponential separations on\nthe sample complexity between 1) value-based learning with perfect\nrepresentation and value-based learning with a good-but-not-perfect\nrepresentation, 2) value-based learning and policy-based learning, 3)\npolicy-based learning and supervised learning and 4) reinforcement learning and\nimitation learning.",
        "Bayesian Decision Trees are known for their probabilistic interpretability.\nHowever, their construction can sometimes be costly. In this article we present\na general Bayesian Decision Tree algorithm applicable to both regression and\nclassification problems. The algorithm does not apply Markov Chain Monte Carlo\nand does not require a pruning step. While it is possible to construct a\nweighted probability tree space we find that one particular tree, the\ngreedy-modal tree (GMT), explains most of the information contained in the\nnumerical examples. This approach seems to perform similarly to Random Forests.",
        "Anomaly detection becomes increasingly important for the dependability and\nserviceability of IT services. As log lines record events during the execution\nof IT services, they are a primary source for diagnostics. Thereby,\nunsupervised methods provide a significant benefit since not all anomalies can\nbe known at training time. Existing unsupervised methods need anomaly examples\nto obtain a suitable decision boundary required for the anomaly detection task.\nThis requirement poses practical limitations. Therefore, we develop A2Log,\nwhich is an unsupervised anomaly detection method consisting of two steps:\nAnomaly scoring and anomaly decision. First, we utilize a self-attention neural\nnetwork to perform the scoring for each log message. Second, we set the\ndecision boundary based on data augmentation of the available normal training\ndata. The method is evaluated on three publicly available datasets and one\nindustry dataset. We show that our approach outperforms existing methods.\nFurthermore, we utilize available anomaly examples to set optimal decision\nboundaries to acquire strong baselines. We show that our approach, which\ndetermines decision boundaries without utilizing anomaly examples, can reach\nscores of the strong baselines.",
        "Style transfer is the process of rendering one image with some content in the\nstyle of another image, representing the style. Recent studies of Liu et al.\n(2017) show that traditional style transfer methods of Gatys et al. (2016) and\nJohnson et al. (2016) fail to reproduce the depth of the content image, which\nis critical for human perception. They suggest to preserve the depth map by\nadditional regularizer in the optimized loss function, forcing preservation of\nthe depth map. However these traditional methods are either computationally\ninefficient or require training a separate neural network for each style. AdaIN\nmethod of Huang et al. (2017) allows efficient transferring of arbitrary style\nwithout training a separate model but is not able to reproduce the depth map of\nthe content image. We propose an extension to this method, allowing depth map\npreservation by applying variable stylization strength. Qualitative analysis\nand results of user evaluation study indicate that the proposed method provides\nbetter stylizations, compared to the original AdaIN style transfer method.",
        "In this paper, we explore using deep reinforcement learning for problems with\nmultiple agents. Most existing methods for deep multi-agent reinforcement\nlearning consider only a small number of agents. When the number of agents\nincreases, the dimensionality of the input and control spaces increase as well,\nand these methods do not scale well. To address this, we propose casting the\nmulti-agent reinforcement learning problem as a distributed optimization\nproblem. Our algorithm assumes that for multi-agent settings, policies of\nindividual agents in a given population live close to each other in parameter\nspace and can be approximated by a single policy. With this simple assumption,\nwe show our algorithm to be extremely effective for reinforcement learning in\nmulti-agent settings. We demonstrate its effectiveness against existing\ncomparable approaches on co-operative and competitive tasks.",
        "Transformer-based models have become ubiquitous in natural language\nprocessing thanks to their large capacity, innate parallelism and high\nperformance. The contextualizing component of a Transformer block is the\n$\\textit{pairwise dot-product}$ attention that has a large $\\Omega(L^2)$ memory\nrequirement for length $L$ sequences, limiting its ability to process long\ndocuments. This has been the subject of substantial interest recently, where\nmultiple approximations were proposed to reduce the quadratic memory\nrequirement using sparse attention matrices. In this work, we propose to\naugment sparse Transformer blocks with a dense attention-based $\\textit{global\nmemory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the\nentire input sequence to each position. Our augmentation has a manageable\n$O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior\nsparse solutions. Moreover, global memory can also be used for sequence\ncompression, by representing a long input sequence with the memory\nrepresentations only. We empirically show that our method leads to substantial\nimprovement on a range of tasks, including (a) synthetic tasks that require\nglobal reasoning, (b) masked language modeling, and (c) reading comprehension.",
        "Graphs have been widely used in data mining and machine learning due to their\nunique representation of real-world objects and their interactions. As graphs\nare getting bigger and bigger nowadays, it is common to see their subgraphs\nseparately collected and stored in multiple local systems. Therefore, it is\nnatural to consider the subgraph federated learning setting, where each local\nsystem holding a small subgraph that may be biased from the distribution of the\nwhole graph. Hence, the subgraph federated learning aims to collaboratively\ntrain a powerful and generalizable graph mining model without directly sharing\ntheir graph data. In this work, towards the novel yet realistic setting of\nsubgraph federated learning, we propose two major techniques: (1) FedSage,\nwhich trains a GraphSage model based on FedAvg to integrate node features, link\nstructures, and task labels on multiple local subgraphs; (2) FedSage+, which\ntrains a missing neighbor generator along FedSage to deal with missing links\nacross local subgraphs. Empirical results on four real-world graph datasets\nwith synthesized subgraph federated learning settings demonstrate the\neffectiveness and efficiency of our proposed techniques. At the same time,\nconsistent theoretical implications are made towards their generalization\nability on the global graphs.",
        "In recent years, deep learning-based feature representation methods have\nshown a promising impact in electroencephalography (EEG)-based brain-computer\ninterface (BCI). Nonetheless, owing to high intra- and inter-subject\nvariabilities, many studies on decoding EEG were designed in a subject-specific\nmanner by using calibration samples, with no concern of its practical use,\nhampered by time-consuming steps and a large data requirement. To this end,\nrecent studies adopted a transfer learning strategy, especially domain\nadaptation techniques. Among those, to our knowledge, an adversarial learning\nhas shown its potential in BCIs. In the meantime, it is known that adversarial\nlearning-based domain adaptation methods are prone to negative transfer that\ndisrupts learning generalized feature representations, applicable to diverse\ndomains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel\nframework that learns class-relevant and subject-invariant feature\nrepresentations in an information-theoretic manner, without using adversarial\nlearning. To be specific, we devise two operational components in a deep\nnetwork that explicitly estimate mutual information between feature\nrepresentations; (1) to decompose features in an intermediate layer into\nclass-relevant and class-irrelevant ones, (2) to enrich class-discriminative\nfeature representation. On two large EEG datasets, we validated the\neffectiveness of our proposed framework by comparing with several comparative\nmethods in performance. Further, we conducted rigorous analyses by performing\nan ablation study in regard to the components in our network, explaining our\nmodel's decision on input EEG signals via layer-wise relevance propagation, and\nvisualizing the distribution of learned features via t-SNE.",
        "The use of visual information for the navigation of unmanned ground vehicles\nin a cross-country environment recently received great attention. However,\nuntil now, the use of textural information has been somewhat less effective\nthan color or laser range information. This manuscript reviews the recent\nachievements in cross-country scene segmentation and addresses their\nshortcomings. It then describes a problem related to classification of high\ndimensional texture features. Finally, it compares three machine learning\nalgorithms aimed at resolving this problem. The experimental results for each\nmachine learning algorithm with the discussion of comparisons are given at the\nend of the manuscript.",
        "The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the\nmultispectral remote sensing imagery provides vital information for the\nlandcover classification, especially concerning the vegetation assessment.\nDespite the usefulness of NIR, common RGB is not always accompanied by it.\nModern achievements in image processing via deep neural networks allow\ngenerating artificial spectral information, such as for the image colorization\nproblem. In this research, we aim to investigate whether this approach can\nproduce not only visually similar images but also an artificial spectral band\nthat can improve the performance of computer vision algorithms for solving\nremote sensing tasks. We study the generative adversarial network (GAN)\napproach in the task of the NIR band generation using just RGB channels of\nhigh-resolution satellite imagery. We evaluate the impact of a generated\nchannel on the model performance for solving the forest segmentation task. Our\nresults show an increase in model accuracy when using generated NIR comparing\nto the baseline model that uses only RGB (0.947 and 0.914 F1-score\naccordingly). Conducted study shows the advantages of generating the extra band\nand its implementation in applied challenges reducing the required amount of\nlabeled data.",
        "Coronary Artery Disease (CAD) is one of the leading causes of death\nworldwide, and so it is very important to correctly diagnose patients with the\ndisease. For medical diagnosis, machine learning is a useful tool, however\nfeatures and algorithms must be carefully selected to get accurate\nclassification. To this effect, three feature selection methods have been used\non 13 input features from the Cleveland dataset with 297 entries, and 7 were\nselected. The selected features were used to train three different classifiers,\nwhich are SVM, Na\\\"ive Bayes and KNN using 10-fold cross-validation. The\nresulting models evaluated using Accuracy, Recall, Specificity and Precision.\nIt is found that the Na\\\"ive Bayes classifier performs the best on this dataset\nand features, outperforming or matching SVM and KNN in all the four evaluation\nparameters used and achieving an accuracy of 84%.",
        "The performance of surface registration relies heavily on the metric used for\nthe alignment error between the source and target shapes. Traditionally, such a\nmetric is based on the point-to-point or point-to-plane distance from the\npoints on the source surface to their closest points on the target surface,\nwhich is susceptible to failure due to instability of the closest-point\ncorrespondence. In this paper, we propose a novel metric based on the\nintersection points between the two shapes and a random straight line, which\ndoes not assume a specific correspondence. We verify the effectiveness of this\nmetric by extensive experiments, including its direct optimization for a single\nregistration problem as well as unsupervised learning for a set of registration\nproblems. The results demonstrate that the algorithms utilizing our proposed\nmetric outperforms the state-of-the-art optimization-based and unsupervised\nlearning-based methods.",
        "Missing values, irregularly collected samples, and multi-resolution signals\ncommonly occur in multivariate time series data, making predictive tasks\ndifficult. These challenges are especially prevalent in the healthcare domain,\nwhere patients' vital signs and electronic records are collected at different\nfrequencies and have occasionally missing information due to the imperfections\nin equipment or patient circumstances. Researchers have handled each of these\nissues differently, often handling missing data through mean value imputation\nand then using sequence models over the multivariate signals while ignoring the\ndifferent resolution of signals. We propose a unified model named\nMulti-resolution Flexible Irregular Time series Network (Multi-FIT). The\nbuilding block for Multi-FIT is the FIT network. The FIT network creates an\ninformative dense representation at each time step using signal information\nsuch as last observed value, time difference since the last observed time stamp\nand overall mean for the signal. Vertical FIT (FIT-V) is a variant of FIT which\nalso models the relationship between different temporal signals while creating\nthe informative dense representations for the signal. The multi-FIT model uses\nmultiple FIT networks for sets of signals with different resolutions, further\nfacilitating the construction of flexible representations. Our model has three\nmain contributions: a.) it does not impute values but rather creates\ninformative representations to provide flexibility to the model for creating\ntask-specific representations b.) it models the relationship between different\nsignals in the form of support signals c.) it models different resolutions in\nparallel before merging them for the final prediction task. The FIT, FIT-V and\nMulti-FIT networks improve upon the state-of-the-art models for three\npredictive tasks, including the forecasting of patient survival.",
        "In this article we give our contribution to the problem of segmentation with\nplug-in procedures. We give general sufficient conditions under which plug in\nprocedure are efficient. We also give an algorithm that satisfy these\nconditions. We give an application of the used algorithm to hyperspectral\nimages segmentation. Hyperspectral images are images that have both spatial and\nspectral coherence with thousands of spectral bands on each pixel. In the\nproposed procedure we combine a reduction dimension technique and a spatial\nregularisation technique. This regularisation is based on the mixlet\nmodelisation of Kolaczyck and Al.",
        "We present an algorithm that learns a coarse 3D representation of objects\nfrom unposed multi-view 2D mask supervision, then uses it to generate detailed\nmask and image texture. In contrast to existing voxel-based methods for unposed\nobject reconstruction, our approach learns to represent the generated shape and\npose with a set of self-supervised canonical 3D anisotropic Gaussians via a\nperspective camera, and a set of per-image transforms. We show that this\napproach can robustly estimate a 3D space for the camera and object, while\nrecent baselines sometimes struggle to reconstruct coherent 3D spaces in this\nsetting. We show results on synthetic datasets with realistic lighting, and\ndemonstrate object insertion with interactive posing. With our work, we help\nmove towards structured representations that handle more real-world variation\nin learning-based object reconstruction.",
        "Complex networks are often either too large for full exploration, partially\naccessible, or partially observed. Downstream learning tasks on these\nincomplete networks can produce low quality results. In addition, reducing the\nincompleteness of the network can be costly and nontrivial. As a result,\nnetwork discovery algorithms optimized for specific downstream learning tasks\ngiven resource collection constraints are of great interest. In this paper, we\nformulate the task-specific network discovery problem in an incomplete network\nsetting as a sequential decision making problem. Our downstream task is\nselective harvesting, the optimal collection of vertices with a particular\nattribute. We propose a framework, called Network Actor Critic (NAC), which\nlearns a policy and notion of future reward in an offline setting via a deep\nreinforcement learning algorithm. The NAC paradigm utilizes a task-specific\nnetwork embedding to reduce the state space complexity. A detailed comparative\nanalysis of popular network embeddings is presented with respect to their role\nin supporting offline planning. Furthermore, a quantitative study is presented\non several synthetic and real benchmarks using NAC and several baselines. We\nshow that offline models of reward and network discovery policies lead to\nsignificantly improved performance when compared to competitive online\ndiscovery algorithms. Finally, we outline learning regimes where planning is\ncritical in addressing sparse and changing reward signals.",
        "Time series are all around in real-world applications. However, unexpected\naccidents for example broken sensors or missing of the signals will cause\nmissing values in time series, making the data hard to be utilized. It then\ndoes harm to the downstream applications such as traditional classification or\nregression, sequential data integration and forecasting tasks, thus raising the\ndemand for data imputation. Currently, time series data imputation is a\nwell-studied problem with different categories of methods. However, these works\nrarely take the temporal relations among the observations and treat the time\nseries as normal structured data, losing the information from the time data. In\nrecent, deep learning models have raised great attention. Time series methods\nbased on deep learning have made progress with the usage of models like RNN,\nsince it captures time information from data. In this paper, we mainly focus on\ntime series imputation technique with deep learning methods, which recently\nmade progress in this field. We will review and discuss their model\narchitectures, their pros and cons as well as their effects to show the\ndevelopment of the time series imputation methods.",
        "We apply network Lasso to semi-supervised regression problems involving\nnetwork structured data. This approach lends quite naturally to highly scalable\nlearning algorithms in the form of message passing over an empirical graph\nwhich represents the network structure of the data. By using a simple\nnon-parametric regression model, which is motivated by a clustering hypothesis,\nwe provide an analysis of the estimation error incurred by network Lasso. This\nanalysis reveals conditions on the the network structure and the available\ntraining data which guarantee network Lasso to be accurate. Remarkably, the\naccuracy of network Lasso is related to the existence of sufficiently large\nnetwork flows over the empirical graph. Thus, our analysis reveals a connection\nbetween network Lasso and maximum flow problems.",
        "In machine learning we often encounter structured output prediction problems\n(SOPPs), i.e. problems where the output space admits a rich internal structure.\nApplication domains where SOPPs naturally occur include natural language\nprocessing, speech recognition, and computer vision. Typical SOPPs have an\nextremely large label set, which grows exponentially as a function of the size\nof the output. Existing generalization analysis implies generalization bounds\nwith at least a square-root dependency on the cardinality $d$ of the label set,\nwhich can be vacuous in practice. In this paper, we significantly improve the\nstate of the art by developing novel high-probability bounds with a logarithmic\ndependency on $d$. Moreover, we leverage the lens of algorithmic stability to\ndevelop generalization bounds in expectation without any dependency on $d$. Our\nresults therefore build a solid theoretical foundation for learning in\nlarge-scale SOPPs. Furthermore, we extend our results to learning with weakly\ndependent data.",
        "Many real-world complex systems can be described as graphs. For a large-scale\ngraph with low sparsity, a node's adjacency vector is a long and sparse\nrepresentation, limiting the practical utilization of existing machine learning\nmethods on nodal features. In practice, graph embedding (graph representation\nlearning) attempts to learn a lower-dimensional representation vector for each\nnode or the whole graph while maintaining the most basic information of graph.\nSince various machine learning methods can efficiently process\nlower-dimensional vectors, graph embedding has recently attracted a lot of\nattention. However, most node embedding or whole graph embedding methods suffer\nfrom the problem of having more sophisticated methodology, hyperparameter\noptimization, and low explainability. This paper proposes a\nhyperparameter-free, extensible, and explainable whole graph embedding method,\ncombining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy\n(E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a\ntrade-off between simplicity and quality under supervised classification\nlearning tasks, using molecular, social, and brain networks. In addition, the\nproposed approach has a good performance in lower-dimensional graph\nvisualization. Overall, the new methodology is simple, hyperparameter-free,\nextensible, and explainable for whole graph embedding with promising potential\nfor exploring graph classification, prediction, and lower-dimensional graph\nvisualization.",
        "Preserving the utility of published datasets while simultaneously providing\nprovable privacy guarantees is a well-known challenge. On the one hand,\ncontext-free privacy solutions, such as differential privacy, provide strong\nprivacy guarantees, but often lead to a significant reduction in utility. On\nthe other hand, context-aware privacy solutions, such as information theoretic\nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data\nholder has access to dataset statistics. We circumvent these limitations by\nintroducing a novel context-aware privacy framework called generative\nadversarial privacy (GAP). GAP leverages recent advancements in generative\nadversarial networks (GANs) to allow the data holder to learn privatization\nschemes from the dataset itself. Under GAP, learning the privacy mechanism is\nformulated as a constrained minimax game between two players: a privatizer that\nsanitizes the dataset in a way that limits the risk of inference attacks on the\nindividuals' private variables, and an adversary that tries to infer the\nprivate variables from the sanitized dataset. To evaluate GAP's performance, we\ninvestigate two simple (yet canonical) statistical dataset models: (a) the\nbinary data model, and (b) the binary Gaussian mixture model. For both models,\nwe derive game-theoretically optimal minimax privacy mechanisms, and show that\nthe privacy mechanisms learned from data (in a generative adversarial fashion)\nmatch the theoretically optimal ones. This demonstrates that our framework can\nbe easily applied in practice, even in the absence of dataset statistics.",
        "Corrupting the input and hidden layers of deep neural networks (DNNs) with\nmultiplicative noise, often drawn from the Bernoulli distribution (or\n'dropout'), provides regularization that has significantly contributed to deep\nlearning's success. However, understanding how multiplicative corruptions\nprevent overfitting has been difficult due to the complexity of a DNN's\nfunctional form. In this paper, we show that when a Gaussian prior is placed on\na DNN's weights, applying multiplicative noise induces a Gaussian scale\nmixture, which can be reparameterized to circumvent the problematic likelihood\nfunction. Analysis can then proceed by using a type-II maximum likelihood\nprocedure to derive a closed-form expression revealing how regularization\nevolves as a function of the network's weights. Results show that\nmultiplicative noise forces weights to become either sparse or invariant to\nrescaling. We find our analysis has implications for model compression as it\nnaturally reveals a weight pruning rule that starkly contrasts with the\ncommonly used signal-to-noise ratio (SNR). While the SNR prunes weights with\nlarge variances, seeing them as noisy, our approach recognizes their robustness\nand retains them. We empirically demonstrate our approach has a strong\nadvantage over the SNR heuristic and is competitive to retraining with soft\ntargets produced from a teacher model.",
        "Graph Neural Networks (GNNs), which generalize the deep neural networks to\ngraph-structured data, have achieved great success in modeling graphs. However,\nas an extension of deep learning for graphs, GNNs lack explainability, which\nlargely limits their adoption in scenarios that demand the transparency of\nmodels. Though many efforts are taken to improve the explainability of deep\nlearning, they mainly focus on i.i.d data, which cannot be directly applied to\nexplain the predictions of GNNs because GNNs utilize both node features and\ngraph topology to make predictions. There are only very few work on the\nexplainability of GNNs and they focus on post-hoc explanations. Since post-hoc\nexplanations are not directly obtained from the GNNs, they can be biased and\nmisrepresent the true explanations. Therefore, in this paper, we study a novel\nproblem of self-explainable GNNs which can simultaneously give predictions and\nexplanations. We propose a new framework which can find $K$-nearest labeled\nnodes for each unlabeled node to give explainable node classification, where\nnearest labeled nodes are found by interpretable similarity module in terms of\nboth node similarity and local structure similarity. Extensive experiments on\nreal-world and synthetic datasets demonstrate the effectiveness of the proposed\nframework for explainable node classification.",
        "The goal of continual learning (CL) is to learn a sequence of tasks without\nsuffering from the phenomenon of catastrophic forgetting. Previous work has\nshown that leveraging memory in the form of a replay buffer can reduce\nperformance degradation on prior tasks. We hypothesize that forgetting can be\nfurther reduced when the model is encouraged to remember the \\textit{evidence}\nfor previously made decisions. As a first step towards exploring this\nhypothesis, we propose a simple novel training paradigm, called Remembering for\nthe Right Reasons (RRR), that additionally stores visual model explanations for\neach example in the buffer and ensures the model has \"the right reasons\" for\nits predictions by encouraging its explanations to remain consistent with those\nused to make decisions at training time. Without this constraint, there is a\ndrift in explanations and increase in forgetting as conventional continual\nlearning algorithms learn new tasks. We demonstrate how RRR can be easily added\nto any memory or regularization-based approach and results in reduced\nforgetting, and more importantly, improved model explanations. We have\nevaluated our approach in the standard and few-shot settings and observed a\nconsistent improvement across various CL approaches using different\narchitectures and techniques to generate model explanations and demonstrated\nour approach showing a promising connection between explainability and\ncontinual learning. Our code is available at\n\\url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.",
        "Graph neural networks (GNNs), which learn the node representations by\nrecursively aggregating information from its neighbors, have become a\npredominant computational tool in many domains. To handle large-scale graphs,\nmost of the existing methods partition the input graph into multiple sub-graphs\n(e.g., through node clustering) and apply batch training to save memory cost.\nHowever, such batch training will lead to label bias within each batch, and\nthen result in over-confidence in model predictions. Since the connected nodes\nwith positively related labels tend to be assigned together, the traditional\ncross-entropy minimization process will attend on the predictions of biased\nclasses in the batch, and may intensify the overfitting issue. To overcome the\nlabel bias problem, we propose the adaptive label smoothing (ALS) method to\nreplace the one-hot hard labels with smoothed ones, which learns to allocate\nlabel confidences from the biased classes to the others. Specifically, ALS\npropagates node labels to aggregate the neighborhood label distribution in a\npre-processing step, and then updates the optimal smoothed labels online to\nadapt to specific graph structure. Experiments on the real-world datasets\ndemonstrate that ALS can be generally applied to the main scalable learning\nframeworks to calibrate the biased labels and improve generalization\nperformances.",
        "The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on Spark AR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on the enhanced data sets and showing that they outperform\nequivalent systems trained on the original data sets (containing faces without\nmasks), when the test benchmark contains masked faces.",
        "Extending the capabilities of robotics to real-world complex, unstructured\nenvironments requires the need of developing better perception systems while\nmaintaining low sample complexity. When dealing with high-dimensional state\nspaces, current methods are either model-free or model-based based on\nreconstruction objectives. The sample inefficiency of the former constitutes a\nmajor barrier for applying them to the real-world. The later, while they\npresent low sample complexity, they learn latent spaces that need to\nreconstruct every single detail of the scene. In real environments, the task\ntypically just represents a small fraction of the scene. Reconstruction\nobjectives suffer in such scenarios as they capture all the unnecessary\ncomponents. In this work, we present MIRO, an information theoretic\nrepresentational learning algorithm for model-based reinforcement learning. We\ndesign a latent space that maximizes the mutual information with the future\ninformation while being able to capture all the information needed for\nplanning. We show that our approach is more robust than reconstruction\nobjectives in the presence of distractors and cluttered scenes",
        "Multiple rotation averaging is an essential task for structure from motion,\nmapping, and robot navigation. The task is to estimate the absolute\norientations of several cameras given some of their noisy relative orientation\nmeasurements. The conventional methods for this task seek parameters of the\nabsolute orientations that agree best with the observed noisy measurements\naccording to a robust cost function. These robust cost functions are highly\nnonlinear and are designed based on certain assumptions about the noise and\noutlier distributions. In this work, we aim to build a neural network that\nlearns the noise patterns from the data and predict/regress the model\nparameters from the noisy relative orientations. The proposed network is a\ncombination of two networks: (1) a view-graph cleaning network, which detects\noutlier edges in the view-graph and rectifies noisy measurements; and (2) a\nfine-tuning network, which fine-tunes an initialization of absolute\norientations bootstrapped from the cleaned graph, in a single step. The\nproposed combined network is very fast, moreover, being trained on a large\nnumber of synthetic graphs, it is more accurate than the conventional iterative\noptimization methods. Although the idea of replacing robust optimization\nmethods by a graph-based network is demonstrated only for multiple rotation\naveraging, it could easily be extended to other graph-based geometric problems,\nfor example, pose-graph optimization.",
        "Several applications of Internet of Things (IoT) technology involve capturing\ndata from multiple sensors resulting in multi-sensor time series. Existing\nneural networks based approaches for such multi-sensor or multivariate time\nseries modeling assume fixed input dimension or number of sensors. Such\napproaches can struggle in the practical setting where different instances of\nthe same device or equipment such as mobiles, wearables, engines, etc. come\nwith different combinations of installed sensors. We consider training neural\nnetwork models from such multi-sensor time series, where the time series have\nvarying input dimensionality owing to availability or installation of a\ndifferent subset of sensors at each source of time series. We propose a novel\nneural network architecture suitable for zero-shot transfer learning allowing\nrobust inference for multivariate time series with previously unseen\ncombination of available dimensions or sensors at test time. Such a\ncombinatorial generalization is achieved by conditioning the layers of a core\nneural network-based time series model with a \"conditioning vector\" that\ncarries information of the available combination of sensors for each time\nseries. This conditioning vector is obtained by summarizing the set of learned\n\"sensor embedding vectors\" corresponding to the available sensors in a time\nseries via a graph neural network. We evaluate the proposed approach on\npublicly available activity recognition and equipment prognostics datasets, and\nshow that the proposed approach allows for better generalization in comparison\nto a deep gated recurrent neural network baseline.",
        "Many recent works demonstrated that Deep Learning models are vulnerable to\nadversarial examples.Fortunately, generating adversarial examples usually\nrequires white-box access to the victim model, and the attacker can only access\nthe APIs opened by cloud platforms. Thus, keeping models in the cloud can\nusually give a (false) sense of security.Unfortunately, cloud-based image\nclassification service is not robust to simple transformations such as Gaussian\nNoise, Salt-and-Pepper Noise, Rotation and Monochromatization. In this\npaper,(1) we propose one novel attack method called Image Fusion(IF) attack,\nwhich achieve a high bypass rate,can be implemented only with OpenCV and is\ndifficult to defend; and (2) we make the first attempt to conduct an extensive\nempirical study of Simple Transformation (ST) attacks against real-world\ncloud-based classification services. Through evaluations on four popular cloud\nplatforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that ST\nattack has a success rate of approximately 100% except Amazon approximately\n50%, IF attack have a success rate over 98% among different classification\nservices. (3) We discuss the possible defenses to address these security\nchallenges.Experiments show that our defense technology can effectively defend\nknown ST attacks.",
        "Deep net architectures have constantly evolved over the past few years,\nleading to significant advancements in a wide array of computer vision tasks.\nHowever, besides high accuracy, many applications also require a low\ncomputational load and limited memory footprint. To date, efficiency has\ntypically been achieved either by architectural choices at the macro level\n(e.g. using skip connections or pruning techniques) or modifications at the\nlevel of the individual layers (e.g. using depth-wise convolutions or channel\nshuffle operations). Interestingly, much less attention has been devoted to the\nrole of the activation functions in constructing efficient nets. Recently,\nKligvasser et al. showed that incorporating spatial connections within the\nactivation functions, enables a significant boost in performance in image\nrestoration tasks, at any given budget of parameters. However, the\neffectiveness of their xUnit module has only been tested on simple small\nmodels, which are not characteristic of those used in high-level vision tasks.\nIn this paper, we adopt and improve the xUnit activation, show how it can be\nincorporated into the DenseNet architecture, and illustrate its high\neffectiveness for classification and image restoration tasks alike. While the\nDenseNet architecture is extremely efficient to begin with, our dense xUnit net\n(DxNet) can typically achieve the same performance with far fewer parameters.\nFor example, on ImageNet, our DxNet outperforms a ReLU-based DenseNet having\n30% more parameters and achieves state-of-the-art results for this budget of\nparameters. Furthermore, in denoising and super-resolution, DxNet significantly\nimproves upon all existing lightweight solutions, including the xUnit-based\nnets of Kligvasser et al.",
        "Classical optimal transport problem seeks a transportation map that preserves\nthe total mass betwenn two probability distributions, requiring their mass to\nbe the same. This may be too restrictive in certain applications such as color\nor shape matching, since the distributions may have arbitrary masses and/or\nthat only a fraction of the total mass has to be transported. Several\nalgorithms have been devised for computing partial Wasserstein metrics that\nrely on an entropic regularization, but when it comes with exact solutions,\nalmost no partial formulation of neither Wasserstein nor Gromov-Wasserstein are\navailable yet. This precludes from working with distributions that do not lie\nin the same metric space or when invariance to rotation or translation is\nneeded. In this paper, we address the partial Wasserstein and\nGromov-Wasserstein problems and propose exact algorithms to solve them. We\nshowcase the new formulation in a positive-unlabeled (PU) learning application.\nTo the best of our knowledge, this is the first application of optimal\ntransport in this context and we first highlight that partial Wasserstein-based\nmetrics prove effective in usual PU learning settings. We then demonstrate that\npartial Gromov-Wasserstein metrics is efficient in scenario where point clouds\ncome from different domains or have different features.",
        "The Information Bottleneck (IB) method (\\cite{tishby2000information})\nprovides an insightful and principled approach for balancing compression and\nprediction for representation learning. The IB objective $I(X;Z)-\\beta I(Y;Z)$\nemploys a Lagrange multiplier $\\beta$ to tune this trade-off. However, in\npractice, not only is $\\beta$ chosen empirically without theoretical guidance,\nthere is also a lack of theoretical understanding between $\\beta$,\nlearnability, the intrinsic nature of the dataset and model capacity. In this\npaper, we show that if $\\beta$ is improperly chosen, learning cannot happen --\nthe trivial representation $P(Z|X)=P(Z)$ becomes the global minimum of the IB\nobjective. We show how this can be avoided, by identifying a sharp phase\ntransition between the unlearnable and the learnable which arises as $\\beta$ is\nvaried. This phase transition defines the concept of IB-Learnability. We prove\nseveral sufficient conditions for IB-Learnability, which provides theoretical\nguidance for choosing a good $\\beta$. We further show that IB-learnability is\ndetermined by the largest confident, typical, and imbalanced subset of the\nexamples (the conspicuous subset), and discuss its relation with model\ncapacity. We give practical algorithms to estimate the minimum $\\beta$ for a\ngiven dataset. We also empirically demonstrate our theoretical conditions with\nanalyses of synthetic datasets, MNIST, and CIFAR10.",
        "This paper represents a cost-effective scene perception system aimed towards\nvisually impaired individual. We use an odroid system integrated with an USB\ncamera and USB laser that can be attached on the chest. The system classifies\nthe detected objects along with its distance from the user and provides a voice\noutput. Experimental results provided in this paper use outdoor traffic scenes.\nThe object detection and classification framework exploits a multi-modal fusion\nbased faster RCNN using motion, sharpening and blurring filters for efficient\nfeature representation.",
        "Validating the safety of autonomous systems generally requires the use of\nhigh-fidelity simulators that adequately capture the variability of real-world\nscenarios. However, it is generally not feasible to exhaustively search the\nspace of simulation scenarios for failures. Adaptive stress testing (AST) is a\nmethod that uses reinforcement learning to find the most likely failure of a\nsystem. AST with a deep reinforcement learning solver has been shown to be\neffective in finding failures across a range of different systems. This\napproach generally involves running many simulations, which can be very\nexpensive when using a high-fidelity simulator. To improve efficiency, we\npresent a method that first finds failures in a low-fidelity simulator. It then\nuses the backward algorithm, which trains a deep neural network policy using a\nsingle expert demonstration, to adapt the low-fidelity failures to\nhigh-fidelity. We have created a series of autonomous vehicle validation case\nstudies that represent some of the ways low-fidelity and high-fidelity\nsimulators can differ, such as time discretization. We demonstrate in a variety\nof case studies that this new AST approach is able to find failures with\nsignificantly fewer high-fidelity simulation steps than are needed when just\nrunning AST directly in high-fidelity. As a proof of concept, we also\ndemonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art\nhigh-fidelity simulator for finding failures in autonomous vehicles.",
        "Recent progress in neural machine translation (NMT) has made it possible to\ntranslate successfully between monolingual language pairs where large parallel\ndata exist, with pre-trained models improving performance even further.\nAlthough there exists work on translating in code-mixed settings (where one of\nthe pairs includes text from two or more languages), it is still unclear what\nrecent success in NMT and language modeling exactly means for translating\ncode-mixed text. We investigate one such context, namely MT from code-mixed\nModern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop\nmodels under different conditions, employing both (i) standard end-to-end\nsequence-to-sequence (S2S) Transformers trained from scratch and (ii)\npre-trained S2S language models (LMs). We are able to acquire reasonable\nperformance using only MSA-EN parallel data with S2S models trained from\nscratch. We also find LMs fine-tuned on data from various Arabic dialects to\nhelp the MSAEA-EN task. Our work is in the context of the Shared Task on\nMachine Translation in Code-Switching. Our best model achieves $\\bf25.72$ BLEU,\nplacing us first on the official shared task evaluation for MSAEA-EN.",
        "Offline methods for reinforcement learning have a potential to help bridge\nthe gap between reinforcement learning research and real-world applications.\nThey make it possible to learn policies from offline datasets, thus overcoming\nconcerns associated with online data collection in the real-world, including\ncost, safety, or ethical concerns. In this paper, we propose a benchmark called\nRL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes\ndata from a diverse range of domains including games (e.g., Atari benchmark)\nand simulated motor control problems (e.g., DM Control Suite). The datasets\ninclude domains that are partially or fully observable, use continuous or\ndiscrete actions, and have stochastic vs. deterministic dynamics. We propose\ndetailed evaluation protocols for each domain in RL Unplugged and provide an\nextensive analysis of supervised learning and offline RL methods using these\nprotocols. We will release data for all our tasks and open-source all\nalgorithms presented in this paper. We hope that our suite of benchmarks will\nincrease the reproducibility of experiments and make it possible to study\nchallenging tasks with a limited computational budget, thus making RL research\nboth more systematic and more accessible across the community. Moving forward,\nwe view RL Unplugged as a living benchmark suite that will evolve and grow with\ndatasets contributed by the research community and ourselves. Our project page\nis available on https://git.io/JJUhd.",
        "This paper addresses distributional offline continuous-time reinforcement\nlearning (DOCTR-L) with stochastic policies for high-dimensional optimal\ncontrol. A soft distributional version of the classical Hamilton-Jacobi-Bellman\n(HJB) equation is given by a semilinear partial differential equation (PDE).\nThis `soft HJB equation' can be learned from offline data without assuming that\nthe latter correspond to a previous optimal or near-optimal policy. A\ndata-driven solution of the soft HJB equation uses methods of Neural PDEs and\nPhysics-Informed Neural Networks developed in the field of Scientific Machine\nLearning (SciML). The suggested approach, dubbed `SciPhy RL', thus reduces\nDOCTR-L to solving neural PDEs from data. Our algorithm called Deep DOCTR-L\nconverts offline high-dimensional data into an optimal policy in one step by\nreducing it to supervised learning, instead of relying on value iteration or\npolicy iteration methods. The method enables a computable approach to the\nquality control of obtained policies in terms of both their expected returns\nand uncertainties about their values.",
        "In facial action unit (AU) recognition tasks, regional feature learning and\nAU relation modeling are two effective aspects which are worth exploring.\nHowever, the limited representation capacity of regional features makes it\ndifficult for relation models to embed AU relationship knowledge. In this\npaper, we propose a novel multi-level adaptive ROI and graph learning (MARGL)\nframework to tackle this problem. Specifically, an adaptive ROI learning module\nis designed to automatically adjust the location and size of the predefined AU\nregions. Meanwhile, besides relationship between AUs, there exists strong\nrelevance between regional features across multiple levels of the backbone\nnetwork as level-wise features focus on different aspects of representation. In\norder to incorporate the intra-level AU relation and inter-level AU regional\nrelevance simultaneously, a multi-level AU relation graph is constructed and\ngraph convolution is performed to further enhance AU regional features of each\nlevel. Experiments on BP4D and DISFA demonstrate the proposed MARGL\nsignificantly outperforms the previous state-of-the-art methods.",
        "Machine learning tools are becoming increasingly powerful and widely used.\nUnfortunately membership attacks, which seek to uncover information from data\nsets used in machine learning, have the potential to limit data sharing. In\nthis paper we consider an approach to increase the privacy protection of data\nsets, as applied to face recognition. Using an auxiliary face recognition\nmodel, we build on the StyleGAN generative adversarial network and feed it with\nlatent codes combining two distinct sub-codes, one encoding visual identity\nfactors, and, the other, non-identity factors. By independently varying these\nvectors during image generation, we create a synthetic data set of fictitious\nface identities. We use this data set to train a face recognition model. The\nmodel performance degrades in comparison to the state-of-the-art of face\nverification. When tested with a simple membership attack our model provides\ngood privacy protection, however the model performance degrades in comparison\nto the state-of-the-art of face verification. We find that the addition of a\nsmall amount of private data greatly improves the performance of our model,\nwhich highlights the limitations of using synthetic data to train machine\nlearning models.",
        "Intrinsically motivated goal exploration processes enable agents to\nautonomously sample goals to explore efficiently complex environments with\nhigh-dimensional continuous actions. They have been applied successfully to\nreal world robots to discover repertoires of policies producing a wide\ndiversity of effects. Often these algorithms relied on engineered goal spaces\nbut it was recently shown that one can use deep representation learning\nalgorithms to learn an adequate goal space in simple environments. However, in\nthe case of more complex environments containing multiple objects or\ndistractors, an efficient exploration requires that the structure of the goal\nspace reflects the one of the environment. In this paper we show that using a\ndisentangled goal space leads to better exploration performances than an\nentangled goal space. We further show that when the representation is\ndisentangled, one can leverage it by sampling goals that maximize learning\nprogress in a modular manner. Finally, we show that the measure of learning\nprogress, used to drive curiosity-driven exploration, can be used\nsimultaneously to discover abstract independently controllable features of the\nenvironment.",
        "Vision transformer (ViT) has recently showed its strong capability in\nachieving comparable results to convolutional neural networks (CNNs) on image\nclassification. However, vanilla ViT simply inherits the same architecture from\nthe natural language processing directly, which is often not optimized for\nvision applications. Motivated by this, in this paper, we propose a new\narchitecture that adopts the pyramid structure and employ a novel\nregional-to-local attention rather than global self-attention in vision\ntransformers. More specifically, our model first generates regional tokens and\nlocal tokens from an image with different patch sizes, where each regional\ntoken is associated with a set of local tokens based on the spatial location.\nThe regional-to-local attention includes two steps: first, the regional\nself-attention extract global information among all regional tokens and then\nthe local self-attention exchanges the information among one regional token and\nthe associated local tokens via self-attention. Therefore, even though local\nself-attention confines the scope in a local region but it can still receive\nglobal information. Extensive experiments on three vision tasks, including\nimage classification, object detection and action recognition, show that our\napproach outperforms or is on par with state-of-the-art ViT variants including\nmany concurrent works. Our source codes and models will be publicly available.",
        "A novel 3D shape classification scheme, based on collaborative representation\nlearning, is investigated in this work. A data-driven feature-extraction\nprocedure, taking the form of a simple projection operator, is in the core of\nour methodology. Provided a shape database, a graph encapsulating the\nstructural relationships among all the available shapes, is first constructed\nand then employed in defining low-dimensional sparse projections. The recently\nintroduced method of CRPs (collaborative representation based projections),\nwhich is based on L2-Graph, is the first variant that is included towards this\nend. A second algorithm, that particularizes the CRPs to shape descriptors that\nare inherently nonnegative, is also introduced as potential alternative. In\nboth cases, the weights in the graph reflecting the database structure are\ncalculated so as to approximate each shape as a sparse linear combination of\nthe remaining dataset objects. By way of solving a generalized eigenanalysis\nproblem, a linear matrix operator is designed that will act as the feature\nextractor. Two popular, inherently high dimensional descriptors, namely\nShapeDNA and Global Point Signature (GPS), are employed in our experimentations\nwith SHREC10, SHREC11 and SCHREC 15 datasets, where shape recognition is cast\nas a multi-class classification problem that is tackled by means of an SVM\n(support vector machine) acting within the reduced dimensional space of the\ncrafted projections. The results are very promising and outperform state of the\nart methods, providing evidence about the highly discriminative nature of the\nintroduced 3D shape representations.",
        "Predicting clinical outcome is remarkably important but challenging. Research\nefforts have been paid on seeking significant biomarkers associated with the\ntherapy response or/and patient survival. However, these biomarkers are\ngenerally costly and invasive, and possibly dissatifactory for novel therapy.\nOn the other hand, multi-modal, heterogeneous, unaligned temporal data is\ncontinuously generated in clinical practice. This paper aims at a unified deep\nlearning approach to predict patient prognosis and therapy response, with\neasily accessible data, e.g., radiographics, laboratory and clinical\ninformation. Prior arts focus on modeling single data modality, or ignore the\ntemporal changes. Importantly, the clinical time series is asynchronous in\npractice, i.e., recorded with irregular intervals. In this study, we formalize\nthe prognosis modeling as a multi-modal asynchronous time series classification\ntask, and propose a MIA-Prognosis framework with Measurement, Intervention and\nAssessment (MIA) information to predict therapy response, where a Simple\nTemporal Attention (SimTA) module is developed to process the asynchronous time\nseries. Experiments on synthetic dataset validate the superiory of SimTA over\nstandard RNN-based approaches. Furthermore, we experiment the proposed method\non an in-house, retrospective dataset of real-world non-small cell lung cancer\npatients under anti-PD-1 immunotherapy. The proposed method achieves promising\nperformance on predicting the immunotherapy response. Notably, our predictive\nmodel could further stratify low-risk and high-risk patients in terms of\nlong-term survival.",
        "Segmentation of the left atrial chamber and assessing its morphology, are\nessential for improving our understanding of atrial fibrillation, the most\ncommon type of cardiac arrhythmia. Automation of this process in 3D gadolinium\nenhanced-MRI (GE-MRI) data is desirable, as manual delineation is\ntime-consuming, challenging and observer-dependent. Recently, deep\nconvolutional neural networks (CNNs) have gained tremendous traction and\nachieved state-of-the-art results in medical image segmentation. However, it is\ndifficult to incorporate local and global information without using contracting\n(pooling) layers, which in turn reduces segmentation accuracy for smaller\nstructures. In this paper, we propose a 3D CNN for volumetric segmentation of\nthe left atrial chamber in LGE-MRI. Our network is based on the well known\nU-Net architecture. We employ a 3D fully convolutional network, with dilated\nconvolutions in the lowest level of the network, and residual connections\nbetween encoder blocks to incorporate local and global knowledge. The results\nshow that including global context through the use of dilated convolutions,\nhelps in domain adaptation, and the overall segmentation accuracy is improved\nin comparison to a 3D U-Net.",
        "We have witnessed the discovery of many techniques for network representation\nlearning in recent years, ranging from encoding the context in random walks to\nembedding the lower order connections, to finding latent space representations\nwith auto-encoders. However, existing techniques are looking mostly into the\nlocal structures in a network, while higher-level properties such as global\ncommunity structures are often neglected. We propose a novel network\nrepresentations learning model framework called RUM (network Representation\nlearning throUgh Multi-level structural information preservation). In RUM, we\nincorporate three essential aspects of a node that capture a network's\ncharacteristics in multiple levels: a node's affiliated local triads, its\nneighborhood relationships, and its global community affiliations. Therefore\nthe framework explicitly and comprehensively preserves the structural\ninformation of a network, extending the encoding process both to the local end\nof the structural information spectrum and to the global end. The framework is\nalso flexible enough to take various community discovery algorithms as its\npreprocessor. Empirical results show that the representations learned by RUM\nhave demonstrated substantial performance advantages in real-life tasks.",
        "3D vehicle detection based on multi-modal fusion is an important task of many\napplications such as autonomous driving. Although significant progress has been\nmade, we still observe two aspects that need to be further improvement: First,\nthe specific gain that camera images can bring to 3D detection is seldom\nexplored by previous works. Second, many fusion algorithms run slowly, which is\nessential for applications with high real-time requirements(autonomous\ndriving). To this end, we propose an end-to-end trainable single-stage\nmulti-modal feature adaptive network in this paper, which uses image\ninformation to effectively reduce false positive of 3D detection and has a fast\ndetection speed. A multi-modal adaptive feature fusion module based on channel\nattention mechanism is proposed to enable the network to adaptively use the\nfeature of each modal. Based on the above mechanism, two fusion technologies\nare proposed to adapt to different usage scenarios: PointAttentionFusion is\nsuitable for filtering simple false positive and faster; DenseAttentionFusion\nis suitable for filtering more difficult false positive and has better overall\nperformance. Experimental results on the KITTI dataset demonstrate significant\nimprovement in filtering false positive over the approach using only point\ncloud data. Furthermore, the proposed method can provide competitive results\nand has the fastest speed compared to the published state-of-the-art\nmulti-modal methods in the KITTI benchmark.",
        "Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.",
        "Generative Adversarial Networks are notoriously challenging to train. The\nunderlying minmax optimization is highly susceptible to the variance of the\nstochastic gradient and the rotational component of the associated game vector\nfield. To tackle these challenges, we propose the Lookahead algorithm for\nminmax optimization, originally developed for single objective minimization\nonly. The backtracking step of our Lookahead-minmax naturally handles the\nrotational game dynamics, a property which was identified to be key for\nenabling gradient ascent descent methods to converge on challenging examples\noften analyzed in the literature. Moreover, it implicitly handles high variance\nwithout using large mini-batches, known to be essential for reaching state of\nthe art performance. Experimental results on MNIST, SVHN, CIFAR-10, and\nImageNet demonstrate a clear advantage of combining Lookahead-minmax with Adam\nor extragradient, in terms of performance and improved stability, for\nnegligible memory and computational cost. Using 30-fold fewer parameters and\n16-fold smaller minibatches we outperform the reported performance of the\nclass-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the\nclass labels, bringing state-of-the-art GAN training within reach of common\ncomputational resources.",
        "A key challenge in video enhancement and action recognition is to fuse useful\ninformation from neighboring frames. Recent works suggest establishing accurate\ncorrespondences between neighboring frames before fusing temporal information.\nHowever, the generated results heavily depend on the quality of correspondence\nestimation. In this paper, we propose a more robust solution: \\emph{sampling\nand fusing multi-level features} across neighborhood frames to generate the\nresults. Based on this idea, we introduce a new module to improve the\ncapability of 3D convolution, namely, learnable sampling 3D convolution\n(\\emph{LS3D-Conv}). We add learnable 2D offsets to 3D convolution which aims to\nsample locations on spatial feature maps across frames. The offsets can be\nlearned for specific tasks. The \\emph{LS3D-Conv} can flexibly replace 3D\nconvolution layers in existing 3D networks and get new architectures, which\nlearns the sampling at multiple feature levels. The experiments on video\ninterpolation, video super-resolution, video denoising, and action recognition\ndemonstrate the effectiveness of our approach.",
        "Matching people across multiple camera views known as person\nre-identification, is a challenging problem due to the change in visual\nappearance caused by varying lighting conditions. The perceived color of the\nsubject appears to be different with respect to illumination. Previous works\nuse color as it is or address these challenges by designing color spaces\nfocusing on a specific cue. In this paper, we propose a data driven approach\nfor learning color patterns from pixels sampled from images across two camera\nviews. The intuition behind this work is that, even though pixel values of same\ncolor would be different across views, they should be encoded with the same\nvalues. We model color feature generation as a learning problem by jointly\nlearning a linear transformation and a dictionary to encode pixel values. We\nalso analyze different photometric invariant color spaces. Using color as the\nonly cue, we compare our approach with all the photometric invariant color\nspaces and show superior performance over all of them. Combining with other\nlearned low-level and high-level features, we obtain promising results in\nViPER, Person Re-ID 2011 and CAVIAR4REID datasets.",
        "We present R-FCN-3000, a large-scale real-time object detector in which\nobjectness detection and classification are decoupled. To obtain the detection\nscore for an RoI, we multiply the objectness score with the fine-grained\nclassification score. Our approach is a modification of the R-FCN architecture\nin which position-sensitive filters are shared across different object classes\nfor performing localization. For fine-grained classification, these\nposition-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9%\non the ImageNet detection dataset and outperforms YOLO-9000 by 18% while\nprocessing 30 images per second. We also show that the objectness learned by\nR-FCN-3000 generalizes to novel classes and the performance increases with the\nnumber of training object classes - supporting the hypothesis that it is\npossible to learn a universal objectness detector. Code will be made available.",
        "This empirical study estimates resilience (adaptive capacity) around the\nperiods of the 2013 heavy flood in Cambodia. We use nearly 1.2 million\nmicrofinance institution (MFI) customer data and implement the unsupervised\nlearning method. Our results highlight the opportunity to develop resilience by\nhaving a better understanding of which areas are likely to be more or less\nresilient based on the characteristics of the MFI customers, and the individual\nchoices or situations that support stronger adaptiveness. We also discuss the\nlimitation of this approach.",
        "A crucial factor to trust Machine Learning (ML) algorithm decisions is a good\nrepresentation of its application field by the training dataset. This is\nparticularly true when parts of the training data have been artificially\ngenerated to overcome common training problems such as lack of data or\nimbalanced dataset. Over the last few years, Generative Adversarial Networks\n(GANs) have shown remarkable results in generating realistic data. However,\nthis ML approach lacks an objective function to evaluate the quality of the\ngenerated data. Numerous GAN applications focus on generating image data mostly\nbecause they can be easily evaluated by a human eye. Less efforts have been\nmade to generate time series data. Assessing their quality is more complicated,\nparticularly for technical data. In this paper, we propose a human-centered\napproach supporting a ML or domain expert to accomplish this task using Visual\nAnalytics (VA) techniques. The presented approach consists of two views, namely\na GAN Iteration View showing similarity metrics between real and generated data\nover the iterations of the generation process and a Detailed Comparative View\nequipped with different time series visualizations such as TimeHistograms, to\ncompare the generated data at different iteration steps. Starting from the GAN\nIteration View, the user can choose suitable iteration steps for detailed\ninspection. We evaluate our approach with a usage scenario that enabled an\nefficient comparison of two different GAN models.",
        "ColorCheckers are reference standards that professional photographers and\nfilmmakers use to ensure predictable results under every lighting condition.\nThe objective of this work is to propose a new fast and robust method for\nautomatic ColorChecker detection. The process is divided into two steps: (1)\nColorCheckers localization and (2) ColorChecker patches recognition. For the\nColorChecker localization, we trained a detection convolutional neural network\nusing synthetic images. The synthetic images are created with the 3D models of\nthe ColorChecker and different background images. The output of the neural\nnetworks are the bounding box of each possible ColorChecker candidates in the\ninput image. Each bounding box defines a cropped image which is evaluated by a\nrecognition system, and each image is canonized with regards to color and\ndimensions. Subsequently, all possible color patches are extracted and grouped\nwith respect to the center's distance. Each group is evaluated as a candidate\nfor a ColorChecker part, and its position in the scene is estimated. Finally, a\ncost function is applied to evaluate the accuracy of the estimation. The method\nis tested using real and synthetic images. The proposed method is fast, robust\nto overlaps and invariant to affine projections. The algorithm also performs\nwell in case of multiple ColorCheckers detection.",
        "This paper reports on WaterGAN, a generative adversarial network (GAN) for\ngenerating realistic underwater images from in-air image and depth pairings in\nan unsupervised pipeline used for color correction of monocular underwater\nimages. Cameras onboard autonomous and remotely operated vehicles can capture\nhigh resolution images to map the seafloor, however, underwater image formation\nis subject to the complex process of light propagation through the water\ncolumn. The raw images retrieved are characteristically different than images\ntaken in air due to effects such as absorption and scattering, which cause\nattenuation of light at different rates for different wavelengths. While this\nphysical process is well described theoretically, the model depends on many\nparameters intrinsic to the water column as well as the objects in the scene.\nThese factors make recovery of these parameters difficult without simplifying\nassumptions or field calibration, hence, restoration of underwater images is a\nnon-trivial problem. Deep learning has demonstrated great success in modeling\ncomplex nonlinear systems but requires a large amount of training data, which\nis difficult to compile in deep sea environments. Using WaterGAN, we generate a\nlarge training dataset of paired imagery, both raw underwater and true color\nin-air, as well as depth data. This data serves as input to a novel end-to-end\nnetwork for color correction of monocular underwater images. Due to the\ndepth-dependent water column effects inherent to underwater environments, we\nshow that our end-to-end network implicitly learns a coarse depth estimate of\nthe underwater scene from monocular underwater images. Our proposed pipeline is\nvalidated with testing on real data collected from both a pure water tank and\nfrom underwater surveys in field testing. Source code is made publicly\navailable with sample datasets and pretrained models.",
        "In practical chiller systems, applying efficient fault diagnosis techniques\ncan significantly reduce energy consumption and improve energy efficiency of\nbuildings. The success of the existing methods for fault diagnosis of chillers\nrelies on the condition that sufficient labeled data are available for\ntraining. However, label acquisition is laborious and costly in practice.\nUsually, the number of labeled data is limited and most data available are\nunlabeled. The existing methods cannot exploit the information contained in\nunlabeled data, which significantly limits the improvement of fault diagnosis\nperformance in chiller systems. To make effective use of unlabeled data to\nfurther improve fault diagnosis performance and reduce the dependency on\nlabeled data, we proposed a novel semi-supervised data-driven fault diagnosis\nmethod for chiller systems based on the semi-generative adversarial network,\nwhich incorporates both unlabeled and labeled data into learning process. The\nsemi-generative adversarial network can learn the information of data\ndistribution from unlabeled data and this information can help to significantly\nimprove the diagnostic performance. Experimental results demonstrate the\neffectiveness of the proposed method. Under the scenario that there are only 80\nlabeled samples and 16000 unlabeled samples, the proposed method can improve\nthe diagnostic accuracy to 84%, while the supervised baseline methods only\nreach the accuracy of 65% at most. Besides, the minimal required number of\nlabeled samples can be reduced by about 60% with the proposed method when there\nare enough unlabeled samples.",
        "In this paper we consider the problem of learning an $\\epsilon$-optimal\npolicy for a discounted Markov Decision Process (MDP). Given an MDP with $S$\nstates, $A$ actions, the discount factor $\\gamma \\in (0,1)$, and an\napproximation threshold $\\epsilon > 0$, we provide a model-free algorithm to\nlearn an $\\epsilon$-optimal policy with sample complexity\n$\\tilde{O}(\\frac{SA\\ln(1/p)}{\\epsilon^2(1-\\gamma)^{5.5}})$ (where the notation\n$\\tilde{O}(\\cdot)$ hides poly-logarithmic factors of $S,A,1/(1-\\gamma)$, and\n$1/\\epsilon$) and success probability $(1-p)$. For small enough $\\epsilon$, we\nshow an improved algorithm with sample complexity\n$\\tilde{O}(\\frac{SA\\ln(1/p)}{\\epsilon^2(1-\\gamma)^{3}})$. While the first bound\nimproves upon all known model-free algorithms and model-based ones with tight\ndependence on $S$, our second algorithm beats all known sample complexity\nbounds and matches the information theoretic lower bound up to logarithmic\nfactors.",
        "In recent years, sentiment analysis and emotion classification are two of the\nmost abundantly used techniques in the field of Natural Language Processing\n(NLP). Although sentiment analysis and emotion classification are used commonly\nin applications such as analyzing customer reviews, the popularity of\ncandidates contesting in elections, and comments about various sporting events;\nhowever, in this study, we have examined their application for epidemic\noutbreak detection. Early outbreak detection is the key to deal with epidemics\neffectively, however, the traditional ways of outbreak detection are\ntime-consuming which inhibits prompt response from the respective departments.\nSocial media platforms such as Twitter, Facebook, Instagram, etc. allow the\nusers to express their thoughts related to different aspects of life, and\ntherefore, serve as a substantial source of information in such situations. The\nproposed study exploits the bilingual (Urdu and English) data from Twitter and\nNEWS websites related to the dengue epidemic in Pakistan, and sentiment\nanalysis and emotion classification are performed to acquire deep insights from\nthe data set for gaining a fair idea related to an epidemic outbreak. Machine\nlearning and deep learning algorithms have been used to train and implement the\nmodels for the execution of both tasks. The comparative performance of each\nmodel has been evaluated using accuracy, precision, recall, and f1-measure.",
        "Graph neural networks (GNNs) have achieved strong performance in various\napplications. In the real world, network data is usually formed in a streaming\nfashion. The distributions of patterns that refer to neighborhood information\nof nodes may shift over time. The GNN model needs to learn the new patterns\nthat cannot yet be captured. But learning incrementally leads to the\ncatastrophic forgetting problem that historical knowledge is overwritten by\nnewly learned knowledge. Therefore, it is important to train GNN model to learn\nnew patterns and maintain existing patterns simultaneously, which few works\nfocus on. In this paper, we propose a streaming GNN model based on continual\nlearning so that the model is trained incrementally and up-to-date node\nrepresentations can be obtained at each time step. Firstly, we design an\napproximation algorithm to detect new coming patterns efficiently based on\ninformation propagation. Secondly, we combine two perspectives of data\nreplaying and model regularization for existing pattern consolidation.\nSpecially, a hierarchy-importance sampling strategy for nodes is designed and a\nweighted regularization term for GNN parameters is derived, achieving greater\nstability and generalization of knowledge consolidation. Our model is evaluated\non real and synthetic data sets and compared with multiple baselines. The\nresults of node classification prove that our model can efficiently update\nmodel parameters and achieve comparable performance to model retraining. In\naddition, we also conduct a case study on the synthetic data, and carry out\nsome specific analysis for each part of our model, illustrating its ability to\nlearn new knowledge and maintain existing knowledge from different\nperspectives.",
        "Recently, deep learning based video object detection has attracted more and\nmore attention. Compared with object detection of static images, video object\ndetection is more challenging due to the motion of objects, while providing\nrich temporal information. The RNN-based algorithm is an effective way to\nenhance detection performance in videos with temporal information. However,\nmost studies in this area only focus on accuracy while ignoring the calculation\ncost and the number of parameters.\n  In this paper, we propose an efficient method that combines channel-reduced\nconvolutional GRU (Squeezed GRU), and Information Entropy map for video object\ndetection (SGE-Net). The experimental results validate the accuracy\nimprovement, computational savings of the Squeezed GRU, and superiority of the\ninformation entropy attention mechanism on the classification performance. The\nmAP has increased by 3.7 contrasted with the baseline, and the number of\nparameters has decreased from 6.33 million to 0.67 million compared with the\nstandard GRU.",
        "Urban air pollution has become a major environmental problem that threatens\npublic health. It has become increasingly important to infer fine-grained urban\nair quality based on existing monitoring stations. One of the challenges is how\nto effectively select some relevant stations for air quality inference. In this\npaper, we propose a novel model based on reinforcement learning for urban air\nquality inference. The model consists of two modules: a station selector and an\nair quality regressor. The station selector dynamically selects the most\nrelevant monitoring stations when inferring air quality. The air quality\nregressor takes in the selected stations and makes air quality inference with\ndeep neural network. We conduct experiments on a real-world air quality dataset\nand our approach achieves the highest performance compared with several popular\nsolutions, and the experiments show significant effectiveness of proposed model\nin tackling problems of air quality inference.",
        "Our objective is video retrieval based on natural language queries. In\naddition, we consider the analogous problem of retrieving sentences or\ngenerating descriptions given an input video. Recent work has addressed the\nproblem by embedding visual and textual inputs into a common space where\nsemantic similarities correlate to distances. We also adopt the embedding\napproach, and make the following contributions: First, we utilize web image\nsearch in sentence embedding process to disambiguate fine-grained visual\nconcepts. Second, we propose embedding models for sentence, image, and video\ninputs whose parameters are learned simultaneously. Finally, we show how the\nproposed model can be applied to description generation. Overall, we observe a\nclear improvement over the state-of-the-art methods in the video and sentence\nretrieval tasks. In description generation, the performance level is comparable\nto the current state-of-the-art, although our embeddings were trained for the\nretrieval tasks.",
        "Facial expression synthesis has drawn much attention in the field of computer\ngraphics and pattern recognition. It has been widely used in face animation and\nrecognition. However, it is still challenging due to the high-level semantic\npresence of large and non-linear face geometry variations. This paper proposes\na Geometry-Guided Generative Adversarial Network (G2-GAN) for photo-realistic\nand identity-preserving facial expression synthesis. We employ facial geometry\n(fiducial points) as a controllable condition to guide facial texture synthesis\nwith specific expression. A pair of generative adversarial subnetworks are\njointly trained towards opposite tasks: expression removal and expression\nsynthesis. The paired networks form a mapping cycle between neutral expression\nand arbitrary expressions, which also facilitate other applications such as\nface transfer and expression invariant face recognition. Experimental results\nshow that our method can generate compelling perceptual results on various\nfacial expression synthesis databases. An expression invariant face recognition\nexperiment is also performed to further show the advantages of our proposed\nmethod.",
        "A deep learning-based monocular depth estimation (MDE) technique is proposed\nfor selection of most informative frames (key frames) of an endoscopic video.\nIn most of the cases, ground truth depth maps of polyps are not readily\navailable and that is why the transfer learning approach is adopted in our\nmethod. An endoscopic modalities generally capture thousands of frames. In this\nscenario, it is quite important to discard low-quality and clinically\nirrelevant frames of an endoscopic video while the most informative frames\nshould be retained for clinical diagnosis. In this view, a key-frame selection\nstrategy is proposed by utilizing the depth information of polyps. In our\nmethod, image moment, edge magnitude, and key-points are considered for\nadaptively selecting the key frames. One important application of our proposed\nmethod could be the 3D reconstruction of polyps with the help of extracted key\nframes. Also, polyps are localized with the help of extracted depth maps.",
        "Normalization operations are essential for state-of-the-art neural networks\nand enable us to train a network from scratch with a large learning rate (LR).\nWe attempt to explain the real effect of Batch Normalization (BN) from the\nperspective of variance transmission by investigating the relationship between\nBN and Weights Normalization (WN). In this work, we demonstrate that the\nproblem of the shift of the average gradient will amplify the variance of every\nconvolutional (conv) layer. We propose Parametric Weights Standardization\n(PWS), a fast and robust to mini-batch size module used for conv filters, to\nsolve the shift of the average gradient. PWS can provide the speed-up of BN.\nBesides, it has less computation and does not change the output of a conv\nlayer. PWS enables the network to converge fast without normalizing the\noutputs. This result enhances the persuasiveness of the shift of the average\ngradient and explains why BN works from the perspective of variance\ntransmission. The code and appendix will be made available on\nhttps://github.com/lyxzzz/PWSConv.",
        "With large-scale integration of renewable generation and distributed energy\nresources (DERs), modern power systems are confronted with new operational\nchallenges, such as growing complexity, increasing uncertainty, and aggravating\nvolatility. Meanwhile, more and more data are becoming available owing to the\nwidespread deployment of smart meters, smart sensors, and upgraded\ncommunication networks. As a result, data-driven control techniques, especially\nreinforcement learning (RL), have attracted surging attention in recent years.\nIn this paper, we provide a tutorial on various RL techniques and how they can\nbe applied to decision-making in power systems. We illustrate RL-based models\nand solutions in three key applications, frequency regulation, voltage control,\nand energy management. We conclude with three critical issues in the\napplication of RL, i.e., safety, scalability, and data. Several potential\nfuture directions are discussed as well.",
        "Sample complexity bounds are a common performance metric in the Reinforcement\nLearning literature. In the discounted cost, infinite horizon setting, all of\nthe known bounds have a factor that is a polynomial in $1/(1-\\gamma)$, where\n$\\gamma < 1$ is the discount factor. For a large discount factor, these bounds\nseem to imply that a very large number of samples is required to achieve an\n$\\varepsilon$-optimal policy. The objective of the present work is to introduce\na new class of algorithms that have sample complexity uniformly bounded for all\n$\\gamma < 1$. One may argue that this is impossible, due to a recent min-max\nlower bound. The explanation is that this previous lower bound is for a\nspecific problem, which we modify, without compromising the ultimate objective\nof obtaining an $\\varepsilon$-optimal policy. Specifically, we show that the\nasymptotic covariance of the Q-learning algorithm with an optimized step-size\nsequence is a quadratic function of $1/(1-\\gamma)$; an expected, and\nessentially known result. The new relative Q-learning algorithm proposed here\nis shown to have asymptotic covariance that is a quadratic in $1/(1- \\rho^*\n\\gamma)$, where $1 - \\rho^* > 0$ is an upper bound on the spectral gap of an\noptimal transition matrix.",
        "Graph neural networks (GNN) have been ubiquitous in graph learning tasks such\nas node classification. Most of GNN methods update the node embedding\niteratively by aggregating its neighbors' information. However, they often\nsuffer from negative disturbance, due to edges connecting nodes with different\nlabels. One approach to alleviate this negative disturbance is to use\nattention, but current attention always considers feature similarity and\nsuffers from the lack of supervision. In this paper, we consider the label\ndependency of graph nodes and propose a decoupling attention mechanism to learn\nboth hard and soft attention. The hard attention is learned on labels for a\nrefined graph structure with fewer inter-class edges. Its purpose is to reduce\nthe aggregation's negative disturbance. The soft attention is learned on\nfeatures maximizing the information gain by message passing over better graph\nstructures. Moreover, the learned attention guides the label propagation and\nthe feature propagation. Extensive experiments are performed on five well-known\nbenchmark graph datasets to verify the effectiveness of the proposed method.",
        "Recently, Generative Adversarial Network (GAN) has been found wide\napplications in style transfer, image-to-image translation and image\nsuper-resolution. In this paper, a color-depth conditional GAN is proposed to\nconcurrently resolve the problems of depth super-resolution and color\nsuper-resolution in 3D videos. Firstly, given the low-resolution depth image\nand low-resolution color image, a generative network is proposed to leverage\nmutual information of color image and depth image to enhance each other in\nconsideration of the geometry structural dependency of color-depth image in the\nsame scene. Secondly, three loss functions, including data loss, total\nvariation loss, and 8-connected gradient difference loss are introduced to\ntrain this generative network in order to keep generated images close to the\nreal ones, in addition to the adversarial loss. Experimental results\ndemonstrate that the proposed approach produces high-quality color image and\ndepth image from low-quality image pair, and it is superior to several other\nleading methods. Besides, we use the same neural network framework to resolve\nthe problem of image smoothing and edge detection at the same time.",
        "Irregularly sampled time series (ISTS) data has irregular temporal intervals\nbetween observations and different sampling rates between sequences. ISTS\ncommonly appears in healthcare, economics, and geoscience. Especially in the\nmedical environment, the widely used Electronic Health Records (EHRs) have\nabundant typical irregularly sampled medical time series (ISMTS) data.\nDeveloping deep learning methods on EHRs data is critical for personalized\ntreatment, precise diagnosis and medical management. However, it is challenging\nto directly use deep learning models for ISMTS data. On the one hand, ISMTS\ndata has the intra-series and inter-series relations. Both the local and global\nstructures should be considered. On the other hand, methods should consider the\ntrade-off between task accuracy and model complexity and remain generality and\ninterpretability. So far, many existing works have tried to solve the above\nproblems and have achieved good results. In this paper, we review these deep\nlearning methods from the perspectives of technology and task. Under the\ntechnology-driven perspective, we summarize them into two categories - missing\ndata-based methods and raw data-based methods. Under the task-driven\nperspective, we also summarize them into two categories - data\nimputation-oriented and downstream task-oriented. For each of them, we point\nout their advantages and disadvantages. Moreover, we implement some\nrepresentative methods and compare them on four medical datasets with two\ntasks. Finally, we discuss the challenges and opportunities in this area.",
        "Deep reinforcement learning (DRL) has gained a lot of attention in recent\nyears, and has been proven to be able to play Atari games and Go at or above\nhuman levels. However, those games are assumed to have a small fixed number of\nactions and could be trained with a simple CNN network. In this paper, we study\na special class of Asian popular card games called Dou Di Zhu, in which two\nadversarial groups of agents must consider numerous card combinations at each\ntime step, leading to huge number of actions. We propose a novel method to\nhandle combinatorial actions, which we call combinational Q-learning (CQL). We\nemploy a two-stage network to reduce action space and also leverage\norder-invariant max-pooling operations to extract relationships between\nprimitive actions. Results show that our method prevails over state-of-the art\nmethods like naive Q-learning and A3C. We develop an easy-to-use card game\nenvironments and train all agents adversarially from sractch, with only\nknowledge of game rules and verify that our agents are comparative to humans.\nOur code to reproduce all reported results will be available online.",
        "This work tackles the problem of temporally coherent face anonymization in\nnatural video streams.We propose JaGAN, a two-stage system starting with\ndetecting and masking out faces with black image patches in all individual\nframes of the video. The second stage leverages a privacy-preserving Video\nGenerative Adversarial Network designed to inpaint the missing image patches\nwith artificially generated faces. Our initial experiments reveal that image\nbased generative models are not capable of inpainting patches showing temporal\ncoherent appearance across neighboring video frames. To address this issue we\nintroduce a newly curated video collection, which is made publicly available\nfor the research community along with this paper. We also introduce the\nIdentity Invariance Score IdI as a means to quantify temporal coherency between\nneighboring frames.",
        "Boundary representation (B-rep) models are the standard way 3D shapes are\ndescribed in Computer-Aided Design (CAD) applications. They combine lightweight\nparametric curves and surfaces with topological information which connects the\ngeometric entities to describe manifolds. In this paper we introduce BRepNet, a\nneural network architecture designed to operate directly on B-rep data\nstructures, avoiding the need to approximate the model as meshes or point\nclouds. BRepNet defines convolutional kernels with respect to oriented coedges\nin the data structure. In the neighborhood of each coedge, a small collection\nof faces, edges and coedges can be identified and patterns in the feature\nvectors from these entities detected by specific learnable parameters. In\naddition, to encourage further deep learning research with B-reps, we publish\nthe Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep\nmodels annotated with information about the modeling operations which created\neach face. We demonstrate that BRepNet can segment these models with higher\naccuracy than methods working on meshes, and point clouds.",
        "Lipschitz constants of neural networks have been explored in various contexts\nin deep learning, such as provable adversarial robustness, estimating\nWasserstein distance, stabilising training of GANs, and formulating invertible\nneural networks. Such works have focused on bounding the Lipschitz constant of\nfully connected or convolutional networks, composed of linear maps and\npointwise non-linearities. In this paper, we investigate the Lipschitz constant\nof self-attention, a non-linear neural network module widely used in sequence\nmodelling. We prove that the standard dot-product self-attention is not\nLipschitz for unbounded input domain, and propose an alternative L2\nself-attention that is Lipschitz. We derive an upper bound on the Lipschitz\nconstant of L2 self-attention and provide empirical evidence for its asymptotic\ntightness. To demonstrate the practical relevance of our theoretical work, we\nformulate invertible self-attention and use it in a Transformer-based\narchitecture for a character-level language modelling task.",
        "Enabling effective and efficient machine learning (ML) over large-scale graph\ndata (e.g., graphs with billions of edges) can have a huge impact on both\nindustrial and scientific applications. However, community efforts to advance\nlarge-scale graph ML have been severely limited by the lack of a suitable\npublic benchmark. For KDD Cup 2021, we present OGB Large-Scale Challenge\n(OGB-LSC), a collection of three real-world datasets for advancing the\nstate-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that\nare orders of magnitude larger than existing ones and covers three core graph\nlearning tasks -- link prediction, graph regression, and node classification.\nFurthermore, OGB-LSC provides dedicated baseline experiments, scaling up\nexpressive graph ML models to the massive datasets. We show that the expressive\nmodels significantly outperform simple scalable baselines, indicating an\nopportunity for dedicated efforts to further improve graph ML at scale. Our\ndatasets and baseline code are released and maintained as part of our OGB\ninitiative (Hu et al., 2020). We hope OGB-LSC at KDD Cup 2021 can empower the\ncommunity to discover innovative solutions for large-scale graph ML.",
        "Real-world planning problems often involve hundreds or even thousands of\nobjects, straining the limits of modern planners. In this work, we address this\nchallenge by learning to predict a small set of objects that, taken together,\nwould be sufficient for finding a plan. We propose a graph neural network\narchitecture for predicting object importance in a single inference pass, thus\nincurring little overhead while greatly reducing the number of objects that\nmust be considered by the planner. Our approach treats the planner and\ntransition model as black boxes, and can be used with any off-the-shelf\nplanner. Empirically, across classical planning, probabilistic planning, and\nrobotic task and motion planning, we find that our method results in planning\nthat is significantly faster than several baselines, including other partial\ngrounding strategies and lifted planners. We conclude that learning to predict\na sufficient set of objects for a planning problem is a simple, powerful, and\ngeneral mechanism for planning in large instances. Video:\nhttps://youtu.be/FWsVJc2fvCE Code: https://git.io/JIsqX",
        "Convolutional Neural Networks (CNN) outperform traditional classification\nmethods in many domains. Recently these methods have gained attention in\nneuroscience and particularly in brain-computer interface (BCI) community.\nHere, we introduce a CNN optimized for classification of brain states from\nmagnetoencephalographic (MEG) measurements. Our CNN design is based on a\ngenerative model of the electromagnetic (EEG and MEG) brain signals and is\nreadily interpretable in neurophysiological terms. We show here that the\nproposed network is able to decode event-related responses as well as\nmodulations of oscillatory brain activity and that it outperforms more complex\nneural networks and traditional classifiers used in the field. Importantly, the\nmodel is robust to inter-individual differences and can successfully generalize\nto new subjects in offline and online classification.",
        "Generative Adversarial Networks (GANs) have recently achieved unprecedented\nsuccess in photo-realistic image synthesis from low-dimensional random noise.\nThe ability to synthesize high-quality content at a large scale brings\npotential risks as the generated samples may lead to misinformation that can\ncreate severe social, political, health, and business hazards. We propose\nSubsetGAN to identify generated content by detecting a subset of anomalous\nnode-activations in the inner layers of pre-trained neural networks. These\nnodes, as a group, maximize a non-parametric measure of divergence away from\nthe expected distribution of activations created from real data. This enable us\nto identify synthesised images without prior knowledge of their distribution.\nSubsetGAN efficiently scores subsets of nodes and returns the group of nodes\nwithin the pre-trained classifier that contributed to the maximum score. The\nclassifier can be a general fake classifier trained over samples from multiple\nsources or the discriminator network from different GANs. Our approach shows\nconsistently higher detection power than existing detection methods across\nseveral state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different\nproportions of generated content.",
        "This paper addresses fast semantic segmentation on video.Video segmentation\noften calls for real-time, or even fasterthan real-time, processing. One common\nrecipe for conserving computation arising from feature extraction is to\npropagate features of few selected keyframes. However, recent advances in fast\nimage segmentation make these solutions less attractive. To leverage fast image\nsegmentation for furthering video segmentation, we propose a simple yet\nefficient propagation framework. Specifically, we perform lightweight flow\nestimation in 1/8-downscaled image space for temporal warping in segmentation\noutpace space. Moreover, we introduce a guided spatially-varying convolution\nfor fusing segmentations derived from the previous and current frames, to\nmitigate propagation error and enable lightweight feature extraction on\nnon-keyframes. Experimental results on Cityscapes and CamVid show that our\nscheme achieves the state-of-the-art accuracy-throughput trade-off on video\nsegmentation.",
        "SSD is one of the state-of-the-art object detection algorithms, and it\ncombines high detection accuracy with real-time speed. However, it is widely\nrecognized that SSD is less accurate in detecting small objects compared to\nlarge objects, because it ignores the context from outside the proposal boxes.\nIn this paper, we present CSSD--a shorthand for context-aware single-shot\nmultibox object detector. CSSD is built on top of SSD, with additional layers\nmodeling multi-scale contexts. We describe two variants of CSSD, which differ\nin their context layers, using dilated convolution layers (DiCSSD) and\ndeconvolution layers (DeCSSD) respectively. The experimental results show that\nthe multi-scale context modeling significantly improves the detection accuracy.\nIn addition, we study the relationship between effective receptive fields\n(ERFs) and the theoretical receptive fields (TRFs), particularly on a VGGNet.\nThe empirical results further strengthen our conclusion that SSD coupled with\ncontext layers achieves better detection results especially for small objects\n($+3.2\\% {\\rm AP}_{@0.5}$ on MS-COCO compared to the newest SSD), while\nmaintaining comparable runtime performance.",
        "Generative Adversarial Networks (GAN) have received wide attention in the\nmachine learning field for their potential to learn high-dimensional, complex\nreal data distribution. Specifically, they do not rely on any assumptions about\nthe distribution and can generate real-like samples from latent space in a\nsimple manner. This powerful property leads GAN to be applied to various\napplications such as image synthesis, image attribute editing, image\ntranslation, domain adaptation and other academic fields. In this paper, we aim\nto discuss the details of GAN for those readers who are familiar with, but do\nnot comprehend GAN deeply or who wish to view GAN from various perspectives. In\naddition, we explain how GAN operates and the fundamental meaning of various\nobjective functions that have been suggested recently. We then focus on how the\nGAN can be combined with an autoencoder framework. Finally, we enumerate the\nGAN variants that are applied to various tasks and other fields for those who\nare interested in exploiting GAN for their research.",
        "Face Anti-spoofing gains increased attentions recently in both academic and\nindustrial fields. With the emergence of various CNN based solutions, the\nmulti-modal(RGB, depth and IR) methods based CNN showed better performance than\nsingle modal classifiers. However, there is a need for improving the\nperformance and reducing the complexity. Therefore, an extreme light network\narchitecture(FeatherNet A/B) is proposed with a streaming module which fixes\nthe weakness of Global Average Pooling and uses less parameters. Our single\nFeatherNet trained by depth image only, provides a higher baseline with 0.00168\nACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure\nwith ``ensemble + cascade'' structure is presented to satisfy the performance\npreferred use cases. Meanwhile, the MMFD dataset is collected to provide more\nattacks and diversity to gain better generalization. We use the fusion method\nin the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the\nresult of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and\n0.9814(TPR@FPR=10e-4).",
        "Albeit intensively studied, false prediction and unclear boundaries are still\nmajor issues of salient object detection. In this paper, we propose a Region\nRefinement Network (RRN), which recurrently filters redundant information and\nexplicitly models boundary information for saliency detection. Different from\nexisting refinement methods, we propose a Region Refinement Module (RRM) that\noptimizes salient region prediction by incorporating supervised attention masks\nin the intermediate refinement stages. The module only brings a minor increase\nin model size and yet significantly reduces false predictions from the\nbackground. To further refine boundary areas, we propose a Boundary Refinement\nLoss (BRL) that adds extra supervision for better distinguishing foreground\nfrom background. BRL is parameter free and easy to train. We further observe\nthat BRL helps retain the integrity in prediction by refining the boundary.\nExtensive experiments on saliency detection datasets show that our refinement\nmodule and loss bring significant improvement to the baseline and can be easily\napplied to different frameworks. We also demonstrate that our proposed model\ngeneralizes well to portrait segmentation and shadow detection tasks.",
        "Depleting lake ice can serve as an indicator for climate change, just like\nsea level rise or glacial retreat. Several Lake Ice Phenological (LIP) events\nserve as sentinels to understand the regional and global climate change. Hence,\nit is useful to monitor long-term lake freezing and thawing patterns. In this\npaper we report a case study for the Oberengadin region of Switzerland, where\nthere are several small- and medium-sized mountain lakes. We observe the LIP\nevents, such as freeze-up, break-up and ice cover duration, across two decades\n(2000-2020) from optical satellite images. We analyse time-series of MODIS\nimagery by estimating spatially resolved maps of lake ice for these Alpine\nlakes with supervised machine learning (and additionally cross-check with VIIRS\ndata when available). To train the classifier we rely on reference data\nannotated manually based on webcam images. From the ice maps we derive\nlong-term LIP trends. Since the webcam data is only available for two winters,\nwe also validate our results against the operational MODIS and VIIRS snow\nproducts. We find a change in complete freeze duration of -0.76 and -0.89 days\nper annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe\nplausible correlations of the LIP trends with climate data measured at nearby\nmeteorological stations. We notice that mean winter air temperature has\nnegative correlation with the freeze duration and break-up events, and positive\ncorrelation with the freeze-up events. Additionally, we observe strong negative\ncorrelation of sunshine during the winter months with the freeze duration and\nbreak-up events.",
        "Despite the progress in automatic detection of radiologic findings from chest\nX-ray (CXR) images in recent years, a quantitative evaluation of the\nexplainability of these models is hampered by the lack of locally labeled\ndatasets for different findings. With the exception of a few expert-labeled\nsmall-scale datasets for specific findings, such as pneumonia and pneumothorax,\nmost of the CXR deep learning models to date are trained on global \"weak\"\nlabels extracted from text reports, or trained via a joint image and\nunstructured text learning strategy. Inspired by the Visual Genome effort in\nthe computer vision community, we constructed the first Chest ImaGenome dataset\nwith a scene graph data structure to describe $242,072$ images. Local\nannotations are automatically produced using a joint rule-based natural\nlanguage processing (NLP) and atlas-based bounding box detection pipeline.\nThrough a radiologist constructed CXR ontology, the annotations for each CXR\nare connected as an anatomy-centered scene graph, useful for image-level\nreasoning and multimodal fusion applications. Overall, we provide: i) $1,256$\ncombinations of relation annotations between $29$ CXR anatomical locations\n(objects with bounding box coordinates) and their attributes, structured as a\nscene graph per image, ii) over $670,000$ localized comparison relations (for\nimproved, worsened, or no change) between the anatomical locations across\nsequential exams, as well as ii) a manually annotated gold standard scene graph\ndataset from $500$ unique patients.",
        "Domain adaptation methods face performance degradation in object detection,\nas the complexity of tasks require more about the transferability of the model.\nWe propose a new perspective on how CNN models gain the transferability,\nviewing the weights of a model as a series of motion patterns. The directions\nof weights, and the gradients, can be divided into domain-specific and\ndomain-invariant parts, and the goal of domain adaptation is to concentrate on\nthe domain-invariant direction while eliminating the disturbance from\ndomain-specific one. Current UDA object detection methods view the two\ndirections as a whole while optimizing, which will cause domain-invariant\ndirection mismatch even if the output features are perfectly aligned. In this\npaper, we propose the domain-specific suppression, an exemplary and\ngeneralizable constraint to the original convolution gradients in\nbackpropagation to detach the two parts of directions and suppress the\ndomain-specific one. We further validate our theoretical analysis and methods\non several domain adaptive object detection tasks, including weather, camera\nconfiguration, and synthetic to real-world adaptation. Our experiment results\nshow significant advance over the state-of-the-art methods in the UDA object\ndetection field, performing a promotion of $10.2\\sim12.2\\%$ mAP on all these\ndomain adaptation scenarios.",
        "RANSAC is an important algorithm in robust optimization and a central\nbuilding block for many computer vision applications. In recent years,\ntraditionally hand-crafted pipelines have been replaced by deep learning\npipelines, which can be trained in an end-to-end fashion. However, RANSAC has\nso far not been used as part of such deep learning pipelines, because its\nhypothesis selection procedure is non-differentiable. In this work, we present\ntwo different ways to overcome this limitation. The most promising approach is\ninspired by reinforcement learning, namely to replace the deterministic\nhypothesis selection by a probabilistic selection for which we can derive the\nexpected loss w.r.t. to all learnable parameters. We call this approach DSAC,\nthe differentiable counterpart of RANSAC. We apply DSAC to the problem of\ncamera localization, where deep learning has so far failed to improve on\ntraditional approaches. We demonstrate that by directly minimizing the expected\nloss of the output camera poses, robustly estimated by RANSAC, we achieve an\nincrease in accuracy. In the future, any deep learning pipeline can use DSAC as\na robust optimization component.",
        "To effectively analyze and design cyberphysical systems (CPS), designers\ntoday have to combat the data deluge problem, i.e., the burden of processing\nintractably large amounts of data produced by complex models and experiments.\nIn this work, we utilize monotonic Parametric Signal Temporal Logic (PSTL) to\ndesign features for unsupervised classification of time series data. This\nenables using off-the-shelf machine learning tools to automatically cluster\nsimilar traces with respect to a given PSTL formula. We demonstrate how this\ntechnique produces interpretable formulas that are amenable to analysis and\nunderstanding using a few representative examples. We illustrate this with case\nstudies related to automotive engine testing, highway traffic analysis, and\nauto-grading massively open online courses.",
        "Purpose of review: This paper presents a review of the current state of the\nart in remote sensing based monitoring of forest disturbances and forest\ndegradation from optical Earth Observation data. Part one comprises an overview\nof currently available optical remote sensing sensors, which can be used for\nforest disturbance and degradation mapping. Part two reviews the two main\ncategories of existing approaches: classical image-to-image change detection\nand time series analysis. Recent findings: With the launch of the Sentinel-2a\nsatellite and available Landsat imagery, time series analysis has become the\nmost promising but also most demanding category of degradation mapping\napproaches. Four time series classification methods are distinguished. The\nmethods are explained and their benefits and drawbacks are discussed. A\nseparate chapter presents a number of recent forest degradation mapping studies\nfor two different ecosystems: temperate forests with a geographical focus on\nEurope and tropical forests with a geographical focus on Africa. Summary: The\nreview revealed that a wide variety of methods for the detection of forest\ndegradation is already available. Today, the main challenge is to transfer\nthese approaches to high resolution time series data from multiple sensors.\nFuture research should also focus on the classification of disturbance types\nand the development of robust up-scalable methods to enable near real time\ndisturbance mapping in support of operational reactive measures.",
        "An instance-weighted variant of the support vector machine (SVM) has\nattracted considerable attention recently since they are useful in various\nmachine learning tasks such as non-stationary data analysis, heteroscedastic\ndata modeling, transfer learning, learning to rank, and transduction. An\nimportant challenge in these scenarios is to overcome the computational\nbottleneck---instance weights often change dynamically or adaptively, and thus\nthe weighted SVM solutions must be repeatedly computed. In this paper, we\ndevelop an algorithm that can efficiently and exactly update the weighted SVM\nsolutions for arbitrary change of instance weights. Technically, this\ncontribution can be regarded as an extension of the conventional solution-path\nalgorithm for a single regularization parameter to multiple instance-weight\nparameters. However, this extension gives rise to a significant problem that\nbreakpoints (at which the solution path turns) have to be identified in\nhigh-dimensional space. To facilitate this, we introduce a parametric\nrepresentation of instance weights. We also provide a geometric interpretation\nin weight space using a notion of critical region: a polyhedron in which the\ncurrent affine solution remains to be optimal. Then we find breakpoints at\nintersections of the solution path and boundaries of polyhedrons. Through\nextensive experiments on various practical applications, we demonstrate the\nusefulness of the proposed algorithm.",
        "Visual place recognition is challenging in the urban environment and is\nusually viewed as a large scale image retrieval task. The intrinsic challenges\nin place recognition exist that the confusing objects such as cars and trees\nfrequently occur in the complex urban scene, and buildings with repetitive\nstructures may cause over-counting and the burstiness problem degrading the\nimage representations. To address these problems, we present an Attention-based\nPyramid Aggregation Network (APANet), which is trained in an end-to-end manner\nfor place recognition. One main component of APANet, the spatial pyramid\npooling, can effectively encode the multi-size buildings containing\ngeo-information. The other one, the attention block, is adopted as a region\nevaluator for suppressing the confusing regional features while highlighting\nthe discriminative ones. When testing, we further propose a simple yet\neffective PCA power whitening strategy, which significantly improves the widely\nused PCA whitening by reasonably limiting the impact of over-counting.\nExperimental evaluations demonstrate that the proposed APANet outperforms the\nstate-of-the-art methods on two place recognition benchmarks, and generalizes\nwell on standard image retrieval datasets.",
        "In this paper, we focus on obtaining 2D and 3D labels, as well as track IDs\nfor objects on the road with the help of a novel 3D Bounding Box Annotation\nToolbox (3D BAT). Our open source, web-based 3D BAT incorporates several smart\nfeatures to improve usability and efficiency. For instance, this annotation\ntoolbox supports semi-automatic labeling of tracks using interpolation, which\nis vital for downstream tasks like tracking, motion planning and motion\nprediction. Moreover, annotations for all camera images are automatically\nobtained by projecting annotations from 3D space into the image domain. In\naddition to the raw image and point cloud feeds, a Masterview consisting of the\ntop view (bird's-eye-view), side view and front views is made available to\nobserve objects of interest from different perspectives. Comparisons of our\nmethod with other publicly available annotation tools reveal that 3D\nannotations can be obtained faster and more efficiently by using our toolbox.",
        "Physiological signals that provide the objective repression of human\naffective states are attracted increasing attention in the emotion recognition\nfield. However, the single signal is difficult to obtain completely and\naccurately description for emotion. Multiple physiological signals fusing\nmodels, building the uniform classification model by means of consistent and\ncomplementary information from different emotions to improve recognition\nperformance. Original fusing models usually choose the particular\nclassification method to recognition, which is ignoring different distribution\nof multiple signals. Aiming above problems, in this work, we propose an emotion\nclassification model through multiple modal physiological signals for different\nemotions. Features are extracted from EEG, EMG, EOG signals for characterizing\nemotional state on valence and arousal levels. For characterization, four bands\nfiltering theta, beta, alpha, gamma for signal preprocessing are adopted and\nthree Hjorth parameters are computing as features. To improve classification\nperformance, an ensemble classifier is built. Experiments are conducted on the\nbenchmark DEAP datasets. For the two-class task, the best result on arousal is\n94.42\\%, the best result on valence is 94.02\\%, respectively. For the\nfour-class task, the highest average classification accuracy is 90.74, and it\nshows good stability. The influence of different peripheral physiological\nsignals for results is also analyzed in this paper.",
        "We propose a method to predict severity of age related macular degeneration\n(AMD) from input optical coherence tomography (OCT) images. Although there is\nno standard clinical severity scale for AMD, we leverage deep learning (DL)\nbased image registration and clustering methods to identify diseased cases and\npredict their severity. Experiments demonstrate our approach's disease\nclassification performance matches state of the art methods. The predicted\ndisease severity performs well on previously unseen data. Registration output\nprovides better explainability than class activation maps regarding label and\nseverity decisions",
        "Over 34,000 objects bigger than 10 cm in length are known to orbit Earth.\nAmong them, only a small percentage are active satellites, while the rest of\nthe population is made of dead satellites, rocket bodies, and debris that pose\na collision threat to operational spacecraft. Furthermore, the predicted growth\nof the space sector and the planned launch of megaconstellations will add even\nmore complexity, therefore causing the collision risk and the burden on space\noperators to increase. Managing this complex framework with internationally\nagreed methods is pivotal and urgent. In this context, we build a novel\nphysics-based probabilistic generative model for synthetically generating\nconjunction data messages, calibrated using real data. By conditioning on\nobservations, we use the model to obtain posterior distributions via Bayesian\ninference. We show that the probabilistic programming approach to conjunction\nassessment can help in making predictions and in finding the parameters that\nexplain the observed data in conjunction data messages, thus shedding more\nlight on key variables and orbital characteristics that more likely lead to\nconjunction events. Moreover, our technique enables the generation of\nphysically accurate synthetic datasets of collisions, answering a fundamental\nneed of the space and machine learning communities working in this area.",
        "Counterfactual explanations can be obtained by identifying the smallest\nchange made to a feature vector to qualitatively influence a prediction; for\nexample, from 'loan rejected' to 'awarded' or from 'high risk of cardiovascular\ndisease' to 'low risk'. Previous approaches often emphasized that\ncounterfactuals should be easily interpretable to humans, motivating sparse\nsolutions with few changes to the feature vectors. However, these approaches\nwould not ensure that the produced counterfactuals be proximate (i.e., not\nlocal outliers) and connected to regions with substantial data density (i.e.,\nclose to correctly classified observations), two requirements known as\ncounterfactual faithfulness. These requirements are fundamental when making\nsuggestions to individuals that are indeed attainable. Our contribution is\ntwofold. On one hand, we suggest to complement the catalogue of counterfactual\nquality measures [1] using a criterion to quantify the degree of difficulty for\na certain counterfactual suggestion. On the other hand, drawing ideas from the\nmanifold learning literature, we develop a framework that generates attainable\ncounterfactuals. We suggest the counterfactual conditional heterogeneous\nvariational autoencoder (C-CHVAE) to identify attainable counterfactuals that\nlie within regions of high data density.",
        "Convolutional Neural Networks (CNNs) are successful deep learning models in\nthe field of computer vision. To get the maximum advantage of CNN model for\nHuman Action Recognition (HAR) using inertial sensor data, in this paper, we\nuse 4 types of spatial domain methods for transforming inertial sensor data to\nactivity images, which are then utilized in a novel fusion framework. These\nfour types of activity images are Signal Images (SI), Gramian Angular Field\n(GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP)\nImages. Furthermore, for creating a multimodal fusion framework and to exploit\nactivity image, we made each type of activity images multimodal by convolving\nwith two spatial domain filters : Prewitt filter and High-boost filter.\nResnet-18, a CNN model, is used to learn deep features from multi-modalities.\nLearned features are extracted from the last pooling layer of each ReNet and\nthen fused by canonical correlation based fusion (CCF) for improving the\naccuracy of human action recognition. These highly informative features are\nserved as input to a multiclass Support Vector Machine (SVM). Experimental\nresults on three publicly available inertial datasets show the superiority of\nthe proposed method over the current state-of-the-art.",
        "In this paper, we propose multi-variable LSTM capable of accurate forecasting\nand variable importance interpretation for time series with exogenous\nvariables. Current attention mechanism in recurrent neural networks mostly\nfocuses on the temporal aspect of data and falls short of characterizing\nvariable importance. To this end, the multi-variable LSTM equipped with\ntensorized hidden states is developed to learn hidden states for individual\nvariables, which give rise to our mixture temporal and variable attention.\nBased on such attention mechanism, we infer and quantify variable importance.\nExtensive experiments using real datasets with Granger-causality test and the\nsynthetic dataset with ground truth demonstrate the prediction performance and\ninterpretability of multi-variable LSTM in comparison to a variety of\nbaselines. It exhibits the prospect of multi-variable LSTM as an end-to-end\nframework for both forecasting and knowledge discovery.",
        "Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.",
        "Convolutional Neural Networks (CNNs) are the go-to model for computer vision.\nRecently, attention-based networks, such as the Vision Transformer, have also\nbecome popular. In this paper we show that while convolutions and attention are\nboth sufficient for good performance, neither of them are necessary. We present\nMLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).\nMLP-Mixer contains two types of layers: one with MLPs applied independently to\nimage patches (i.e. \"mixing\" the per-location features), and one with MLPs\napplied across patches (i.e. \"mixing\" spatial information). When trained on\nlarge datasets, or with modern regularization schemes, MLP-Mixer attains\ncompetitive scores on image classification benchmarks, with pre-training and\ninference cost comparable to state-of-the-art models. We hope that these\nresults spark further research beyond the realms of well established CNNs and\nTransformers.",
        "Hybrid modeling, the combination of first principle and machine learning\nmodels, is an emerging research field that gathers more and more attention.\nEven if hybrid models produce formidable results for academic examples, there\nare still different technical challenges that hinder the use of hybrid modeling\nin real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a\nnumerical ODE solver and an ANN, we are paving the way for the use of a variety\nof first principle models from different modeling tools as parts of hybrid\nmodels. This contribution handles the hybrid modeling of a complex, real-world\nexample: Starting with a simplified 1D-fluid model of the human cardiovascular\nsystem (arterial side), the aim is to learn neglected physical effects like\narterial elasticity from data. We will show that the hybrid modeling process is\nmore comfortable, needs less system knowledge and is therefore less error-prone\ncompared to modeling solely based on first principle. Further, the resulting\nhybrid model has improved in computation performance, compared to a pure first\nprinciple white-box model, while still fulfilling the requirements regarding\naccuracy of the considered hemodynamic quantities. The use of the presented\ntechniques is explained in a general manner and the considered use-case can\nserve as example for other modeling and simulation applications in and beyond\nthe medical domain.",
        "3D object detection is an essential task in autonomous driving. Recent\ntechniques excel with highly accurate detection rates, provided the 3D input\ndata is obtained from precise but expensive LiDAR technology. Approaches based\non cheaper monocular or stereo imagery data have, until now, resulted in\ndrastically lower accuracies --- a gap that is commonly attributed to poor\nimage-based depth estimation. However, in this paper we argue that it is not\nthe quality of the data but its representation that accounts for the majority\nof the difference. Taking the inner workings of convolutional neural networks\ninto consideration, we propose to convert image-based depth maps to\npseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With\nthis representation we can apply different existing LiDAR-based detection\nalgorithms. On the popular KITTI benchmark, our approach achieves impressive\nimprovements over the existing state-of-the-art in image-based performance ---\nraising the detection accuracy of objects within the 30m range from the\nprevious state-of-the-art of 22% to an unprecedented 74%. At the time of\nsubmission our algorithm holds the highest entry on the KITTI 3D object\ndetection leaderboard for stereo-image-based approaches. Our code is publicly\navailable at https://github.com/mileyan/pseudo_lidar.",
        "We consider the problem of anomaly detection with a small set of partially\nlabeled anomaly examples and a large-scale unlabeled dataset. This is a common\nscenario in many important applications. Existing related methods either\nexclusively fit the limited anomaly examples that typically do not span the\nentire set of anomalies, or proceed with unsupervised learning from the\nunlabeled data. We propose here instead a deep reinforcement learning-based\napproach that enables an end-to-end optimization of the detection of both\nlabeled and unlabeled anomalies. This approach learns the known abnormality by\nautomatically interacting with an anomaly-biased simulation environment, while\ncontinuously extending the learned abnormality to novel classes of anomaly\n(i.e., unknown anomalies) by actively exploring possible anomalies in the\nunlabeled data. This is achieved by jointly optimizing the exploitation of the\nsmall labeled anomaly data and the exploration of the rare unlabeled anomalies.\nExtensive experiments on 48 real-world datasets show that our model\nsignificantly outperforms five state-of-the-art competing methods.",
        "While neural networks have acted as a strong unifying force in the design of\nmodern AI systems, the neural network architectures themselves remain highly\nheterogeneous due to the variety of tasks to be solved. In this chapter, we\nexplore how to adapt the Layer-wise Relevance Propagation (LRP) technique used\nfor explaining the predictions of feed-forward networks to the LSTM\narchitecture used for sequential data modeling and forecasting. The special\naccumulators and gated interactions present in the LSTM require both a new\npropagation scheme and an extension of the underlying theoretical framework to\ndeliver faithful explanations.",
        "The successful application of deep learning to many visual recognition tasks\nrelies heavily on the availability of a large amount of labeled data which is\nusually expensive to obtain. The few-shot learning problem has attracted\nincreasing attention from researchers for building a robust model upon only a\nfew labeled samples. Most existing works tackle this problem under the\nmeta-learning framework by mimicking the few-shot learning task with an\nepisodic training strategy. In this paper, we propose a new transfer-learning\nframework for semi-supervised few-shot learning to fully utilize the auxiliary\ninformation from labeled base-class data and unlabeled novel-class data. The\nframework consists of three components: 1) pre-training a feature extractor on\nbase-class data; 2) using the feature extractor to initialize the classifier\nweights for the novel classes; and 3) further updating the model with a\nsemi-supervised learning method. Under the proposed framework, we develop a\nnovel method for semi-supervised few-shot learning called TransMatch by\ninstantiating the three components with Imprinting and MixMatch. Extensive\nexperiments on two popular benchmark datasets for few-shot learning,\nCUB-200-2011 and miniImageNet, demonstrate that our proposed method can\neffectively utilize the auxiliary information from labeled base-class data and\nunlabeled novel-class data to significantly improve the accuracy of few-shot\nlearning task.",
        "We present a unified, efficient and effective framework for point-cloud based\n3D object detection. Our two-stage approach utilizes both voxel representation\nand raw point cloud data to exploit respective advantages. The first stage\nnetwork, with voxel representation as input, only consists of light\nconvolutional operations, producing a small number of high-quality initial\npredictions. Coordinate and indexed convolutional feature of each point in\ninitial prediction are effectively fused with the attention mechanism,\npreserving both accurate localization and context information. The second stage\nworks on interior points with their fused feature for further refining the\nprediction. Our method is evaluated on KITTI dataset, in terms of both 3D and\nBird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS\ndetection rate.",
        "A recent source of concern for the security of neural networks is the\nemergence of clean-label dataset poisoning attacks, wherein correctly labeled\npoison samples are injected into the training dataset. While these poison\nsamples look legitimate to the human observer, they contain malicious\ncharacteristics that trigger a targeted misclassification during inference. We\npropose a scalable and transferable clean-label poisoning attack against\ntransfer learning, which creates poison images with their center close to the\ntarget image in the feature space. Our attack, Bullseye Polytope, improves the\nattack success rate of the current state-of-the-art by 26.75% in end-to-end\ntransfer learning, while increasing attack speed by a factor of 12. We further\nextend Bullseye Polytope to a more practical attack model by including multiple\nimages of the same object (e.g., from different angles) when crafting the\npoison samples. We demonstrate that this extension improves attack\ntransferability by over 16% to unseen images (of the same object) without using\nextra poison samples.",
        "The performance of object detection, to a great extent, depends on the\navailability of large annotated datasets. To alleviate the annotation cost, the\nresearch community has explored a number of ways to exploit unlabeled or weakly\nlabeled data. However, such efforts have met with limited success so far. In\nthis work, we revisit the problem with a pragmatic standpoint, trying to\nexplore a new balance between detection performance and annotation cost by\njointly exploiting fully and weakly annotated data. Specifically, we propose a\nweakly- and semi-supervised object detection framework (WSSOD), which involves\na two-stage learning procedure. An agent detector is first trained on a joint\ndataset and then used to predict pseudo bounding boxes on weakly-annotated\nimages. The underlying assumptions in the current as well as common\nsemi-supervised pipelines are also carefully examined under a unified EM\nformulation. On top of this framework, weakly-supervised loss (WSL), label\nattention and random pseudo-label sampling (RPS) strategies are introduced to\nrelax these assumptions, bringing additional improvement on the efficacy of the\ndetection pipeline. The proposed framework demonstrates remarkable performance\non PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to\nthose obtained in fully-supervised settings, with only one third of the\nannotations.",
        "Images can be segmented by first using a classifier to predict an affinity\ngraph that reflects the degree to which image pixels must be grouped together\nand then partitioning the graph to yield a segmentation. Machine learning has\nbeen applied to the affinity classifier to produce affinity graphs that are\ngood in the sense of minimizing edge misclassification rates. However, this\nerror measure is only indirectly related to the quality of segmentations\nproduced by ultimately partitioning the affinity graph. We present the first\nmachine learning algorithm for training a classifier to produce affinity graphs\nthat are good in the sense of producing segmentations that directly minimize\nthe Rand index, a well known segmentation performance measure. The Rand index\nmeasures segmentation performance by quantifying the classification of the\nconnectivity of image pixel pairs after segmentation. By using the simple graph\npartitioning algorithm of finding the connected components of the thresholded\naffinity graph, we are able to train an affinity classifier to directly\nminimize the Rand index of segmentations resulting from the graph partitioning.\nOur learning algorithm corresponds to the learning of maximin affinities\nbetween image pixel pairs, which are predictive of the pixel-pair connectivity.",
        "Surrogate explainers of black-box machine learning predictions are of\nparamount importance in the field of eXplainable Artificial Intelligence since\nthey can be applied to any type of data (images, text and tabular), are\nmodel-agnostic and are post-hoc (i.e., can be retrofitted). The Local\nInterpretable Model-agnostic Explanations (LIME) algorithm is often mistakenly\nunified with a more general framework of surrogate explainers, which may lead\nto a belief that it is the solution to surrogate explainability. In this paper\nwe empower the community to \"build LIME yourself\" (bLIMEy) by proposing a\nprincipled algorithmic framework for building custom local surrogate explainers\nof black-box model predictions, including LIME itself. To this end, we\ndemonstrate how to decompose the surrogate explainers family into\nalgorithmically independent and interoperable modules and discuss the influence\nof these component choices on the functional capabilities of the resulting\nexplainer, using the example of LIME.",
        "In many real-world reinforcement learning (RL) problems, besides optimizing\nthe main objective function, an agent must concurrently avoid violating a\nnumber of constraints. In particular, besides optimizing performance it is\ncrucial to guarantee the safety of an agent during training as well as\ndeployment (e.g. a robot should avoid taking actions - exploratory or not -\nwhich irrevocably harm its hardware). To incorporate safety in RL, we derive\nalgorithms under the framework of constrained Markov decision problems (CMDPs),\nan extension of the standard Markov decision problems (MDPs) augmented with\nconstraints on expected cumulative costs. Our approach hinges on a novel\n\\emph{Lyapunov} method. We define and present a method for constructing\nLyapunov functions, which provide an effective way to guarantee the global\nsafety of a behavior policy during training via a set of local, linear\nconstraints. Leveraging these theoretical underpinnings, we show how to use the\nLyapunov approach to systematically transform dynamic programming (DP) and RL\nalgorithms into their safe counterparts. To illustrate their effectiveness, we\nevaluate these algorithms in several CMDP planning and decision-making tasks on\na safety benchmark domain. Our results show that our proposed method\nsignificantly outperforms existing baselines in balancing constraint\nsatisfaction and performance.",
        "We introduced a high-resolution equirectangular panorama (360-degree, virtual\nreality) dataset for object detection and propose a multi-projection variant of\nYOLO detector. The main challenge with equirectangular panorama image are i)\nthe lack of annotated training data, ii) high-resolution imagery and iii)\nsevere geometric distortions of objects near the panorama projection poles. In\nthis work, we solve the challenges by i) using training examples available in\nthe \"conventional datasets\" (ImageNet and COCO), ii) employing only\nlow-resolution images that require only moderate GPU computing power and\nmemory, and iii) our multi-projection YOLO handles projection distortions by\nmaking multiple stereographic sub-projections. In our experiments, YOLO\noutperforms the other state-of-art detector, Faster RCNN and our\nmulti-projection YOLO achieves the best accuracy with low-resolution input.",
        "Real-time generic object detection on mobile platforms is a crucial but\nchallenging computer vision task. However, previous CNN-based detectors suffer\nfrom enormous computational cost, which hinders them from real-time inference\nin computation-constrained scenarios. In this paper, we investigate the\neffectiveness of two-stage detectors in real-time generic detection and propose\na lightweight two-stage detector named ThunderNet. In the backbone part, we\nanalyze the drawbacks in previous lightweight backbones and present a\nlightweight backbone designed for object detection. In the detection part, we\nexploit an extremely efficient RPN and detection head design. To generate more\ndiscriminative feature representation, we design two efficient architecture\nblocks, Context Enhancement Module and Spatial Attention Module. At last, we\ninvestigate the balance between the input resolution, the backbone, and the\ndetection head. Compared with lightweight one-stage detectors, ThunderNet\nachieves superior performance with only 40% of the computational cost on PASCAL\nVOC and COCO benchmarks. Without bells and whistles, our model runs at 24.1 fps\non an ARM-based device. To the best of our knowledge, this is the first\nreal-time detector reported on ARM platforms. Code will be released for paper\nreproduction.",
        "Compared to RGB semantic segmentation, RGBD semantic segmentation can achieve\nbetter performance by taking depth information into consideration. However, it\nis still problematic for contemporary segmenters to effectively exploit RGBD\ninformation since the feature distributions of RGB and depth (D) images vary\nsignificantly in different scenes. In this paper, we propose an Attention\nComplementary Network (ACNet) that selectively gathers features from RGB and\ndepth branches. The main contributions lie in the Attention Complementary\nModule (ACM) and the architecture with three parallel branches. More precisely,\nACM is a channel attention-based module that extracts weighted features from\nRGB and depth branches. The architecture preserves the inference of the\noriginal RGB and depth branches, and enables the fusion branch at the same\ntime. Based on the above structures, ACNet is capable of exploiting more\nhigh-quality features from different channels. We evaluate our model on\nSUN-RGBD and NYUDv2 datasets, and prove that our model outperforms\nstate-of-the-art methods. In particular, a mIoU score of 48.3\\% on NYUDv2 test\nset is achieved with ResNet50. We will release our source code based on PyTorch\nand the trained segmentation model at https://github.com/anheidelonghu/ACNet.",
        "Deep neural network (DNN) based salient object detection in images based on\nhigh-quality labels is expensive. Alternative unsupervised approaches rely on\ncareful selection of multiple handcrafted saliency methods to generate noisy\npseudo-ground-truth labels. In this work, we propose a two-stage mechanism for\nrobust unsupervised object saliency prediction, where the first stage involves\nrefinement of the noisy pseudo labels generated from different handcrafted\nmethods. Each handcrafted method is substituted by a deep network that learns\nto generate the pseudo labels. These labels are refined incrementally in\nmultiple iterations via our proposed self-supervision technique. In the second\nstage, the refined labels produced from multiple networks representing multiple\nsaliency methods are used to train the actual saliency detection network. We\nshow that this self-learning procedure outperforms all the existing\nunsupervised methods over different datasets. Results are even comparable to\nthose of fully-supervised state-of-the-art approaches. The code is available at\nhttps://tinyurl.com/wtlhgo3 .",
        "In many graphs such as social networks, nodes have associated attributes\nrepresenting their behavior. Predicting node attributes in such graphs is an\nimportant problem with applications in many domains like recommendation\nsystems, privacy preservation, and targeted advertisement. Attributes values\ncan be predicted by analyzing patterns and correlations among attributes and\nemploying classification/regression algorithms. However, these approaches do\nnot utilize readily available network topology information. In this regard,\ninterconnections between different attributes of nodes can be exploited to\nimprove the prediction accuracy. In this paper, we propose an approach to\nrepresent a node by a feature map with respect to an attribute $a_i$ (which is\nused as input for machine learning algorithms) using all attributes of\nneighbors to predict attributes values for $a_i$. We perform extensive\nexperimentation on ten real-world datasets and show that the proposed feature\nmap significantly improves the prediction accuracy as compared to baseline\napproaches on these datasets.",
        "Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.",
        "We propose a novel graph cross network (GXN) to achieve comprehensive feature\nlearning from multiple scales of a graph. Based on trainable hierarchical\nrepresentations of a graph, GXN enables the interchange of intermediate\nfeatures across scales to promote information flow. Two key ingredients of GXN\ninclude a novel vertex infomax pooling (VIPool), which creates multiscale\ngraphs in a trainable manner, and a novel feature-crossing layer, enabling\nfeature interchange across scales. The proposed VIPool selects the most\ninformative subset of vertices based on the neural estimation of mutual\ninformation between vertex features and neighborhood features. The intuition\nbehind is that a vertex is informative when it can maximally reflect its\nneighboring information. The proposed feature-crossing layer fuses intermediate\nfeatures between two scales for mutual enhancement by improving information\nflow and enriching multiscale features at hidden layers. The cross shape of the\nfeature-crossing layer distinguishes GXN from many other multiscale\narchitectures. Experimental results show that the proposed GXN improves the\nclassification accuracy by 2.12% and 1.15% on average for graph classification\nand vertex classification, respectively. Based on the same network, the\nproposed VIPool consistently outperforms other graph-pooling methods.",
        "Privacy protection on human biological information has drawn increasing\nattention in recent years, among which face anonymization plays an importance\nrole. We propose a novel approach which protects identity information of facial\nimages from leakage with slightest modification. Specifically, we disentangle\nidentity representation from other facial attributes leveraging the power of\ngenerative adversarial networks trained on a conditional multi-scale\nreconstruction (CMR) loss and an identity loss. We evaulate the disentangle\nability of our model, and propose an effective method for identity\nanonymization, namely Anonymous Identity Generation (AIG), to reach the goal of\nface anonymization meanwhile maintaining similarity to the original image as\nmuch as possible. Quantitative and qualitative results demonstrate our method's\nsuperiority compared with the SOTAs on both visual quality and anonymization\nsuccess rate.",
        "Recent work on graph generative models has made remarkable progress towards\ngenerating increasingly realistic graphs, as measured by global graph features\nsuch as degree distribution, density, and clustering coefficients. Deep\ngenerative models have also made significant advances through better modelling\nof the local correlations in the graph topology, which have been very useful\nfor predicting unobserved graph components, such as the existence of a link or\nthe class of a node, from nearby observed graph components. A complete\nscientific understanding of graph data should address both global and local\nstructure. In this paper, we propose a joint model for both as complementary\nobjectives in a graph VAE framework. Global structure is captured by\nincorporating graph kernels in a probabilistic model whose loss function is\nclosely related to the maximum mean discrepancy(MMD) between the global\nstructures of the reconstructed and the input graphs. The ELBO objective\nderived from the model regularizes a standard local link reconstruction term\nwith an MMD term. Our experiments demonstrate a significant improvement in the\nrealism of the generated graph structures, typically by 1-2 orders of magnitude\nof graph structure metrics, compared to leading graph VAEand GAN models. Local\nlink reconstruction improves as well in many cases.",
        "Single-view depth estimation suffers from the problem that a network trained\non images from one camera does not generalize to images taken with a different\ncamera model. Thus, changing the camera model requires collecting an entirely\nnew training dataset. In this work, we propose a new type of convolution that\ncan take the camera parameters into account, thus allowing neural networks to\nlearn calibration-aware patterns. Experiments confirm that this improves the\ngeneralization capabilities of depth prediction networks considerably, and\nclearly outperforms the state of the art when the train and test images are\nacquired with different cameras.",
        "A new paradigm is proposed for autonomous driving. The new paradigm lies\nbetween the end-to-end and pipelined approaches, and is inspired by how humans\nsolve the problem. While it relies on scene understanding, the latter only\nconsiders objects that could originate hazard. These are denoted as\naction-inducing, since changes in their state should trigger vehicle actions.\nThey also define a set of explanations for these actions, which should be\nproduced jointly with the latter. An extension of the BDD100K dataset,\nannotated for a set of 4 actions and 21 explanations, is proposed. A new\nmulti-task formulation of the problem, which optimizes the accuracy of both\naction commands and explanations, is then introduced. A CNN architecture is\nfinally proposed to solve this problem, by combining reasoning about action\ninducing objects and global scene context. Experimental results show that the\nrequirement of explanations improves the recognition of action-inducing\nobjects, which in turn leads to better action predictions.",
        "Time series anomalies can offer information relevant to critical situations\nfacing various fields, from finance and aerospace to the IT, security, and\nmedical domains. However, detecting anomalies in time series data is\nparticularly challenging due to the vague definition of anomalies and said\ndata's frequent lack of labels and highly complex temporal correlations.\nCurrent state-of-the-art unsupervised machine learning methods for anomaly\ndetection suffer from scalability and portability issues, and may have high\nfalse positive rates. In this paper, we propose TadGAN, an unsupervised anomaly\ndetection approach built on Generative Adversarial Networks (GANs). To capture\nthe temporal correlations of time series distributions, we use LSTM Recurrent\nNeural Networks as base models for Generators and Critics. TadGAN is trained\nwith cycle consistency loss to allow for effective time-series data\nreconstruction. We further propose several novel methods to compute\nreconstruction errors, as well as different approaches to combine\nreconstruction errors and Critic outputs to compute anomaly scores. To\ndemonstrate the performance and generalizability of our approach, we test\nseveral anomaly scoring techniques and report the best-suited one. We compare\nour approach to 8 baseline anomaly detection methods on 11 datasets from\nmultiple reputable sources such as NASA, Yahoo, Numenta, Amazon, and Twitter.\nThe results show that our approach can effectively detect anomalies and\noutperform baseline methods in most cases (6 out of 11). Notably, our method\nhas the highest averaged F1 score across all the datasets. Our code is open\nsource and is available as a benchmarking tool.",
        "We introduce a new, efficient, principled and backpropagation-compatible\nalgorithm for learning a probability distribution on the weights of a neural\nnetwork, called Bayes by Backprop. It regularises the weights by minimising a\ncompression cost, known as the variational free energy or the expected lower\nbound on the marginal likelihood. We show that this principled kind of\nregularisation yields comparable performance to dropout on MNIST\nclassification. We then demonstrate how the learnt uncertainty in the weights\ncan be used to improve generalisation in non-linear regression problems, and\nhow this weight uncertainty can be used to drive the exploration-exploitation\ntrade-off in reinforcement learning.",
        "GNNs have been proven to perform highly effective in various node-level,\nedge-level, and graph-level prediction tasks in several domains. Existing\napproaches mainly focus on static graphs. However, many graphs change over time\nwith their edge may disappear, or node or edge attribute may alter from one\ntime to the other. It is essential to consider such evolution in representation\nlearning of nodes in time varying graphs. In this paper, we propose a Temporal\nMultilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding\napproach for dynamic graph that incorporates the interdependence of temporal\nrelations into embedding computation. We evaluate the performance of TMP-GNN on\ntwo different representations of temporal multilayered graphs. The performance\nis assessed against the most popular GNNs on node-level prediction tasks. Then,\nwe incorporate TMP-GNN into a deep learning framework to estimate missing data\nand compare the performance with their corresponding competent GNNs from our\nformer experiment, and a baseline method. Experimental results on four\nreal-world datasets yield up to 58% of lower ROC AUC for pairwise node\nclassification task, and 96% of lower MAE in missing feature estimation,\nparticularly for graphs with a relatively high number of nodes and lower mean\ndegree of connectivity.",
        "The online learning of deep neural networks is an interesting problem of\nmachine learning because, for example, major IT companies want to manage the\ninformation of the massive data uploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong learning. We aim to train\ndeep models from new data that consists of new classes, distributions, and\ntasks at minimal computational cost, which we call online deep learning.\nUnfortunately, deep neural network learning through classical online and\nincremental methods does not work well in both theory and practice. In this\npaper, we introduce dual memory architectures for online incremental deep\nlearning. The proposed architecture consists of deep representation learners\nand fast learnable shallow kernel networks, both of which synergize to track\nthe information of new data. During the training phase, we use various online,\nincremental ensemble, and transfer learning techniques in order to achieve\nlower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image\nrecognition tasks, the proposed dual memory architectures performs much better\nthan the classical online and incremental ensemble algorithm, and their\naccuracies are similar to that of the batch learner.",
        "Camera and lidar are important sensor modalities for robotics in general and\nself-driving cars in particular. The sensors provide complementary information\noffering an opportunity for tight sensor-fusion. Surprisingly, lidar-only\nmethods outperform fusion methods on the main benchmark datasets, suggesting a\ngap in the literature. In this work, we propose PointPainting: a sequential\nfusion method to fill this gap. PointPainting works by projecting lidar points\ninto the output of an image-only semantic segmentation network and appending\nthe class scores to each point. The appended (painted) point cloud can then be\nfed to any lidar-only method. Experiments show large improvements on three\ndifferent state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on\nthe KITTI and nuScenes datasets. The painted version of PointRCNN represents a\nnew state of the art on the KITTI leaderboard for the bird's-eye view detection\ntask. In ablation, we study how the effects of Painting depends on the quality\nand format of the semantic segmentation output, and demonstrate how latency can\nbe minimized through pipelining.",
        "This paper addresses the problem of depth estimation from a single still\nimage. Inspired by recent works on multi- scale convolutional neural networks\n(CNN), we propose a deep model which fuses complementary information derived\nfrom multiple CNN side outputs. Different from previous methods, the\nintegration is obtained by means of continuous Conditional Random Fields\n(CRFs). In particular, we propose two different variations, one based on a\ncascade of multiple CRFs, the other on a unified graphical model. By designing\na novel CNN implementation of mean-field updates for continuous CRFs, we show\nthat both proposed models can be regarded as sequential deep networks and that\ntraining can be performed end-to-end. Through extensive experimental evaluation\nwe demonstrate the effective- ness of the proposed approach and establish new\nstate of the art results on publicly available datasets.",
        "In this paper, we propose an end-to-end framework for instance segmentation.\nBased on the recently introduced DETR [1], our method, termed SOLQ, segments\nobjects by learning unified queries. In SOLQ, each query represents one object\nand has multiple representations: class, location and mask. The object queries\nlearned perform classification, box regression and mask encoding simultaneously\nin an unified vector form. During training phase, the mask vectors encoded are\nsupervised by the compression coding of raw spatial masks. In inference time,\nmask vectors produced can be directly transformed to spatial masks by the\ninverse process of compression coding. Experimental results show that SOLQ can\nachieve state-of-the-art performance, surpassing most of existing approaches.\nMoreover, the joint learning of unified query representation can greatly\nimprove the detection performance of original DETR. We hope our SOLQ can serve\nas a strong baseline for the Transformer-based instance segmentation. Code is\navailable at https://github.com/megvii-research/SOLQ.",
        "In deep reinforcement learning (RL) tasks, an efficient exploration mechanism\nshould be able to encourage an agent to take actions that lead to less frequent\nstates which may yield higher accumulative future return. However, both knowing\nabout the future and evaluating the frequentness of states are non-trivial\ntasks, especially for deep RL domains, where a state is represented by\nhigh-dimensional image frames. In this paper, we propose a novel informed\nexploration framework for deep RL, where we build the capability for an RL\nagent to predict over the future transitions and evaluate the frequentness for\nthe predicted future frames in a meaningful manner. To this end, we train a\ndeep prediction model to predict future frames given a state-action pair, and a\nconvolutional autoencoder model to hash over the seen frames. In addition, to\nutilize the counts derived from the seen frames to evaluate the frequentness\nfor the predicted frames, we tackle the challenge of matching the predicted\nfuture frames and their corresponding seen frames at the latent feature level.\nIn this way, we derive a reliable metric for evaluating the novelty of the\nfuture direction pointed by each action, and hence inform the agent to explore\nthe least frequent one.",
        "Adversarial Imitation Learning (AIL) is a class of algorithms in\nReinforcement learning (RL), which tries to imitate an expert without taking\nany reward from the environment and does not provide expert behavior directly\nto the policy training. Rather, an agent learns a policy distribution that\nminimizes the difference from expert behavior in an adversarial setting.\nAdversarial Inverse Reinforcement Learning (AIRL) leverages the idea of AIL,\nintegrates a reward function approximation along with learning the policy, and\nshows the utility of IRL in the transfer learning setting. But the reward\nfunction approximator that enables transfer learning does not perform well in\nimitation tasks. We propose an Off-Policy Adversarial Inverse Reinforcement\nLearning (Off-policy-AIRL) algorithm which is sample efficient as well as gives\ngood imitation performance compared to the state-of-the-art AIL algorithm in\nthe continuous control tasks. For the same reward function approximator, we\nshow the utility of learning our algorithm over AIL by using the learned reward\nfunction to retrain the policy over a task under significant variation where\nexpert demonstrations are absent.",
        "Restricted Boltzmann Machines (RBMs) are generative models which can learn\nuseful representations from samples of a dataset in an unsupervised fashion.\nThey have been widely employed as an unsupervised pre-training method in\nmachine learning. RBMs have been modified to model time series in two main\nways: The Temporal RBM stacks a number of RBMs laterally and introduces\ntemporal dependencies between the hidden layer units; The Conditional RBM, on\nthe other hand, considers past samples of the dataset as a conditional bias and\nlearns a representation which takes these into account. Here we propose a new\ntraining method for both the TRBM and the CRBM, which enforces the dynamic\nstructure of temporal datasets. We do so by treating the temporal models as\ndenoising autoencoders, considering past frames of the dataset as corrupted\nversions of the present frame and minimizing the reconstruction error of the\npresent data by the model. We call this approach Temporal Autoencoding. This\nleads to a significant improvement in the performance of both models in a\nfilling-in-frames task across a number of datasets. The error reduction for\nmotion capture data is 56\\% for the CRBM and 80\\% for the TRBM. Taking the\nposterior mean prediction instead of single samples further improves the\nmodel's estimates, decreasing the error by as much as 91\\% for the CRBM on\nmotion capture data. We also trained the model to perform forecasting on a\nlarge number of datasets and have found TA pretraining to consistently improve\nthe performance of the forecasts. Furthermore, by looking at the prediction\nerror across time, we can see that this improvement reflects a better\nrepresentation of the dynamics of the data as opposed to a bias towards\nreconstructing the observed data on a short time scale.",
        "Vision transformers (ViTs) process input images as sequences of patches via\nself-attention; a radically different architecture than convolutional neural\nnetworks (CNNs). This makes it interesting to study the adversarial feature\nspace of ViT models and their transferability. In particular, we observe that\nadversarial patterns found via conventional adversarial attacks show very low\nblack-box transferability even for large ViT models. However, we show that this\nphenomenon is only due to the sub-optimal attack procedures that do not\nleverage the true representation potential of ViTs. A deep ViT is composed of\nmultiple blocks, with a consistent architecture comprising of self-attention\nand feed-forward layers, where each block is capable of independently producing\na class token. Formulating an attack using only the last class token\n(conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial\ntransferability of ViTs. Using the compositional nature of ViT models, we\nenhance the transferability of existing attacks by introducing two novel\nstrategies specific to the architecture of ViT models. (i) Self-Ensemble: We\npropose a method to find multiple discriminative pathways by dissecting a\nsingle ViT model into an ensemble of networks. This allows explicitly utilizing\nclass-specific information at each ViT block. (ii) Token Refinement: We then\npropose to refine the tokens to further enhance the discriminative capacity at\neach block of ViT. Our token refinement systematically combines the class\ntokens with structural information preserved within the patch tokens. An\nadversarial attack, when applied to such refined tokens within the ensemble of\nclassifiers found in a single vision transformer, has significantly higher\ntransferability.",
        "As a new classification platform, deep learning has recently received\nincreasing attention from researchers and has been successfully applied to many\ndomains. In some domains, like bioinformatics and robotics, it is very\ndifficult to construct a large-scale well-annotated dataset due to the expense\nof data acquisition and costly annotation, which limits its development.\nTransfer learning relaxes the hypothesis that the training data must be\nindependent and identically distributed (i.i.d.) with the test data, which\nmotivates us to use transfer learning to solve the problem of insufficient\ntraining data. This survey focuses on reviewing the current researches of\ntransfer learning by using deep neural network and its applications. We defined\ndeep transfer learning, category and review the recent research works based on\nthe techniques used in deep transfer learning.",
        "In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.",
        "For classification tasks, dictionary learning based methods have attracted\nlots of attention in recent years. One popular way to achieve this purpose is\nto introduce label information to generate a discriminative dictionary to\nrepresent samples. However, compared with traditional dictionary learning, this\ncategory of methods only achieves significant improvements in supervised\nlearning, and has little positive influence on semi-supervised or unsupervised\nlearning. To tackle this issue, we propose a Dynamic Label Dictionary Learning\n(DLDL) algorithm to generate the soft label matrix for unlabeled data.\nSpecifically, we employ hypergraph manifold regularization to keep the\nrelations among original data, transformed data, and soft labels consistent. We\ndemonstrate the efficiency of the proposed DLDL approach on two remote sensing\ndatasets.",
        "In complex transfer learning scenarios new tasks might not be tightly linked\nto previous tasks. Approaches that transfer information contained only in the\nfinal parameters of a source model will therefore struggle. Instead, transfer\nlearning at a higher level of abstraction is needed. We propose Leap, a\nframework that achieves this by transferring knowledge across learning\nprocesses. We associate each task with a manifold on which the training process\ntravels from initialization to final parameters and construct a meta-learning\nobjective that minimizes the expected length of this path. Our framework\nleverages only information obtained during training and can be computed on the\nfly at negligible cost. We demonstrate that our framework outperforms competing\nmethods, both in meta-learning and transfer learning, on a set of computer\nvision tasks. Finally, we demonstrate that Leap can transfer knowledge across\nlearning processes in demanding reinforcement learning environments (Atari)\nthat involve millions of gradient steps.",
        "We present a simple and effective deep convolutional neural network (CNN)\nmodel for video deblurring. The proposed algorithm mainly consists of optical\nflow estimation from intermediate latent frames and latent frame restoration\nsteps. It first develops a deep CNN model to estimate optical flow from\nintermediate latent frames and then restores the latent frames based on the\nestimated optical flow. To better explore the temporal information from videos,\nwe develop a temporal sharpness prior to constrain the deep CNN model to help\nthe latent frame restoration. We develop an effective cascaded training\napproach and jointly train the proposed CNN model in an end-to-end manner. We\nshow that exploring the domain knowledge of video deblurring is able to make\nthe deep CNN model more compact and efficient. Extensive experimental results\nshow that the proposed algorithm performs favorably against state-of-the-art\nmethods on the benchmark datasets as well as real-world videos.",
        "This paper presents a novel method for pedestrian detection and tracking by\nfusing camera and LiDAR sensor data. To deal with the challenges associated\nwith the autonomous driving scenarios, an integrated tracking and detection\nframework is proposed. The detection phase is performed by converting LiDAR\nstreams to computationally tractable depth images, and then, a deep neural\nnetwork is developed to identify pedestrian candidates both in RGB and depth\nimages. To provide accurate information, the detection phase is further\nenhanced by fusing multi-modal sensor information using the Kalman filter. The\ntracking phase is a combination of the Kalman filter prediction and an optical\nflow algorithm to track multiple pedestrians in a scene. We evaluate our\nframework on a real public driving dataset. Experimental results demonstrate\nthat the proposed method achieves significant performance improvement over a\nbaseline method that solely uses image-based pedestrian detection.",
        "We consider the problem of semantic image segmentation using deep\nconvolutional neural networks. We propose a novel network architecture called\nthe label refinement network that predicts segmentation labels in a\ncoarse-to-fine fashion at several resolutions. The segmentation labels at a\ncoarse resolution are used together with convolutional features to obtain finer\nresolution segmentation labels. We define loss functions at several stages in\nthe network to provide supervisions at different stages. Our experimental\nresults on several standard datasets demonstrate that the proposed model\nprovides an effective way of producing pixel-wise dense image labeling.",
        "Neural networks are the pinnacle of Artificial Intelligence, as in recent\nyears we witnessed many novel architectures, learning and optimization\ntechniques for deep learning. Capitalizing on the fact that neural networks\ninherently constitute multipartite graphs among neuron layers, we aim to\nanalyze directly their structure to extract meaningful information that can\nimprove the learning process. To our knowledge graph mining techniques for\nenhancing learning in neural networks have not been thoroughly investigated. In\nthis paper we propose an adapted version of the k-core structure for the\ncomplete weighted multipartite graph extracted from a deep learning\narchitecture. As a multipartite graph is a combination of bipartite graphs,\nthat are in turn the incidence graphs of hypergraphs, we design k-hypercore\ndecomposition, the hypergraph analogue of k-core degeneracy. We applied\nk-hypercore to several neural network architectures, more specifically to\nconvolutional neural networks and multilayer perceptrons for image recognition\ntasks after a very short pretraining. Then we used the information provided by\nthe hypercore numbers of the neurons to re-initialize the weights of the neural\nnetwork, thus biasing the gradient optimization scheme. Extensive experiments\nproved that k-hypercore outperforms the state-of-the-art initialization\nmethods.",
        "RGB-D semantic segmentation has attracted increasing attention over the past\nfew years. Existing methods mostly employ homogeneous convolution operators to\nconsume the RGB and depth features, ignoring their intrinsic differences. In\nfact, the RGB values capture the photometric appearance properties in the\nprojected image space, while the depth feature encodes both the shape of a\nlocal geometry as well as the base (whereabout) of it in a larger context.\nCompared with the base, the shape probably is more inherent and has a stronger\nconnection to the semantics, and thus is more critical for segmentation\naccuracy. Inspired by this observation, we introduce a Shape-aware\nConvolutional layer (ShapeConv) for processing the depth feature, where the\ndepth feature is firstly decomposed into a shape-component and a\nbase-component, next two learnable weights are introduced to cooperate with\nthem independently, and finally a convolution is applied on the re-weighted\ncombination of these two components. ShapeConv is model-agnostic and can be\neasily integrated into most CNNs to replace vanilla convolutional layers for\nsemantic segmentation. Extensive experiments on three challenging indoor RGB-D\nsemantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID,\ndemonstrate the effectiveness of our ShapeConv when employing it over five\npopular architectures. Moreover, the performance of CNNs with ShapeConv is\nboosted without introducing any computation and memory increase in the\ninference phase. The reason is that the learnt weights for balancing the\nimportance between the shape and base components in ShapeConv become constants\nin the inference phase, and thus can be fused into the following convolution,\nresulting in a network that is identical to one with vanilla convolutional\nlayers.",
        "Illumination effects cause problems for many computer vision algorithms. We\npresent a user-friendly interactive system for robust illumination-invariant\nimage generation. Compared with the previous automated illumination-invariant\nimage derivation approaches, our system enables users to specify a particular\nkind of illumination variation for removal. The derivation of\nillumination-invariant image is guided by the user input. The input is a stroke\nthat defines an area covering a set of pixels whose intensities are influenced\npredominately by the illumination variation. This additional flexibility\nenhances the robustness for processing non-linearly rendered images and the\nimages of the scenes where their illumination variations are difficult to\nestimate automatically. Finally, we present some evaluation results of our\nmethod.",
        "It is well known that human gaze carries significant information about visual\nattention. However, there are three main difficulties in incorporating the gaze\ndata in an attention mechanism of deep neural networks: 1) the gaze fixation\npoints are likely to have measurement errors due to blinking and rapid eye\nmovements; 2) it is unclear when and how much the gaze data is correlated with\nvisual attention; and 3) gaze data is not always available in many real-world\nsituations. In this work, we introduce an effective probabilistic approach to\nintegrate human gaze into spatiotemporal attention for egocentric activity\nrecognition. Specifically, we represent the locations of gaze fixation points\nas structured discrete latent variables to model their uncertainties. In\naddition, we model the distribution of gaze fixations using a variational\nmethod. The gaze distribution is learned during the training process so that\nthe ground-truth annotations of gaze locations are no longer needed in testing\nsituations since they are predicted from the learned gaze distribution. The\npredicted gaze locations are used to provide informative attentional cues to\nimprove the recognition performance. Our method outperforms all the previous\nstate-of-the-art approaches on EGTEA, which is a large-scale dataset for\negocentric activity recognition provided with gaze measurements. We also\nperform an ablation study and qualitative analysis to demonstrate that our\nattention mechanism is effective.",
        "Artificial intelligence (AI) has been transforming the practice of drug\ndiscovery in the past decade. Various AI techniques have been used in a wide\nrange of applications, such as virtual screening and drug design. In this\nsurvey, we first give an overview on drug discovery and discuss related\napplications, which can be reduced to two major tasks, i.e., molecular property\nprediction and molecule generation. We then discuss common data resources,\nmolecule representations and benchmark platforms. Furthermore, to summarize the\nprogress of AI in drug discovery, we present the relevant AI techniques\nincluding model architectures and learning paradigms in the papers surveyed. We\nexpect that this survey will serve as a guide for researchers who are\ninterested in working at the interface of artificial intelligence and drug\ndiscovery. We also provide a GitHub repository\n(https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection\nof papers and codes, if applicable, as a learning resource, which is regularly\nupdated.",
        "Anderson acceleration is an old and simple method for accelerating the\ncomputation of a fixed point. However, as far as we know and quite\nsurprisingly, it has never been applied to dynamic programming or reinforcement\nlearning. In this paper, we explain briefly what Anderson acceleration is and\nhow it can be applied to value iteration, this being supported by preliminary\nexperiments showing a significant speed up of convergence, that we critically\ndiscuss. We also discuss how this idea could be applied more generally to\n(deep) reinforcement learning.",
        "A significant issue in training deep neural networks to solve supervised\nlearning tasks is the need for large numbers of labelled datapoints. The goal\nof semi-supervised learning is to leverage ubiquitous unlabelled data, together\nwith small quantities of labelled data, to achieve high task performance.\nThough substantial recent progress has been made in developing semi-supervised\nalgorithms that are effective for comparatively small datasets, many of these\ntechniques do not scale readily to the large (unlaballed) datasets\ncharacteristic of real-world applications. In this paper we introduce a novel\napproach to scalable semi-supervised learning, called Local Label Propagation\n(LLP). Extending ideas from recent work on unsupervised embedding learning, LLP\nfirst embeds datapoints, labelled and otherwise, in a common latent space using\na deep neural network. It then propagates pseudolabels from known to unknown\ndatapoints in a manner that depends on the local geometry of the embedding,\ntaking into account both inter-point distance and local data density as a\nweighting on propagation likelihood. The parameters of the deep embedding are\nthen trained to simultaneously maximize pseudolabel categorization performance\nas well as a metric of the clustering of datapoints within each psuedo-label\ngroup, iteratively alternating stages of network training and label\npropagation. We illustrate the utility of the LLP method on the ImageNet\ndataset, achieving results that outperform previous state-of-the-art scalable\nsemi-supervised learning algorithms by large margins, consistently across a\nwide variety of training regimes. We also show that the feature representation\nlearned with LLP transfers well to scene recognition in the Places 205 dataset.",
        "Several methods exist to infer causal networks from massive volumes of\nobservational data. However, almost all existing methods require a considerable\nlength of time series data to capture cause and effect relationships. In\ncontrast, memory-less transition networks or Markov Chain data, which refers to\none-step transitions to and from an event, have not been explored for causality\ninference even though such data is widely available. We find that causal\nnetwork can be inferred from characteristics of four unique distribution zones\naround each event. We call this Composition of Transitions and show that cause,\neffect, and random events exhibit different behavior in their compositions. We\napplied machine learning models to learn these different behaviors and to infer\ncausality. We name this new method Causality Inference using Composition of\nTransitions (CICT). To evaluate CICT, we used an administrative inpatient\nhealthcare dataset to set up a network of patients transitions between\ndifferent diagnoses. We show that CICT is highly accurate in inferring whether\nthe transition between a pair of events is causal or random and performs well\nin identifying the direction of causality in a bi-directional association.",
        "This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On the COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP. Further incorporating with the Object365 pre-trained model, the\ndetection accuracy reaches 61.3 mAP and the instance segmentation accuracy\nreaches 53.0 mAP, pushing the new state-of-the-art.",
        "We introduce Automorphism-based graph neural networks (Autobahn), a new\nfamily of graph neural networks. In an Autobahn, we decompose the graph into a\ncollection of subgraphs and apply local convolutions that are equivariant to\neach subgraph's automorphism group. Specific choices of local neighborhoods and\nsubgraphs recover existing architectures such as message passing neural\nnetworks. Our formalism also encompasses novel architectures: as an example, we\nintroduce a graph neural network that decomposes the graph into paths and\ncycles. The resulting convolutions reflect the natural way that parts of the\ngraph can transform, preserving the intuitive meaning of convolution without\nsacrificing global permutation equivariance. We validate our approach by\napplying Autobahn to molecular graphs, where it achieves state-of-the-art\nresults.",
        "Deep reinforcement learning (DRL) is a booming area of artificial\nintelligence. Many practical applications of DRL naturally involve more than\none collaborative learners, making it important to study DRL in a multi-agent\ncontext. Previous research showed that effective learning in complex\nmulti-agent systems demands for highly coordinated environment exploration\namong all the participating agents. Many researchers attempted to cope with\nthis challenge through learning centralized value functions. However, the\ncommon strategy for every agent to learn their local policies directly often\nfail to nurture strong inter-agent collaboration and can be sample inefficient\nwhenever agents alter their communication channels. To address these issues, we\npropose a new framework known as centralized training and exploration with\ndecentralized execution via policy distillation. Guided by this framework and\nthe maximum-entropy learning technique, we will first train agents' policies\nwith shared global component to foster coordinated and effective learning.\nLocally executable policies will be derived subsequently from the trained\nglobal policies via policy distillation. Experiments show that our new\nframework and algorithm can achieve significantly better performance and higher\nsample efficiency than a cutting-edge baseline on several multi-agent DRL\nbenchmarks.",
        "In classical reinforcement learning, when exploring an environment, agents\naccept arbitrary short term loss for long term gain. This is infeasible for\nsafety critical applications, such as robotics, where even a single unsafe\naction may cause system failure. In this paper, we address the problem of\nsafely exploring finite Markov decision processes (MDP). We define safety in\nterms of an, a priori unknown, safety constraint that depends on states and\nactions. We aim to explore the MDP under this constraint, assuming that the\nunknown function satisfies regularity conditions expressed via a Gaussian\nprocess prior. We develop a novel algorithm for this task and prove that it is\nable to completely explore the safely reachable part of the MDP without\nviolating the safety constraint. To achieve this, it cautiously explores safe\nstates and actions in order to gain statistical confidence about the safety of\nunvisited state-action pairs from noisy observations collected while navigating\nthe environment. Moreover, the algorithm explicitly considers reachability when\nexploring the MDP, ensuring that it does not get stuck in any state with no\nsafe way out. We demonstrate our method on digital terrain models for the task\nof exploring an unknown map with a rover.",
        "Generative Adversarial Networks (GAN) have attracted much research attention\nrecently, leading to impressive results for natural image generation. However,\nto date little success was observed in using GAN generated images for improving\nclassification tasks. Here we attempt to explore, in the context of car license\nplate recognition, whether it is possible to generate synthetic training data\nusing GAN to improve recognition accuracy. With a carefully-designed pipeline,\nwe show that the answer is affirmative. First, a large-scale image set is\ngenerated using the generator of GAN, without manual annotation. Then, these\nimages are fed to a deep convolutional neural network (DCNN) followed by a\nbidirectional recurrent neural network (BRNN) with long short-term memory\n(LSTM), which performs the feature learning and sequence labelling. Finally,\nthe pre-trained model is fine-tuned on real images. Our experimental results on\na few data sets demonstrate the effectiveness of using GAN images: an\nimprovement of 7.5% over a strong baseline with moderate-sized real data being\navailable. We show that the proposed framework achieves competitive recognition\naccuracy on challenging test datasets. We also leverage the depthwise separate\nconvolution to construct a lightweight convolutional RNN, which is about half\nsize and 2x faster on CPU. Combining this framework and the proposed pipeline,\nwe make progress in performing accurate recognition on mobile and embedded\ndevices.",
        "Visualizing the perceptual content by analyzing human functional magnetic\nresonance imaging (fMRI) has been an active research area. However, due to its\nhigh dimensionality, complex dimensional structure, and small number of samples\navailable, reconstructing realistic images from fMRI remains challenging.\nRecently with the development of convolutional neural network (CNN) and\ngenerative adversarial network (GAN), mapping multi-voxel fMRI data to complex,\nrealistic images has been made possible. In this paper, we propose a model,\nDCNN-GAN, by combining a reconstruction network and GAN. We utilize the CNN for\nhierarchical feature extraction and the DCNN-GAN to reconstruct more realistic\nimages. Extensive experiments have been conducted, showing that our method\noutperforms previous works, regarding reconstruction quality and computational\ncost.",
        "Within the last decade, neural network based predictors have demonstrated\nimpressive - and at times super-human - capabilities. This performance is often\npaid for with an intransparent prediction process and thus has sparked numerous\ncontributions in the novel field of explainable artificial intelligence (XAI).\nIn this paper, we focus on a popular and widely used method of XAI, the\nLayer-wise Relevance Propagation (LRP). Since its initial proposition LRP has\nevolved as a method, and a best practice for applying the method has tacitly\nemerged, based however on humanly observed evidence alone. In this paper we\ninvestigate - and for the first time quantify - the effect of this current best\npractice on feedforward neural networks in a visual object detection setting.\nThe results verify that the layer-dependent approach to LRP applied in recent\nliterature better represents the model's reasoning, and at the same time\nincreases the object localization and class discriminativity of LRP.",
        "Unsupervised learning poses one of the most difficult challenges in computer\nvision today. The task has an immense practical value with many applications in\nartificial intelligence and emerging technologies, as large quantities of\nunlabeled videos can be collected at relatively low cost. In this paper, we\naddress the unsupervised learning problem in the context of detecting the main\nforeground objects in single images. We train a student deep network to predict\nthe output of a teacher pathway that performs unsupervised object discovery in\nvideos or large image collections. Our approach is different from published\nmethods on unsupervised object discovery. We move the unsupervised learning\nphase during training time, then at test time we apply the standard\nfeed-forward processing along the student pathway. This strategy has the\nbenefit of allowing increased generalization possibilities during training,\nwhile remaining fast at testing. Our unsupervised learning algorithm can run\nover several generations of student-teacher training. Thus, a group of student\nnetworks trained in the first generation collectively create the teacher at the\nnext generation. In experiments our method achieves top results on three\ncurrent datasets for object discovery in video, unsupervised image segmentation\nand saliency detection. At test time the proposed system is fast, being one to\ntwo orders of magnitude faster than published unsupervised methods.",
        "In recent years, low-rank based tensor completion, which is a higher-order\nextension of matrix completion, has received considerable attention. However,\nthe low-rank assumption is not sufficient for the recovery of visual data, such\nas color and 3D images, where the ratio of missing data is extremely high. In\nthis paper, we consider \"smoothness\" constraints as well as low-rank\napproximations, and propose an efficient algorithm for performing tensor\ncompletion that is particularly powerful regarding visual data. The proposed\nmethod admits significant advantages, owing to the integration of smooth\nPARAFAC decomposition for incomplete tensors and the efficient selection of\nmodels in order to minimize the tensor rank. Thus, our proposed method is\ntermed as \"smooth PARAFAC tensor completion (SPC).\" In order to impose the\nsmoothness constraints, we employ two strategies, total variation (SPC-TV) and\nquadratic variation (SPC-QV), and invoke the corresponding algorithms for model\nlearning. Extensive experimental evaluations on both synthetic and real-world\nvisual data illustrate the significant improvements of our method, in terms of\nboth prediction performance and efficiency, compared with many state-of-the-art\ntensor completion methods.",
        "Attention is a general reasoning mechanism than can flexibly deal with image\ninformation, but its memory requirements had made it so far impractical for\nhigh resolution image generation. We present Grid Partitioned Attention (GPA),\na new approximate attention algorithm that leverages a sparse inductive bias\nfor higher computational and memory efficiency in image domains: queries attend\nonly to few keys, spatially close queries attend to close keys due to\ncorrelations. Our paper introduces the new attention layer, analyzes its\ncomplexity and how the trade-off between memory usage and model power can be\ntuned by the hyper-parameters.We will show how such attention enables novel\ndeep learning architectures with copying modules that are especially useful for\nconditional image generation tasks like pose morphing. Our contributions are\n(i) algorithm and code1of the novel GPA layer, (ii) a novel deep\nattention-copying architecture, and (iii) new state-of-the art experimental\nresults in human pose morphing generation benchmarks.",
        "Mixture-of-Experts (MoE) with sparse conditional computation has been proved\nan effective architecture for scaling attention-based models to more parameters\nwith comparable computation cost. In this paper, we propose Sparse-MLP, scaling\nthe recent MLP-Mixer model with sparse MoE layers, to achieve a more\ncomputation-efficient architecture. We replace a subset of dense MLP blocks in\nthe MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two\nstages of MoE layers: one with MLP experts mixing information within channels\nalong image patch dimension, one with MLP experts mixing information within\npatches along the channel dimension. Besides, to reduce computational cost in\nrouting and improve expert capacity, we design Re-represent layers in each\nSparse block. These layers are to re-scale image representations by two simple\nbut effective linear transformations. When pre-training on ImageNet-1k with\nMoCo v3 algorithm, our models can outperform dense MLP models by 2.5\\% on\nImageNet Top-1 accuracy with fewer parameters and computational cost. On\nsmall-scale downstream image classification tasks, i.e. Cifar10 and Cifar100,\nour Sparse-MLP can still achieve better performance than baselines.",
        "Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.",
        "Feature importance ranking has become a powerful tool for explainable AI.\nHowever, its nature of combinatorial optimization poses a great challenge for\ndeep learning. In this paper, we propose a novel dual-net architecture\nconsisting of operator and selector for discovery of an optimal feature subset\nof a fixed size and ranking the importance of those features in the optimal\nsubset simultaneously. During learning, the operator is trained for a\nsupervised learning task via optimal feature subset candidates generated by the\nselector that learns predicting the learning performance of the operator\nworking on different optimal subset candidates. We develop an alternate\nlearning algorithm that trains two nets jointly and incorporates a stochastic\nlocal search procedure into learning to address the combinatorial optimization\nchallenge. In deployment, the selector generates an optimal feature subset and\nranks feature importance, while the operator makes predictions based on the\noptimal subset for test data. A thorough evaluation on synthetic, benchmark and\nreal data sets suggests that our approach outperforms several state-of-the-art\nfeature importance ranking and supervised feature selection methods. (Our\nsource code is available: https://github.com/maksym33/FeatureImportanceDL)",
        "Generating realistic images from scene graphs asks neural networks to be able\nto reason about object relationships and compositionality. As a relatively new\ntask, how to properly ensure the generated images comply with scene graphs or\nhow to measure task performance remains an open question. In this paper, we\npropose to harness scene graph context to improve image generation from scene\ngraphs. We introduce a scene graph context network that pools features\ngenerated by a graph convolutional neural network that are then provided to\nboth the image generation network and the adversarial loss. With the context\nnetwork, our model is trained to not only generate realistic looking images,\nbut also to better preserve non-spatial object relationships. We also define\ntwo novel evaluation metrics, the relation score and the mean opinion relation\nscore, for this task that directly evaluate scene graph compliance. We use both\nquantitative and qualitative studies to demonstrate that our pro-posed model\noutperforms the state-of-the-art on this challenging task.",
        "3D object representation learning is a fundamental challenge in computer\nvision to infer about the 3D world. Recent advances in deep learning have shown\ntheir efficiency in 3D object recognition, among which view-based methods have\nperformed best so far. However, feature learning of multiple views in existing\nmethods is mostly performed in a supervised fashion, which often requires a\nlarge amount of data labels with high costs. In contrast, self-supervised\nlearning aims to learn multi-view feature representations without involving\nlabeled data. To this end, we propose a novel self-supervised paradigm to learn\nMulti-View Transformation Equivariant Representations (MV-TER), exploring the\nequivariant transformations of a 3D object and its projected multiple views.\nSpecifically, we perform a 3D transformation on a 3D object, and obtain\nmultiple views before and after the transformation via projection. Then, we\nself-train a representation to capture the intrinsic 3D object representation\nby decoding 3D transformation parameters from the fused feature representations\nof multiple views before and after the transformation. Experimental results\ndemonstrate that the proposed MV-TER significantly outperforms the\nstate-of-the-art view-based approaches in 3D object classification and\nretrieval tasks, and show the generalization to real-world datasets.",
        "Conventional nonlinear subspace learning techniques (e.g., manifold learning)\nusually introduce some drawbacks in explainability (explicit mapping) and\ncost-effectiveness (linearization), generalization capability (out-of-sample),\nand representability (spatial-spectral discrimination). To overcome these\nshortcomings, a novel linearized subspace analysis technique with\nspatial-spectral manifold alignment is developed for a semi-supervised\nhyperspectral dimensionality reduction (HDR), called joint and progressive\nsubspace analysis (JPSA). The JPSA learns a high-level, semantically\nmeaningful, joint spatial-spectral feature representation from hyperspectral\ndata by 1) jointly learning latent subspaces and a linear classifier to find an\neffective projection direction favorable for classification; 2) progressively\nsearching several intermediate states of subspaces to approach an optimal\nmapping from the original space to a potential more discriminative subspace; 3)\nspatially and spectrally aligning manifold structure in each learned latent\nsubspace in order to preserve the same or similar topological property between\nthe compressed data and the original data. A simple but effective classifier,\ni.e., nearest neighbor (NN), is explored as a potential application for\nvalidating the algorithm performance of different HDR approaches. Extensive\nexperiments are conducted to demonstrate the superiority and effectiveness of\nthe proposed JPSA on two widely-used hyperspectral datasets: Indian Pines\n(92.98\\%) and the University of Houston (86.09\\%) in comparison with previous\nstate-of-the-art HDR methods. The demo of this basic work (i.e., ECCV2018) is\nopenly available at https://github.com/danfenghong/ECCV2018_J-Play.",
        "Deep reinforcement learning (DRL) has achieved great successes in many\nsimulated tasks. The sample inefficiency problem makes applying traditional DRL\nmethods to real-world robots a great challenge. Generative Adversarial\nImitation Learning (GAIL) -- a general model-free imitation learning method,\nallows robots to directly learn policies from expert trajectories in large\nenvironments. However, GAIL shares the limitation of other imitation learning\nmethods that they can seldom surpass the performance of demonstrations. In this\npaper, to address the limit of GAIL, we propose GAN-Based Interactive\nReinforcement Learning (GAIRL) from demonstration and human evaluative feedback\nby combining the advantages of GAIL and interactive reinforcement learning. We\ntested our proposed method in six physics-based control tasks, ranging from\nsimple low-dimensional control tasks -- Cart Pole and Mountain Car, to\ndifficult high-dimensional tasks -- Inverted Double Pendulum, Lunar Lander,\nHopper and HalfCheetah. Our results suggest that with both optimal and\nsuboptimal demonstrations, a GAIRL agent can always learn a more stable policy\nwith optimal or close to optimal performance, while the performance of the GAIL\nagent is upper bounded by the performance of demonstrations or even worse than\nit. In addition, our results indicate the reason that GAIRL is superior over\nGAIL is the complementary effect of demonstrations and human evaluative\nfeedback.",
        "Machine learning methods based on statistical principles have proven highly\nsuccessful in dealing with a wide variety of data analysis and analytics tasks.\nTraditional data models are mostly concerned with independent identically\ndistributed data. The recent success of end-to-end modelling scheme using deep\nneural networks equipped with effective structures such as convolutional layers\nor skip connections allows the extension to more sophisticated and structured\npractical data, such as natural language, images, videos, etc. On the\napplication side, vector fields are an extremely useful type of data in\nempirical sciences, as well as signal processing, e.g. non-parametric\ntransformations of 3D point clouds using 3D vector fields, the modelling of the\nfluid flow in earth science, and the modelling of physical fields.\n  This review article is dedicated to recent computational tools of vector\nfields, including vector data representations, predictive model of spatial\ndata, as well as applications in computer vision, signal processing, and\nempirical sciences.",
        "In this paper, we propose a novel label propagation based method for saliency\ndetection. A key observation is that saliency in an image can be estimated by\npropagating the labels extracted from the most certain background and object\nregions. For most natural images, some boundary superpixels serve as the\nbackground labels and the saliency of other superpixels are determined by\nranking their similarities to the boundary labels based on an inner propagation\nscheme. For images of complex scenes, we further deploy a 3-cue-center-biased\nobjectness measure to pick out and propagate foreground labels. A\nco-transduction algorithm is devised to fuse both boundary and objectness\nlabels based on an inter propagation scheme. The compactness criterion decides\nwhether the incorporation of objectness labels is necessary, thus greatly\nenhancing computational efficiency. Results on five benchmark datasets with\npixel-wise accurate annotations show that the proposed method achieves superior\nperformance compared with the newest state-of-the-arts in terms of different\nevaluation metrics.",
        "While visual object detection with deep learning has received much attention\nin the past decade, cases when heavy intra-class occlusions occur have not been\nstudied thoroughly. In this work, we propose a Non-Maximum-Suppression (NMS)\nalgorithm that dramatically improves the detection recall while maintaining\nhigh precision in scenes with heavy occlusions. Our NMS algorithm is derived\nfrom a novel embedding mechanism, in which the semantic and geometric features\nof the detected boxes are jointly exploited. The embedding makes it possible to\ndetermine whether two heavily-overlapping boxes belong to the same object in\nthe physical world. Our approach is particularly useful for car detection and\npedestrian detection in urban scenes where occlusions often happen. We show the\neffectiveness of our approach by creating a model called SG-Det (short for\nSemantics and Geometry Detection) and testing SG-Det on two widely-adopted\ndatasets, KITTI and CityPersons for which it achieves state-of-the-art\nperformance.",
        "Despite the recent progress, 3D multi-person pose estimation from monocular\nvideos is still challenging due to the commonly encountered problem of missing\ninformation caused by occlusion, partially out-of-frame target persons, and\ninaccurate person detection. To tackle this problem, we propose a novel\nframework integrating graph convolutional networks (GCNs) and temporal\nconvolutional networks (TCNs) to robustly estimate camera-centric multi-person\n3D poses that do not require camera parameters. In particular, we introduce a\nhuman-joint GCN, which, unlike the existing GCN, is based on a directed graph\nthat employs the 2D pose estimator's confidence scores to improve the pose\nestimation results. We also introduce a human-bone GCN, which models the bone\nconnections and provides more information beyond human joints. The two GCNs\nwork together to estimate the spatial frame-wise 3D poses and can make use of\nboth visible joint and bone information in the target frame to estimate the\noccluded or missing human-part information. To further refine the 3D pose\nestimation, we use our temporal convolutional networks (TCNs) to enforce the\ntemporal and human-dynamics constraints. We use a joint-TCN to estimate\nperson-centric 3D poses across frames, and propose a velocity-TCN to estimate\nthe speed of 3D joints to ensure the consistency of the 3D pose estimation in\nconsecutive frames. Finally, to estimate the 3D human poses for multiple\npersons, we propose a root-TCN that estimates camera-centric 3D poses without\nrequiring camera parameters. Quantitative and qualitative evaluations\ndemonstrate the effectiveness of the proposed method.",
        "Over the past years, computer vision community has contributed to enormous\nprogress in semantic image segmentation, a per-pixel classification task,\ncrucial for dense scene understanding and rapidly becoming vital in lots of\nreal-world applications, including driverless cars and medical imaging. Most\nrecent models are now reaching previously unthinkable numbers (e.g., 89% mean\niou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a\nrange of other metrics provide the general picture of model performance, in\nthis paper we aim to extend them into other meaningful and important for\napplications characteristics, answering such questions as 'how accurate the\nmodel segmentation is on small objects in the general scene?', or 'what are the\nsources of uncertainty that cause the model to make an erroneous prediction?'.\nBesides establishing a methodology that covers the performance of a single\nmodel from different perspectives, we also showcase several extensions that can\nbe worth pursuing in order to further improve current results in semantic\nsegmentation.",
        "Graph Neural Nets (GNNs) have received increasing attentions, partially due\nto their superior performance in many node and graph classification tasks.\nHowever, there is a lack of understanding on what they are learning and how\nsophisticated the learned graph functions are. In this work, we propose a\ndissection of GNNs on graph classification into two parts: 1) the graph\nfiltering, where graph-based neighbor aggregations are performed, and 2) the\nset function, where a set of hidden node features are composed for prediction.\nTo study the importance of both parts, we propose to linearize them separately.\nWe first linearize the graph filtering function, resulting Graph Feature\nNetwork (GFN), which is a simple lightweight neural net defined on a\n\\textit{set} of graph augmented features. Further linearization of GFN's set\nfunction results in Graph Linear Network (GLN), which is a linear function.\nEmpirically we perform evaluations on common graph classification benchmarks.\nTo our surprise, we find that, despite the simplification, GFN could match or\nexceed the best accuracies produced by recently proposed GNNs (with a fraction\nof computation cost), while GLN underperforms significantly. Our results\ndemonstrate the importance of non-linear set function, and suggest that linear\ngraph filtering with non-linear set function is an efficient and powerful\nscheme for modeling existing graph classification benchmarks.",
        "Autonomous assembly of objects is an essential task in robotics and 3D\ncomputer vision. It has been studied extensively in robotics as a problem of\nmotion planning, actuator control and obstacle avoidance. However, the task of\ndeveloping a generalized framework for assembly robust to structural variants\nremains relatively unexplored. In this work, we tackle this problem using a\nrecurrent graph learning framework considering inter-part relations and the\nprogressive update of the part pose. Our network can learn more plausible\npredictions of shape structure by accounting for priorly assembled parts.\nCompared to the current state-of-the-art, our network yields up to 10%\nimprovement in part accuracy and up to 15% improvement in connectivity accuracy\non the PartNet dataset. Moreover, our resulting latent space facilitates\nexciting applications such as shape recovery from the point-cloud components.\nWe conduct extensive experiments to justify our design choices and demonstrate\nthe effectiveness of the proposed framework.",
        "Time series data play an important role in many applications and their\nanalysis reveals crucial information for understanding the underlying\nprocesses. Among the many time series learning tasks of great importance, we\nhere focus on semi-supervised learning based on a graph representation of the\ndata. Two main aspects are involved in this task. A suitable distance measure\nto evaluate the similarities between time series, and a learning method to make\npredictions based on these distances. However, the relationship between the two\naspects has never been studied systematically in the context of graph-based\nlearning. We describe four different distance measures, including (Soft) DTW\nand MPDist, a distance measure based on the Matrix Profile, as well as four\nsuccessful semi-supervised learning methods, including the graph Allen--Cahn\nmethod and a Graph Convolutional Neural Network. We then compare the\nperformance of the algorithms on binary classification data sets. In our\nfindings we compare the chosen graph-based methods using all distance measures\nand observe that the results vary strongly with respect to the accuracy. As\npredicted by the ``no free lunch'' theorem, no clear best combination to employ\nin all cases is found. Our study provides a reproducible framework for future\nwork in the direction of semi-supervised learning for time series with a focus\non graph representations.",
        "The standard petrography test method for measuring air voids in concrete\n(ASTM C457) requires a meticulous and long examination of sample phase\ncomposition under a stereomicroscope. The high expertise and specialized\nequipment discourage this test for routine concrete quality control. Though the\ntask can be alleviated with the aid of color-based image segmentation,\nadditional surface color treatment is required. Recently, deep learning\nalgorithms using convolutional neural networks (CNN) have achieved\nunprecedented segmentation performance on image testing benchmarks. In this\nstudy, we investigated the feasibility of using CNN to conduct concrete\nsegmentation without the use of color treatment. The CNN demonstrated a strong\npotential to process a wide range of concretes, including those not involved in\nmodel training. The experimental results showed that CNN outperforms the\ncolor-based segmentation by a considerable margin, and has comparable accuracy\nto human experts. Furthermore, the segmentation time is reduced to mere\nseconds.",
        "State-of-the-art image captioning methods mostly focus on improving visual\nfeatures, less attention has been paid to utilizing the inherent properties of\nlanguage to boost captioning performance. In this paper, we show that\nvocabulary coherence between words and syntactic paradigm of sentences are also\nimportant to generate high-quality image caption. Following the conventional\nencoder-decoder framework, we propose the Reflective Decoding Network (RDN) for\nimage captioning, which enhances both the long-sequence dependency and position\nperception of words in a caption decoder. Our model learns to collaboratively\nattend on both visual and textual features and meanwhile perceive each word's\nrelative position in the sentence to maximize the information delivered in the\ngenerated caption. We evaluate the effectiveness of our RDN on the COCO image\ncaptioning datasets and achieve superior performance over the previous methods.\nFurther experiments reveal that our approach is particularly advantageous for\nhard cases with complex scenes to describe by captions.",
        "Preparing and scanning histopathology slides consists of several steps, each\nwith a multitude of parameters. The parameters can vary between pathology labs\nand within the same lab over time, resulting in significant variability of the\ntissue appearance that hampers the generalization of automatic image analysis\nmethods. Typically, this is addressed with ad-hoc approaches such as staining\nnormalization that aim to reduce the appearance variability. In this paper, we\npropose a systematic solution based on domain-adversarial neural networks. We\nhypothesize that removing the domain information from the model representation\nleads to better generalization. We tested our hypothesis for the problem of\nmitosis detection in breast cancer histopathology images and made a comparative\nanalysis with two other approaches. We show that combining color augmentation\nwith domain-adversarial training is a better alternative than standard\napproaches to improve the generalization of deep learning methods.",
        "Brain aging is a widely studied longitudinal process throughout which the\nbrain undergoes considerable morphological changes and various machine learning\napproaches have been proposed to analyze it. Within this context, brain age\nprediction from structural MR images and age-specific brain morphology template\ngeneration are two problems that have attracted much attention. While most\napproaches tackle these tasks independently, we assume that they are inverse\ndirections of the same functional bidirectional relationship between a brain's\nmorphology and an age variable. In this paper, we propose to model this\nrelationship with a single conditional normalizing flow, which unifies brain\nage prediction and age-conditioned generative modeling in a novel way. In an\ninitial evaluation of this idea, we show that our normalizing flow brain aging\nmodel can accurately predict brain age while also being able to generate\nage-specific brain morphology templates that realistically represent the\ntypical aging trend in a healthy population. This work is a step towards\nunified modeling of functional relationships between 3D brain morphology and\nclinical variables of interest with powerful normalizing flows.",
        "This paper presents a method to interpret the success of knowledge\ndistillation by quantifying and analyzing task-relevant and task-irrelevant\nvisual concepts that are encoded in intermediate layers of a deep neural\nnetwork (DNN). More specifically, three hypotheses are proposed as follows. 1.\nKnowledge distillation makes the DNN learn more visual concepts than learning\nfrom raw data. 2. Knowledge distillation ensures that the DNN is prone to\nlearning various visual concepts simultaneously. Whereas, in the scenario of\nlearning from raw data, the DNN learns visual concepts sequentially. 3.\nKnowledge distillation yields more stable optimization directions than learning\nfrom raw data. Accordingly, we design three types of mathematical metrics to\nevaluate feature representations of the DNN. In experiments, we diagnosed\nvarious DNNs, and above hypotheses were verified.",
        "This paper presents a new Graph Neural Network (GNN) type using feature-wise\nlinear modulation (FiLM). Many standard GNN variants propagate information\nalong the edges of a graph by computing \"messages\" based only on the\nrepresentation of the source of each edge. In GNN-FiLM, the representation of\nthe target node of an edge is additionally used to compute a transformation\nthat can be applied to all incoming messages, allowing feature-wise modulation\nof the passed information.\n  Results of experiments comparing different GNN architectures on three tasks\nfrom the literature are presented, based on re-implementations of baseline\nmethods. Hyperparameters for all methods were found using extensive search,\nyielding somewhat surprising results: differences between baseline models are\nsmaller than reported in the literature. Nonetheless, GNN-FiLM outperforms\nbaseline methods on a regression task on molecular graphs and performs\ncompetitively on other tasks.",
        "Visual relations, such as \"person ride bike\" and \"bike next to car\", offer a\ncomprehensive scene understanding of an image, and have already shown their\ngreat utility in connecting computer vision and natural language. However, due\nto the challenging combinatorial complexity of modeling\nsubject-predicate-object relation triplets, very little work has been done to\nlocalize and predict visual relations. Inspired by the recent advances in\nrelational representation learning of knowledge bases and convolutional object\ndetection networks, we propose a Visual Translation Embedding network (VTransE)\nfor visual relation detection. VTransE places objects in a low-dimensional\nrelation space where a relation can be modeled as a simple vector translation,\ni.e., subject + predicate $\\approx$ object. We propose a novel feature\nextraction layer that enables object-relation knowledge transfer in a\nfully-convolutional fashion that supports training and inference in a single\nforward/backward pass. To the best of our knowledge, VTransE is the first\nend-to-end relation detection network. We demonstrate the effectiveness of\nVTransE over other state-of-the-art methods on two large-scale datasets: Visual\nRelationship and Visual Genome. Note that even though VTransE is a purely\nvisual model, it is still competitive to the Lu's multi-modal model with\nlanguage priors.",
        "Recent vision-language (VL) studies have shown remarkable progress by\nlearning generic representations from massive image-text pairs with transformer\nmodels and then fine-tuning on downstream VL tasks. While existing research has\nbeen focused on achieving high accuracy with large pre-trained models, building\na lightweight model is of great value in practice but is less explored. In this\npaper, we propose a smaller and faster VL model, MiniVLM, which can be\nfinetuned with good performance on various downstream tasks like its larger\ncounterpart. MiniVLM consists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We design a Two-stage\nEfficient feature Extractor (TEE), inspired by the one-stage EfficientDet\nnetwork, to significantly reduce the time cost of visual feature extraction by\n$95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce\nthe computation cost of the transformer module after comparing different\ncompact BERT models. In addition, we improve the MiniVLM pre-training by adding\n$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality image tags obtained from\na strong tagging model to enhance cross-modality alignment. The large models\nare used offline without adding any overhead in fine-tuning and inference. With\nthe above design choices, our MiniVLM reduces the model size by $73\\%$ and the\ninference time cost by $94\\%$ while being able to retain $94-97\\%$ of the\naccuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the\nstate-of-the-art VL research for on-the-edge applications.",
        "This paper takes a step towards temporal reasoning in a dynamically changing\nvideo, not in the pixel space that constitutes its frames, but in a latent\nspace that describes the non-linear dynamics of the objects in its world. We\nintroduce the Kalman variational auto-encoder, a framework for unsupervised\nlearning of sequential data that disentangles two latent representations: an\nobject's representation, coming from a recognition model, and a latent state\ndescribing its dynamics. As a result, the evolution of the world can be\nimagined and missing data imputed, both without the need to generate high\ndimensional frames at each time step. The model is trained end-to-end on videos\nof a variety of simulated physical systems, and outperforms competing methods\nin generative and missing data imputation tasks.",
        "The motion-and-time analysis has been a popular research topic in operations\nresearch, especially for analyzing work performances in manufacturing and\nservice operations. It is regaining attention as continuous improvement tools\nfor lean manufacturing and smart factory. This paper develops a framework for\ndata-driven analysis of work motions and studies their correlations to work\nspeeds or execution rates, using data collected from modern motion sensors. The\npast analyses largely relied on manual steps involving time-consuming\nstop-watching and video-taping, followed by manual data analysis. While modern\nsensing devices have automated the collection of motion data, the motion\nanalytics that transform the new data into knowledge are largely\nunderdeveloped. Unsolved technical questions include: How the motion and time\ninformation can be extracted from the motion sensor data, how work motions and\nexecution rates are statistically modeled and compared, and what are the\nstatistical correlations of motions to the rates? In this paper, we develop a\nnovel mathematical framework for motion and time analysis with motion sensor\ndata, by defining new mathematical representation spaces of human motions and\nexecution rates and by developing statistical tools on these new spaces. This\nmethodological research is demonstrated using five use cases applied to\nmanufacturing motion data.",
        "Most image data available are often stored in a compressed format, from which\nJPEG is the most widespread. To feed this data on a convolutional neural\nnetwork (CNN), a preliminary decoding process is required to obtain RGB pixels,\ndemanding a high computational load and memory usage. For this reason, the\ndesign of CNNs for processing JPEG compressed data has gained attention in\nrecent years. In most existing works, typical CNN architectures are adapted to\nfacilitate the learning with the DCT coefficients rather than RGB pixels.\nAlthough they are effective, their architectural changes either raise the\ncomputational costs or neglect relevant information from DCT inputs. In this\npaper, we examine different ways of speeding up CNNs designed for DCT inputs,\nexploiting learning strategies to reduce the computational complexity by taking\nfull advantage of DCT inputs. Our experiments were conducted on the ImageNet\ndataset. Results show that learning how to combine all DCT inputs in a\ndata-driven fashion is better than discarding them by hand, and its combination\nwith a reduction of layers has proven to be effective for reducing the\ncomputational costs while retaining accuracy.",
        "Recent deep learning based image inpainting methods which utilize contextual\ninformation and two-stage architecture have exhibited remarkable performance.\nHowever, the two-stage architecture is time-consuming, the contextual\ninformation lack high-level semantics and ignores both the semantic relevance\nand distance information of hole's feature patches, these limitations result in\nblurry textures and distorted structures of final result. Motivated by these\nobservations, we propose a new deep generative model-based approach, which\ntrains a shared network twice with different targets and utilizes a single\nnetwork during the testing phase, so that we can effectively save inference\ntime. Specifically, the targets of two training steps are structure\nreconstruction and texture generation respectively. During the second training,\nwe first propose a Pyramid Filling Block (PF-block) to utilize the high-level\nfeatures that the hole regions has been filled to guide the filling process of\nlow-level features progressively, the missing content can be filled from deep\nto shallow in a pyramid fashion. Then, inspired by the classical bilateral\nfilter [30], we propose the Bilateral Attention layer (BA-layer) to optimize\nfilled feature map, which synthesizes feature patches at each position by\ncomputing weighted sums of the surrounding feature patches, these weights are\nderived by considering both distance and value relationships between feature\npatches, thus making the visually plausible inpainting results. Finally,\nexperiments on multiple publicly available datasets show the superior\nperformance of our approach.",
        "This paper proposes a method for video smoke detection using synthetic smoke\nsamples. The virtual data can automatically offer precise and rich annotated\nsamples. However, the learning of smoke representations will be hurt by the\nappearance gap between real and synthetic smoke samples. The existed researches\nmainly work on the adaptation to samples extracted from original annotated\nsamples. These methods take the object detection and domain adaptation as two\nindependent parts. To train a strong detector with rich synthetic samples, we\nconstruct the adaptation to the detection layer of state-of-the-art\nsingle-model detectors (SSD and MS-CNN). The training procedure is an\nend-to-end stage. The classification, location and adaptation are combined in\nthe learning. The performance of the proposed model surpasses the original\nbaseline in our experiments. Meanwhile, our results show that the detectors\nbased on the adversarial adaptation are superior to the detectors based on the\ndiscrepancy adaptation. Code will be made publicly available on\nhttp://smoke.ustc.edu.cn. Moreover, the domain adaptation for two-stage\ndetector is described in Appendix A.",
        "Domain Adaptation (DA) methods are widely used in medical image segmentation\ntasks to tackle the problem of differently distributed train (source) and test\n(target) data. We consider the supervised DA task with a limited number of\nannotated samples from the target domain. It corresponds to one of the most\nrelevant clinical setups: building a sufficiently accurate model on the minimum\npossible amount of annotated data. Existing methods mostly fine-tune specific\nlayers of the pretrained Convolutional Neural Network (CNN). However, there is\nno consensus on which layers are better to fine-tune, e.g. the first layers for\nimages with low-level domain shift or the deeper layers for images with\nhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecture\nthat automatically chooses the layers which should be optimally fine-tuned.\nMore specifically, on the target domain, our method additionally learns the\npolicy that indicates whether a specific layer should be fine-tuned or reused\nfrom the pretrained network. We show that our method performs at the same level\nas the best of the nonflexible fine-tuning methods even under the extreme\nscarcity of annotated data. Secondly, we show that SpotTUnet policy provides a\nlayer-wise visualization of the domain shift impact on the network, which could\nbe further used to develop robust domain generalization methods. In order to\nextensively evaluate SpotTUnet performance, we use a publicly available dataset\nof brain MR images (CC359), characterized by explicit domain shift. We release\na reproducible experimental pipeline.",
        "Deep Reinforcement Learning (DRL) has been applied to address a variety of\ncooperative multi-agent problems with either discrete action spaces or\ncontinuous action spaces. However, to the best of our knowledge, no previous\nwork has ever succeeded in applying DRL to multi-agent problems with\ndiscrete-continuous hybrid (or parameterized) action spaces which is very\ncommon in practice. Our work fills this gap by proposing two novel algorithms:\nDeep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent\nHierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized\ntraining but decentralized execution paradigm: different levels of\ncommunication between different agents are used to facilitate the training\nprocess, while each agent executes its policy independently based on local\nobservations during execution. Our empirical results on several challenging\ntasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN\nand Deep MAHHQN are effective and significantly outperform existing independent\ndeep parameterized Q-learning method.",
        "In this work we seek to bridge the concepts of topographic organization and\nequivariance in neural networks. To accomplish this, we introduce the\nTopographic VAE: a novel method for efficiently training deep generative models\nwith topographically organized latent variables. We show that such a model\nindeed learns to organize its activations according to salient characteristics\nsuch as digit class, width, and style on MNIST. Furthermore, through\ntopographic organization over time (i.e. temporal coherence), we demonstrate\nhow predefined latent space transformation operators can be encouraged for\nobserved transformed input sequences -- a primitive form of unsupervised\nlearned equivariance. We demonstrate that this model successfully learns sets\nof approximately equivariant features (i.e. \"capsules\") directly from sequences\nand achieves higher likelihood on correspondingly transforming test sequences.\nEquivariance is verified quantitatively by measuring the approximate\ncommutativity of the inference network and the sequence transformations.\nFinally, we demonstrate approximate equivariance to complex transformations,\nexpanding upon the capabilities of existing group equivariant neural networks.",
        "In this paper, we introduce a Deep Convolutional Analysis Dictionary Model\n(DeepCAM) by learning convolutional dictionaries instead of unstructured\ndictionaries as in the case of deep analysis dictionary model introduced in the\ncompanion paper. Convolutional dictionaries are more suitable for processing\nhigh-dimensional signals like for example images and have only a small number\nof free parameters. By exploiting the properties of a convolutional dictionary,\nwe present an efficient convolutional analysis dictionary learning approach. A\nL-layer DeepCAM consists of L layers of convolutional analysis dictionary and\nelement-wise soft-thresholding pairs and a single layer of convolutional\nsynthesis dictionary. Similar to DeepAM, each convolutional analysis dictionary\nis composed of a convolutional Information Preserving Analysis Dictionary\n(IPAD) and a convolutional Clustering Analysis Dictionary (CAD). The IPAD and\nthe CAD are learned using variations of the proposed learning algorithm. We\ndemonstrate that DeepCAM is an effective multilayer convolutional model and, on\nsingle image super-resolution, achieves performance comparable with other\nmethods while also showing good generalization capabilities.",
        "For sake of reliability, it is necessary for models in real-world\napplications to be both powerful and globally interpretable. Simple\nclassifiers, e.g., Logistic Regression (LR), are globally interpretable, but\nnot powerful enough to model complex nonlinear interactions among features in\ntabular data. Meanwhile, Deep Neural Networks (DNNs) have shown great\neffectiveness for modeling tabular data, but is not globally interpretable. In\nthis work, we find local piece-wise interpretations in DNN of a specific\nfeature are usually inconsistent in different samples, which is caused by\nfeature interactions in the hidden layers. Accordingly, we can design an\nautomatic feature crossing method to find feature interactions in DNN, and use\nthem as cross features in LR. We give definition of the interpretation\ninconsistency in DNN, based on which a novel feature crossing method called\nDNN2LR is proposed. Extensive experiments have been conducted on four public\ndatasets and two real-world datasets. The final model, i.e., a LR model\nempowered with cross features, generated by DNN2LR can outperform the complex\nDNN model, as well as several state-of-the-art feature crossing methods. The\nexperimental results strongly verify the effectiveness and efficiency of\nDNN2LR, especially on real-world datasets with large numbers of feature fields.",
        "The segmentation of animals from camera-trap images is a difficult task. To\nillustrate, there are various challenges due to environmental conditions and\nhardware limitation in these images. We proposed a multi-layer robust principal\ncomponent analysis (multi-layer RPCA) approach for background subtraction. Our\nmethod computes sparse and low-rank images from a weighted sum of descriptors,\nusing color and texture features as case of study for camera-trap images\nsegmentation. The segmentation algorithm is composed of histogram equalization\nor Gaussian filtering as pre-processing, and morphological filters with active\ncontour as post-processing. The parameters of our multi-layer RPCA were\noptimized with an exhaustive search. The database consists of camera-trap\nimages from the Colombian forest taken by the Instituto de Investigaci\\'on de\nRecursos Biol\\'ogicos Alexander von Humboldt. We analyzed the performance of\nour method in inherent and therefore challenging situations of camera-trap\nimages. Furthermore, we compared our method with some state-of-the-art\nalgorithms of background subtraction, where our multi-layer RPCA outperformed\nthese other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average\nfine-grained F-measure for color and infrared sequences, respectively. To our\nbest knowledge, this paper is the first work proposing multi-layer RPCA and\nusing it for camera-trap images segmentation.",
        "We consider the task of unsupervised extraction of meaningful latent\nrepresentations of speech by applying autoencoding neural networks to speech\nwaveforms. The goal is to learn a representation able to capture high level\nsemantic content from the signal, e.g.\\ phoneme identities, while being\ninvariant to confounding low level details in the signal such as the underlying\npitch contour or background noise. Since the learned representation is tuned to\ncontain only phonetic content, we resort to using a high capacity WaveNet\ndecoder to infer information discarded by the encoder from previous samples.\nMoreover, the behavior of autoencoder models depends on the kind of constraint\nthat is applied to the latent representation. We compare three variants: a\nsimple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder\n(VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of\nlearned representations in terms of speaker independence, the ability to\npredict phonetic content, and the ability to accurately reconstruct individual\nspectrogram frames. Moreover, for discrete encodings extracted using the\nVQ-VAE, we measure the ease of mapping them to phonemes. We introduce a\nregularization scheme that forces the representations to focus on the phonetic\ncontent of the utterance and report performance comparable with the top entries\nin the ZeroSpeech 2017 unsupervised acoustic unit discovery task.",
        "The past decade has witnessed significant progress on detecting objects in\naerial images that are often distributed with large scale variations and\narbitrary orientations. However most of existing methods rely on heuristically\ndefined anchors with different scales, angles and aspect ratios and usually\nsuffer from severe misalignment between anchor boxes and axis-aligned\nconvolutional features, which leads to the common inconsistency between the\nclassification score and localization accuracy. To address this issue, we\npropose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:\na Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The\nFAM can generate high-quality anchors with an Anchor Refinement Network and\nadaptively align the convolutional features according to the anchor boxes with\na novel Alignment Convolution. The ODM first adopts active rotating filters to\nencode the orientation information and then produces orientation-sensitive and\norientation-invariant features to alleviate the inconsistency between\nclassification score and localization accuracy. Besides, we further explore the\napproach to detect objects in large-size images, which leads to a better\ntrade-off between speed and accuracy. Extensive experiments demonstrate that\nour method can achieve state-of-the-art performance on two commonly used aerial\nobjects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The\ncode is available at https://github.com/csuhan/s2anet.",
        "We study the ability of neural networks to steer or control trajectories of\ncontinuous time non-linear dynamical systems on graphs, which we represent with\nneural ordinary differential equations (neural ODEs). To do so, we introduce a\nneural-ODE control (NODEC) framework and find that it can learn control signals\nthat drive graph dynamical systems into desired target states. While we use\nloss functions that do not constrain the control energy, our results show that\nNODEC produces low energy control signals. Finally, we showcase the performance\nand versatility of NODEC by using it to control a system of more than one\nthousand coupled, non-linear ODEs.",
        "Ordinal regression predicts the objects' labels that exhibit a natural\nordering, which is important to many managerial problems such as credit scoring\nand clinical diagnosis. In these problems, the ability to explain how the\nattributes affect the prediction is critical to users. However, most, if not\nall, existing ordinal regression models simplify such explanation in the form\nof constant coefficients for the main and interaction effects of individual\nattributes. Such explanation cannot characterize the contributions of\nattributes at different value scales. To address this challenge, we propose a\nnew explainable ordinal regression model, namely, the Explainable Ordinal\nFactorization Model (XOFM). XOFM uses the piece-wise linear functions to\napproximate the actual contributions of individual attributes and their\ninteractions. Moreover, XOFM introduces a novel ordinal transformation process\nto assign each object the probabilities of belonging to multiple relevant\nclasses, instead of fixing boundaries to differentiate classes. XOFM is based\non the Factorization Machines to handle the potential sparsity problem as a\nresult of discretizing the attribute scales. Comprehensive experiments with\nbenchmark datasets and baseline models demonstrate that the proposed XOFM\nexhibits superior explainability and leads to state-of-the-art prediction\naccuracy.",
        "The recent explosive interest on transformers has suggested their potential\nto become powerful ``universal\" models for computer vision tasks, such as\nclassification, detection, and segmentation. While those attempts mainly study\nthe discriminative models, we explore transformers on some more notoriously\ndifficult vision tasks, e.g., generative adversarial networks (GANs). Our goal\nis to conduct the first pilot study in building a GAN completely free of\nconvolutions, using only pure transformer-based architectures. Our vanilla GAN\narchitecture, dubbed TransGAN, consists of a memory-friendly transformer-based\ngenerator that progressively increases feature resolution, and correspondingly\na multi-scale discriminator to capture simultaneously semantic contexts and\nlow-level textures. On top of them, we introduce the new module of grid\nself-attention for alleviating the memory bottleneck further, in order to scale\nup TransGAN to high-resolution generation. We also develop a unique training\nrecipe including a series of techniques that can mitigate the training\ninstability issues of TransGAN, such as data augmentation, modified\nnormalization, and relative position encoding. Our best architecture achieves\nhighly competitive performance compared to current state-of-the-art GANs using\nconvolutional backbones. Specifically, TransGAN sets new state-of-the-art\ninception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2.\nWhen it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as\non CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual\nexamples with high fidelity and impressive texture details. In addition, we\ndive deep into the transformer-based generation models to understand how their\nbehaviors differ from convolutional ones, by visualizing training dynamics. The\ncode is available at https://github.com/VITA-Group/TransGAN.",
        "In the deep metric learning approach to image segmentation, a convolutional\nnet densely generates feature vectors at the pixels of an image. Pairs of\nfeature vectors are trained to be similar or different, depending on whether\nthe corresponding pixels belong to same or different ground truth segments. To\nsegment a new image, the feature vectors are computed and clustered. Both\nempirically and theoretically, it is unclear whether or when deep metric\nlearning is superior to the more conventional approach of directly predicting\nan affinity graph with a convolutional net. We compare the two approaches using\nbrain images from serial section electron microscopy images, which constitute\nan especially challenging example of instance segmentation. We first show that\nseed-based postprocessing of the feature vectors, as originally proposed,\nproduces inferior accuracy because it is difficult for the convolutional net to\npredict feature vectors that remain uniform across large objects. Then we\nconsider postprocessing by thresholding a nearest neighbor graph followed by\nconnected components. In this case, segmentations from a \"metric graph\" turn\nout to be competitive or even superior to segmentations from a directly\npredicted affinity graph. To explain these findings theoretically, we invoke\nthe property that the metric function satisfies the triangle inequality. Then\nwe show with an example where this constraint suppresses noise, causing\nconnected components to more robustly segment a metric graph than an\nunconstrained affinity graph.",
        "Data size is the bottleneck for developing deep saliency models, because\ncollecting eye-movement data is very time consuming and expensive. Most of\ncurrent studies on human attention and saliency modeling have used high quality\nstereotype stimuli. In real world, however, captured images undergo various\ntypes of transformations. Can we use these transformations to augment existing\nsaliency datasets? Here, we first create a novel saliency dataset including\nfixations of 10 observers over 1900 images degraded by 19 types of\ntransformations. Second, by analyzing eye movements, we find that observers\nlook at different locations over transformed versus original images. Third, we\nutilize the new data over transformed images, called data augmentation\ntransformation (DAT), to train deep saliency models. We find that label\npreserving DATs with negligible impact on human gaze boost saliency prediction,\nwhereas some other DATs that severely impact human gaze degrade the\nperformance. These label preserving valid augmentation transformations provide\na solution to enlarge existing saliency datasets. Finally, we introduce a novel\nsaliency model based on generative adversarial network (dubbed GazeGAN). A\nmodified UNet is proposed as the generator of the GazeGAN, which combines\nclassic skip connections with a novel center-surround connection (CSC), in\norder to leverage multi level features. We also propose a histogram loss based\non Alternative Chi Square Distance (ACS HistLoss) to refine the saliency map in\nterms of luminance distribution. Extensive experiments and comparisons over 3\ndatasets indicate that GazeGAN achieves the best performance in terms of\npopular saliency evaluation metrics, and is more robust to various\nperturbations. Our code and data are available at:\nhttps://github.com/CZHQuality/Sal-CFS-GAN.",
        "The paper introduces the Hidden Tree Markov Network (HTN), a\nneuro-probabilistic hybrid fusing the representation power of generative models\nfor trees with the incremental and discriminative learning capabilities of\nneural networks. We put forward a modular architecture in which multiple\ngenerative models of limited complexity are trained to learn structural feature\ndetectors whose outputs are then combined and integrated by neural layers at a\nlater stage. In this respect, the model is both deep, thanks to the unfolding\nof the generative models on the input structures, as well as wide, given the\npotentially large number of generative modules that can be trained in parallel.\nExperimental results show that the proposed approach can outperform\nstate-of-the-art syntactic kernels as well as generative kernels built on the\nsame probabilistic model as the HTN.",
        "The increasing availability of healthcare data requires accurate analysis of\ndisease diagnosis, progression, and realtime monitoring to provide improved\ntreatments to the patients. In this context, Machine Learning (ML) models are\nused to extract valuable features and insights from high-dimensional and\nheterogeneous healthcare data to detect different diseases and patient\nactivities in a Smart Healthcare System (SHS). However, recent researches show\nthat ML models used in different application domains are vulnerable to\nadversarial attacks. In this paper, we introduce a new type of adversarial\nattacks to exploit the ML classifiers used in a SHS. We consider an adversary\nwho has partial knowledge of data distribution, SHS model, and ML algorithm to\nperform both targeted and untargeted attacks. Employing these adversarial\ncapabilities, we manipulate medical device readings to alter patient status\n(disease-affected, normal condition, activities, etc.) in the outcome of the\nSHS. Our attack utilizes five different adversarial ML algorithms (HopSkipJump,\nFast Gradient Method, Crafting Decision Tree, Carlini & Wagner, Zeroth Order\nOptimization) to perform different malicious activities (e.g., data poisoning,\nmisclassify outputs, etc.) on a SHS. Moreover, based on the training and\ntesting phase capabilities of an adversary, we perform white box and black box\nattacks on a SHS. We evaluate the performance of our work in different SHS\nsettings and medical devices. Our extensive evaluation shows that our proposed\nadversarial attack can significantly degrade the performance of a ML-based SHS\nin detecting diseases and normal activities of the patients correctly, which\neventually leads to erroneous treatment.",
        "Machine learning based methods achieves impressive results in object\nclassification and detection. Utilizing representative data of the visual world\nduring the training phase is crucial to achieve good performance with such data\ndriven approaches. However, it not always possible to access bias-free datasets\nthus, robustness to biased data is a desirable property for a learning system.\nCapsule Networks have been introduced recently and their tolerance to biased\ndata has received little attention. This paper aims to fill this gap and\nproposes two experimental scenarios to assess the tolerance to imbalanced\ntraining data and to determine the generalization performance of a model with\nunfamiliar affine transformations of the images. This paper assesses dynamic\nrouting and EM routing based Capsule Networks and proposes a comparison with\nConvolutional Neural Networks in the two tested scenarios. The presented\nresults provide new insights into the behaviour of capsule networks.",
        "As a subset of unsupervised representation learning, self-supervised\nrepresentation learning adopts self-defined signals as supervision and uses the\nlearned representation for downstream tasks, such as object detection and image\ncaptioning. Many proposed approaches for self-supervised learning follow\nnaturally a multi-view perspective, where the input (e.g., original images) and\nthe self-supervised signals (e.g., augmented images) can be seen as two\nredundant views of the data. Building from this multi-view perspective, this\npaper provides an information-theoretical framework to better understand the\nproperties that encourage successful self-supervised learning. Specifically, we\ndemonstrate that self-supervised learned representations can extract\ntask-relevant information and discard task-irrelevant information. Our\ntheoretical framework paves the way to a larger space of self-supervised\nlearning objective design. In particular, we propose a composite objective that\nbridges the gap between prior contrastive and predictive learning objectives,\nand introduce an additional objective term to discard task-irrelevant\ninformation. To verify our analysis, we conduct controlled experiments to\nevaluate the impact of the composite objectives. We also explore our\nframework's empirical generalization beyond the multi-view perspective, where\nthe cross-view redundancy may not be clearly observed.",
        "A variety of modeling techniques have been developed in the past decade to\nreduce the computational expense and improve the accuracy of modeling. In this\nstudy, a new framework of modeling is suggested. Compared with other popular\nmethods, a distinctive characteristic is \"from image based model to analysis\nbased model (e.g. stress, strain, and deformation)\". In such a framework, a\nreconstruction neural network (ReConNN) model designed for simulation-based\nphysical field's reconstruction is proposed. The ReConNN contains two submodels\nthat are convolutional neural network (CNN) and generative adversarial net-work\n(GAN). The CNN is employed to construct the mapping between contour images of\nphysical field and objective function. Subsequently, the GAN is utilized to\ngenerate more images which are similar to the existing contour images. Finally,\nLagrange polynomial is applied to complete the reconstruction. However, the\nexisting CNN models are commonly applied to the classification tasks, which\nseem to be difficult to handle with regression tasks of images. Meanwhile, the\nexisting GAN architectures are insufficient to generate high-accuracy \"pseudo\ncontour images\". Therefore, a ReConNN model based on a Convolution in\nConvolution (CIC) and a Convolutional AutoEncoder based on Wasserstein\nGenerative Adversarial Network (WGAN-CAE) is suggested. To evaluate the\nperformance of the proposed model representatively, a classical topology\noptimization procedure is considered. Then the ReConNN is utilized to the\nreconstruction of heat transfer process of a pin fin heat sink. It demonstrates\nthat the proposed ReConNN model is proved to be a potential capability to\nreconstruct physical field for multidisciplinary, such as structural\noptimization.",
        "Recent exploration methods have proven to be a recipe for improving\nsample-efficiency in deep reinforcement learning (RL). However, efficient\nexploration in high-dimensional observation spaces still remains a challenge.\nThis paper presents Random Encoders for Efficient Exploration (RE3), an\nexploration method that utilizes state entropy as an intrinsic reward. In order\nto estimate state entropy in environments with high-dimensional observations,\nwe utilize a k-nearest neighbor entropy estimator in the low-dimensional\nrepresentation space of a convolutional encoder. In particular, we find that\nthe state entropy can be estimated in a stable and compute-efficient manner by\nutilizing a randomly initialized encoder, which is fixed throughout training.\nOur experiments show that RE3 significantly improves the sample-efficiency of\nboth model-free and model-based RL methods on locomotion and navigation tasks\nfrom DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3\nallows learning diverse behaviors without extrinsic rewards, effectively\nimproving sample-efficiency in downstream tasks. Source code and videos are\navailable at https://sites.google.com/view/re3-rl.",
        "This paper investigates the idea of encoding object-centered representations\nin the design of the reward function and policy architectures of a\nlanguage-guided reinforcement learning agent. This is done using a combination\nof object-wise permutation invariant networks inspired from Deep Sets and\ngated-attention mechanisms. In a 2D procedurally-generated world where agents\ntargeting goals in natural language navigate and interact with objects, we show\nthat these architectures demonstrate strong generalization capacities to\nout-of-distribution goals. We study the generalization to varying numbers of\nobjects at test time and further extend the object-centered architectures to\ngoals involving relational reasoning.",
        "State-of-the-art stereo matching networks have difficulties in generalizing\nto new unseen environments due to significant domain differences, such as\ncolor, illumination, contrast, and texture. In this paper, we aim at designing\na domain-invariant stereo matching network (DSMNet) that generalizes well to\nunseen scenes. To achieve this goal, we propose i) a novel \"domain\nnormalization\" approach that regularizes the distribution of learned\nrepresentations to allow them to be invariant to domain differences, and ii) a\ntrainable non-local graph-based filter for extracting robust structural and\ngeometric representations that can further enhance domain-invariant\ngeneralizations. When trained on synthetic data and generalized to real test\nsets, our model performs significantly better than all state-of-the-art models.\nIt even outperforms some deep learning models (e.g. MC-CNN) fine-tuned with\ntest-domain data.",
        "Dictionary based classifiers are a family of algorithms for time series\nclassification (TSC), that focus on capturing the frequency of pattern\noccurrences in a time series. The ensemble based Bag of Symbolic Fourier\nApproximation Symbols (BOSS) was found to be a top performing TSC algorithm in\na recent evaluation, as well as the best performing dictionary based\nclassifier. A recent addition to the category, the Word Extraction for Time\nSeries Classification (WEASEL), claims an improvement on this performance. Both\nof these algorithms however have non-trivial scalability issues, taking a\nconsiderable amount of build time and space on larger datasets. We evaluate\nchanges to the way BOSS chooses classifiers for its ensemble, replacing its\nparameter search with random selection. This change allows for the easy\nimplementation of contracting, setting a build time limit for the classifier\nand check-pointing, saving progress during the classifiers build. To\ndifferentiate between the two BOSS ensemble methods we refer to our randomised\nversion as RBOSS. Additionally we test the application of common ensembling\ntechniques to help retain accuracy from the loss of the BOSS parameter search.\nWe achieve a significant reduction in build time without a significant change\nin accuracy on average when compared to BOSS by creating a size $n$ weighted\nensemble selecting the best performers from $k$ randomly chosen parameter sets.\nOur experiments are conducted on datasets from the recently expanded UCR time\nseries archive. We demonstrate the usability improvements to RBOSS with a case\nstudy using a large whale acoustics dataset for which BOSS proved infeasible.",
        "3D object classification and segmentation using deep neural networks has been\nextremely successful. As the problem of identifying 3D objects has many\nsafety-critical applications, the neural networks have to be robust against\nadversarial changes to the input data set. There is a growing body of research\non generating human-imperceptible adversarial attacks and defenses against them\nin the 2D image classification domain. However, 3D objects have various\ndifferences with 2D images, and this specific domain has not been rigorously\nstudied so far.\n  We present a preliminary evaluation of adversarial attacks on deep 3D point\ncloud classifiers, namely PointNet and PointNet++, by evaluating both white-box\nand black-box adversarial attacks that were proposed for 2D images and\nextending those attacks to reduce the perceptibility of the perturbations in 3D\nspace. We also show the high effectiveness of simple defenses against those\nattacks by proposing new defenses that exploit the unique structure of 3D point\nclouds. Finally, we attempt to explain the effectiveness of the defenses\nthrough the intrinsic structures of both the point clouds and the neural\nnetwork architectures. Overall, we find that networks that process 3D point\ncloud data are weak to adversarial attacks, but they are also more easily\ndefensible compared to 2D image classifiers. Our investigation will provide the\ngroundwork for future studies on improving the robustness of deep neural\nnetworks that handle 3D data.",
        "Stochastic gradient descent (SGD) undergoes complicated multiplicative noise\nfor the mean-square loss. We use this property of the SGD noise to derive a\nstochastic differential equation (SDE) with simpler additive noise by\nperforming a non-uniform transformation of the time variable. In the SDE, the\ngradient of the loss is replaced by that of the logarithmized loss.\nConsequently, we show that, near a local or global minimum, the stationary\ndistribution $P_\\mathrm{ss}(\\theta)$ of the network parameters $\\theta$ follows\na power-law with respect to the loss function $L(\\theta)$, i.e.\n$P_\\mathrm{ss}(\\theta)\\propto L(\\theta)^{-\\phi}$ with the exponent $\\phi$\nspecified by the mini-batch size, the learning rate, and the Hessian at the\nminimum. We obtain the escape rate formula from a local minimum, which is\ndetermined not by the loss barrier height $\\Delta L=L(\\theta^s)-L(\\theta^*)$\nbetween a minimum $\\theta^*$ and a saddle $\\theta^s$ but by the logarithmized\nloss barrier height $\\Delta\\log L=\\log[L(\\theta^s)/L(\\theta^*)]$. Our\nescape-rate formula explains an empirical fact that SGD prefers flat minima\nwith low effective dimensions.",
        "Multi-agent control problems constitute an interesting area of application\nfor deep reinforcement learning models with continuous action spaces. Such\nreal-world applications, however, typically come with critical safety\nconstraints that must not be violated. In order to ensure safety, we enhance\nthe well-known multi-agent deep deterministic policy gradient (MADDPG)\nframework by adding a safety layer to the deep policy network. In particular,\nwe extend the idea of linearizing the single-step transition dynamics, as was\ndone for single-agent systems in Safe DDPG (Dalal et al., 2018), to multi-agent\nsettings. We additionally propose to circumvent infeasibility problems in the\naction correction step using soft constraints (Kerrigan & Maciejowski, 2000).\nResults from the theory of exact penalty functions can be used to guarantee\nconstraint satisfaction of the soft constraints under mild assumptions. We\nempirically find that the soft formulation achieves a dramatic decrease in\nconstraint violations, making safety available even during the learning\nprocedure.",
        "Through deep learning and computer vision techniques, driving manoeuvres can\nbe predicted accurately a few seconds in advance. Even though adapting a\nlearned model to new drivers and different vehicles is key for robust\ndriver-assistance systems, this problem has received little attention so far.\nThis work proposes to tackle this challenge through domain adaptation, a\ntechnique closely related to transfer learning. A proof of concept for the\napplication of a Domain-Adversarial Recurrent Neural Network (DA-RNN) to\nmulti-modal time series driving data is presented, in which domain-invariant\nfeatures are learned by maximizing the loss of an auxiliary domain classifier.\nOur implementation is evaluated using a leave-one-driver-out approach on\nindividual drivers from the Brain4Cars dataset, as well as using a new dataset\nacquired through driving simulations, yielding an average increase in\nperformance of 30% and 114% respectively compared to no adaptation. We also\nshow the importance of fine-tuning sections of the network to optimise the\nextraction of domain-independent features. The results demonstrate the\napplicability of the approach to driver-assistance systems as well as training\nand simulation environments.",
        "An important problem in causal inference is to break down the total effect of\ntreatment into different causal pathways and quantify the causal effect in each\npathway. Causal mediation analysis (CMA) is a formal statistical approach for\nidentifying and estimating these causal effects. Central to CMA is the\nsequential ignorability assumption that implies all pre-treatment confounders\nare measured and they can capture different types of confounding, e.g.,\npost-treatment confounders and hidden confounders. Typically unverifiable in\nobservational studies, this assumption restrains both the coverage and\npracticality of conventional methods. This work, therefore, aims to circumvent\nthe stringent assumption by following a causal graph with a unified confounder\nand its proxy variables. Our core contribution is an algorithm that combines\ndeep latent-variable models and proxy strategy to jointly infer a unified\nsurrogate confounder and estimate different causal effects in CMA from observed\nvariables. Empirical evaluations using both synthetic and semi-synthetic\ndatasets validate the effectiveness of the proposed method.",
        "The heavy-tailed distributions of corrupted outliers and singular values of\nall channels in low-level vision have proven effective priors for many\napplications such as background modeling, photometric stereo and image\nalignment. And they can be well modeled by a hyper-Laplacian. However, the use\nof such distributions generally leads to challenging non-convex, non-smooth and\nnon-Lipschitz problems, and makes existing algorithms very slow for large-scale\napplications. Together with the analytic solutions to lp-norm minimization with\ntwo specific values of p, i.e., p=1/2 and p=2/3, we propose two novel bilinear\nfactor matrix norm minimization models for robust principal component analysis.\nWe first define the double nuclear norm and Frobenius/nuclear hybrid norm\npenalties, and then prove that they are in essence the Schatten-1/2 and 2/3\nquasi-norms, respectively, which lead to much more tractable and scalable\nLipschitz optimization problems. Our experimental analysis shows that both our\nmethods yield more accurate solutions than original Schatten quasi-norm\nminimization, even when the number of observations is very limited. Finally, we\napply our penalties to various low-level vision problems, e.g., text removal,\nmoving object detection, image alignment and inpainting, and show that our\nmethods usually outperform the state-of-the-art methods.",
        "The explore{exploit dilemma is one of the central challenges in Reinforcement\nLearning (RL). Bayesian RL solves the dilemma by providing the agent with\ninformation in the form of a prior distribution over environments; however,\nfull Bayesian planning is intractable. Planning with the mean MDP is a common\nmyopic approximation of Bayesian planning. We derive a novel reward bonus that\nis a function of the posterior distribution over environments, which, when\nadded to the reward in planning with the mean MDP, results in an agent which\nexplores efficiently and effectively. Although our method is similar to\nexisting methods when given an uninformative or unstructured prior, unlike\nexisting methods, our method can exploit structured priors. We prove that our\nmethod results in a polynomial sample complexity and empirically demonstrate\nits advantages in a structured exploration task.",
        "We present a novel method for reliably explaining the predictions of neural\nnetworks. We consider an explanation reliable if it identifies input features\nrelevant to the model output by considering the input and the neighboring data\npoints. Our method is built on top of the assumption of smooth landscape in a\nloss function of the model prediction: locally consistent loss and gradient\nprofile. A theoretical analysis established in this study suggests that those\nlocally smooth model explanations are learned using a batch of noisy copies of\nthe input with the L1 regularization for a saliency map. Extensive experiments\nsupport the analysis results, revealing that the proposed saliency maps\nretrieve the original classes of adversarial examples crafted against both\nnaturally and adversarially trained models, significantly outperforming\nprevious methods. We further demonstrated that such good performance results\nfrom the learning capability of this method to identify input features that are\ntruly relevant to the model output of the input and the neighboring data\npoints, fulfilling the requirements of a reliable explanation.",
        "Recently, satellites with high temporal resolution have fostered wide\nattention in various practical applications. Due to limitations of bandwidth\nand hardware cost, however, the spatial resolution of such satellites is\nconsiderably low, largely limiting their potentials in scenarios that require\nspatially explicit information. To improve image resolution, numerous\napproaches based on training low-high resolution pairs have been proposed to\naddress the super-resolution (SR) task. Despite their success, however,\nlow/high spatial resolution pairs are usually difficult to obtain in satellites\nwith a high temporal resolution, making such approaches in SR impractical to\nuse. In this paper, we proposed a new unsupervised learning framework, called\n\"MIP\", which achieves SR tasks without low/high resolution image pairs. First,\nrandom noise maps are fed into a designed generative adversarial network (GAN)\nfor reconstruction. Then, the proposed method converts the reference image to\nlatent space as the migration image prior. Finally, we update the input noise\nvia an implicit method, and further transfer the texture and structured\ninformation from the reference image. Extensive experimental results on the\nDraper dataset show that MIP achieves significant improvements over\nstate-of-the-art methods both quantitatively and qualitatively. The proposed\nMIP is open-sourced at http://github.com/jiaming-wang/MIP.",
        "How can we estimate the importance of nodes in a knowledge graph (KG)? A KG\nis a multi-relational graph that has proven valuable for many tasks including\nquestion answering and semantic search. In this paper, we present GENI, a\nmethod for tackling the problem of estimating node importance in KGs, which\nenables several downstream applications such as item recommendation and\nresource allocation. While a number of approaches have been developed to\naddress this problem for general graphs, they do not fully utilize information\navailable in KGs, or lack flexibility needed to model complex relationship\nbetween entities and their importance. To address these limitations, we explore\nsupervised machine learning algorithms. In particular, building upon recent\nadvancement of graph neural networks (GNNs), we develop GENI, a GNN-based\nmethod designed to deal with distinctive challenges involved with predicting\nnode importance in KGs. Our method performs an aggregation of importance scores\ninstead of aggregating node embeddings via predicate-aware attention mechanism\nand flexible centrality adjustment. In our evaluation of GENI and existing\nmethods on predicting node importance in real-world KGs with different\ncharacteristics, GENI achieves 5-17% higher NDCG@100 than the state of the art.",
        "Quite a few people in the world have to stay under permanent surveillance for\nhealth reasons; they include diabetic people or people with some other chronic\nconditions, the elderly and the disabled.These groups may face heightened risk\nof having life-threatening falls or of being struck by a syncope. Due to\nlimited availability of resources a substantial part of people at risk can not\nreceive necessary monitoring and thus are exposed to excessive danger.\nNowadays, this problem is usually solved via applying Human Activity\nRecognition (HAR) methods. HAR is a perspective and fast-paced Data Science\nfield, which has a wide range of application areas such as healthcare, sport,\nsecurity etc. However, the currently techniques of recognition are markedly\nlacking in accuracy, hence, the present paper suggests a highly accurate method\nfor human activity classification. Wepropose a new workflow to address the HAR\nproblem and evaluate it on the UniMiB SHAR dataset, which consists of the\naccelerometer signals. The model we suggest is based on continuous wavelet\ntransform (CWT) and convolutional neural networks (CNNs). Wavelet transform\nlocalizes signal features both in time and frequency domains and after that a\nCNN extracts these features and recognizes activity. It is also worth noting\nthat CWT converts 1D accelerometer signal into 2D images and thus enables to\nobtain better results as 2D networks have a significantly higher predictive\ncapacity. In the course of the work we build a convolutional neural network and\nvary such model parameters as number of spatial axes, number of layers, number\nof neurons in each layer, image size, type of mother wavelet, the order of zero\nmoment of mother wavelet etc. Besides, we also apply models with residual\nblocks which resulted in significantly higher metric values. Finally, we\nsucceed to reach 99.26 % accuracy and it is a worthy performance for this\nproblem.",
        "Although deep networks have been widely adopted, one of their shortcomings\nhas been their blackbox nature. One particularly difficult problem in machine\nlearning is multivariate time series (MVTS) classification. MVTS data arise in\nmany applications and are becoming ever more pervasive due to explosive growth\nof sensors and IoT devices. Here, we propose a novel network (IETNet) that\nidentifies the important channels in the classification decision for each\ninstance of inference. This feature also enables identification and removal of\nnon-predictive variables which would otherwise lead to overfit and/or\ninaccurate model. IETNet is an end-to-end network that combines temporal\nfeature extraction, variable selection, and joint variable interaction into a\nsingle learning framework. IETNet utilizes an 1D convolutions for temporal\nfeatures, a novel channel gate layer for variable-class assignment using an\nattention layer to perform cross channel reasoning and perform classification\nobjective. To gain insight into the learned temporal features and channels, we\nextract region of interest attention map along both time and channels. The\nviability of this network is demonstrated through a multivariate time series\ndata from N body simulations and spacecraft sensor data.",
        "Correlative microscopy aims at combining two or more modalities to gain more\ninformation than the one provided by one modality on the same biological\nstructure. Registration is needed at different steps of correlative\nmicroscopies workflows. Biologists want to select the image content used for\nregistration not to introduce bias in the correlation of unknown structures.\nIntensity-based methods might not allow this selection and might be too slow\nwhen the images are very large. We propose an approach based on point clouds\ncreated from selected content by the biologist. These point clouds may be prone\nto big differences in densities but also missing parts and outliers. In this\npaper we present a method of registration for point clouds based on graph\nbuilding and graph matching, and compare the method to iterative closest point\nbased methods.",
        "When we fine-tune a well-trained deep learning model for a new set of\nclasses, the network learns new concepts but gradually forgets the knowledge of\nold training. In some real-life applications, we may be interested in learning\nnew classes without forgetting the capability of previous experience. Such\nlearning without forgetting problem is often investigated using 2D image\nrecognition tasks. In this paper, considering the growth of depth camera\ntechnology, we address the same problem for the 3D point cloud object data.\nThis problem becomes more challenging in the 3D domain than 2D because of the\nunavailability of large datasets and powerful pretrained backbone models. We\ninvestigate knowledge distillation techniques on 3D data to reduce catastrophic\nforgetting of the previous training. Moreover, we improve the distillation\nprocess by using semantic word vectors of object classes. We observe that\nexploring the interrelation of old and new knowledge during training helps to\nlearn new concepts without forgetting old ones. Experimenting on three 3D point\ncloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic\n(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish\nnew baseline results on learning without forgetting for 3D data. This research\nwill instigate many future works in this area.",
        "Predicting 3D human pose from a single monoscopic video can be highly\nchallenging due to factors such as low resolution, motion blur and occlusion,\nin addition to the fundamental ambiguity in estimating 3D from 2D. Approaches\nthat directly regress the 3D pose from independent images can be particularly\nsusceptible to these factors and result in jitter, noise and/or inconsistencies\nin skeletal estimation. Much of which can be overcome if the temporal evolution\nof the scene and skeleton are taken into account. However, rather than tracking\nbody parts and trying to temporally smooth them, we propose a novel transformer\nbased network that can learn a distribution over both pose and motion in an\nunsupervised fashion. We call our approach Skeletor. Skeletor overcomes\ninaccuracies in detection and corrects partial or entire skeleton corruption.\nSkeletor uses strong priors learn from on 25 million frames to correct skeleton\nsequences smoothly and consistently. Skeletor can achieve this as it implicitly\nlearns the spatio-temporal context of human motion via a transformer based\nneural network. Extensive experiments show that Skeletor achieves improved\nperformance on 3D human pose estimation and further provides benefits for\ndownstream tasks such as sign language translation.",
        "Reinforcement learning methods have recently been very successful at\nperforming complex sequential tasks like playing Atari games, Go and Poker.\nThese algorithms have outperformed humans in several tasks by learning from\nscratch, using only scalar rewards obtained through interaction with their\nenvironment. While there certainly has been considerable independent innovation\nto produce such results, many core ideas in reinforcement learning are inspired\nby phenomena in animal learning, psychology and neuroscience. In this paper, we\ncomprehensively review a large number of findings in both neuroscience and\npsychology that evidence reinforcement learning as a promising candidate for\nmodeling learning and decision making in the brain. In doing so, we construct a\nmapping between various classes of modern RL algorithms and specific findings\nin both neurophysiological and behavioral literature. We then discuss the\nimplications of this observed relationship between RL, neuroscience and\npsychology and its role in advancing research in both AI and brain science.",
        "Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences. In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art.",
        "Although significant progress has been made in synthesizing high-quality and\nvisually realistic face images by unconditional Generative Adversarial Networks\n(GANs), there still lacks of control over the generation process in order to\nachieve semantic face editing. In addition, it remains very challenging to\nmaintain other face information untouched while editing the target attributes.\nIn this paper, we propose a novel learning framework, called GuidedStyle, to\nachieve semantic face editing on StyleGAN by guiding the image generation\nprocess with a knowledge network. Furthermore, we allow an attention mechanism\nin StyleGAN generator to adaptively select a single layer for style\nmanipulation. As a result, our method is able to perform disentangled and\ncontrollable edits along various attributes, including smiling, eyeglasses,\ngender, mustache and hair color. Both qualitative and quantitative results\ndemonstrate the superiority of our method over other competing methods for\nsemantic face editing. Moreover, we show that our model can be also applied to\ndifferent types of real and artistic face editing, demonstrating strong\ngeneralization ability.",
        "With increasing applications of 3D hand pose estimation in various\nhuman-computer interaction applications, convolution neural networks (CNNs)\nbased estimation models have been actively explored. However, the existing\nmodels require complex architectures or redundant computational resources to\ntrade with the acceptable accuracy. To tackle this limitation, this paper\nproposes HandFoldingNet, an accurate and efficient hand pose estimator that\nregresses the hand joint locations from the normalized 3D hand point cloud\ninput. The proposed model utilizes a folding-based decoder that folds a given\n2D hand skeleton into the corresponding joint coordinates. For higher\nestimation accuracy, folding is guided by multi-scale features, which include\nboth global and joint-wise local features. Experimental results show that the\nproposed model outperforms the existing methods on three hand pose benchmark\ndatasets with the lowest model parameter requirement. Code is available at\nhttps://github.com/cwc1260/HandFold.",
        "In this work we propose for the first time a transformer-based framework for\nunsupervised representation learning of multivariate time series. Pre-trained\nmodels can be potentially used for downstream tasks such as regression and\nclassification, forecasting and missing value imputation. By evaluating our\nmodels on several benchmark datasets for multivariate time series regression\nand classification, we show that not only does our modeling approach represent\nthe most successful method employing unsupervised learning of multivariate time\nseries presented to date, but also that it exceeds the current state-of-the-art\nperformance of supervised methods; it does so even when the number of training\nsamples is very limited, while offering computational efficiency. Finally, we\ndemonstrate that unsupervised pre-training of our transformer models offers a\nsubstantial performance benefit over fully supervised learning, even without\nleveraging additional unlabeled data, i.e., by reusing the same data samples\nthrough the unsupervised objective.",
        "In this paper we introduce a new approach to computing hidden features of\nsampled vector fields. The basic idea is to convert the vector field data to a\ngraph structure and use tools designed for automatic, unsupervised analysis of\ngraphs. Using a few data sets we show that the collected features of the vector\nfields are correlated with the dynamics known for analytic models which\ngenerates the data. In particular the method may be useful in analysis of data\nsets where the analytic model is poorly understood or not known.",
        "Deep RL approaches build much of their success on the ability of the deep\nneural network to generate useful internal representations. Nevertheless, they\nsuffer from a high sample-complexity and starting with a good input\nrepresentation can have a significant impact on the performance. In this paper,\nwe exploit the fact that the underlying Markov decision process (MDP)\nrepresents a graph, which enables us to incorporate the topological information\nfor effective state representation learning.\n  Motivated by the recent success of node representations for several graph\nanalytical tasks we specifically investigate the capability of node\nrepresentation learning methods to effectively encode the topology of the\nunderlying MDP in Deep RL. To this end we perform a comparative analysis of\nseveral models chosen from 4 different classes of representation learning\nalgorithms for policy learning in grid-world navigation tasks, which are\nrepresentative of a large class of RL problems. We find that all embedding\nmethods outperform the commonly used matrix representation of grid-world\nenvironments in all of the studied cases. Moreoever, graph convolution based\nmethods are outperformed by simpler random walk based methods and graph linear\nautoencoders.",
        "We report competitive results on object detection and instance segmentation\non the COCO dataset using standard models trained from random initialization.\nThe results are no worse than their ImageNet pre-training counterparts even\nwhen using the hyper-parameters of the baseline system (Mask R-CNN) that were\noptimized for fine-tuning pre-trained models, with the sole exception of\nincreasing the number of training iterations so the randomly initialized models\nmay converge. Training from random initialization is surprisingly robust; our\nresults hold even when: (i) using only 10% of the training data, (ii) for\ndeeper and wider models, and (iii) for multiple tasks and metrics. Experiments\nshow that ImageNet pre-training speeds up convergence early in training, but\ndoes not necessarily provide regularization or improve final target task\naccuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection\nwithout using any external data---a result on par with the top COCO 2017\ncompetition results that used ImageNet pre-training. These observations\nchallenge the conventional wisdom of ImageNet pre-training for dependent tasks\nand we expect these discoveries will encourage people to rethink the current de\nfacto paradigm of `pre-training and fine-tuning' in computer vision.",
        "In recent years, Deep Learning methods have achieved state of the art\nperformance in a vast range of machine learning tasks, including image\nclassification and multilingual automatic text translation. These architectures\nare trained to solve machine learning tasks in an end-to-end fashion. In order\nto reach top-tier performance, these architectures often require a very large\nnumber of trainable parameters. There are multiple undesirable consequences,\nand in order to tackle these issues, it is desired to be able to open the black\nboxes of deep learning architectures. Problematically, doing so is difficult\ndue to the high dimensionality of representations and the stochasticity of the\ntraining process. In this thesis, we investigate these architectures by\nintroducing a graph formalism based on the recent advances in Graph Signal\nProcessing (GSP). Namely, we use graphs to represent the latent spaces of deep\nneural networks. We showcase that this graph formalism allows us to answer\nvarious questions including: ensuring generalization abilities, reducing the\namount of arbitrary choices in the design of the learning process, improving\nrobustness to small perturbations added to the inputs, and reducing\ncomputational complexity",
        "Computational intelligence-based ocean characteristics forecasting\napplications, such as Significant Wave Height (SWH) prediction, are crucial for\navoiding social and economic loss in coastal cities. Compared to the\ntraditional empirical-based or numerical-based forecasting models, \"soft\ncomputing\" approaches, including machine learning and deep learning models,\nhave shown numerous success in recent years. In this paper, we focus on\nenabling the deep learning model to learn both short-term and long-term\nspatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural\nNetwork (WGNN) approach is proposed to integrate the advantages of wavelet\ntransform and graph neural network. Several parallel graph neural networks are\nseparately trained on wavelet decomposed data, and the reconstruction of each\nmodel's prediction forms the final SWH prediction. Experimental results show\nthat the proposed WGNN approach outperforms other models, including the\nnumerical models, the machine learning models, and several deep learning\nmodels.",
        "Activity detection from first-person videos (FPV) captured using a wearable\ncamera is an active research field with potential applications in many sectors,\nincluding healthcare, law enforcement, and rehabilitation. State-of-the-art\nmethods use optical flow-based hybrid techniques that rely on features derived\nfrom the motion of objects from consecutive frames. In this work, we developed\na two-stream network, the \\emph{SegCodeNet}, that uses a network branch\ncontaining video-streams with color-coded semantic segmentation masks of\nrelevant objects in addition to the original RGB video-stream. We also include\na stream-wise attention gating that prioritizes between the two streams and a\nframe-wise attention module that prioritizes the video frames that contain\nrelevant features. Experiments are conducted on an FPV dataset containing $18$\nactivity classes in office environments. In comparison to a single-stream\nnetwork, the proposed two-stream method achieves an absolute improvement of\n$14.366\\%$ and $10.324\\%$ for averaged F1 score and accuracy, respectively,\nwhen average results are compared for three different frame sizes\n$224\\times224$, $112\\times112$, and $64\\times64$. The proposed method provides\nsignificant performance gains for lower-resolution images with absolute\nimprovements of $17\\%$ and $26\\%$ in F1 score for input dimensions of\n$112\\times112$ and $64\\times64$, respectively. The best performance is achieved\nfor a frame size of $224\\times224$ yielding an F1 score and accuracy of\n$90.176\\%$ and $90.799\\%$ which outperforms the state-of-the-art Inflated 3D\nConvNet (I3D) \\cite{carreira2017quo} method by an absolute margin of $4.529\\%$\nand $2.419\\%$, respectively.",
        "The goal of automatic Sign Language Production (SLP) is to translate spoken\nlanguage to a continuous stream of sign language video at a level comparable to\na human translator. If this was achievable, then it would revolutionise Deaf\nhearing communications. Previous work on predominantly isolated SLP has shown\nthe need for architectures that are better suited to the continuous domain of\nfull sign sequences.\n  In this paper, we propose Progressive Transformers, a novel architecture that\ncan translate from discrete spoken language sentences to continuous 3D skeleton\npose outputs representing sign language. We present two model configurations,\nan end-to-end network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary.\n  Our transformer network architecture introduces a counter that enables\ncontinuous sequence generation at training and inference. We also provide\nseveral data augmentation processes to overcome the problem of drift and\nimprove the performance of SLP models. We propose a back translation evaluation\nmechanism for SLP, presenting benchmark quantitative results on the challenging\nRWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future\nresearch.",
        "Neural volumetric representations such as Neural Radiance Fields (NeRF) have\nemerged as a compelling technique for learning to represent 3D scenes from\nimages with the goal of rendering photorealistic images of the scene from\nunobserved viewpoints. However, NeRF's computational requirements are\nprohibitive for real-time applications: rendering views from a trained NeRF\nrequires querying a multilayer perceptron (MLP) hundreds of times per ray. We\npresent a method to train a NeRF, then precompute and store (i.e. \"bake\") it as\na novel representation called a Sparse Neural Radiance Grid (SNeRG) that\nenables real-time rendering on commodity hardware. To achieve this, we\nintroduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid\nrepresentation with learned feature vectors. The resulting scene representation\nretains NeRF's ability to render fine geometric details and view-dependent\nappearance, is compact (averaging less than 90 MB per scene), and can be\nrendered in real-time (higher than 30 frames per second on a laptop GPU).\nActual screen captures are shown in our video.",
        "By a \"covering\" we mean a Gaussian mixture model fit to observed data.\nApproximations of the Bayes factor can be availed of to judge model fit to the\ndata within a given Gaussian mixture model. Between families of Gaussian\nmixture models, we propose the R\\'enyi quadratic entropy as an excellent and\ntractable model comparison framework. We exemplify this using the segmentation\nof an MRI image volume, based (1) on a direct Gaussian mixture model applied to\nthe marginal distribution function, and (2) Gaussian model fit through k-means\napplied to the 4D multivalued image volume furnished by the wavelet transform.\nVisual preference for one model over another is not immediate. The R\\'enyi\nquadratic entropy allows us to show clearly that one of these modelings is\nsuperior to the other.",
        "Hyperspectral cameras can provide unique spectral signatures for consistently\ndistinguishing materials that can be used to solve surveillance tasks. In this\npaper, we propose a novel real-time hyperspectral likelihood maps-aided\ntracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving\nobject tracking system generally consists of registration, object detection,\nand tracking modules. We focus on the target detection part and remove the\nnecessity to build any offline classifiers and tune a large amount of\nhyperparameters, instead learning a generative target model in an online manner\nfor hyperspectral channels ranging from visible to infrared wavelengths. The\nkey idea is that, our adaptive fusion method can combine likelihood maps from\nmultiple bands of hyperspectral imagery into one single more distinctive\nrepresentation increasing the margin between mean value of foreground and\nbackground pixels in the fused map. Experimental results show that the HLT not\nonly outperforms all established fusion methods but is on par with the current\nstate-of-the-art hyperspectral target tracking frameworks.",
        "Estimation of information theoretic quantities such as mutual information and\nits conditional variant has drawn interest in recent times owing to their\nmultifaceted applications. Newly proposed neural estimators for these\nquantities have overcome severe drawbacks of classical $k$NN-based estimators\nin high dimensions. In this work, we focus on conditional mutual information\n(CMI) estimation by utilizing its formulation as a minmax optimization problem.\nSuch a formulation leads to a joint training procedure similar to that of\ngenerative adversarial networks. We find that our proposed estimator provides\nbetter estimates than the existing approaches on a variety of simulated data\nsets comprising linear and non-linear relations between variables. As an\napplication of CMI estimation, we deploy our estimator for conditional\nindependence (CI) testing on real data and obtain better results than\nstate-of-the-art CI testers.",
        "We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.",
        "Non-intrusive load monitoring (NILM) helps disaggregate the household's main\nelectricity consumption to energy usages of individual appliances, thus greatly\ncutting down the cost in fine-grained household load monitoring. To address the\narisen privacy concern in NILM applications, federated learning (FL) could be\nleveraged for NILM model training and sharing. When applying the FL paradigm in\nreal-world NILM applications, however, we are faced with the challenges of edge\nresource restriction, edge model personalization and edge training data\nscarcity.\n  In this paper we present FedNILM, a practical FL paradigm for NILM\napplications at the edge client. Specifically, FedNILM is designed to deliver\nprivacy-preserving and personalized NILM services to large-scale edge clients,\nby leveraging i) secure data aggregation through federated learning, ii)\nefficient cloud model compression via filter pruning and multi-task learning,\nand iii) personalized edge model building with unsupervised transfer learning.\nOur experiments on real-world energy data show that, FedNILM is able to achieve\npersonalized energy disaggregation with the state-of-the-art accuracy, while\nensuring privacy preserving at the edge client.",
        "Removing camera motion blur from a single light field is a challenging task\nsince it is highly ill-posed inverse problem. The problem becomes even worse\nwhen blur kernel varies spatially due to scene depth variation and high-order\ncamera motion. In this paper, we propose a novel algorithm to estimate all blur\nmodel variables jointly, including latent sub-aperture image, camera motion,\nand scene depth from the blurred 4D light field. Exploiting multi-view nature\nof a light field relieves the inverse property of the optimization by utilizing\nstrong depth cues and multi-view blur observation. The proposed joint\nestimation achieves high quality light field deblurring and depth estimation\nsimultaneously under arbitrary 6-DOF camera motion and unconstrained scene\ndepth. Intensive experiment on real and synthetic blurred light field confirms\nthat the proposed algorithm outperforms the state-of-the-art light field\ndeblurring and depth estimation methods.",
        "The success of deep learning heavily depends on the availability of large\nlabeled training sets. However, it is hard to get large labeled datasets in\nmedical image domain because of the strict privacy concern and costly labeling\nefforts. Contrastive learning, an unsupervised learning technique, has been\nproved powerful in learning image-level representations from unlabeled data.\nThe learned encoder can then be transferred or fine-tuned to improve the\nperformance of downstream tasks with limited labels. A critical step in\ncontrastive learning is the generation of contrastive data pairs, which is\nrelatively simple for natural image classification but quite challenging for\nmedical image segmentation due to the existence of the same tissue or organ\nacross the dataset. As a result, when applied to medical image segmentation,\nmost state-of-the-art contrastive learning frameworks inevitably introduce a\nlot of false-negative pairs and result in degraded segmentation quality. To\naddress this issue, we propose a novel positional contrastive learning (PCL)\nframework to generate contrastive data pairs by leveraging the position\ninformation in volumetric medical images. Experimental results on CT and MRI\ndatasets demonstrate that the proposed PCL method can substantially improve the\nsegmentation performance compared to existing methods in both semi-supervised\nsetting and transfer learning setting.",
        "3D object detection has seen quick progress thanks to advances in deep\nlearning on point clouds. A few recent works have even shown state-of-the-art\nperformance with just point clouds input (e.g. VoteNet). However, point cloud\ndata have inherent limitations. They are sparse, lack color information and\noften suffer from sensor noise. Images, on the other hand, have high resolution\nand rich texture. Thus they can complement the 3D geometry provided by point\nclouds. Yet how to effectively use image information to assist point cloud\nbased detection is still an open question. In this work, we build on top of\nVoteNet and propose a 3D detection architecture called ImVoteNet specialized\nfor RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes\nin point clouds. Compared to prior work on multi-modal detection, we explicitly\nextract both geometric and semantic features from the 2D images. We leverage\ncamera parameters to lift these features to 3D. To improve the synergy of 2D-3D\nfeature fusion, we also propose a multi-tower training scheme. We validate our\nmodel on the challenging SUN RGB-D dataset, advancing state-of-the-art results\nby 5.7 mAP. We also provide rich ablation studies to analyze the contribution\nof each design choice.",
        "We propose a method for modeling and learning turn-taking behaviors for\naccessing a shared resource. We model the individual behavior for each agent in\nan interaction and then use a multi-agent fusion model to generate a summary\nover the expected actions of the group to render the model independent of the\nnumber of agents. The individual behavior models are weighted finite state\ntransducers (WFSTs) with weights dynamically updated during interactions, and\nthe multi-agent fusion model is a logistic regression classifier.\n  We test our models in a multi-agent tower-building environment, where a\nQ-learning agent learns to interact with rule-based agents. Our approach\naccurately models the underlying behavior patterns of the rule-based agents\nwith accuracy ranging between 0.63 and 1.0 depending on the stochasticity of\nthe other agent behaviors. In addition we show using KL-divergence that the\nmodel accurately captures the distribution of next actions when interacting\nwith both a single agent (KL-divergence < 0.1) and with multiple agents\n(KL-divergence < 0.37). Finally, we demonstrate that our behavior model can be\nused by a Q-learning agent to take turns in an interactive turn-taking\nenvironment.",
        "We consider the problem of binary image generation with given properties.\nThis problem arises in a number of practical applications, including generation\nof artificial porous medium for an electrode of lithium-ion batteries, for\ncomposed materials, etc. A generated image represents a porous medium and, as\nsuch, it is subject to two sets of constraints: topological constraints on the\nstructure and process constraints on the physical process over this structure.\nTo perform image generation we need to define a mapping from a porous medium to\nits physical process parameters. For a given geometry of a porous medium, this\nmapping can be done by solving a partial differential equation (PDE). However,\nembedding a PDE solver into the search procedure is computationally expensive.\nWe use a binarized neural network to approximate a PDE solver. This allows us\nto encode the entire problem as a logical formula. Our main contribution is\nthat, for the first time, we show that this problem can be tackled using\ndecision procedures. Our experiments show that our model is able to produce\nrandom constrained images that satisfy both topological and process\nconstraints.",
        "We propose a single-stage Human-Object Interaction (HOI) detection method\nthat has outperformed all existing methods on HICO-DET dataset at 37 fps on a\nsingle Titan XP GPU. It is the first real-time HOI detection method.\nConventional HOI detection methods are composed of two stages, i.e.,\nhuman-object proposals generation, and proposals classification. Their\neffectiveness and efficiency are limited by the sequential and separate\narchitecture. In this paper, we propose a Parallel Point Detection and Matching\n(PPDM) HOI detection framework. In PPDM, an HOI is defined as a point triplet <\nhuman point, interaction point, object point>. Human and object points are the\ncenter of the detection boxes, and the interaction point is the midpoint of the\nhuman and object points. PPDM contains two parallel branches, namely point\ndetection branch and point matching branch. The point detection branch predicts\nthree points. Simultaneously, the point matching branch predicts two\ndisplacements from the interaction point to its corresponding human and object\npoints. The human point and the object point originated from the same\ninteraction point are considered as matched pairs. In our novel parallel\narchitecture, the interaction points implicitly provide context and\nregularization for human and object detection. The isolated detection boxes are\nunlikely to form meaning HOI triplets are suppressed, which increases the\nprecision of HOI detection. Moreover, the matching between human and object\ndetection boxes is only applied around limited numbers of filtered candidate\ninteraction points, which saves much computational cost. Additionally, we build\na new application-oriented database named HOI-A, which severs as a good\nsupplement to the existing datasets. The source code and the dataset will be\nmade publicly available to facilitate the development of HOI detection.",
        "We study the problem of aligning two sets of 3D geometric primitives given\nknown correspondences. Our first contribution is to show that this primitive\nalignment framework unifies five perception problems including point cloud\nregistration, primitive (mesh) registration, category-level 3D registration,\nabsolution pose estimation (APE), and category-level APE. Our second\ncontribution is to propose DynAMical Pose estimation (DAMP), the first general\nand practical algorithm to solve primitive alignment problem by simulating\nrigid body dynamics arising from virtual springs and damping, where the springs\nspan the shortest distances between corresponding primitives. We evaluate DAMP\nin simulated and real datasets across all five problems, and demonstrate (i)\nDAMP always converges to the globally optimal solution in the first three\nproblems with 3D-3D correspondences; (ii) although DAMP sometimes converges to\nsuboptimal solutions in the last two problems with 2D-3D correspondences, using\na scheme for escaping local minima, DAMP always succeeds. Our third\ncontribution is to demystify the surprising empirical performance of DAMP and\nformally prove a global convergence result in the case of point cloud\nregistration by charactering local stability of the equilibrium points of the\nunderlying dynamical system.",
        "The energy consumption of the HVAC system accounts for a significant portion\nof the energy consumption of the public building system, and using an efficient\nenergy consumption prediction model can assist it in carrying out effective\nenergy-saving transformation. Unlike the traditional energy consumption\nprediction model, this paper extracts features from large data sets using\nXGBoost, trains them separately to obtain multiple models, then fuses them with\nLightGBM's independent prediction results using MAE, infers energy consumption\nrelated variables, and successfully applies this model to the self-developed\nInternet of Things platform.",
        "Semantic segmentation of medical images is a crucial step for the\nquantification of healthy anatomy and diseases alike. The majority of the\ncurrent state-of-the-art segmentation algorithms are based on deep neural\nnetworks and rely on large datasets with full pixel-wise annotations. Producing\nsuch annotations can often only be done by medical professionals and requires\nlarge amounts of valuable time. Training a medical image segmentation network\nwith weak annotations remains a relatively unexplored topic. In this work we\ninvestigate training strategies to learn the parameters of a pixel-wise\nsegmentation network from scribble annotations alone. We evaluate the\ntechniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation\ndatasets. We find that the networks trained on scribbles suffer from a\nremarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)\nwith respect to a network trained on full annotations.",
        "Gear drives are one of the most widely used transmission system in many\nmachinery. Sound signals of a rotating machine contain the dynamic information\nabout its health conditions. Not much information available in the literature\nreporting suitability of sound signals for fault diagnosis applications.\nMaximum numbers of literature are based on FFT (Fast Fourier Transform)\nanalysis and have its own limitations with non-stationary signals like the ones\nfrom gears. In this paper, attempt has been made in using sound signals\nacquired from gears in good and simulated faulty conditions for the purpose of\nfault diagnosis through a machine learning approach. The descriptive\nstatistical features were extracted from the acquired sound signals and the\npredominant features were selected using J48 decision tree technique. The\nselected features were then used for classification using Large Margin\nK-nearest neighbor approach. The paper also discusses the effect of various\nparameters on classification accuracy.",
        "Substantial progress has been made on modeling rigid 3D objects using deep\nimplicit representations. Yet, extending these methods to learn neural models\nof human shape is still in its infancy. Human bodies are complex and the key\nchallenge is to learn a representation that generalizes such that it can\nexpress body shape deformations for unseen subjects in unseen,\nhighly-articulated, poses. To address this challenge, we introduce LEAP\n(LEarning Articulated occupancy of People), a novel neural occupancy\nrepresentation of the human body. Given a set of bone transformations (i.e.\njoint locations and rotations) and a query point in space, LEAP first maps the\nquery point to a canonical space via learned linear blend skinning (LBS)\nfunctions and then efficiently queries the occupancy value via an occupancy\nnetwork that models accurate identity- and pose-dependent deformations in the\ncanonical space. Experiments show that our canonicalized occupancy estimation\nwith the learned LBS functions greatly improves the generalization capability\nof the learned occupancy representation across various human shapes and poses,\noutperforming existing solutions in all settings.",
        "Scene understanding has been of high interest in computer vision. It\nencompasses not only identifying objects in a scene, but also their\nrelationships within the given context. With this goal, a recent line of works\ntackles 3D semantic segmentation and scene layout prediction. In our work we\nfocus on scene graphs, a data structure that organizes the entities of a scene\nin a graph, where objects are nodes and their relationships modeled as edges.\nWe leverage inference on scene graphs as a way to carry out 3D scene\nunderstanding, mapping objects and their relationships. In particular, we\npropose a learned method that regresses a scene graph from the point cloud of a\nscene. Our novel architecture is based on PointNet and Graph Convolutional\nNetworks (GCN). In addition, we introduce 3DSSG, a semi-automatically generated\ndataset, that contains semantically rich scene graphs of 3D scenes. We show the\napplication of our method in a domain-agnostic retrieval task, where graphs\nserve as an intermediate representation for 3D-3D and 2D-3D matching.",
        "To date, research on sensor-equipped mobile devices has primarily focused on\nthe purely supervised task of human activity recognition (walking, running,\netc), demonstrating limited success in inferring high-level health outcomes\nfrom low-level signals, such as acceleration. Here, we present a novel\nself-supervised representation learning method using activity and heart rate\n(HR) signals without semantic labels. With a deep neural network, we set HR\nresponses as the supervisory signal for the activity data, leveraging their\nunderlying physiological relationship.\n  We evaluate our model in the largest free-living combined-sensing dataset\n(comprising more than 280,000 hours of wrist accelerometer & wearable ECG data)\nand show that the resulting embeddings can generalize in various downstream\ntasks through transfer learning with linear classifiers, capturing\nphysiologically meaningful, personalized information. For instance, they can be\nused to predict (higher than 70 AUC) variables associated with individuals'\nhealth, fitness and demographic characteristics, outperforming unsupervised\nautoencoders and common bio-markers. Overall, we propose the first multimodal\nself-supervised method for behavioral and physiological data with implications\nfor large-scale health and lifestyle monitoring.",
        "Convolutional neural networks have recently demonstrated high-quality\nreconstruction for single image super-resolution. However, existing methods\noften require a large number of network parameters and entail heavy\ncomputational loads at runtime for generating high-accuracy super-resolution\nresults. In this paper, we propose the deep Laplacian Pyramid Super-Resolution\nNetwork for fast and accurate image super-resolution. The proposed network\nprogressively reconstructs the sub-band residuals of high-resolution images at\nmultiple pyramid levels. In contrast to existing methods that involve the\nbicubic interpolation for pre-processing (which results in large feature maps),\nthe proposed method directly extracts features from the low-resolution input\nspace and thereby entails low computational loads. We train the proposed\nnetwork with deep supervision using the robust Charbonnier loss functions and\nachieve high-quality image reconstruction. Furthermore, we utilize the\nrecursive layers to share parameters across as well as within pyramid levels,\nand thus drastically reduce the number of parameters. Extensive quantitative\nand qualitative evaluations on benchmark datasets show that the proposed\nalgorithm performs favorably against the state-of-the-art methods in terms of\nrun-time and image quality.",
        "Rivers and canals flowing through cities are often used illegally for dumping\nthe trash. This contaminates freshwater channels as well as causes blockage in\nsewerage resulting in urban flooding. When this contaminated water reaches\nagricultural fields, it results in degradation of soil and poses critical\nenvironmental as well as economic threats. The dumped trash is often found\nfloating on the water surface. The trash could be disfigured, partially\nsubmerged, decomposed into smaller pieces, clumped together with other objects\nwhich obscure its shape and creates a challenging detection problem. This paper\nproposes a method for the detection of visible trash floating on the water\nsurface of the canals in urban areas. We also provide a large dataset, first of\nits kind, trash in water channels that contains object-level annotations. A\nnovel attention layer is proposed that improves the detection of smaller\nobjects. Towards the end of this paper, we provide a detailed comparison of our\nmethod with state-of-the-art object detectors and show that our method\nsignificantly improves the detection of smaller objects. The dataset will be\nmade publicly available.",
        "The state-of-the-art performance for object detection has been significantly\nimproved over the past two years. Besides the introduction of powerful deep\nneural networks such as GoogleNet and VGG, novel object detection frameworks\nsuch as R-CNN and its successors, Fast R-CNN and Faster R-CNN, play an\nessential role in improving the state-of-the-art. Despite their effectiveness\non still images, those frameworks are not specifically designed for object\ndetection from videos. Temporal and contextual information of videos are not\nfully investigated and utilized. In this work, we propose a deep learning\nframework that incorporates temporal and contextual information from tubelets\nobtained in videos, which dramatically improves the baseline performance of\nexisting still-image detection frameworks when they are applied to videos. It\nis called T-CNN, i.e. tubelets with convolutional neueral networks. The\nproposed framework won the recently introduced object-detection-from-video\n(VID) task with provided data in the ImageNet Large-Scale Visual Recognition\nChallenge 2015 (ILSVRC2015).",
        "We introduce a family of multilayer graph kernels and establish new links\nbetween graph convolutional neural networks and kernel methods. Our approach\ngeneralizes convolutional kernel networks to graph-structured data, by\nrepresenting graphs as a sequence of kernel feature maps, where each node\ncarries information about local graph substructures. On the one hand, the\nkernel point of view offers an unsupervised, expressive, and easy-to-regularize\ndata representation, which is useful when limited samples are available. On the\nother hand, our model can also be trained end-to-end on large-scale data,\nleading to new types of graph convolutional neural networks. We show that our\nmethod achieves competitive performance on several graph classification\nbenchmarks, while offering simple model interpretation. Our code is freely\navailable at https://github.com/claying/GCKN.",
        "Speech is the most common communication method between humans and involves\nthe perception of both auditory and visual channels. Automatic speech\nrecognition focuses on interpreting the audio signals, but it has been\ndemonstrated that video can provide information that is complementary to the\naudio. Thus, the study of automatic lip-reading is important and is still an\nopen problem. One of the key challenges is the definition of the visual\nelementary units (the visemes) and their vocabulary. Many researchers have\nanalyzed the importance of the phoneme to viseme mapping and have proposed\nviseme vocabularies with lengths between 11 and 15 visemes. These viseme\nvocabularies have usually been manually defined by their linguistic properties\nand in some cases using decision trees or clustering techniques. In this work,\nwe focus on the automatic construction of an optimal viseme vocabulary based on\nthe association of phonemes with similar appearance. To this end, we construct\nan automatic system that uses local appearance descriptors to extract the main\ncharacteristics of the mouth region and HMMs to model the statistic relations\nof both viseme and phoneme sequences. To compare the performance of the system\ndifferent descriptors (PCA, DCT and SIFT) are analyzed. We test our system in a\nSpanish corpus of continuous speech. Our results indicate that we are able to\nrecognize approximately 58% of the visemes, 47% of the phonemes and 23% of the\nwords in a continuous speech scenario and that the optimal viseme vocabulary\nfor Spanish is composed by 20 visemes.",
        "The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.",
        "(Partial) ranking loss is a commonly used evaluation measure for multi-label\nclassification, which is usually optimized with convex surrogates for\ncomputational efficiency. Prior theoretical work on multi-label ranking mainly\nfocuses on (Fisher) consistency analyses. However, there is a gap between\nexisting theory and practice -- some pairwise losses can lead to promising\nperformance but lack consistency, while some univariate losses are consistent\nbut usually have no clear superiority in practice. In this paper, we attempt to\nfill this gap through a systematic study from two complementary perspectives of\nconsistency and generalization error bounds of learning algorithms. Our results\nshow that learning algorithms with the consistent univariate loss have an error\nbound of $O(c)$ ($c$ is the number of labels), while algorithms with the\ninconsistent pairwise loss depend on $O(\\sqrt{c})$ as shown in prior work. This\nexplains that the latter can achieve better performance than the former in\npractice. Moreover, we present an inconsistent reweighted univariate loss-based\nlearning algorithm that enjoys an error bound of $O(\\sqrt{c})$ for promising\nperformance as well as the computational efficiency of univariate losses.\nFinally, experimental results validate our theoretical analyses.",
        "Mutual Information between agent Actions and environment States (MIAS)\nquantifies the influence of agent on its environment. Recently, it was found\nthat the maximization of MIAS can be used as an intrinsic motivation for\nartificial agents. In literature, the term empowerment is used to represent the\nmaximum of MIAS at a certain state. While empowerment has been shown to solve a\nbroad range of reinforcement learning problems, its calculation in arbitrary\ndynamics is a challenging problem because it relies on the estimation of mutual\ninformation. Existing approaches, which rely on sampling, are limited to low\ndimensional spaces, because high-confidence distribution-free lower bounds for\nmutual information require exponential number of samples. In this work, we\ndevelop a novel approach for the estimation of empowerment in unknown dynamics\nfrom visual observation only, without the need to sample for MIAS. The core\nidea is to represent the relation between action sequences and future states\nusing a stochastic dynamic model in latent space with a specific form. This\nallows us to efficiently compute empowerment with the \"Water-Filling\" algorithm\nfrom information theory. We construct this embedding with deep neural networks\ntrained on a sophisticated objective function. Our experimental results show\nthat the designed embedding preserves information-theoretic properties of the\noriginal dynamics.",
        "Several works have addressed the problem of incorporating constraints in the\nreinforcement learning (RL) framework, however majority of them can only\nguarantee the satisfaction of soft constraints. In this work, we address the\nproblem of satisfying hard state constraints in a model-free RL setting with\nthe deterministic system dynamics. The proposed algorithm is developed for the\ndiscrete state and action space and utilizes a multi-class support vector\nmachine (SVM) to represent the policy. The state constraints are incorporated\nin the SVM optimization framework to derive an analytical solution for\ndetermining the policy parameters. This final policy converges to a solution\nwhich is guaranteed to satisfy the constraints. Additionally, the proposed\nformulation adheres to the Q-learning framework and thus, also guarantees\nconvergence to the optimal solution. The algorithm is demonstrated with\nmultiple example problems.",
        "The 'Clever Hans' effect occurs when the learned model produces correct\npredictions based on the 'wrong' features. This effect which undermines the\ngeneralization capability of an ML model and goes undetected by standard\nvalidation techniques has been frequently observed for supervised learning\nwhere the training algorithm leverages spurious correlations in the data. The\nquestion whether Clever Hans also occurs in unsupervised learning, and in which\nform, has received so far almost no attention. Therefore, this paper will\ncontribute an explainable AI (XAI) procedure that can highlight the relevant\nfeatures used by popular anomaly detection models of different type. Our\nanalysis reveals that the Clever Hans effect is widespread in anomaly detection\nand occurs in many (unexpected) forms. Interestingly, the observed Clever Hans\neffects are in this case not so much due to the data, but due to the anomaly\ndetection models themselves whose structure makes them unable to detect the\ntruly relevant features, even though vast amounts of data points are available.\nOverall, our work contributes a warning against an unrestrained use of existing\nanomaly detection models in practical applications, but it also points at a\npossible way out of the Clever Hans dilemma, specifically, by allowing multiple\nanomaly models to mutually cancel their individual structural weaknesses to\njointly produce a better and more trustworthy anomaly detector.",
        "Solving tasks with sparse rewards is a main challenge in reinforcement\nlearning. While hierarchical controllers are an intuitive approach to this\nproblem, current methods often require manual reward shaping, alternating\ntraining phases, or manually defined sub tasks. We introduce modulated policy\nhierarchies (MPH), that can learn end-to-end to solve tasks from sparse\nrewards. To achieve this, we study different modulation signals and exploration\nfor hierarchical controllers. Specifically, we find that communicating via\nbit-vectors is more efficient than selecting one out of multiple skills, as it\nenables mixing between them. To facilitate exploration, MPH uses its different\ntime scales for temporally extended intrinsic motivation at each level of the\nhierarchy. We evaluate MPH on the robotics tasks of pushing and sparse block\nstacking, where it outperforms recent baselines.",
        "Deep neural networks are applied to a wide range of problems in recent years.\nIn this work, Convolutional Neural Network (CNN) is applied to the problem of\ndetermining the depth from a single camera image (monocular depth). Eight\ndifferent networks are designed to perform depth estimation, each of them\nsuitable for a feature level. Networks with different pooling sizes determine\ndifferent feature levels. After designing a set of networks, these models may\nbe combined into a single network topology using graph optimization techniques.\nThis \"Semi Parallel Deep Neural Network (SPDNN)\" eliminates duplicated common\nnetwork layers, and can be further optimized by retraining to achieve an\nimproved model compared to the individual topologies. In this study, four SPDNN\nmodels are trained and have been evaluated at 2 stages on the KITTI dataset.\nThe ground truth images in the first part of the experiment are provided by the\nbenchmark, and for the second part, the ground truth images are the depth map\nresults from applying a state-of-the-art stereo matching method. The results of\nthis evaluation demonstrate that using post-processing techniques to refine the\ntarget of the network increases the accuracy of depth estimation on individual\nmono images. The second evaluation shows that using segmentation data alongside\nthe original data as the input can improve the depth estimation results to a\npoint where performance is comparable with stereo depth estimation. The\ncomputational time is also discussed in this study.",
        "We consider two important aspects in understanding and editing images:\nmodeling regular, program-like texture or patterns in 2D planes, and 3D posing\nof these planes in the scene. Unlike prior work on image-based program\nsynthesis, which assumes the image contains a single visible 2D plane, we\npresent Box Program Induction (BPI), which infers a program-like scene\nrepresentation that simultaneously models repeated structure on multiple 2D\nplanes, the 3D position and orientation of the planes, and camera parameters,\nall from a single image. Our model assumes a box prior, i.e., that the image\ncaptures either an inner view or an outer view of a box in 3D. It uses neural\nnetworks to infer visual cues such as vanishing points, wireframe lines to\nguide a search-based algorithm to find the program that best explains the\nimage. Such a holistic, structured scene representation enables 3D-aware\ninteractive image editing operations such as inpainting missing pixels,\nchanging camera parameters, and extrapolate the image contents.",
        "This paper builds on the connection between graph neural networks and\ntraditional dynamical systems. We propose continuous graph neural networks\n(CGNN), which generalise existing graph neural networks with discrete dynamics\nin that they can be viewed as a specific discretisation scheme. The key idea is\nhow to characterise the continuous dynamics of node representations, i.e. the\nderivatives of node representations, w.r.t. time. Inspired by existing\ndiffusion-based methods on graphs (e.g. PageRank and epidemic models on social\nnetworks), we define the derivatives as a combination of the current node\nrepresentations, the representations of neighbors, and the initial values of\nthe nodes. We propose and analyse two possible dynamics on graphs---including\neach dimension of node representations (a.k.a. the feature channel) change\nindependently or interact with each other---both with theoretical\njustification. The proposed continuous graph neural networks are robust to\nover-smoothing and hence allow us to build deeper networks, which in turn are\nable to capture the long-range dependencies between nodes. Experimental results\non the task of node classification demonstrate the effectiveness of our\nproposed approach over competitive baselines.",
        "Quantitative bone single-photon emission computed tomography (QBSPECT) has\nthe potential to provide a better quantitative assessment of bone metastasis\nthan planar bone scintigraphy due to its ability to better quantify activity in\noverlapping structures. An important element of assessing response of bone\nmetastasis is accurate image segmentation. However, limited by the properties\nof QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs)\nstill relies heavily on the manual delineation by experts. This work proposes a\nfast and robust automated segmentation method for partitioning a QBSPECT image\ninto lesion, bone, and background. We present a new unsupervised segmentation\nloss function and its semi- and supervised variants for training a\nconvolutional neural network (ConvNet). The loss functions were developed based\non the objective function of the classical Fuzzy C-means (FCM) algorithm. We\nconducted a comprehensive study to compare our proposed methods with ConvNets\ntrained using supervised loss functions and conventional clustering methods.\nThe Dice similarity coefficient (DSC) and several other metrics were used as\nfigures of merit as applied to the task of delineating lesion and bone in both\nsimulated and clinical SPECT/CT images. We experimentally demonstrated that the\nproposed methods yielded good segmentation results on a clinical dataset even\nthough the training was done using realistic simulated images. A ConvNet-based\nimage segmentation method that uses novel loss functions was developed and\nevaluated. The method can operate in unsupervised, semi-supervised, or\nfully-supervised modes depending on the availability of annotated training\ndata. The results demonstrated that the proposed method provides fast and\nrobust lesion and bone segmentation for QBSPECT/CT. The method can potentially\nbe applied to other medical image segmentation applications.",
        "Transformer is a ubiquitous model for natural language processing and has\nattracted wide attentions in computer vision. The attention maps are\nindispensable for a transformer model to encode the dependencies among input\ntokens. However, they are learned independently in each layer and sometimes\nfail to capture precise patterns. In this paper, we propose a novel and generic\nmechanism based on evolving attention to improve the performance of\ntransformers. On one hand, the attention maps in different layers share common\nknowledge, thus the ones in preceding layers can instruct the attention in\nsucceeding layers through residual connections. On the other hand, low-level\nand high-level attentions vary in the level of abstraction, so we adopt\nconvolutional layers to model the evolutionary process of attention maps. The\nproposed evolving attention mechanism achieves significant performance\nimprovement over various state-of-the-art models for multiple tasks, including\nimage classification, natural language understanding and machine translation.",
        "Deep learning applied to the reconstruction of 3D shapes has seen growing\ninterest. A popular approach to 3D reconstruction and generation in recent\nyears has been the CNN encoder-decoder model usually applied in voxel space.\nHowever, this often scales very poorly with the resolution limiting the\neffectiveness of these models. Several sophisticated alternatives for decoding\nto 3D shapes have been proposed typically relying on complex deep learning\narchitectures for the decoder model. In this work, we show that this additional\ncomplexity is not necessary, and that we can actually obtain high quality 3D\nreconstruction using a linear decoder, obtained from principal component\nanalysis on the signed distance function (SDF) of the surface. This approach\nallows easily scaling to larger resolutions. We show in multiple experiments\nthat our approach is competitive with state-of-the-art methods. It also allows\nthe decoder to be fine-tuned on the target task using a loss designed\nspecifically for SDF transforms, obtaining further gains.",
        "Finding visual correspondence between local features is key to many computer\nvision problems. While defining features with larger contextual scales usually\nimplies greater discriminativeness, it could also lead to less spatial accuracy\nof the features. We propose AutoScaler, a scale-attention network to explicitly\noptimize this trade-off in visual correspondence tasks. Our network consists of\na weight-sharing feature network to compute multi-scale feature maps and an\nattention network to combine them optimally in the scale space. This allows our\nnetwork to have adaptive receptive field sizes over different scales of the\ninput. The entire network is trained end-to-end in a siamese framework for\nvisual correspondence tasks. Our method achieves favorable results compared to\nstate-of-the-art methods on challenging optical flow and semantic matching\nbenchmarks, including Sintel, KITTI and CUB-2011. We also show that our method\ncan generalize to improve hand-crafted descriptors (e.g Daisy) on general\nvisual correspondence tasks. Finally, our attention network can generate\nvisually interpretable scale attention maps.",
        "Generative adversarial networks (GANs) have been successfully used for\nconsiderable computer vision tasks, especially the image-to-image translation.\nHowever, generators in these networks are of complicated architectures with\nlarge number of parameters and huge computational complexities. Existing\nmethods are mainly designed for compressing and speeding-up deep neural\nnetworks in the classification task, and cannot be directly applied on GANs for\nimage translation, due to their different objectives and training procedures.\nTo this end, we develop a novel co-evolutionary approach for reducing their\nmemory usage and FLOPs simultaneously. In practice, generators for two image\ndomains are encoded as two populations and synergistically optimized for\ninvestigating the most important convolution filters iteratively. Fitness of\neach individual is calculated using the number of parameters, a\ndiscriminator-aware regularization, and the cycle consistency. Extensive\nexperiments conducted on benchmark datasets demonstrate the effectiveness of\nthe proposed method for obtaining compact and effective generators.",
        "The recent advances of convolutional detectors show impressive performance\nimprovement for large scale object detection. However, in general, the\ndetection performance usually decreases as the object classes to be detected\nincreases, and it is a practically challenging problem to train a dominant\nmodel for all classes due to the limitations of detection models and datasets.\nIn most cases, therefore, there are distinct performance differences of the\nmodern convolutional detectors for each object class detection. In this paper,\nin order to build an ensemble detector for large scale object detection, we\npresent a conceptually simple but very effective class-wise ensemble detection\nwhich is named as Rank of Experts. We first decompose an intractable problem of\nfinding the best detections for all object classes into small subproblems of\nfinding the best ones for each object class. We then solve the detection\nproblem by ranking detectors in order of the average precision rate for each\nclass, and then aggregate the responses of the top ranked detectors (i.e.\nexperts) for class-wise ensemble detection. The main benefit of our method is\neasy to implement and does not require any joint training of experts for\nensemble. Based on the proposed Rank of Experts, we won the 2nd place in the\nILSVRC 2017 object detection competition.",
        "Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.",
        "Computer vision has flourished in recent years thanks to Deep Learning\nadvancements, fast and scalable hardware solutions and large availability of\nstructured image data. Convolutional Neural Networks trained on supervised\ntasks with backpropagation learn to extract meaningful representations from raw\npixels automatically, and surpass shallow methods in image understanding.\nThough convenient, data-driven feature learning is prone to dataset bias: a\nnetwork learns its parameters from training signals alone, and will usually\nperform poorly if train and test distribution differ. To alleviate this\nproblem, research on Domain Generalization (DG), Domain Adaptation (DA) and\ntheir variations is increasing. This thesis contributes to these research\ntopics by presenting novel and effective ways to solve the dataset bias problem\nin its various settings. We propose new frameworks for Domain Generalization\nand Domain Adaptation which make use of feature aggregation strategies and\nvisual transformations via data-augmentation and multi-task integration of\nself-supervision. We also design an algorithm that adapts an object detection\nmodel to any out of distribution sample at test time. With through\nexperimentation, we show how our proposed solutions outperform competitive\nstate-of-the-art approaches in established DG and DA benchmarks.",
        "In recent years, huge amounts of unstructured textual data on the Internet\nare a big difficulty for AI algorithms to provide the best recommendations for\nusers and their search queries. Since the Internet became widespread, a lot of\nresearch has been done in the field of Natural Language Processing (NLP) and\nmachine learning. Almost every solution transforms documents into Vector Space\nModels (VSM) in order to apply AI algorithms over them. One such approach is\nbased on Case-Based Reasoning (CBR). Therefore, the most important part of\nthose systems is to compute the similarity between numerical data points. In\n2016, the new similarity TS-SS metric is proposed, which showed\nstate-of-the-art results in the field of textual mining for unsupervised\nlearning. However, no one before has investigated its performances for\nsupervised learning (classification task). In this work, we devised a CBR\nsystem capable of finding the most similar documents for a given query aiming\nto investigate performances of the new state-of-the-art metric, TS-SS, in\naddition to the two other geometrical similarity measures --- Euclidean\ndistance and Cosine similarity --- that showed the best predictive results over\nseveral benchmark corpora. The results show surprising inappropriateness of\nTS-SS measure for high dimensional features.",
        "Semantic and instance segmentation algorithms are two general yet distinct\nimage segmentation solutions powered by Convolution Neural Network. While\nsemantic segmentation benefits extensively from the end-to-end training\nstrategy, instance segmentation is frequently framed as a multi-stage task,\nsupported by learning-based discrimination and post-process clustering.\nIndependent optimizations on substages instigate the accumulation of\nsegmentation errors. In this work, we propose to embed prior clustering\ninformation into an embedding learning framework FCRNet, stimulating the\none-stage instance segmentation. FCRNet relieves the complexity of post process\nby incorporating the number of clustering groups into the embedding space. The\nsuperior performance of FCRNet is verified and compared with other methods on\nthe nucleus dataset BBBC006.",
        "Scene Parsing is a crucial step to enable autonomous systems to understand\nand interact with their surroundings. Supervised deep learning methods have\nmade great progress in solving scene parsing problems, however, come at the\ncost of laborious manual pixel-level annotation. To alleviate this effort\nsynthetic data as well as weak supervision have both been investigated.\nNonetheless, synthetically generated data still suffers from severe domain\nshift while weak labels are often imprecise. Moreover, most existing works for\nweakly supervised scene parsing are limited to salient foreground objects. The\naim of this work is hence twofold: Exploit synthetic data where feasible and\nintegrate weak supervision where necessary. More concretely, we address this\ngoal by utilizing depth as transfer domain because its synthetic-to-real\ndiscrepancy is much lower than for color. At the same time, we perform weak\nlocalization from easily obtainable image level labels and integrate both using\na novel contour-based scheme. Our approach is implemented as a teacher-student\nlearning framework to solve the transfer learning problem by generating a\npseudo ground truth. Using only depth-based adaptation, this approach already\noutperforms previous transfer learning approaches on the popular indoor scene\nparsing SUN RGB-D dataset. Our proposed two-stage integration more than halves\nthe gap towards fully supervised methods when compared to previous\nstate-of-the-art in transfer learning.",
        "Despite the growing discriminative capabilities of modern deep learning\nmethods for recognition tasks, the inner workings of the state-of-art models\nstill remain mostly black-boxes. In this paper, we propose a systematic\ninterpretation of model parameters and hidden representations of Residual\nTemporal Convolutional Networks (Res-TCN) for action recognition in time-series\ndata. We also propose a Feature Map Decoder as part of the interpretation\nanalysis, which outputs a representation of model's hidden variables in the\nsame domain as the input. Such analysis empowers us to expose model's\ncharacteristic learning patterns in an interpretable way. For example, through\nthe diagnosis analysis, we discovered that our model has learned to achieve\nview-point invariance by implicitly learning to perform rotational\nnormalization of the input to a more discriminative view. Based on the findings\nfrom the model interpretation analysis, we propose a targeted refinement\ntechnique, which can generalize to various other recognition models. The\nproposed work introduces a three-stage paradigm for model learning: training,\ninterpretable diagnosis and targeted refinement. We validate our approach on\nskeleton based 3D human action recognition benchmark of NTU RGB+D. We show that\nthe proposed workflow is an effective model learning strategy and the resulting\nMulti-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the\nstate-of-the-art performance on NTU RGB+D.",
        "In this paper, we propose a new product knowledge graph (PKG) embedding\napproach for learning the intrinsic product relations as product knowledge for\ne-commerce. We define the key entities and summarize the pivotal product\nrelations that are critical for general e-commerce applications including\nmarketing, advertisement, search ranking and recommendation. We first provide a\ncomprehensive comparison between PKG and ordinary knowledge graph (KG) and then\nillustrate why KG embedding methods are not suitable for PKG learning. We\nconstruct a self-attention-enhanced distributed representation learning model\nfor learning PKG embeddings from raw customer activity data in an end-to-end\nfashion. We design an effective multi-task learning schema to fully leverage\nthe multi-modal e-commerce data. The Poincare embedding is also employed to\nhandle complex entity structures. We use a real-world dataset from\ngrocery.walmart.com to evaluate the performances on knowledge completion,\nsearch ranking and recommendation. The proposed approach compares favourably to\nbaselines in knowledge completion and downstream tasks.",
        "Real-time CNN-based object detection models for applications like\nsurveillance can achieve high accuracy but are computationally expensive.\nRecent works have shown 10 to 100x reduction in computation cost for inference\nby using domain-specific networks. However, prior works have focused on\ninference only. If the domain model requires frequent retraining, training\ncosts can pose a significant bottleneck. To address this, we propose Dataset\nCulling: a pipeline to reduce the size of the dataset for training, based on\nthe prediction difficulty. Images that are easy to classify are filtered out\nsince they contribute little to improving the accuracy. The difficulty is\nmeasured using our proposed confidence loss metric with little computational\noverhead. Dataset Culling is extended to optimize the image resolution to\nfurther improve training and inference costs. We develop fixed-angle,\nlong-duration video datasets across several domains, and we show that the\ndataset size can be culled by a factor of 300x to reduce the total training\ntime by 47x with no accuracy loss or even with slight improvement. Codes are\navailable: https://github.com/kentaroy47/DatasetCulling",
        "A common shortfall of supervised learning for medical imaging is the greedy\nneed for human annotations, which is often expensive and time-consuming to\nobtain. This paper proposes a semi-supervised classification method for three\nkinds of apicomplexan parasites and non-infected host cells microscopic images,\nwhich uses a small number of labeled data and a large number of unlabeled data\nfor training. There are two challenges in microscopic image recognition. The\nfirst is that salient structures of the microscopic images are more fuzzy and\nintricate than natural images' on a real-world scale. The second is that\ninsignificant textures, like background staining, lightness, and contrast\nlevel, vary a lot in samples from different clinical scenarios. To address\nthese challenges, we aim to learn a distinguishable and appearance-invariant\nrepresentation by contrastive learning strategy. On one hand, macroscopic\nimages, which share similar shape characteristics in morphology, are introduced\nto contrast for structure enhancement. On the other hand, different appearance\ntransformations, including color distortion and flittering, are utilized to\ncontrast for texture elimination. In the case where only 1% of microscopic\nimages are labeled, the proposed method reaches an accuracy of 94.90% in a\ngeneralized testing set.",
        "We present an end-to-end, interpretable, deep-learning architecture to learn\na graph kernel that predicts the outcome of chronic disease drug prescription.\nThis is achieved through a deep metric learning collaborative with a Support\nVector Machine objective using a graphical representation of Electronic Health\nRecords. We formulate the predictive model as a binary graph classification\nproblem with an adaptive learned graph kernel through novel cross-global\nattention node matching between patient graphs, simultaneously computing on\nmultiple graphs without training pair or triplet generation. Results using the\nTaiwanese National Health Insurance Research Database demonstrate that our\napproach outperforms current start-of-the-art models both in terms of accuracy\nand interpretability.",
        "Identification of disease genes, which are a set of genes associated with a\ndisease, plays an important role in understanding and curing diseases. In this\npaper, we present a biomedical knowledge graph designed specifically for this\nproblem, propose a novel machine learning method that identifies disease genes\non such graphs by leveraging recent advances in network biology and graph\nrepresentation learning, study the effects of various relation types on\nprediction performance, and empirically demonstrate that our algorithms\noutperform its closest state-of-the-art competitor in disease gene\nidentification by 24.1%. We also show that we achieve higher precision than\nOpen Targets, the leading initiative for target identification, with respect to\npredicting drug targets in clinical trials for Parkinson's disease.",
        "Representing and reasoning about 3D structures of macromolecules is emerging\nas a distinct challenge in machine learning. Here, we extend recent work on\ngeometric vector perceptrons and apply equivariant graph neural networks to a\nwide range of tasks from structural biology. Our method outperforms all\nreference architectures on three out of eight tasks in the ATOM3D benchmark, is\ntied for first on two others, and is competitive with equivariant networks\nusing higher-order representations and spherical harmonic convolutions. In\naddition, we demonstrate that transfer learning can further improve performance\non certain downstream tasks. Code is available at\nhttps://github.com/drorlab/gvp-pytorch.",
        "A framework for unsupervised group activity analysis from a single video is\nhere presented. Our working hypothesis is that human actions lie on a union of\nlow-dimensional subspaces, and thus can be efficiently modeled as sparse linear\ncombinations of atoms from a learned dictionary representing the action's\nprimitives. Contrary to prior art, and with the primary goal of spatio-temporal\naction grouping, in this work only one single video segment is available for\nboth unsupervised learning and analysis without any prior training information.\nAfter extracting simple features at a single spatio-temporal scale, we learn a\ndictionary for each individual in the video during each short time lapse. These\ndictionaries allow us to compare the individuals' actions by producing an\naffinity matrix which contains sufficient discriminative information about the\nactions in the scene leading to grouping with simple and efficient tools. With\ndiverse publicly available real videos, we demonstrate the effectiveness of the\nproposed framework and its robustness to cluttered backgrounds, changes of\nhuman appearance, and action variability.",
        "Recently, Transformer networks have redefined the state of the art in many\nNLP tasks. However, these models suffer from quadratic computational cost in\nthe input sequence length $n$ to compute pairwise attention in each layer. This\nhas prompted recent research into sparse Transformers that sparsify the\nconnections in the attention layers. While empirically promising for long\nsequences, fundamental questions remain unanswered: Can sparse Transformers\napproximate any arbitrary sequence-to-sequence function, similar to their dense\ncounterparts? How does the sparsity pattern and the sparsity level affect their\nperformance? In this paper, we address these questions and provide a unifying\nframework that captures existing sparse attention models. We propose sufficient\nconditions under which we prove that a sparse attention model can universally\napproximate any sequence-to-sequence function. Surprisingly, our results show\nthat sparse Transformers with only $O(n)$ connections per attention layer can\napproximate the same function class as the dense model with $n^2$ connections.\nLastly, we present experiments comparing different patterns/levels of sparsity\non standard NLP tasks.",
        "We tackle the image reassembly problem with wide space between the fragments,\nin such a way that the patterns and colors continuity is mostly unusable. The\nspacing emulates the erosion of which the archaeological fragments suffer. We\ncrop-square the fragments borders to compel our algorithm to learn from the\ncontent of the fragments. We also complicate the image reassembly by removing\nfragments and adding pieces from other sources. We use a two-step method to\nobtain the reassemblies: 1) a neural network predicts the positions of the\nfragments despite the gaps between them; 2) a graph that leads to the best\nreassemblies is made from these predictions. In this paper, we notably\ninvestigate the effect of branch-cut in the graph of reassemblies. We also\nprovide a comparison with the literature, solve complex images reassemblies,\nexplore at length the dataset, and propose a new metric that suits its\nspecificities.\n  Keywords: image reassembly, jigsaw puzzle, deep learning, graph, branch-cut,\ncultural heritage",
        "We study provably-efficient reinforcement learning in non-episodic factored\nMarkov decision processes (FMDPs). All previous regret minimization algorithms\nin this setting made the strong assumption that the factored structure of the\nFMDP is known to the learner in advance. In this paper, we provide the first\nalgorithm that learns the structure of the FMDP while minimizing the regret.\nOur algorithm is based on the optimism in face of uncertainty principle,\ncombined with a simple statistical method for structure learning, and can be\nimplemented efficiently given oracle-access to an FMDP planner. In addition, we\ngive a variant of our algorithm that remains efficient even when the oracle is\nlimited to non-factored actions, which is the case with almost all existing\napproximate planners. Finally, we also provide a novel lower bound for the\nknown structure case that matches the best known regret bound of Chen et al.\n(2020).",
        "Recent work in image processing suggests that operating on (overlapping)\npatches in an image may lead to state-of-the-art results. This has been\ndemonstrated for a variety of problems including denoising, inpainting,\ndeblurring, and super-resolution. The work reported in [1,2] takes an extra\nstep forward by showing that ordering these patches to form an approximate\nshortest path can be leveraged for better processing. The core idea is to apply\na simple filter on the resulting 1D smoothed signal obtained after the\npatch-permutation. This idea has been also explored in combination with a\nwavelet pyramid, leading eventually to a sophisticated and highly effective\nregularizer for inverse problems in imaging. In this work we further study the\npatch-permutation concept, and harness it to propose a new simple yet effective\nregularization for image restoration problems. Our approach builds on the\nclassic Maximum A'posteriori probability (MAP), with a penalty function\nconsisting of a regular log-likelihood term and a novel permutation-based\nregularization term. Using a plain 1D Laplacian, the proposed regularization\nforces robust smoothness (L1) on the permuted pixels. Since the permutation\noriginates from patch-ordering, we propose to accumulate the smoothness terms\nover all the patches' pixels. Furthermore, we take into account the found\ndistances between adjacent patches in the ordering, by weighting the Laplacian\noutcome. We demonstrate the proposed scheme on a diverse set of problems: (i)\nsevere Poisson image denoising, (ii) Gaussian image denoising, (iii) image\ndeblurring, and (iv) single image super-resolution. In all these cases, we use\nrecent methods that handle these problems as initialization to our scheme. This\nis followed by an L-BFGS optimization of the above-described penalty function,\nleading to state-of-the-art results, and especially so for highly ill-posed\ncases.",
        "The field of view (FOV) of convolutional neural networks is highly related to\nthe accuracy of inference. Dilated convolutions are known as an effective\nsolution to the problems which require large FOVs. However, for general-purpose\nhardware or dedicated hardware, it usually takes extra time to handle dilated\nconvolutions compared with standard convolutions. In this paper, we propose a\nnetwork module, Cascaded and Separable Structure of Dilated (CASSOD)\nConvolution, and a special hardware system to handle the CASSOD networks\nefficiently. A CASSOD-Net includes multiple cascaded $2 \\times 2$ dilated\nfilters, which can be used to replace the traditional $3 \\times 3$ dilated\nfilters without decreasing the accuracy of inference. Two example applications,\nface detection and image segmentation, are tested with dilated convolutions and\nthe proposed CASSOD modules. The new network for face detection achieves higher\naccuracy than the previous work with only 47% of filter weights in the dilated\nconvolution layers of the context module. Moreover, the proposed hardware\nsystem can accelerate the computations of dilated convolutions, and it is 2.78\ntimes faster than traditional hardware systems when the filter size is $3\n\\times 3$.",
        "Convolution neural network (CNN) has been widely used in Single Image Super\nResolution (SISR) so that SISR has been a great success recently. As the\nnetwork deepens, the learning ability of network becomes more and more\npowerful. However, most SISR methods based on CNN do not make full use of\nhierarchical feature and the learning ability of network. These features cannot\nbe extracted directly by subsequent layers, so the previous layer hierarchical\ninformation has little impact on the output and performance of subsequent\nlayers relatively poor. To solve above problem, a novel Multi-Level Feature\nFusion network (MLRN) is proposed, which can take full use of global\nintermediate features. We also introduce Feature Skip Fusion Block (FSFblock)\nas basic module. Each block can be extracted directly to the raw multiscale\nfeature and fusion multi-level feature, then learn feature spatial correlation.\nThe correlation among the features of the holistic approach leads to a\ncontinuous global memory of information mechanism. Extensive experiments on\npublic datasets show that the method proposed by MLRN can be implemented, which\nis favorable performance for the most advanced methods.",
        "When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.",
        "Extreme Learning Machines (ELMs) have become a popular tool in the field of\nArtificial Intelligence due to their very high training speed and\ngeneralization capabilities. Another advantage is that they have a single\nhyper-parameter that must be tuned up: the number of hidden nodes. Most\ntraditional approaches dictate that this parameter should be chosen smaller\nthan the number of available training samples in order to avoid over-fitting.\nIn fact, it has been proved that choosing the number of hidden nodes equal to\nthe number of training samples yields a perfect training classification with\nprobability 1 (w.r.t. the random parameter initialization). In this article we\nargue that in spite of this, in some cases it may be beneficial to choose a\nmuch larger number of hidden nodes, depending on certain properties of the\ndata. We explain why this happens and show some examples to illustrate how the\nmodel behaves. In addition, we present a pruning algorithm to cope with the\nadditional computational burden associated to the enlarged ELM. Experimental\nresults using electroencephalography (EEG) signals show an improvement in\nperformance with respect to traditional ELM approaches, while diminishing the\nextra computing time associated to the use of large architectures.",
        "Due to a variety of motions across different frames, it is highly challenging\nto learn an effective spatiotemporal representation for accurate video saliency\nprediction (VSP). To address this issue, we develop an effective spatiotemporal\nfeature alignment network tailored to VSP, mainly including two key\nsub-networks: a multi-scale deformable convolutional alignment network (MDAN)\nand a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network.\nThe MDAN learns to align the features of the neighboring frames to the\nreference one in a coarse-to-fine manner, which can well handle various\nmotions. Specifically, the MDAN owns a pyramidal feature hierarchy structure\nthat first leverages deformable convolution (Dconv) to align the\nlower-resolution features across frames, and then aggregates the aligned\nfeatures to align the higher-resolution features, progressively enhancing the\nfeatures from top to bottom. The output of MDAN is then fed into the\nBi-ConvLSTM for further enhancement, which captures the useful long-time\ntemporal information along forward and backward timing directions to\neffectively guide attention orientation shift prediction under complex scene\ntransformation. Finally, the enhanced features are decoded to generate the\npredicted saliency map. The proposed model is trained end-to-end without any\nintricate post processing. Extensive evaluations on four VSP benchmark datasets\ndemonstrate that the proposed method achieves favorable performance against\nstate-of-the-art methods. The source codes and all the results will be\nreleased.",
        "Graph-structured data exist in numerous applications in real life. As a\nstate-of-the-art graph neural network, the graph convolutional network (GCN)\nplays an important role in processing graph-structured data. However, a recent\nstudy reported that GCNs are also vulnerable to adversarial attacks, which\nmeans that GCN models may suffer malicious attacks with unnoticeable\nmodifications of the data. Among all the adversarial attacks on GCNs, there is\na special kind of attack method called the universal adversarial attack, which\ngenerates a perturbation that can be applied to any sample and causes GCN\nmodels to output incorrect results. Although universal adversarial attacks in\ncomputer vision have been extensively researched, there are few research works\non universal adversarial attacks on graph structured data. In this paper, we\npropose a targeted universal adversarial attack against GCNs. Our method\nemploys a few nodes as the attack nodes. The attack capability of the attack\nnodes is enhanced through a small number of fake nodes connected to them.\nDuring an attack, any victim node will be misclassified by the GCN as the\nattack node class as long as it is linked to them. The experiments on three\npopular datasets show that the average attack success rate of the proposed\nattack on any victim node in the graph reaches 83% when using only 3 attack\nnodes and 6 fake nodes. We hope that our work will make the community aware of\nthe threat of this type of attack and raise the attention given to its future\ndefense.",
        "Several works in computer vision have demonstrated the effectiveness of\nactive learning for adapting the recognition model when new unlabeled data\nbecomes available. Most of these works consider that labels obtained from the\nannotator are correct. However, in a practical scenario, as the quality of the\nlabels depends on the annotator, some of the labels might be wrong, which\nresults in degraded recognition performance. In this paper, we address the\nproblems of i) how a system can identify which of the queried labels are wrong\nand ii) how a multi-class active learning system can be adapted to minimize the\nnegative impact of label noise. Towards solving the problems, we propose a\nnoisy label filtering based learning approach where the inter-relationship\n(context) that is quite common in natural data is utilized to detect the wrong\nlabels. We construct a graphical representation of the unlabeled data to encode\nthese relationships and obtain new beliefs on the graph when noisy labels are\navailable. Comparing the new beliefs with the prior relational information, we\ngenerate a dissimilarity score to detect the incorrect labels and update the\nrecognition model with correct labels which result in better recognition\nperformance. This is demonstrated in three different applications: scene\nclassification, activity classification, and document classification.",
        "We present an approach to labeling short video clips with English verbs as\nevent descriptions. A key distinguishing aspect of this work is that it labels\nvideos with verbs that describe the spatiotemporal interaction between event\nparticipants, humans and objects interacting with each other, abstracting away\nall object-class information and fine-grained image characteristics, and\nrelying solely on the coarse-grained motion of the event participants. We apply\nour approach to a large set of 22 distinct verb classes and a corpus of 2,584\nvideos, yielding two surprising outcomes. First, a classification accuracy of\ngreater than 70% on a 1-out-of-22 labeling task and greater than 85% on a\nvariety of 1-out-of-10 subsets of this labeling task is independent of the\nchoice of which of two different time-series classifiers we employ. Second, we\nachieve this level of accuracy using a highly impoverished intermediate\nrepresentation consisting solely of the bounding boxes of one or two event\nparticipants as a function of time. This indicates that successful event\nrecognition depends more on the choice of appropriate features that\ncharacterize the linguistic invariants of the event classes than on the\nparticular classifier algorithms.",
        "A critical flaw of existing inverse reinforcement learning (IRL) methods is\ntheir inability to significantly outperform the demonstrator. This is because\nIRL typically seeks a reward function that makes the demonstrator appear\nnear-optimal, rather than inferring the underlying intentions of the\ndemonstrator that may have been poorly executed in practice. In this paper, we\nintroduce a novel reward-learning-from-observation algorithm, Trajectory-ranked\nReward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately)\nranked demonstrations in order to infer high-quality reward functions from a\nset of potentially poor demonstrations. When combined with deep reinforcement\nlearning, T-REX outperforms state-of-the-art imitation learning and IRL methods\non multiple Atari and MuJoCo benchmark tasks and achieves performance that is\noften more than twice the performance of the best demonstration. We also\ndemonstrate that T-REX is robust to ranking noise and can accurately\nextrapolate intention by simply watching a learner noisily improve at a task\nover time.",
        "Locating discriminative parts plays a key role in fine-grained visual\nclassification due to the high similarities between different objects. Recent\nworks based on convolutional neural networks utilize the feature maps taken\nfrom the last convolutional layer to mine discriminative regions. However, the\nlast convolutional layer tends to focus on the whole object due to the large\nreceptive field, which leads to a reduced ability to spot the differences. To\naddress this issue, we propose a novel Granularity-Aware Convolutional Neural\nNetwork (GA-CNN) that progressively explores discriminative features.\nSpecifically, GA-CNN utilizes the differences of the receptive fields at\ndifferent layers to learn multi-granularity features, and it exploits larger\ngranularity information based on the smaller granularity information found at\nthe previous stages. To further boost the performance, we introduce an\nobject-attentive module that can effectively localize the object given a raw\nimage. GA-CNN does not need bounding boxes/part annotations and can be trained\nend-to-end. Extensive experimental results show that our approach achieves\nstate-of-the-art performances on three benchmark datasets.",
        "We consider the problem of representation learning for temporal interaction\ngraphs where a network of entities with complex interactions over an extended\nperiod of time is modeled as a graph with a rich set of node and edge\nattributes. In particular, an edge between a node-pair within the graph\ncorresponds to a multi-dimensional time-series. To fully capture and model the\ndynamics of the network, we propose GTEA, a framework of representation\nlearning for temporal interaction graphs with per-edge time-based aggregation.\nUnder GTEA, a Graph Neural Network (GNN) is integrated with a state-of-the-art\nsequence model, such as LSTM, Transformer and their time-aware variants. The\nsequence model generates edge embeddings to encode temporal interaction\npatterns between each pair of nodes, while the GNN-based backbone learns the\ntopological dependencies and relationships among different nodes. GTEA also\nincorporates a sparsity-inducing self-attention mechanism to distinguish and\nfocus on the more important neighbors of each node during the aggregation\nprocess. By capturing temporal interactive dynamics together with\nmulti-dimensional node and edge attributes in a network, GTEA can learn\nfine-grained representations for a temporal interaction graph to enable or\nfacilitate other downstream data analytic tasks. Experimental results show that\nGTEA outperforms state-of-the-art schemes including GraphSAGE, APPNP, and TGAT\nby delivering higher accuracy (100.00%, 98.51%, 98.05% ,79.90%) and macro-F1\nscore (100.00%, 98.51%, 96.68% ,79.90%) over four large-scale real-world\ndatasets for binary/ multi-class node classification.",
        "Co-creative Procedural Content Generation via Machine Learning (PCGML) refers\nto systems where a PCGML agent and a human work together to produce output\ncontent. One of the limitations of co-creative PCGML is that it requires\nco-creative training data for a PCGML agent to learn to interact with humans.\nHowever, acquiring this data is a difficult and time-consuming process. In this\nwork, we propose approximating human-AI interaction data and employing transfer\nlearning to adapt learned co-creative knowledge from one game to a different\ngame. We explore this approach for co-creative Zelda dungeon room generation.",
        "Reinforcement learning was carried out in a simulated environment to learn\ncontinuous velocity control over multiple motor axes. This was then applied to\na real-world optical tweezers experiment with the objective of moving a\nlaser-trapped microsphere to a target location whilst avoiding collisions with\nother free-moving microspheres. The concept of training a neural network in a\nvirtual environment has significant potential in the application of machine\nlearning for experimental optimization and control, as the neural network can\ndiscover optimal methods for problem solving without the risk of damage to\nequipment, and at a speed not limited by movement in the physical environment.\nAs the neural network treats both virtual and physical environments\nequivalently, we show that the network can also be applied to an augmented\nenvironment, where a virtual environment is combined with the physical\nenvironment. This technique may have the potential to unlock capabilities\nassociated with mixed and augmented reality, such as enforcing safety limits\nfor machine motion or as a method of inputting observations from additional\nsensors.",
        "Artificial Intelligence (AI) is becoming a critical component in the defense\nindustry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT\nsought to vet the feasibility of AI algorithms capable of piloting an F-16 in\nsimulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM)\napproach combines a hierarchical architecture with maximum-entropy\nreinforcement learning (RL), integrates expert knowledge through reward\nshaping, and supports modularity of policies. This approach achieved a $2^{nd}$\nplace finish in the final ADT event (among eight total competitors) and\ndefeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course\nin match play.",
        "We introduce a new spectral method for image segmentation that incorporates\nlong range relationships for global appearance modeling. The approach combines\ntwo different graphs, one is a sparse graph that captures spatial relationships\nbetween nearby pixels and another is a dense graph that captures pairwise\nsimilarity between all pairs of pixels. We extend the spectral method for\nNormalized Cuts to this setting by combining the transition matrices of Markov\nchains associated with each graph. We also derive an efficient method that uses\nimportance sampling for sparsifying the dense graph of appearance\nrelationships. This leads to a practical algorithm for segmenting\nhigh-resolution images. The resulting method can segment challenging images\nwithout any filtering or pre-processing.",
        "Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit, which limit the performance of both tasks. In this paper, we propose a\nnetwork injected with contextual information (CI-Net) to solve the problem.\nSpecifically, we introduce self-attention block in the encoder to generate\nattention map. With supervision from the ideal attention map created by\nsemantic label, the network is embedded with contextual information so that it\ncould understand scene better and utilize correlated features to make accurate\nprediction. Besides, a feature sharing module is constructed to make the\ntask-specific features deeply fused and a consistency loss is devised to make\nthe features mutually guided. We evaluate the proposed CI-Net on the\nNYU-Depth-v2 and SUN-RGBD datasets. The experimental results validate that our\nproposed CI-Net could effectively improve the accuracy of semantic segmentation\nand depth estimation.",
        "In this work, we propose to employ information-geometric tools to optimize a\ngraph neural network architecture such as the graph convolutional networks.\nMore specifically, we develop optimization algorithms for the graph-based\nsemi-supervised learning by employing the natural gradient information in the\noptimization process. This allows us to efficiently exploit the geometry of the\nunderlying statistical model or parameter space for optimization and inference.\nTo the best of our knowledge, this is the first work that has utilized the\nnatural gradient for the optimization of graph neural networks that can be\nextended to other semi-supervised problems. Efficient computations algorithms\nare developed and extensive numerical studies are conducted to demonstrate the\nsuperior performance of our algorithms over existing algorithms such as ADAM\nand SGD.",
        "Deep neural networks (DNNs) have demonstrated their great potential in recent\nyears, exceeding the per-formance of human experts in a wide range of\napplications. Due to their large sizes, however, compressiontechniques such as\nweight quantization and pruning are usually applied before they can be\naccommodated onthe edge. It is generally believed that quantization leads to\nperformance degradation, and plenty of existingworks have explored quantization\nstrategies aiming at minimum accuracy loss. In this paper, we argue\nthatquantization, which essentially imposes regularization on weight\nrepresentations, can sometimes help toimprove accuracy. We conduct\ncomprehensive experiments on three widely used applications: fully con-nected\nnetwork (FCN) for biomedical image segmentation, convolutional neural network\n(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)\nfor automatic speech recognition, and experi-mental results show that\nquantization can improve the accuracy by 1%, 1.95%, 4.23% on the three\napplicationsrespectively with 3.5x-6.4x memory reduction.",
        "The framework of variational autoencoders allows us to efficiently learn deep\nlatent-variable models, such that the model's marginal distribution over\nobserved variables fits the data. Often, we're interested in going a step\nfurther, and want to approximate the true joint distribution over observed and\nlatent variables, including the true prior and posterior distributions over\nlatent variables. This is known to be generally impossible due to\nunidentifiability of the model. We address this issue by showing that for a\nbroad family of deep latent-variable models, identification of the true joint\ndistribution over observed and latent variables is actually possible up to very\nsimple transformations, thus achieving a principled and powerful form of\ndisentanglement. Our result requires a factorized prior distribution over the\nlatent variables that is conditioned on an additionally observed variable, such\nas a class label or almost any other observation. We build on recent\ndevelopments in nonlinear ICA, which we extend to the case with noisy,\nundercomplete or discrete observations, integrated in a maximum likelihood\nframework. The result also trivially contains identifiable flow-based\ngenerative models as a special case.",
        "We propose a novel training procedure for improving the performance of\ngenerative adversarial networks (GANs), especially to bidirectional GANs.\nFirst, we enforce that the empirical distribution of the inverse inference\nnetwork matches the prior distribution, which favors the generator network\nreproducibility on the seen samples. Second, we have found that the marginal\nlog-likelihood of the samples shows a severe overrepresentation of a certain\ntype of samples. To address this issue, we propose to train the bidirectional\nGAN using a non-uniform sampling for the mini-batch selection, resulting in\nimproved quality and variety in generated samples measured quantitatively and\nby visual inspection. We illustrate our new procedure with the well-known\nCIFAR10, Fashion MNIST and CelebA datasets.",
        "Object detection is widely studied in computer vision filed. In recent years,\ncertain representative deep learning based detection methods along with solid\nbenchmarks are proposed, which boosts the development of related researchs.\nHowever, existing detection methods still suffer from undesirable performance\nunder challenges such as camouflage, blur, inter-class similarity, intra-class\nvariance and complex environment. To address this issue, we propose LGA-RCNN\nwhich utilizes a loss-guided attention (LGA) module to highlight representative\nregion of objects. Then, those highlighted local information are fused with\nglobal information for precise classification and localization.",
        "We present a method for improving human design of chairs. The goal of the\nmethod is generating enormous chair candidates in order to facilitate human\ndesigner by creating sketches and 3d models accordingly based on the generated\nchair design. It consists of an image synthesis module, which learns the\nunderlying distribution of training dataset, a super-resolution module, which\nimprove quality of generated image and human involvements. Finally, we manually\npick one of the generated candidates to create a real life chair for\nillustration.",
        "Recently, various auxiliary tasks have been proposed to accelerate\nrepresentation learning and improve sample efficiency in deep reinforcement\nlearning (RL). However, existing auxiliary tasks do not take the\ncharacteristics of RL problems into consideration and are unsupervised. By\nleveraging returns, the most important feedback signals in RL, we propose a\nnovel auxiliary task that forces the learnt representations to discriminate\nstate-action pairs with different returns. Our auxiliary loss is theoretically\njustified to learn representations that capture the structure of a new form of\nstate-action abstraction, under which state-action pairs with similar return\ndistributions are aggregated together. In low data regime, our algorithm\noutperforms strong baselines on complex tasks in Atari games and DeepMind\nControl suite, and achieves even better performance when combined with existing\nauxiliary tasks.",
        "Attention mechanism has demonstrated great potential in fine-grained visual\nrecognition tasks. In this paper, we present a counterfactual attention\nlearning method to learn more effective attention based on causal inference.\nUnlike most existing methods that learn visual attention based on conventional\nlikelihood, we propose to learn the attention with counterfactual causality,\nwhich provides a tool to measure the attention quality and a powerful\nsupervisory signal to guide the learning process. Specifically, we analyze the\neffect of the learned visual attention on network prediction through\ncounterfactual intervention and maximize the effect to encourage the network to\nlearn more useful attention for fine-grained image recognition. Empirically, we\nevaluate our method on a wide range of fine-grained recognition tasks where\nattention plays a crucial role, including fine-grained image categorization,\nperson re-identification, and vehicle re-identification. The consistent\nimprovement on all benchmarks demonstrates the effectiveness of our method.\nCode is available at https://github.com/raoyongming/CAL",
        "In this paper we show how using satellite images can improve the accuracy of\nhousing price estimation models. Using Los Angeles County's property assessment\ndataset, by transferring learning from an Inception-v3 model pretrained on\nImageNet, we could achieve an improvement of ~10% in R-squared score compared\nto two baseline models that only use non-image features of the house.",
        "Deep Neural Network (DNN) models have vulnerabilities related to security\nconcerns, with attackers usually employing complex hacking techniques to expose\ntheir structures. Data poisoning-enabled perturbation attacks are complex\nadversarial ones that inject false data into models. They negatively impact the\nlearning process, with no benefit to deeper networks, as they degrade a model's\naccuracy and convergence rates. In this paper, we propose an\nattack-agnostic-based defense method for mitigating their influence. In it, a\nDefensive Feature Layer (DFL) is integrated with a well-known DNN architecture\nwhich assists in neutralizing the effects of illegitimate perturbation samples\nin the feature space. To boost the robustness and trustworthiness of this\nmethod for correctly classifying attacked input samples, we regularize the\nhidden space of a trained model with a discriminative loss function called\nPolarized Contrastive Loss (PCL). It improves discrimination among samples in\ndifferent classes and maintains the resemblance of those in the same class.\nAlso, we integrate a DFL and PCL in a compact model for defending against data\npoisoning attacks. This method is trained and tested using the CIFAR-10 and\nMNIST datasets with data poisoning-enabled perturbation attacks, with the\nexperimental results revealing its excellent performance compared with those of\nrecent peer techniques.",
        "In this paper, we investigate the problem of learning disentangled\nrepresentations. Given a pair of images sharing some attributes, we aim to\ncreate a low-dimensional representation which is split into two parts: a shared\nrepresentation that captures the common information between the images and an\nexclusive representation that contains the specific information of each image.\nTo address this issue, we propose a model based on mutual information\nestimation without relying on image reconstruction or image generation. Mutual\ninformation maximization is performed to capture the attributes of data in the\nshared and exclusive representations while we minimize the mutual information\nbetween the shared and exclusive representation to enforce representation\ndisentanglement. We show that these representations are useful to perform\ndownstream tasks such as image classification and image retrieval based on the\nshared or exclusive component. Moreover, classification results show that our\nmodel outperforms the state-of-the-art model based on VAE/GAN approaches in\nrepresentation disentanglement.",
        "Although recent works have developed methods that can generate estimations\n(or imputations) of the missing entries in a dataset to facilitate downstream\nanalysis, most depend on assumptions that may not align with real-world\napplications and could suffer from poor performance in subsequent tasks. This\nis particularly true if the data have large missingness rates or a small\npopulation. More importantly, the imputation error could be propagated into the\nprediction step that follows, causing the gradients used to train the\nprediction models to be biased. Consequently, in this work, we introduce the\nimportance guided stochastic gradient descent (IGSGD) method to train\nmultilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly\nperform inference from inputs containing missing values without imputation.\nSpecifically, we employ reinforcement learning (RL) to adjust the gradients\nused to train the models via back-propagation. This not only reduces bias but\nallows the model to exploit the underlying information behind missingness\npatterns. We test the proposed approach on real-world time-series (i.e.,\nMIMIC-III), tabular data obtained from an eye clinic, and a standard dataset\n(i.e., MNIST), where our imputation-free predictions outperform the traditional\ntwo-step imputation-based predictions using state-of-the-art imputation\nmethods.",
        "We address the problem of learning binary decision trees that partition data\nfor some downstream task. We propose to learn discrete parameters (i.e., for\ntree traversals and node pruning) and continuous parameters (i.e., for tree\nsplit functions and prediction functions) simultaneously using argmin\ndifferentiation. We do so by sparsely relaxing a mixed-integer program for the\ndiscrete parameters, to allow gradients to pass through the program to\ncontinuous parameters. We derive customized algorithms to efficiently compute\nthe forward and backward passes. This means that our tree learning procedure\ncan be used as an (implicit) layer in arbitrary deep networks, and can be\noptimized with arbitrary loss functions. We demonstrate that our approach\nproduces binary trees that are competitive with existing single tree and\nensemble approaches, in both supervised and unsupervised settings. Further,\napart from greedy approaches (which do not have competitive accuracies), our\nmethod is faster to train than all other tree-learning baselines we compare\nwith. The code for reproducing the results is available at\nhttps://github.com/vzantedeschi/LatentTrees.",
        "Complex computer simulators are increasingly used across fields of science as\ngenerative models tying parameters of an underlying theory to experimental\nobservations. Inference in this setup is often difficult, as simulators rarely\nadmit a tractable density or likelihood function. We introduce Adversarial\nVariational Optimization (AVO), a likelihood-free inference algorithm for\nfitting a non-differentiable generative model incorporating ideas from\ngenerative adversarial networks, variational optimization and empirical Bayes.\nWe adapt the training procedure of generative adversarial networks by replacing\nthe differentiable generative network with a domain-specific simulator. We\nsolve the resulting non-differentiable minimax problem by minimizing\nvariational upper bounds of the two adversarial objectives. Effectively, the\nprocedure results in learning a proposal distribution over simulator\nparameters, such that the JS divergence between the marginal distribution of\nthe synthetic data and the empirical distribution of observed data is\nminimized. We evaluate and compare the method with simulators producing both\ndiscrete and continuous data.",
        "With the increasing popularity of augmented and virtual reality, retailers\nare now focusing more towards customer satisfaction to increase the amount of\nsales. Although augmented reality is not a new concept but it has gained much\nneeded attention over the past few years. Our present work is targeted towards\nthis direction which may be used to enhance user experience in various virtual\nand augmented reality based applications. We propose a model to change skin\ntone of a person. Given any input image of a person or a group of persons with\nsome value indicating the desired change of skin color towards fairness or\ndarkness, this method can change the skin tone of the persons in the image.\nThis is an unsupervised method and also unconstrained in terms of pose,\nillumination, number of persons in the image etc. The goal of this work is to\nreduce the time and effort which is generally required for changing the skin\ntone using existing applications (e.g., Photoshop) by professionals or novice.\nTo establish the efficacy of this method we have compared our result with that\nof some popular photo editor and also with the result of some existing\nbenchmark method related to human attribute manipulation. Rigorous experiments\non different datasets show the effectiveness of this method in terms of\nsynthesizing perceptually convincing outputs.",
        "In many real-world problems, complex dependencies are present both among\nsamples and among features. The Kronecker sum or the Cartesian product of two\ngraphs, each modeling dependencies across features and across samples, has been\nused as an inverse covariance matrix for a matrix-variate Gaussian\ndistribution, as an alternative to a Kronecker-product inverse covariance\nmatrix, due to its more intuitive sparse structure. However, the existing\nmethods for sparse Kronecker-sum inverse covariance estimation are limited in\nthat they do not scale to more than a few hundred features and samples and that\nthe unidentifiable parameters pose challenges in estimation. In this paper, we\nintroduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse\ncovariance estimation, based on Newton's method combined with\neigendecomposition of the two graphs for exploiting the structure of Kronecker\nsum. EiGLasso further reduces computation time by approximating the Hessian\nbased on the eigendecomposition of the sample and feature graphs. EiGLasso\nachieves quadratic convergence with the exact Hessian and linear convergence\nwith the approximate Hessian. We describe a simple new approach to estimating\nthe unidentifiable parameters that generalizes the existing methods. On\nsimulated and real-world data, we demonstrate that EiGLasso achieves two to\nthree orders-of-magnitude speed-up compared to the existing methods.",
        "Recently there has been an increasing trend to use deep learning frameworks\nfor both 2D consumer images and for 3D medical images. However, there has been\nlittle effort to use deep frameworks for volumetric vascular segmentation. We\nwanted to address this by providing a freely available dataset of 12 annotated\ntwo-photon vasculature microscopy stacks. We demonstrated the use of deep\nlearning framework consisting both 2D and 3D convolutional filters (ConvNet).\nOur hybrid 2D-3D architecture produced promising segmentation result. We\nderived the architectures from Lee et al. who used the ZNN framework initially\ndesigned for electron microscope image segmentation. We hope that by sharing\nour volumetric vasculature datasets, we will inspire other researchers to\nexperiment with vasculature dataset and improve the used network architectures.",
        "Knowing when a classifier's prediction can be trusted is useful in many\napplications and critical for safely using AI. While the bulk of the effort in\nmachine learning research has been towards improving classifier performance,\nunderstanding when a classifier's predictions should and should not be trusted\nhas received far less attention. The standard approach is to use the\nclassifier's discriminant or confidence score; however, we show there exists an\nalternative that is more effective in many situations. We propose a new score,\ncalled the trust score, which measures the agreement between the classifier and\na modified nearest-neighbor classifier on the testing example. We show\nempirically that high (low) trust scores produce surprisingly high precision at\nidentifying correctly (incorrectly) classified examples, consistently\noutperforming the classifier's confidence score as well as many other\nbaselines. Further, under some mild distributional assumptions, we show that if\nthe trust score for an example is high (low), the classifier will likely agree\n(disagree) with the Bayes-optimal classifier. Our guarantees consist of\nnon-asymptotic rates of statistical consistency under various nonparametric\nsettings and build on recent developments in topological data analysis.",
        "Data-driven discovery of partial differential equations (PDEs) has attracted\nincreasing attention in recent years. Although significant progress has been\nmade, certain unresolved issues remain. For example, for PDEs with high-order\nderivatives, the performance of existing methods is unsatisfactory, especially\nwhen the data are sparse and noisy. It is also difficult to discover\nheterogeneous parametric PDEs where heterogeneous parameters are embedded in\nthe partial differential operators. In this work, a new framework combining\ndeep-learning and integral form is proposed to handle the above-mentioned\nproblems simultaneously, and improve the accuracy and stability of PDE\ndiscovery. In the framework, a deep neural network is firstly trained with\nobservation data to generate meta-data and calculate derivatives. Then, a\nunified integral form is defined, and the genetic algorithm is employed to\ndiscover the best structure. Finally, the value of parameters is calculated,\nand whether the parameters are constants or variables is identified. Numerical\nexperiments proved that our proposed algorithm is more robust to noise and more\naccurate compared with existing methods due to the utilization of integral\nform. Our proposed algorithm is also able to discover PDEs with high-order\nderivatives or heterogeneous parameters accurately with sparse and noisy data.",
        "In this paper, a robust vehicle local position estimation with the help of\nsingle camera sensor and GPS is presented. A modified Inverse Perspective\nMapping, illuminant Invariant techniques and object detection based approach is\nused to localize the vehicle in the road. Vehicles current lane, its position\nfrom road boundary and other cars are used to define its local position. For\nthis purpose Lane markings are detected using a Laplacian edge feature, robust\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\ncolor space and illuminant invariant techniques. Lanes are assumed to be as\nparabolic model and fitted using robust RANSAC. This method can reliably detect\nall lanes of the road, estimate lane departure angle and local position of\nvehicle relative to lanes, road boundary and other cars. Different type of\nobstacle like pedestrians, vehicles are detected using HOG feature based\ndeformable part model.",
        "Face morphing attacks aim at creating face images that are verifiable to be\nthe face of multiple identities, which can lead to building faulty identity\nlinks in operations like border checks. While creating a morphed face detector\n(MFD), training on all possible attack types is essential to achieve good\ndetection performance. Therefore, investigating new methods of creating\nmorphing attacks drives the generalizability of MADs. Creating morphing attacks\nwas performed on the image level, by landmark interpolation, or on the\nlatent-space level, by manipulating latent vectors in a generative adversarial\nnetwork. The earlier results in varying blending artifacts and the latter\nresults in synthetic-like striping artifacts. This work presents the novel\nmorphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using\na GAN-based generation, as well as, eliminate the manipulation in the latent\nspace, resulting in visibly realistic morphed images compared to previous\nworks. The generated ReGenMorph appearance is compared to recent morphing\napproaches and evaluated for face recognition vulnerability and attack\ndetectability, whether as known or unknown attacks.",
        "A key aspect for the forklifts is the state-of-health (SoH) assessment to\nensure the safety and the reliability of uninterrupted power source.\nForecasting the battery SoH well is imperative to enable preventive maintenance\nand hence to reduce the costs. This paper demonstrates the capabilities of\ngradient boosting regression for predicting the SoH timeseries under\ncircumstances when there is little prior information available about the\nbatteries. We compared the gradient boosting method with light gradient\nboosting, extra trees, extreme gradient boosting, random forests, long\nshort-term memory networks and with combined convolutional neural network and\nlong short-term memory networks methods. We used multiple predictors and lagged\ntarget signal decomposition results as additional predictors and compared the\nyielded prediction results with different sets of predictors for each method.\nFor this work, we are in possession of a unique data set of 45 lithium-ion\nbattery packs with large variation in the data. The best model that we derived\nwas validated by a novel walk-forward algorithm that also calculates point-wise\nconfidence intervals for the predictions; we yielded reasonable predictions and\nconfidence intervals for the predictions. Furthermore, we verified this model\nagainst five other lithium-ion battery packs; the best model generalised to\ngreater extent to this set of battery packs. The results about the final model\nsuggest that we were able to enhance the results in respect to previously\ndeveloped models. Moreover, we further validated the model for extracting cycle\ncounts presented in our previous work with data from new forklifts; their\nbattery packs completed around 3000 cycles in a 10-year service period, which\ncorresponds to the cycle life for commercial Nickel-Cobalt-Manganese (NMC)\ncells.",
        "This paper introduces a new encoder-decoder architecture that is trained to\nreconstruct images by disentangling the salient information of the image and\nthe values of attributes directly in the latent space. As a result, after\ntraining, our model can generate different realistic versions of an input image\nby varying the attribute values. By using continuous attribute values, we can\nchoose how much a specific attribute is perceivable in the generated image.\nThis property could allow for applications where users can modify an image\nusing sliding knobs, like faders on a mixing console, to change the facial\nexpression of a portrait, or to update the color of some objects. Compared to\nthe state-of-the-art which mostly relies on training adversarial networks in\npixel space by altering attribute values at train time, our approach results in\nmuch simpler training schemes and nicely scales to multiple attributes. We\npresent evidence that our model can significantly change the perceived value of\nthe attributes while preserving the naturalness of images.",
        "This paper proposes an approach for rapid bounding box annotation for object\ndetection datasets. The procedure consists of two stages: The first step is to\nannotate a part of the dataset manually, and the second step proposes\nannotations for the remaining samples using a model trained with the first\nstage annotations. We experimentally study which first/second stage split\nminimizes to total workload. In addition, we introduce a new fully labeled\nobject detection dataset collected from indoor scenes. Compared to other indoor\ndatasets, our collection has more class categories, different backgrounds,\nlighting conditions, occlusion and high intra-class differences. We train deep\nlearning based object detectors with a number of state-of-the-art models and\ncompare them in terms of speed and accuracy. The fully annotated dataset is\nreleased freely available for the research community.",
        "In this paper, we propose a novel object proposal generation scheme by\nformulating a graph-based salient edge classification framework that utilizes\nthe edge context. In the proposed method, we construct a Bayesian probabilistic\nedge map to assign a saliency value to the edgelets by exploiting low level\nedge features. A Conditional Random Field is then learned to effectively\ncombine these features for edge classification with object/non-object label. We\npropose an objectness score for the generated windows by analyzing the salient\nedge density inside the bounding box. Extensive experiments on PASCAL VOC 2007\ndataset demonstrate that the proposed method gives competitive performance\nagainst 10 popular generic object detection techniques while using fewer number\nof proposals.",
        "In this work, we propose a novel approach that predicts the relationships\nbetween various entities in an image in a weakly supervised manner by relying\non image captions and object bounding box annotations as the sole source of\nsupervision. Our proposed approach uses a top-down attention mechanism to align\nentities in captions to objects in the image, and then leverage the syntactic\nstructure of the captions to align the relations. We use these alignments to\ntrain a relation classification network, thereby obtaining both grounded\ncaptions and dense relationships. We demonstrate the effectiveness of our model\non the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of\n25% on the relationships present in the image. We also show that the model\nsuccessfully predicts relations that are not present in the corresponding\ncaptions.",
        "Finding overcomplete latent representations of data has applications in data\nanalysis, signal processing, machine learning, theoretical neuroscience and\nmany other fields. In an overcomplete representation, the number of latent\nfeatures exceeds the data dimensionality, which is useful when the data is\nundersampled by the measurements (compressed sensing, information bottlenecks\nin neural systems) or composed from multiple complete sets of linear features,\neach spanning the data space. Independent Components Analysis (ICA) is a linear\ntechnique for learning sparse latent representations, which typically has a\nlower computational cost than sparse coding, its nonlinear, recurrent\ncounterpart. While well suited for finding complete representations, we show\nthat overcompleteness poses a challenge to existing ICA algorithms.\nSpecifically, the coherence control in existing ICA algorithms, necessary to\nprevent the formation of duplicate dictionary features, is ill-suited in the\novercomplete case. We show that in this case several existing ICA algorithms\nhave undesirable global minima that maximize coherence. Further, by comparing\nICA algorithms on synthetic data and natural images to the computationally more\nexpensive sparse coding solution, we show that the coherence control biases the\nexploration of the data manifold, sometimes yielding suboptimal solutions. We\nprovide a theoretical explanation of these failures and, based on the theory,\npropose improved overcomplete ICA algorithms. All told, this study contributes\nnew insights into and methods for coherence control for linear ICA, some of\nwhich are applicable to many other, potentially nonlinear, unsupervised\nlearning methods.",
        "The reasonable employment of RGB and depth data show great significance in\npromoting the development of computer vision tasks and robot-environment\ninteraction. However, there are different advantages and disadvantages in the\nearly and late fusion of the two types of data. Besides, due to the diversity\nof object information, using a single type of data in a specific scenario tends\nto result in semantic misleading. Based on the above considerations, we propose\nan adaptively-cooperative fusion network (ACFNet) with ResinRes structure for\nsalient object detection. This structure is designed to flexibly utilize the\nadvantages of feature fusion in early and late stages. Secondly, an\nadaptively-cooperative semantic guidance (ACG) scheme is designed to suppress\ninaccurate features in the guidance phase. Further, we proposed a type-based\nattention module (TAM) to optimize the network and enhance the multi-scale\nperception of different objects. For different objects, the features generated\nby different types of convolution are enhanced or suppressed by the gated\nmechanism for segmentation optimization. ACG and TAM optimize the transfer of\nfeature streams according to their data attributes and convolution attributes,\nrespectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate\nthat the proposed network performs favorably against 18 state-of-the-art\nalgorithms.",
        "Generative adversarial networks have achieved remarkable performance on\nvarious tasks but suffer from training instability. Despite many training\nstrategies proposed to improve training stability, this issue remains as a\nchallenge. In this paper, we investigate the training instability from the\nperspective of adversarial samples and reveal that adversarial training on fake\nsamples is implemented in vanilla GANs, but adversarial training on real\nsamples has long been overlooked. Consequently, the discriminator is extremely\nvulnerable to adversarial perturbation and the gradient given by the\ndiscriminator contains non-informative adversarial noises, which hinders the\ngenerator from catching the pattern of real samples. Here, we develop\nadversarial symmetric GANs (AS-GANs) that incorporate adversarial training of\nthe discriminator on real samples into vanilla GANs, making adversarial\ntraining symmetrical. The discriminator is therefore more robust and provides\nmore informative gradient with less adversarial noise, thereby stabilizing\ntraining and accelerating convergence. The effectiveness of the AS-GANs is\nverified on image generation on CIFAR-10 , CelebA, and LSUN with varied network\narchitectures. Not only the training is more stabilized, but the FID scores of\ngenerated samples are consistently improved by a large margin compared to the\nbaseline. The bridging of adversarial samples and adversarial networks provides\na new approach to further develop adversarial networks.",
        "Despite the significant progress of deep reinforcement learning (RL) in\nsolving sequential decision making problems, RL agents often overfit to\ntraining environments and struggle to adapt to new, unseen environments. This\nprevents robust applications of RL in real world situations, where system\ndynamics may deviate wildly from the training settings. In this work, our\nprimary contribution is to propose an information theoretic regularization\nobjective and an annealing-based optimization method to achieve better\ngeneralization ability in RL agents. We demonstrate the extreme generalization\nbenefits of our approach in different domains ranging from maze navigation to\nrobotic tasks; for the first time, we show that agents can generalize to test\nparameters more than 10 standard deviations away from the training parameter\ndistribution. This work provides a principled way to improve generalization in\nRL by gradually removing information that is redundant for task-solving; it\nopens doors for the systematic study of generalization from training to\nextremely different testing settings, focusing on the established connections\nbetween information theory and machine learning.",
        "We propose a novel neural network architecture based on dual quaternions\nwhich allow for a compact representation of informations with a main focus on\ndescribing rigid body movements. To cover the dynamic behavior inherent to\nrigid body movements, we propose recurrent architectures in the neural network.\nTo further model the interactions between individual rigid bodies as well as\nexternal inputs efficiently, we incorporate a novel attention mechanism\nemploying dual quaternion algebra. The introduced architecture is trainable by\nmeans of gradient based algorithms. We apply our approach to a parcel\nprediction problem where a rigid body with an initial position, orientation,\nvelocity and angular velocity moves through a fixed simulation environment\nwhich exhibits rich interactions between the parcel and the boundaries.",
        "We present a data-driven algorithm and mathematical model for anomaly\nalarming at directional drilling. The algorithm is based on machine learning.\nIt compares the real-time drilling telemetry with one corresponding to past\naccidents and analyses the level of similarity. The model performs a\ntime-series comparison using aggregated statistics and Gradient Boosting\nclassification. It is trained on historical data containing the drilling\ntelemetry of $80$ wells drilled within $19$ oilfields. The model can detect an\nanomaly and identify its type by comparing the real-time measurements while\ndrilling with the ones from the database of past accidents. Validation tests\nshow that our algorithm identifies half of the anomalies with about $0.53$\nfalse alarms per day on average. The model performance ensures sufficient time\nand cost savings as it enables partial prevention of the failures and accidents\nat the well construction.",
        "We present a novel variational generative adversarial network (VGAN) based on\nWasserstein loss to learn a latent representation from a face image that is\ninvariant to identity but preserves head-pose information. This facilitates\nsynthesis of a realistic face image with the same head pose as a given input\nimage, but with a different identity. One application of this network is in\nprivacy-sensitive scenarios; after identity replacement in an image, utility,\nsuch as head pose, can still be recovered. Extensive experimental validation on\nsynthetic and real human-face image datasets performed under 3 threat scenarios\nconfirms the ability of the proposed network to preserve head pose of the input\nimage, mask the input identity, and synthesize a good-quality realistic face\nimage of a desired identity. We also show that our network can be used to\nperform pose-preserving identity morphing and identity-preserving pose\nmorphing. The proposed method improves over a recent state-of-the-art method in\nterms of quantitative metrics as well as synthesized image quality.",
        "In the real world, out-of-distribution samples, noise and distortions exist\nin test data. Existing deep networks developed for point cloud data analysis\nare prone to overfitting and a partial change in test data leads to\nunpredictable behaviour of the networks. In this paper, we propose a smart yet\nsimple deep network for analysis of 3D models using `orderly disorder' theory.\nOrderly disorder is a way of describing the complex structure of disorders\nwithin complex systems. Our method extracts the deep patterns inside a 3D\nobject via creating a dynamic link to seek the most stable patterns and at\nonce, throws away the unstable ones. Patterns are more robust to changes in\ndata distribution, especially those that appear in the top layers. Features are\nextracted via an innovative cloning decomposition technique and then linked to\neach other to form stable complex patterns. Our model alleviates the\nvanishing-gradient problem, strengthens dynamic link propagation and\nsubstantially reduces the number of parameters. Extensive experiments on\nchallenging benchmark datasets verify the superiority of our light network on\nthe segmentation and classification tasks, especially in the presence of noise\nwherein our network's performance drops less than 10% while the\nstate-of-the-art networks fail to work.",
        "This paper presents a Bayesian image segmentation model based on Potts prior\nand loopy belief propagation. The proposed Bayesian model involves several\nterms, including the pairwise interactions of Potts models, and the average\nvectors and covariant matrices of Gauss distributions in color image modeling.\nThese terms are often referred to as hyperparameters in statistical machine\nlearning theory. In order to determine these hyperparameters, we propose a new\nscheme for hyperparameter estimation based on conditional maximization of\nentropy in the Potts prior. The algorithm is given based on loopy belief\npropagation. In addition, we compare our conditional maximum entropy framework\nwith the conventional maximum likelihood framework, and also clarify how the\nfirst order phase transitions in LBP's for Potts models influence our\nhyperparameter estimation procedures.",
        "A concept of using Neural Ordinary Differential Equations(NODE) for Transfer\nLearning has been introduced. In this paper we use the EfficientNets to explore\ntransfer learning on CIFAR-10 dataset. We use NODE for fine-tuning our model.\nUsing NODE for fine tuning provides more stability during training and\nvalidation.These continuous depth blocks can also have a trade off between\nnumerical precision and speed .Using Neural ODEs for transfer learning has\nresulted in much stable convergence of the loss function.",
        "Curriculum learning can improve neural network training by guiding the\noptimization to desirable optima. We propose a novel curriculum learning\napproach for image classification that adapts the loss function by changing the\nlabel representation. The idea is to use a probability distribution over\nclasses as target label, where the class probabilities reflect the similarity\nto the true class. Gradually, this label representation is shifted towards the\nstandard one-hot-encoding. That is, in the beginning minor mistakes are\ncorrected less than large mistakes, resembling a teaching process in which\nbroad concepts are explained first before subtle differences are taught.\n  The class similarity can be based on prior knowledge. For the special case of\nthe labels being natural words, we propose a generic way to automatically\ncompute the similarities. The natural words are embedded into Euclidean space\nusing a standard word embedding. The probability of each class is then a\nfunction of the cosine similarity between the vector representations of the\nclass and the true label. The proposed label-similarity curriculum learning\n(LCL) approach was empirically evaluated using several popular deep learning\narchitectures for image classification tasks applied to five datasets including\nImageNet, CIFAR100, and AWA2. In all scenarios, LCL was able to improve the\nclassification accuracy on the test data compared to standard training.",
        "Inferring spectral signatures from ground based natural images has acquired a\nlot of interest in applied deep learning. In contrast to the spectra of ground\nbased images, aerial spectral images have low spatial resolution and suffer\nfrom higher noise interference. In this paper, we train a conditional\nadversarial network to learn an inverse mapping from a trichromatic space to 31\nspectral bands within 400 to 700 nm. The network is trained on AeroCampus, a\nfirst of its kind aerial hyperspectral dataset. AeroCampus consists of high\nspatial resolution color images and low spatial resolution hyperspectral images\n(HSI). Color images synthesized from 31 spectral bands are used to train our\nnetwork. With a baseline root mean square error of 2.48 on the synthesized RGB\ntest data, we show that it is possible to generate spectral signatures in\naerial imagery.",
        "Pedestrian trajectory prediction is valuable for understanding human motion\nbehaviors and it is challenging because of the social influence from other\npedestrians, the scene constraints and the multimodal possibilities of\npredicted trajectories. Most existing methods only focus on two of the above\nthree key elements. In order to jointly consider all these elements, we propose\na novel trajectory prediction method named Scene Gated Social Graph (SGSG). In\nthe proposed SGSG, dynamic graphs are used to describe the social relationship\namong pedestrians. The social and scene influences are taken into account\nthrough the scene gated social graph features which combine the encoded social\ngraph features and semantic scene features. In addition, a VAE module is\nincorporated to learn the scene gated social feature and sample latent\nvariables for generating multiple trajectories that are socially and\nenvironmentally acceptable. We compare our SGSG against twenty state-of-the-art\npedestrian trajectory prediction methods and the results show that the proposed\nmethod achieves superior performance on two widely used trajectory prediction\nbenchmarks.",
        "Recent learning-based approaches, in which models are trained by single-view\nimages have shown promising results for monocular 3D face reconstruction, but\nthey suffer from the ill-posed face pose and depth ambiguity issue. In contrast\nto previous works that only enforce 2D feature constraints, we propose a\nself-supervised training architecture by leveraging the multi-view geometry\nconsistency, which provides reliable constraints on face pose and depth\nestimation. We first propose an occlusion-aware view synthesis method to apply\nmulti-view geometry consistency to self-supervised learning. Then we design\nthree novel loss functions for multi-view consistency, including the pixel\nconsistency loss, the depth consistency loss, and the facial landmark-based\nepipolar loss. Our method is accurate and robust, especially under large\nvariations of expressions, poses, and illumination conditions. Comprehensive\nexperiments on the face alignment and 3D face reconstruction benchmarks have\ndemonstrated superiority over state-of-the-art methods. Our code and model are\nreleased in https://github.com/jiaxiangshang/MGCNet.",
        "Despite the tremendous success of deep neural networks in various learning\nproblems, it has been observed that adding an intentionally designed\nadversarial perturbation to inputs of these architectures leads to erroneous\nclassification with high confidence in the prediction. In this work, we propose\na general framework based on the perturbation analysis of learning algorithms\nwhich consists of convex programming and is able to recover many current\nadversarial attacks as special cases. The framework can be used to propose\nnovel attacks against learning algorithms for classification and regression\ntasks under various new constraints with closed form solutions in many\ninstances. In particular we derive new attacks against classification\nalgorithms which are shown to achieve comparable performances to notable\nexisting attacks. The framework is then used to generate adversarial\nperturbations for regression tasks which include single pixel and single subset\nattacks. By applying this method to autoencoding and image colorization tasks,\nit is shown that adversarial perturbations can effectively perturb the output\nof regression tasks as well.",
        "Besides accuracy, the model size of convolutional neural networks (CNN)\nmodels is another important factor considering limited hardware resources in\npractical applications. For example, employing deep neural networks on mobile\nsystems requires the design of accurate yet fast CNN for low latency in\nclassification and object detection. To fulfill the need, we aim at obtaining\nCNN models with both high testing accuracy and small size to address resource\nconstraints in many embedded devices. In particular, this paper focuses on\nproposing a generic reinforcement learning-based model compression approach in\na two-stage compression pipeline: pruning and quantization. The first stage of\ncompression, i.e., pruning, is achieved via exploiting deep reinforcement\nlearning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise\nchannel pruning and element-wise variational pruning via information dropout.\nThe second stage, i.e., quantization, is achieved via a similar DRL approach\nbut focuses on obtaining the optimal bits representation for individual layers.\nWe further conduct experimental results on CIFAR-10 and ImageNet datasets. For\nthe CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x\nfrom 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet\ndataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to\n4.14MB with no accuracy loss.",
        "Hypertext transfer protocol (HTTP) is one of the most widely used protocols\non the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use\nHTTP as the transport mechanism. Therefore, it is crucial to develop an\nintelligent solution that would allow to effectively detect and filter out\nanomalies in HTTP traffic. Currently, most of the anomaly detection systems are\neither rule-based or trained using manually selected features. We propose\nutilizing modern unsupervised language representation model for embedding HTTP\nrequests and then using it to classify anomalies in the traffic. The solution\nis motivated by methods used in Natural Language Processing (NLP) such as\nDoc2Vec which could potentially capture the true understanding of HTTP\nmessages, and therefore improve the efficiency of Intrusion Detection System.\nIn our work, we not only aim at generating a suitable embedding space, but also\nat the interpretability of the proposed model. We decided to use the current\nstate-of-the-art RoBERTa, which, as far as we know, has never been used in a\nsimilar problem. To verify how the solution would work in real word conditions,\nwe train the model using only legitimate traffic. We also try to explain the\nresults based on clusters that occur in the vectorized requests space and a\nsimple logistic regression classifier. We compared our approach with the\nsimilar, previously proposed methods. We evaluate the feasibility of our method\non three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared\nourselves. The results we show are comparable to others or better, and most\nimportantly - interpretable.",
        "Compared with common image segmentation tasks targeted at low-resolution\nimages, higher resolution detailed image segmentation receives much less\nattention. In this paper, we propose and study a task named Meticulous Object\nSegmentation (MOS), which is focused on segmenting well-defined foreground\nobjects with elaborate shapes in high resolution images (e.g. 2k - 4k). To this\nend, we propose the MeticulousNet which leverages a dedicated decoder to\ncapture the object boundary details. Specifically, we design a Hierarchical\nPoint-wise Refining (HierPR) block to better delineate object boundaries, and\nreformulate the decoding process as a recursive coarse to fine refinement of\nthe object mask. To evaluate segmentation quality near object boundaries, we\npropose the Meticulosity Quality (MQ) score considering both the mask coverage\nand boundary precision. In addition, we collect a MOS benchmark dataset\nincluding 600 high quality images with complex objects. We provide\ncomprehensive empirical evidence showing that MeticulousNet can reveal\npixel-accurate segmentation boundaries and is superior to state-of-the-art\nmethods for high resolution object segmentation tasks.",
        "Bayesian optimization is a powerful tool to optimize a black-box function,\nthe evaluation of which is time-consuming or costly. In this paper, we propose\na new approach to Bayesian optimization called GP-MGC, which maximizes\nmultiscale graph correlation with respect to the global maximum to determine\nthe next query point. We present our evaluation of GP-MGC in applications\ninvolving both synthetic benchmark functions and real-world datasets and\ndemonstrate that GP-MGC performs as well as or even better than\nstate-of-the-art methods such as max-value entropy search and GP-UCB.",
        "In this paper, we analyze and extend an online learning framework known as\nContext-Attentive Bandit, motivated by various practical applications, from\nmedical diagnosis to dialog systems, where due to observation costs only a\nsmall subset of a potentially large number of context variables can be observed\nat each iteration;however, the agent has a freedom to choose which variables to\nobserve. We derive a novel algorithm, called Context-Attentive Thompson\nSampling (CATS), which builds upon the Linear Thompson Sampling approach,\nadapting it to Context-Attentive Bandit setting. We provide a theoretical\nregret analysis and an extensive empirical evaluation demonstrating advantages\nof the proposed approach over several baseline methods on a variety of\nreal-life datasets",
        "Continual acquisition of novel experience without interfering previously\nlearned knowledge, i.e. continual learning, is critical for artificial neural\nnetworks, but limited by catastrophic forgetting. A neural network adjusts its\nparameters when learning a new task, but then fails to conduct the old tasks\nwell. By contrast, the brain has a powerful ability to continually learn new\nexperience without catastrophic interference. The underlying neural mechanisms\npossibly attribute to the interplay of hippocampus-dependent memory system and\nneocortex-dependent memory system, mediated by prefrontal cortex. Specifically,\nthe two memory systems develop specialized mechanisms to consolidate\ninformation as more specific forms and more generalized forms, respectively,\nand complement the two forms of information in the interplay. Inspired by such\nbrain strategy, we propose a novel approach named triple memory networks (TMNs)\nfor continual learning. TMNs model the interplay of hippocampus, prefrontal\ncortex and sensory cortex (a neocortex region) as a triple-network architecture\nof generative adversarial networks (GAN). The input information is encoded as\nspecific representation of the data distributions in a generator, or\ngeneralized knowledge of solving tasks in a discriminator and a classifier,\nwith implementing appropriate brain-inspired algorithms to alleviate\ncatastrophic forgetting in each module. Particularly, the generator replays\ngenerated data of the learned tasks to the discriminator and the classifier,\nboth of which are implemented with a weight consolidation regularizer to\ncomplement the lost information in generation process. TMNs achieve new\nstate-of-the-art performance on a variety of class-incremental learning\nbenchmarks on MNIST, SVHN, CIFAR-10 and ImageNet-50, comparing with strong\nbaseline methods.",
        "Multi-view subspace clustering has been applied to applications such as image\nprocessing and video surveillance, and has attracted increasing attention. Most\nexisting methods learn view-specific self-representation matrices, and\nconstruct a combined affinity matrix from multiple views. The affinity\nconstruction process is time-consuming, and the combined affinity matrix is not\nguaranteed to reflect the whole true subspace structure. To overcome these\nissues, the Latent Complete Row Space Recovery (LCRSR) method is proposed.\nConcretely, LCRSR is based on the assumption that the multi-view observations\nare generated from an underlying latent representation, which is further\nassumed to collect the authentic samples drawn exactly from multiple subspaces.\nLCRSR is able to recover the row space of the latent representation, which not\nonly carries complete information from multiple views but also determines the\nsubspace membership under certain conditions. LCRSR does not involve the graph\nconstruction procedure and is solved with an efficient and convergent\nalgorithm, thereby being more scalable to large-scale datasets. The\neffectiveness and efficiency of LCRSR are validated by clustering various kinds\nof multi-view data and illustrated in the background subtraction task.",
        "Guided super-resolution (GSR) of thermal images using visible range images is\nchallenging because of the difference in the spectral-range between the images.\nThis in turn means that there is significant texture-mismatch between the\nimages, which manifests as blur and ghosting artifacts in the super-resolved\nthermal image. To tackle this, we propose a novel algorithm for GSR based on\npyramidal edge-maps extracted from the visible image. Our proposed network has\ntwo sub-networks. The first sub-network super-resolves the low-resolution\nthermal image while the second obtains edge-maps from the visible image at a\ngrowing perceptual scale and integrates them into the super-resolution\nsub-network with the help of attention-based fusion. Extraction and integration\nof multi-level edges allows the super-resolution network to process\ntexture-to-object level information progressively, enabling more\nstraightforward identification of overlapping edges between the input images.\nExtensive experiments show that our model outperforms the state-of-the-art GSR\nmethods, both quantitatively and qualitatively.",
        "Hidden Markov models have successfully been applied as models of discrete\ntime series in many fields. Often, when applied in practice, the parameters of\nthese models have to be estimated. The currently predominating identification\nmethods, such as maximum-likelihood estimation and especially\nexpectation-maximization, are iterative and prone to have problems with local\nminima. A non-iterative method employing a spectral subspace-like approach has\nrecently been proposed in the machine learning literature. This paper evaluates\nthe performance of this algorithm, and compares it to the performance of the\nexpectation-maximization algorithm, on a number of numerical examples. We find\nthat the performance is mixed; it successfully identifies some systems with\nrelatively few available observations, but fails completely for some systems\neven when a large amount of observations is available. An open question is how\nthis discrepancy can be explained. We provide some indications that it could be\nrelated to how well-conditioned some system parameters are.",
        "Graph convolutional networks (GCNs) are widely used in graph-based\napplications such as graph classification and segmentation. However, current\nGCNs have limitations on implementation such as network architectures due to\ntheir irregular inputs. In contrast, convolutional neural networks (CNNs) are\ncapable of extracting rich features from large-scale input data, but they do\nnot support general graph inputs. To bridge the gap between GCNs and CNNs, in\nthis paper we study the problem of how to effectively and efficiently map\ngeneral graphs to 2D grids that CNNs can be directly applied to, while\npreserving graph topology as much as possible. We therefore propose two novel\ngraph-to-grid mapping schemes, namely, {\\em graph-preserving grid layout\n(GPGL)} and its extension {\\em Hierarchical GPGL (H-GPGL)} for computational\nefficiency. We formulate the GPGL problem as integer programming and further\npropose an approximate yet efficient solver based on a penalized Kamada-Kawai\nmethod, a well-known optimization algorithm in 2D graph drawing. We propose a\nnovel vertex separation penalty that encourages graph vertices to lay on the\ngrid without any overlap. Along with this image representation, even extra 2D\nmaxpooling layers contribute to the PointNet, a widely applied point-based\nneural network. We demonstrate the empirical success of GPGL on general graph\nclassification with small graphs and H-GPGL on 3D point cloud segmentation with\nlarge graphs, based on 2D CNNs including VGG16, ResNet50 and multi-scale maxout\n(MSM) CNN.",
        "The Generative Adversarial Network (GAN) has recently been applied to\ngenerate synthetic images from text. Despite significant advances, most current\nstate-of-the-art algorithms are regular-grid region based; when attention is\nused, it is mainly applied between individual regular-grid regions and a word.\nThese approaches are sufficient to generate images that contain a single object\nin its foreground, such as a \"bird\" or \"flower\". However, natural languages\noften involve complex foreground objects and the background may also constitute\na variable portion of the generated image. Therefore, the regular-grid based\nimage attention weights may not necessarily concentrate on the intended\nforeground region(s), which in turn, results in an unnatural looking image.\nAdditionally, individual words such as \"a\", \"blue\" and \"shirt\" do not\nnecessarily provide a full visual context unless they are applied together. For\nthis reason, in our paper, we proposed a novel method in which we introduced an\nadditional set of attentions between true-grid regions and word phrases. The\ntrue-grid region is derived using a set of auxiliary bounding boxes. These\nauxiliary bounding boxes serve as superior location indicators to where the\nalignment and attention should be drawn with the word phrases. Word phrases are\nderived from analysing Part-of-Speech (POS) results. We perform experiments on\nthis novel network architecture using the Microsoft Common Objects in Context\n(MSCOCO) dataset and the model generates $256 \\times 256$ conditioned on a\nshort sentence description. Our proposed approach is capable of generating more\nrealistic images compared with the current state-of-the-art algorithms.",
        "Flat surfaces captured by 3D point clouds are often used for localization,\nmapping, and modeling. Dense point cloud processing has high computation and\nmemory costs making low-dimensional representations of flat surfaces such as\npolygons desirable. We present Polylidar3D, a non-convex polygon extraction\nalgorithm which takes as input unorganized 3D point clouds (e.g., LiDAR data),\norganized point clouds (e.g., range images), or user-provided meshes.\nNon-convex polygons represent flat surfaces in an environment with interior\ncutouts representing obstacles or holes. The Polylidar3D front-end transforms\ninput data into a half-edge triangular mesh. This representation provides a\ncommon level of input data abstraction for subsequent back-end processing. The\nPolylidar3D back-end is composed of four core algorithms: mesh smoothing,\ndominant plane normal estimation, planar segment extraction, and finally\npolygon extraction. Polylidar3D is shown to be quite fast, making use of CPU\nmulti-threading and GPU acceleration when available. We demonstrate\nPolylidar3D's versatility and speed with real-world datasets including aerial\nLiDAR point clouds for rooftop mapping, autonomous driving LiDAR point clouds\nfor road surface detection, and RGBD cameras for indoor floor/wall detection.\nWe also evaluate Polylidar3D on a challenging planar segmentation benchmark\ndataset. Results consistently show excellent speed and accuracy.",
        "Graph Neural Networks (GNNs) are deep learning methods which provide the\ncurrent state of the art performance in node classification tasks. GNNs often\nassume homophily -- neighboring nodes having similar features and labels--, and\ntherefore may not be at their full potential when dealing with non-homophilic\ngraphs. In this work, we focus on addressing this limitation and enable Graph\nAttention Networks (GAT), a commonly used variant of GNNs, to explore the\nstructural information within each graph locality. Inspired by the positional\nencoding in the Transformers, we propose a framework, termed Graph Attentional\nNetworks with Positional Embeddings (GAT-POS), to enhance GATs with positional\nembeddings which capture structural and positional information of the nodes in\nthe graph. In this framework, the positional embeddings are learned by a model\npredictive of the graph context, plugged into an enhanced GAT architecture,\nwhich is able to leverage both the positional and content information of each\nnode. The model is trained jointly to optimize for the task of node\nclassification as well as the task of predicting graph context. Experimental\nresults show that GAT-POS reaches remarkable improvement compared to strong GNN\nbaselines and recent structural embedding enhanced GNNs on non-homophilic\ngraphs.",
        "Potential Based Reward Shaping combined with a potential function based on\nappropriately defined abstract knowledge has been shown to significantly\nimprove learning speed in Reinforcement Learning. MultiGrid Reinforcement\nLearning (MRL) has further shown that such abstract knowledge in the form of a\npotential function can be learned almost solely from agent interaction with the\nenvironment. However, we show that MRL faces the problem of not extending well\nto work with Deep Learning. In this paper we extend and improve MRL to take\nadvantage of modern Deep Learning algorithms such as Deep Q-Networks (DQN). We\nshow that DQN augmented with our approach perform significantly better on\ncontinuous control tasks than its Vanilla counterpart and DQN augmented with\nMRL.",
        "Generative Adversarial Networks (GANs) have recently advanced image synthesis\nby learning the underlying distribution of the observed data. However, how the\nfeatures learned from solving the task of image generation are applicable to\nother vision tasks remains seldom explored. In this work, we show that learning\nto synthesize images can bring remarkable hierarchical visual features that are\ngeneralizable across a wide range of applications. Specifically, we consider\nthe pre-trained StyleGAN generator as a learned loss function and utilize its\nlayer-wise representation to train a novel hierarchical encoder. The visual\nfeature produced by our encoder, termed as Generative Hierarchical Feature\n(GH-Feat), has strong transferability to both generative and discriminative\ntasks, including image editing, image harmonization, image classification, face\nverification, landmark detection, and layout prediction. Extensive qualitative\nand quantitative experimental results demonstrate the appealing performance of\nGH-Feat.",
        "Machine learning has been utilized to perform tasks in many different domains\nsuch as classification, object detection, image segmentation and natural\nlanguage analysis. Data labeling has always been one of the most important\ntasks in machine learning. However, labeling large amounts of data increases\nthe monetary cost in machine learning. As a result, researchers started to\nfocus on reducing data annotation and labeling costs. Transfer learning was\ndesigned and widely used as an efficient approach that can reasonably reduce\nthe negative impact of limited data, which in turn, reduces the data\npreparation cost. Even transferring previous knowledge from a source domain\nreduces the amount of data needed in a target domain. However, large amounts of\nannotated data are still demanded to build robust models and improve the\nprediction accuracy of the model. Therefore, researchers started to pay more\nattention on auto annotation and labeling. In this survey paper, we provide a\nreview of previous techniques that focuses on optimized data annotation and\nlabeling for video, audio, and text data.",
        "This paper studies the neural architecture search (NAS) problem for\ndeveloping efficient generator networks. Compared with deep models for visual\nrecognition tasks, generative adversarial network (GAN) are usually designed to\nconduct various complex image generation. We first discover an intact search\nspace of generator networks including three dimensionalities, i.e., path,\noperator, channel for fully excavating the network performance. To reduce the\nhuge search cost, we explore a coarse-to-fine search strategy which divides the\noverall search process into three sub-optimization problems accordingly. In\naddition, a fair supernet training approach is utilized to ensure that all\nsub-networks can be updated fairly and stably. Experiments results on\nbenchmarks show that we can provide generator networks with better image\nquality and lower computational costs over the state-of-the-art methods. For\nexample, with our method, it takes only about 8 GPU hours on the entire\nedges-to-shoes dataset to get a 2.56 MB model with a 24.13 FID score and 10 GPU\nhours on the entire Urban100 dataset to get a 1.49 MB model with a 24.94 PSNR\nscore.",
        "Discovering concepts (or temporal abstractions) in an unsupervised manner\nfrom demonstration data in the absence of an environment is an important\nproblem. Organizing these discovered concepts hierarchically at different\nlevels of abstraction is useful in discovering patterns, building ontologies,\nand generating tutorials from demonstration data. However, recent work to\ndiscover such concepts without access to any environment does not discover\nrelationships (or a hierarchy) between these discovered concepts. In this\npaper, we present a Transformer-based concept abstraction architecture UNHCLE\n(pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way\nfrom demonstration data. We empirically demonstrate how UNHCLE discovers\nmeaningful hierarchies using datasets from Chess and Cooking domains. Finally,\nwe show how UNHCLE learns meaningful language labels for concepts by using\ndemonstration data augmented with natural language for cooking and chess. All\nof our code is available at https://github.com/UNHCLE/UNHCLE",
        "The outcome of Jacobian singular values regularization was studied for\nsupervised learning problems. It also was shown that Jacobian conditioning\nregularization can help to avoid the ``mode-collapse'' problem in Generative\nAdversarial Networks. In this paper, we try to answer the following question:\nCan information about policy conditioning help to shape a more stable and\ngeneral policy of reinforcement learning agents? To answer this question, we\nconduct a study of Jacobian conditioning behavior during policy optimization.\nTo the best of our knowledge, this is the first work that research condition\nnumber in reinforcement learning agents. We propose a conditioning\nregularization algorithm and test its performance on the range of continuous\ncontrol tasks. Finally, we compare algorithms on the CoinRun environment with\nseparated train end test levels to analyze how conditioning regularization\ncontributes to agents' generalization.",
        "Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights: (1) depthwise Convolution and\nself-Attention can be naturally unified via simple relative attention; (2)\nvertically stacking convolution layers and attention layers in a principled way\nis surprisingly effective in improving generalization, capacity and efficiency.\nExperiments show that our CoAtNets achieve state-of-the-art performance under\ndifferent resource constraints across various datasets: Without extra data,\nCoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M\nimages from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching\nViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;\nNotably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1\naccuracy on ImageNet, establishing a new state-of-the-art result.",
        "The purpose of this paper is to outline a generalised model for representing\nhybrids of relational-categorical, symbolic, perceptual-sensory and\nperceptual-latent data, so as to embody, in the same architectural data layer,\nrepresentations for the input, output and latent tensors. This variety of\nrepresentation is currently used by various machine-learning models in computer\nvision, NLP/NLU, reinforcement learning which allows for direct application of\ncross-domain queries and functions. This is achieved by endowing a directed\nTensor-Typed Multi-Graph with at least some edge attributes which represent the\nembeddings from various latent spaces, so as to define, construct and compute\nnew similarity and distance relationships between and across tensorial forms,\nincluding visual, linguistic, auditory latent representations, thus stitching\nthe logical-categorical view of the observed universe to the\nBayesian/statistical view.",
        "We present a framework, which we call Molecule Deep $Q$-Networks (MolDQN),\nfor molecule optimization by combining domain knowledge of chemistry and\nstate-of-the-art reinforcement learning techniques (double $Q$-learning and\nrandomized value functions). We directly define modifications on molecules,\nthereby ensuring 100\\% chemical validity. Further, we operate without\npre-training on any dataset to avoid possible bias from the choice of that set.\nInspired by problems faced during medicinal chemistry lead optimization, we\nextend our model with multi-objective reinforcement learning, which maximizes\ndrug-likeness while maintaining similarity to the original molecule. We further\nshow the path through chemical space to achieve optimization for a molecule to\nunderstand how the model works.",
        "We investigate the problem of identifying adversarial attacks on image-based\nneural networks. We present intriguing experimental results showing significant\ndiscrepancies between the explanations generated for the predictions of a model\non clean and adversarial data. Utilizing this intuition, we propose a framework\nwhich can identify whether a given input is adversarial based on the\nexplanations given by the model. Code for our experiments can be found here:\nhttps://github.com/seansaito/Explaining-Away-Attacks-Against-Neural-Networks.",
        "While multi-agent interactions can be naturally modeled as a graph, the\nenvironment has traditionally been considered as a black box. We propose to\ncreate a shared agent-entity graph, where agents and environmental entities\nform vertices, and edges exist between the vertices which can communicate with\neach other. Agents learn to cooperate by exchanging messages along the edges of\nthis graph. Our proposed multi-agent reinforcement learning framework is\ninvariant to the number of agents or entities present in the system as well as\npermutation invariance, both of which are desirable properties for any\nmulti-agent system representation. We present state-of-the-art results on\ncoverage, formation and line control tasks for multi-agent teams in a fully\ndecentralized framework and further show that the learned policies quickly\ntransfer to scenarios with different team sizes along with strong zero-shot\ngeneralization performance. This is an important step towards developing\nmulti-agent teams which can be realistically deployed in the real world without\nassuming complete prior knowledge or instantaneous communication at unbounded\ndistances.",
        "Neural implicit 3D representations have emerged as a powerful paradigm for\nreconstructing surfaces from multi-view images and synthesizing novel views.\nUnfortunately, existing methods such as DVR or IDR require accurate per-pixel\nobject masks as supervision. At the same time, neural radiance fields have\nrevolutionized novel view synthesis. However, NeRF's estimated volume density\ndoes not admit accurate surface reconstruction. Our key insight is that\nimplicit surface models and radiance fields can be formulated in a unified way,\nenabling both surface and volume rendering using the same model. This unified\nperspective enables novel, more efficient sampling procedures and the ability\nto reconstruct accurate surfaces without input masks. We compare our method on\nthe DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments\ndemonstrate that we outperform NeRF in terms of reconstruction quality while\nperforming on par with IDR without requiring masks.",
        "Designing off-policy reinforcement learning algorithms is typically a very\nchallenging task, because a desirable iteration update often involves an\nexpectation over an on-policy distribution. Prior off-policy actor-critic (AC)\nalgorithms have introduced a new critic that uses the density ratio for\nadjusting the distribution mismatch in order to stabilize the convergence, but\nat the cost of potentially introducing high biases due to the estimation errors\nof both the density ratio and value function. In this paper, we develop a\ndoubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take\nadvantage of learned nuisance functions to reduce estimation errors. Moreover,\nDR-Off-PAC adopts a single timescale structure, in which both actor and critics\nare updated simultaneously with constant stepsize, and is thus more sample\nefficient than prior algorithms that adopt either two timescale or nested-loop\nstructure. We study the finite-time convergence rate and characterize the\nsample complexity for DR-Off-PAC to attain an $\\epsilon$-accurate optimal\npolicy. We also show that the overall convergence of DR-Off-PAC is doubly\nrobust to the approximation errors that depend only on the expressive power of\napproximation functions. To the best of our knowledge, our study establishes\nthe first overall sample complexity analysis for a single time-scale off-policy\nAC algorithm.",
        "Face anti-spoofing approach based on domain generalization(DG) has drawn\ngrowing attention due to its robustness forunseen scenarios. Existing DG\nmethods assume that the do-main label is known.However, in real-world\napplications, thecollected dataset always contains mixture domains, where\nthedomain label is unknown. In this case, most of existing meth-ods may not\nwork. Further, even if we can obtain the domainlabel as existing methods, we\nthink this is just a sub-optimalpartition. To overcome the limitation, we\npropose domain dy-namic adjustment meta-learning (D2AM) without using do-main\nlabels, which iteratively divides mixture domains viadiscriminative domain\nrepresentation and trains a generaliz-able face anti-spoofing with\nmeta-learning. Specifically, wedesign a domain feature based on Instance\nNormalization(IN) and propose a domain representation learning module(DRLM) to\nextract discriminative domain features for cluster-ing. Moreover, to reduce the\nside effect of outliers on cluster-ing performance, we additionally utilize\nmaximum mean dis-crepancy (MMD) to align the distribution of sample featuresto\na prior distribution, which improves the reliability of clus tering. Extensive\nexperiments show that the proposed methodoutperforms conventional DG-based face\nanti-spoofing meth-ods, including those utilizing domain labels. Furthermore,\nweenhance the interpretability through visualizatio",
        "In this paper, we propose a very compact embedded CNN processor design based\non a modified logarithmic computing method using very low bit-width\nrepresentation. Our high-quality CNN processor can easily fit into edge\ndevices. For Yolov2, our processing circuit takes only 0.15 mm2 using TSMC 40\nnm cell library. The key idea is to constrain the activation and weight values\nof all layers uniformly to be within the range [-1, 1] and produce low\nbit-width logarithmic representation. With the uniform representations, we\ndevise a unified, reusable CNN computing kernel and significantly reduce\ncomputing resources. The proposed approach has been extensively evaluated on\nmany popular image classification CNN models (AlexNet, VGG16, and ResNet-18/34)\nand object detection models (Yolov2). The hardware-implemented results show\nthat our design consumes only minimal computing and storage resources, yet\nattains very high accuracy. The design is thoroughly verified on FPGAs, and the\nSoC integration is underway with promising results. With extremely efficient\nresource and energy usage, our design is excellent for edge computing purposes.",
        "Self-supervised monocular depth estimation has become an appealing solution\nto the lack of ground truth labels, but its reconstruction loss often produces\nover-smoothed results across object boundaries and is incapable of handling\nocclusion explicitly. In this paper, we propose a new approach to leverage\npseudo ground truth depth maps of stereo images generated from self-supervised\nstereo matching methods. The confidence map of the pseudo ground truth depth\nmap is estimated to mitigate performance degeneration by inaccurate pseudo\ndepth maps. To cope with the prediction error of the confidence map itself, we\nalso leverage the threshold network that learns the threshold dynamically\nconditioned on the pseudo depth maps. The pseudo depth labels filtered out by\nthe thresholded confidence map are used to supervise the monocular depth\nnetwork. Furthermore, we propose the probabilistic framework that refines the\nmonocular depth map with the help of its uncertainty map through the\npixel-adaptive convolution (PAC) layer. Experimental results demonstrate\nsuperior performance to state-of-the-art monocular depth estimation methods.\nLastly, we exhibit that the proposed threshold learning can also be used to\nimprove the performance of existing confidence estimation approaches.",
        "LIDAR point clouds and RGB-images are both extremely essential for 3D object\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\nthese two types of data effectively. However, their fusion methods based on\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\npropose a novel fusion approach named Point-based Attentive Cont-conv\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\nExcept for continuous convolution, we additionally add a Point-Pooling and an\nAttentive Aggregation to make the fused features more expressive. Moreover,\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\nsub-network to extract full-resolution semantic feature maps from images and\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\nthe effectiveness of the PACF module and the expressive semantic features from\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\nDetection benchmark, and our method can achieve state-of-the-art on the metric\nof 3D AP.",
        "Learning classifier systems (LCSs) are population-based predictive systems\nthat were originally envisioned as agents to act in reinforcement learning (RL)\nenvironments. These systems can suffer from population bloat and so are\namenable to compaction techniques that try to strike a balance between\npopulation size and performance. A well-studied LCS architecture is XCSF, which\nin the RL setting acts as a Q-function approximator. We apply XCSF to a\ndeterministic and stochastic variant of the FrozenLake8x8 environment from\nOpenAI Gym, with its performance compared in terms of function approximation\nerror and policy accuracy to the optimal Q-functions and policies produced by\nsolving the environments via dynamic programming. We then introduce a novel\ncompaction algorithm (Greedy Niche Mass Compaction - GNMC) and study its\noperation on XCSF's trained populations. Results show that given a suitable\nparametrisation, GNMC preserves or even slightly improves function\napproximation error while yielding a significant reduction in population size.\nReasonable preservation of policy accuracy also occurs, and we link this metric\nto the commonly used steps-to-goal metric in maze-like environments,\nillustrating how the metrics are complementary rather than competitive.",
        "The huge amount of video data produced daily by camera-based systems, such as\nsurveilance, medical and telecommunication systems, emerges the need for\neffective video summarization (VS) methods. These methods should be capable of\ncreating an overview of the video content. In this paper, we propose a novel VS\nmethod based on a Generative Adversarial Network (GAN) model pre-trained with\nhuman eye fixations. The main contribution of the proposed method is that it\ncan provide perceptually compatible video summaries by combining both perceived\ncolor and spatiotemporal visual attention cues in a unsupervised scheme.\nSeveral fusion approaches are considered for robustness under uncertainty, and\npersonalization. The proposed method is evaluated in comparison to\nstate-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental\nresults conclude that SalSum outperforms the state-of-the-art approaches by\nproviding the highest f-measure score on the VSUMM benchmark.",
        "Hateful memes are an emerging method of spreading hate on the internet,\nrelying on both images and text to convey a hateful message. We take an\ninterpretable approach to hateful meme detection, using machine learning and\nsimple heuristics to identify the features most important to classifying a meme\nas hateful. In the process, we build a gradient-boosted decision tree and an\nLSTM-based model that achieve comparable performance (73.8 validation and 72.7\ntest auROC) to the gold standard of humans and state-of-the-art transformer\nmodels on this challenging task.",
        "OCR algorithms have received a significant improvement in performance\nrecently, mainly due to the increase in the capabilities of artificial\nintelligence algorithms. However, this advancement is not evenly distributed\nover all languages. Urdu is among the languages which did not receive much\nattention, especially in the font independent perspective. There exists no\nautomated system that can reliably recognize printed Urdu text in images and\nvideos across different fonts. To help bridge this gap, we have developed\nQaida, a large scale data set with 256 fonts, and a complete Urdu lexicon. We\nhave also developed a Convolutional Neural Network (CNN) based classification\nmodel which can recognize Urdu ligatures with 84.2% accuracy. Moreover, we\ndemonstrate that our recognition network can not only recognize the text in the\nfonts it is trained on but can also reliably recognize text in unseen (new)\nfonts. To this end, this paper makes following contributions: (i) we introduce\na large scale, multiple fonts based data set for printed Urdu text\nrecognition;(ii) we have designed, trained and evaluated a CNN based model for\nUrdu text recognition; (iii) we experiment with incremental learning methods to\nproduce state-of-the-art results for Urdu text recognition. All the experiment\nchoices were thoroughly validated via detailed empirical analysis. We believe\nthat this study can serve as the basis for further improvement in the\nperformance of font independent Urdu OCR systems.",
        "High dimensional data analysis for exploration and discovery includes three\nfundamental tasks: dimensionality reduction, clustering, and visualization.\nWhen the three associated tasks are done separately, as is often the case thus\nfar, inconsistencies can occur among the tasks in terms of data geometry and\nothers. This can lead to confusing or misleading data interpretation. In this\npaper, we propose a novel neural network-based method, called Consistent\nRepresentation Learning (CRL), to accomplish the three associated tasks\nend-to-end and improve the consistencies. The CRL network consists of two\nnonlinear dimensionality reduction (NLDR) transformations: (1) one from the\ninput data space to the latent feature space for clustering, and (2) the other\nfrom the clustering space to the final 2D or 3D space for visualization.\nImportantly, the two NLDR transformations are performed to best satisfy local\ngeometry preserving (LGP) constraints across the spaces or network layers, to\nimprove data consistencies along with the processing flow. Also, we propose a\nnovel metric, clustering-visualization inconsistency (CVI), for evaluating the\ninconsistencies. Extensive comparative results show that the proposed CRL\nneural network method outperforms the popular t-SNE and UMAP-based and other\ncontemporary clustering and visualization algorithms in terms of evaluation\nmetrics and visualization.",
        "Traditional color images only depict color intensities in red, green and blue\nchannels, often making object trackers fail in challenging scenarios, e.g.,\nbackground clutter and rapid changes of target appearance. Alternatively,\nmaterial information of targets contained in a large amount of bands of\nhyperspectral images (HSI) is more robust to these difficult conditions. In\nthis paper, we conduct a comprehensive study on how material information can be\nutilized to boost object tracking from three aspects: benchmark dataset,\nmaterial feature representation and material based tracking. In terms of\nbenchmark, we construct a dataset of fully-annotated videos, which contain both\nhyperspectral and color sequences of the same scene. Material information is\nrepresented by spectral-spatial histogram of multidimensional gradient, which\ndescribes the 3D local spectral-spatial structure in an HSI, and fractional\nabundances of constituted material components which encode the underlying\nmaterial distribution. These two types of features are embedded into\ncorrelation filters, yielding material based tracking. Experimental results on\nthe collected benchmark dataset show the potentials and advantages of material\nbased object tracking.",
        "Denoising autoencoders (DAEs) have proven useful for unsupervised\nrepresentation learning, but a thorough theoretical understanding is still\nlacking of how the input noise influences learning. Here we develop theory for\nhow noise influences learning in DAEs. By focusing on linear DAEs, we are able\nto derive analytic expressions that exactly describe their learning dynamics.\nWe verify our theoretical predictions with simulations as well as experiments\non MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise\nallows DAEs to ignore low variance directions in the inputs while learning to\nreconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs\nto standard regularised autoencoders, we show that noise has a similar\nregularisation effect to weight decay, but with faster training dynamics. We\nalso show that our theoretical predictions approximate learning dynamics on\nreal-world data and qualitatively match observed dynamics in nonlinear DAEs.",
        "A novel locally statistical active contour model (ACM) for image segmentation\nin the presence of intensity inhomogeneity is presented in this paper. The\ninhomogeneous objects are modeled as Gaussian distributions of different means\nand variances, and a moving window is used to map the original image into\nanother domain, where the intensity distributions of inhomogeneous objects are\nstill Gaussian but are better separated. The means of the Gaussian\ndistributions in the transformed domain can be adaptively estimated by\nmultiplying a bias field with the original signal within the window. A\nstatistical energy functional is then defined for each local region, which\ncombines the bias field, the level set function, and the constant approximating\nthe true signal of the corresponding object. Experiments on both synthetic and\nreal images demonstrate the superiority of our proposed algorithm to\nstate-of-the-art and representative methods.",
        "It is well established that training deep neural networks gives useful\nrepresentations that capture essential features of the inputs. However, these\nrepresentations are poorly understood in theory and practice. In the context of\nsupervised learning an important question is whether these representations\ncapture features informative for classification, while filtering out\nnon-informative noisy ones. We explore a formalization of this question by\nconsidering a generative process where each class is associated with a\nhigh-dimensional manifold and different classes define different manifolds.\nUnder this model, each input is produced using two latent vectors: (i) a\n\"manifold identifier\" $\\gamma$ and; (ii)~a \"transformation parameter\" $\\theta$\nthat shifts examples along the surface of a manifold. E.g., $\\gamma$ might\nrepresent a canonical image of a dog, and $\\theta$ might stand for variations\nin pose, background or lighting. We provide theoretical and empirical evidence\nthat neural representations can be viewed as LSH-like functions that map each\ninput to an embedding that is a function of solely the informative $\\gamma$ and\ninvariant to $\\theta$, effectively recovering the manifold identifier $\\gamma$.\nAn important consequence of this behavior is one-shot learning to unseen\nclasses.",
        "Although deep learning has been applied to successfully address many data\nmining problems, relatively limited work has been done on deep learning for\nanomaly detection. Existing deep anomaly detection methods, which focus on\nlearning new feature representations to enable downstream anomaly detection\nmethods, perform indirect optimization of anomaly scores, leading to\ndata-inefficient learning and suboptimal anomaly scoring. Also, they are\ntypically designed as unsupervised learning due to the lack of large-scale\nlabeled anomaly data. As a result, they are difficult to leverage prior\nknowledge (e.g., a few labeled anomalies) when such information is available as\nin many real-world anomaly detection applications.\n  This paper introduces a novel anomaly detection framework and its\ninstantiation to address these problems. Instead of representation learning,\nour method fulfills an end-to-end learning of anomaly scores by a neural\ndeviation learning, in which we leverage a few (e.g., multiple to dozens)\nlabeled anomalies and a prior probability to enforce statistically significant\ndeviations of the anomaly scores of anomalies from that of normal data objects\nin the upper tail. Extensive results show that our method can be trained\nsubstantially more data-efficiently and achieves significantly better anomaly\nscoring than state-of-the-art competing methods.",
        "We attempt to give a unifying view of the various recent attempts to (i)\nimprove the interpretability of tree-based models and (ii) debias the the\ndefault variable-importance measure in random Forests, Gini importance. In\nparticular, we demonstrate a common thread among the out-of-bag based bias\ncorrection methods and their connection to local explanation for trees. In\naddition, we point out a bias caused by the inclusion of inbag data in the\nnewly developed explainable AI for trees algorithms.",
        "Often we wish to predict a large number of variables that depend on each\nother as well as on other observed variables. Structured prediction methods are\nessentially a combination of classification and graphical modeling, combining\nthe ability of graphical models to compactly model multivariate data with the\nability of classification methods to perform prediction using large sets of\ninput features. This tutorial describes conditional random fields, a popular\nprobabilistic method for structured prediction. CRFs have seen wide application\nin natural language processing, computer vision, and bioinformatics. We\ndescribe methods for inference and parameter estimation for CRFs, including\npractical issues for implementing large scale CRFs. We do not assume previous\nknowledge of graphical modeling, so this tutorial is intended to be useful to\npractitioners in a wide variety of fields.",
        "We address the problem of Bayesian reinforcement learning using efficient\nmodel-based online planning. We propose an optimism-free Bayes-adaptive\nalgorithm to induce deeper and sparser exploration with a theoretical bound on\nits performance relative to the Bayes optimal policy, with a lower\ncomputational complexity. The main novelty is the use of a candidate policy\ngenerator, to generate long-term options in the planning tree (over beliefs),\nwhich allows us to create much sparser and deeper trees. Experimental results\non different environments show that in comparison to the state-of-the-art, our\nalgorithm is both computationally more efficient, and obtains significantly\nhigher reward in discrete environments.",
        "Many important problems can be formulated as reasoning in knowledge graphs.\nRepresentation learning has proved extremely effective for transductive\nreasoning, in which one needs to make new predictions for already observed\nentities. This is true for both attributed graphs(where each entity has an\ninitial feature vector) and non-attributed graphs (where the only initial\ninformation derives from known relations with other entities). For\nout-of-sample reasoning, where one needs to make predictions for entities that\nwere unseen at training time, much prior work considers attributed graph.\nHowever, this problem is surprisingly under-explored for non-attributed graphs.\nIn this paper, we study the out-of-sample representation learning problem for\nnon-attributed knowledge graphs, create benchmark datasets for this task,\ndevelop several models and baselines, and provide empirical analyses and\ncomparisons of the proposed models and baselines.",
        "We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)\nembeddings of conditional distributions and vector-valued regressors. This\nconnection introduces a natural regularized loss function which the RKHS\nembeddings minimise, providing an intuitive understanding of the embeddings and\na justification for their use. Furthermore, the equivalence allows the\napplication of vector-valued regression methods and results to the problem of\nlearning conditional distributions. Using this link we derive a sparse version\nof the embedding by considering alternative formulations. Further, by applying\nconvergence results for vector-valued regression to the embedding problem we\nderive minimax convergence rates which are O(\\log(n)/n) -- compared to current\nstate of the art rates of O(n^{-1/4}) -- and are valid under milder and more\nintuitive assumptions. These minimax upper rates coincide with lower rates up\nto a logarithmic factor, showing that the embedding method achieves nearly\noptimal rates. We study our sparse embedding algorithm in a reinforcement\nlearning task where the algorithm shows significant improvement in sparsity\nover an incomplete Cholesky decomposition.",
        "A big part of achieving Artificial General Intelligence(AGI) is to build a\nmachine that can see and listen like humans. Much work has focused on designing\nmodels for image classification, video classification, object detection, pose\nestimation, speech recognition, etc., and has achieved significant progress in\nrecent years thanks to deep learning. However, understanding the world is not\nenough. An AI agent also needs to know how to talk, especially how to\ncommunicate with a human. While perception (vision, for example) is more common\nacross animal species, the use of complicated language is unique to humans and\nis one of the most important aspects of intelligence.\n  In this thesis, we focus on generating textual output given visual input. In\nChapter 3, we focus on generating the referring expression, a text description\nfor an object in the image so that a receiver can infer which object is being\ndescribed. We use a comprehension machine to directly guide the generated\nreferring expressions to be more discriminative. In Chapter 4, we introduce a\nmethod that encourages discriminability in image caption generation. We show\nthat more discriminative captioning models generate more descriptive captions.\nIn Chapter 5, we study how training objectives and sampling methods affect the\nmodels' ability to generate diverse captions. We find that a popular captioning\ntraining strategy will be detrimental to the diversity of generated captions.\nIn Chapter 6, we propose a model that can control the length of generated\ncaptions. By changing the desired length, one can influence the style and\ndescriptiveness of the captions. Finally, in Chapter 7, we rank/generate\ninformative image tags according to their information utility. The proposed\nmethod better matches what humans think are the most important tags for the\nimages.",
        "We introduce a new unsupervised learning problem: clustering wide-sense\nstationary ergodic stochastic processes. A covariance-based dissimilarity\nmeasure together with asymptotically consistent algorithms is designed for\nclustering offline and online datasets, respectively. We also suggest a formal\ncriterion on the efficiency of dissimilarity measures, and discuss of some\napproach to improve the efficiency of our clustering algorithms, when they are\napplied to cluster particular type of processes, such as self-similar processes\nwith wide-sense stationary ergodic increments. Clustering synthetic data and\nreal-world data are provided as examples of applications.",
        "Recently, Deep-Neural-Network (DNN) based edge prediction is progressing\nfast. Although the DNN based schemes outperform the traditional edge detectors,\nthey have much higher computational complexity. It could be that the DNN based\nedge detectors often adopt the neural net structures designed for high-level\ncomputer vision tasks, such as image segmentation and object recognition. Edge\ndetection is a rather local and simple job, the over-complicated architecture\nand massive parameters may be unnecessary. Therefore, we propose a traditional\nmethod inspired framework to produce good edges with minimal complexity. We\nsimplify the network architecture to include Feature Extractor, Enrichment, and\nSummarizer, which roughly correspond to gradient, low pass filter, and pixel\nconnection in the traditional edge detection schemes. The proposed structure\ncan effectively reduce the complexity and retain the edge prediction quality.\nOur TIN2 (Traditional Inspired Network) model has an accuracy higher than the\nrecent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.",
        "The real-world data usually exhibits heterogeneous properties such as\nmodalities, views, or resources, which brings some unique challenges wherein\nthe key is Heterogeneous Representation Learning (HRL) termed in this paper.\nThis brief survey covers the topic of HRL, centered around several major\nlearning settings and real-world applications. First of all, from the\nmathematical perspective, we present a unified learning framework which is able\nto model most existing learning settings with the heterogeneous inputs. After\nthat, we conduct a comprehensive discussion on the HRL framework by reviewing\nsome selected learning problems along with the mathematics perspectives,\nincluding multi-view learning, heterogeneous transfer learning, Learning using\nprivileged information and heterogeneous multi-task learning. For each learning\ntask, we also discuss some applications under these learning problems and\ninstantiates the terms in the mathematical framework. Finally, we highlight the\nchallenges that are less-touched in HRL and present future research directions.\nTo the best of our knowledge, there is no such framework to unify these\nheterogeneous problems, and this survey would benefit the community.",
        "Point-cloud registration (PCR) is an important task in various applications\nsuch as robotic manipulation, augmented and virtual reality, SLAM, etc. PCR is\nan optimization problem involving minimization over two different types of\ninterdependent variables: transformation parameters and point-to-point\ncorrespondences. Recent developments in deep-learning have produced\ncomputationally fast approaches for PCR. The loss functions that are optimized\nin these networks are based on the error in the transformation parameters. We\nhypothesize that these methods would perform significantly better if they\ncalculated their loss function using correspondence error instead of only using\nerror in transformation parameters. We define correspondence error as a metric\nbased on incorrectly matched point pairs. We provide a fundamental explanation\nfor why this is the case and test our hypothesis by modifying existing methods\nto use correspondence-based loss instead of transformation-based loss. These\nexperiments show that the modified networks converge faster and register more\naccurately even at larger misalignment when compared to the original networks.",
        "We study a reinforcement learning setting, where the state transition\nfunction is a convex combination of a stochastic continuous function and a\ndeterministic function. Such a setting generalizes the widely-studied\nstochastic state transition setting, namely the setting of deterministic policy\ngradient (DPG).\n  We firstly give a simple example to illustrate that the deterministic policy\ngradient may be infinite under deterministic state transitions, and introduce a\ntheoretical technique to prove the existence of the policy gradient in this\ngeneralized setting. Using this technique, we prove that the deterministic\npolicy gradient indeed exists for a certain set of discount factors, and\nfurther prove two conditions that guarantee the existence for all discount\nfactors. We then derive a closed form of the policy gradient whenever exists.\nFurthermore, to overcome the challenge of high sample complexity of DPG in this\nsetting, we propose the Generalized Deterministic Policy Gradient (GDPG)\nalgorithm. The main innovation of the algorithm is a new method of applying\nmodel-based techniques to the model-free algorithm, the deep deterministic\npolicy gradient algorithm (DDPG). GDPG optimize the long-term rewards of the\nmodel-based augmented MDP subject to a constraint that the long-rewards of the\nMDP is less than the original one.\n  We finally conduct extensive experiments comparing GDPG with state-of-the-art\nmethods and the direct model-based extension method of DDPG on several standard\ncontinuous control benchmarks. Results demonstrate that GDPG substantially\noutperforms DDPG, the model-based extension of DDPG and other baselines in\nterms of both convergence and long-term rewards in most environments.",
        "We present Convolutional Oriented Boundaries (COB), which produces multiscale\noriented contours and region hierarchies starting from generic image\nclassification Convolutional Neural Networks (CNNs). COB is computationally\nefficient, because it requires a single CNN forward pass for multi-scale\ncontour detection and it uses a novel sparse boundary representation for\nhierarchical segmentation; it gives a significant leap in performance over the\nstate-of-the-art, and it generalizes very well to unseen categories and\ndatasets. Particularly, we show that learning to estimate not only contour\nstrength but also orientation provides more accurate results. We perform\nextensive experiments for low-level applications on BSDS, PASCAL Context,\nPASCAL Segmentation, and NYUD to evaluate boundary detection performance,\nshowing that COB provides state-of-the-art contours and region hierarchies in\nall datasets. We also evaluate COB on high-level tasks when coupled with\nmultiple pipelines for object proposals, semantic contours, semantic\nsegmentation, and object detection on MS-COCO, SBD, and PASCAL; showing that\nCOB also improves the results for all tasks.",
        "Deep neural networks (DNNs) based methods have achieved great success in\nsingle image super-resolution (SISR). However, existing state-of-the-art SISR\ntechniques are designed like black boxes lacking transparency and\ninterpretability. Moreover, the improvement in visual quality is often at the\nprice of increased model complexity due to black-box design. In this paper, we\npresent and advocate an explainable approach toward SISR named model-guided\ndeep unfolding network (MoG-DUN). Targeting at breaking the coherence barrier,\nwe opt to work with a well-established image prior named nonlocal\nauto-regressive model and use it to guide our DNN design. By integrating deep\ndenoising and nonlocal regularization as trainable modules within a deep\nlearning framework, we can unfold the iterative process of model-based SISR\ninto a multi-stage concatenation of building blocks with three interconnected\nmodules (denoising, nonlocal-AR, and reconstruction). The design of all three\nmodules leverages the latest advances including dense/skip connections as well\nas fast nonlocal implementation. In addition to explainability, MoG-DUN is\naccurate (producing fewer aliasing artifacts), computationally efficient (with\nreduced model parameters), and versatile (capable of handling multiple\ndegradations). The superiority of the proposed MoG-DUN method to existing\nstate-of-the-art image SR methods including RCAN, SRMDNF, and SRFBN is\nsubstantiated by extensive experiments on several popular datasets and various\ndegradation scenarios.",
        "We present an image segmentation method that iteratively evolves a polygon.\nAt each iteration, the vertices of the polygon are displaced based on the local\nvalue of a 2D shift map that is inferred from the input image via an\nencoder-decoder architecture. The main training loss that is used is the\ndifference between the polygon shape and the ground truth segmentation mask.\nThe network employs a neural renderer to create the polygon from its vertices,\nmaking the process fully differentiable. We demonstrate that our method\noutperforms the state of the art segmentation networks and deep active contour\nsolutions in a variety of benchmarks, including medical imaging and aerial\nimages. Our code is available at https://github.com/shirgur/ACDRNet.",
        "Whether to attract viewer attention to a particular object, give the\nimpression of depth or simply reproduce human-like scene perception, shallow\ndepth of field images are used extensively by professional and amateur\nphotographers alike. To this end, high quality optical systems are used in DSLR\ncameras to focus on a specific depth plane while producing visually pleasing\nbokeh. We propose a physically motivated pipeline to mimic this effect from\nall-in-focus stereo images, typically retrieved by mobile cameras. It is\ncapable to change the focal plane a posteriori at 76 FPS on KITTI images to\nenable real-time applications. As our portmanteau suggests, SteReFo\ninterrelates stereo-based depth estimation and refocusing efficiently. In\ncontrast to other approaches, our pipeline is simultaneously fully\ndifferentiable, physically motivated, and agnostic to scene content. It also\nenables computational video focus tracking for moving objects in addition to\nrefocusing of static images. We evaluate our approach on the publicly available\ndatasets SceneFlow, KITTI, CityScapes and quantify the quality of architectural\nchanges.",
        "Emotion recognition is a classic field of research with a typical setup\nextracting features and feeding them through a classifier for prediction. On\nthe other hand, generative models jointly capture the distributional\nrelationship between emotions and the feature profiles. Relatively recently,\nGenerative Adversarial Networks (GANs) have surfaced as a new class of\ngenerative models and have shown considerable success in modeling distributions\nin the fields of computer vision and natural language understanding. In this\nwork, we experiment with variants of GAN architectures to generate feature\nvectors corresponding to an emotion in two ways: (i) A generator is trained\nwith samples from a mixture prior. Each mixture component corresponds to an\nemotional class and can be sampled to generate features from the corresponding\nemotion. (ii) A one-hot vector corresponding to an emotion can be explicitly\nused to generate the features. We perform analysis on such models and also\npropose different metrics used to measure the performance of the GAN models in\ntheir ability to generate realistic synthetic samples. Apart from evaluation on\na given dataset of interest, we perform a cross-corpus study where we study the\nutility of the synthetic samples as additional training data in low resource\nconditions.",
        "In this paper, we propose a novel subspace learning framework for one-class\nclassification. The proposed framework presents the problem in the form of\ngraph embedding. It includes the previously proposed subspace one-class\ntechniques as its special cases and provides further insight on what these\ntechniques actually optimize. The framework allows to incorporate other\nmeaningful optimization goals via the graph preserving criterion and reveals\nspectral and spectral regression-based solutions as alternatives to the\npreviously used gradient-based technique. We combine the subspace learning\nframework iteratively with Support Vector Data Description applied in the\nsubspace to formulate Graph-Embedded Subspace Support Vector Data Description.\nWe experimentally analyzed the performance of newly proposed different\nvariants. We demonstrate improved performance against the baselines and the\nrecently proposed subspace learning methods for one-class classification.",
        "Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors\nhave tremendous potential for fast autonomous or remote-controlled semantic\nscene analysis, e.g., for disaster examination. In this work, we propose a UAV\nsystem for real-time semantic inference and fusion of multiple sensor\nmodalities. Semantic segmentation of LiDAR scans and RGB images, as well as\nobject detection on RGB and thermal images, run online onboard the UAV computer\nusing lightweight CNN architectures and embedded inference accelerators. We\nfollow a late fusion approach where semantic information from multiple\nmodalities augments 3D point clouds and image segmentation masks while also\ngenerating an allocentric semantic map. Our system provides augmented semantic\nimages and point clouds with $\\approx\\,$9$\\,$Hz. We evaluate the integrated\nsystem in real-world experiments in an urban environment.",
        "Near-infrared gray images captured together with corresponding visible color\nimages have recently proven useful for image restoration and classification.\nThis paper introduces a new coloring method to add colors to near-infrared gray\nimages based on a contrast-preserving mapping model. A naive coloring method\ndirectly adds the colors from the visible color image to the near-infrared gray\nimage; however, this method results in an unrealistic image because of the\ndiscrepancies in brightness and image structure between the captured\nnear-infrared gray image and the visible color image. To solve the discrepancy\nproblem, first we present a new contrast-preserving mapping model to create a\nnew near-infrared gray image with a similar appearance in the luminance plane\nto the visible color image, while preserving the contrast and details of the\ncaptured near-infrared gray image. Then based on the proposed\ncontrast-preserving mapping model, we develop a method to derive realistic\ncolors that can be added to the newly created near-infrared gray image.\nExperimental results show that the proposed method can not only preserve the\nlocal contrasts and details of the captured near-infrared gray image, but\ntransfers the realistic colors from the visible color image to the newly\ncreated near-infrared gray image. Experimental results also show that the\nproposed approach can be applied to near-infrared denoising.",
        "Real-world videos contain many complex actions with inherent relationships\nbetween action classes. In this work, we propose an attention-based\narchitecture that models these action relationships for the task of temporal\naction localization in untrimmed videos. As opposed to previous works that\nleverage video-level co-occurrence of actions, we distinguish the relationships\nbetween actions that occur at the same time-step and actions that occur at\ndifferent time-steps (i.e. those which precede or follow each other). We define\nthese distinct relationships as action dependencies. We propose to improve\naction localization performance by modeling these action dependencies in a\nnovel attention-based Multi-Label Action Dependency (MLAD)layer. The MLAD layer\nconsists of two branches: a Co-occurrence Dependency Branch and a Temporal\nDependency Branch to model co-occurrence action dependencies and temporal\naction dependencies, respectively. We observe that existing metrics used for\nmulti-label classification do not explicitly measure how well action\ndependencies are modeled, therefore, we propose novel metrics that consider\nboth co-occurrence and temporal dependencies between action classes. Through\nempirical evaluation and extensive analysis, we show improved performance over\nstate-of-the-art methods on multi-label action localization\nbenchmarks(MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.",
        "Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the\nlargest egocentric video benchmark, offering a unique viewpoint on people's\ninteraction with objects, their attention, and even intention. In this paper,\nwe detail how this large-scale dataset was captured by 32 participants in their\nnative kitchen environments, and densely annotated with actions and object\ninteractions. Our videos depict nonscripted daily activities, as recording is\nstarted every time a participant entered their kitchen. Recording took place in\n4 countries by participants belonging to 10 different nationalities, resulting\nin highly diverse kitchen habits and cooking styles. Our dataset features 55\nhours of video consisting of 11.5M frames, which we densely labelled for a\ntotal of 39.6K action segments and 454.2K object bounding boxes. Our annotation\nis unique in that we had the participants narrate their own videos after\nrecording, thus reflecting true intention, and we crowd-sourced ground-truths\nbased on these. We describe our object, action and. anticipation challenges,\nand evaluate several baselines over two test splits, seen and unseen kitchens.\nWe introduce new baselines that highlight the multimodal nature of the dataset\nand the importance of explicit temporal modelling to discriminate fine-grained\nactions e.g. 'closing a tap' from 'opening' it up.",
        "Due to an increase in the number of image achieves, Content-Based Image\nRetrieval (CBIR) has gained attention for research community of computer\nvision. The image visual contents are represented in a feature space in the\nform of numerical values that is considered as a feature vector of image.\nImages belonging to different classes may contain the common visuals and shapes\nthat can result in the closeness of computed feature space of two different\nimages belonging to separate classes. Due to this reason, feature extraction\nand image representation is selected with appropriate features as it directly\naffects the performance of image retrieval system. The commonly used visual\nfeatures are image spatial layout, color, texture and shape. Image feature\nspace is combined to achieve the discriminating ability that is not possible to\nachieve when the features are used separately. Due to this reason, in this\npaper, we aim to explore the low-level feature combination that are based on\ncolor and shape features. We selected color moments and color histogram to\nrepresent color while shape is represented by using invariant moments. We\nselected this combination, as these features are reported intuitive, compact\nand robust for image representation. We evaluated the performance of our\nproposed research by using the Corel, Coil and Ground Truth (GT) image\ndatasets. We evaluated the proposed low-level feature fusion by calculating the\nprecision, recall and time required for feature extraction. The precision,\nrecall and feature extraction values obtained from the proposed low-level\nfeature fusion outperforms the existing research of CBIR.",
        "Deep learning based image segmentation methods have achieved great success,\neven having human-level accuracy in some applications. However, due to the\nblack box nature of deep learning, the best method may fail in some situations.\nThus predicting segmentation quality without ground truth would be very crucial\nespecially in clinical practice. Recently, people proposed to train neural\nnetworks to estimate the quality score by regression. Although it can achieve\npromising prediction accuracy, the network suffers robustness problem, e.g. it\nis vulnerable to adversarial attacks. In this paper, we propose to alleviate\nthis problem by utilizing the difference between the input image and the\nreconstructed image, which is conditioned on the segmentation to be assessed,\nto lower the chance to overfit to the undesired image features from the\noriginal input image, and thus to increase the robustness. Results on ACDC17\ndataset demonstrated our method is promising.",
        "Predicting the link between two nodes is a fundamental problem for graph data\nanalytics. In attributed graphs, both the structure and attribute information\ncan be utilized for link prediction. Most existing studies focus on\ntransductive link prediction where both nodes are already in the graph.\nHowever, many real-world applications require inductive prediction for new\nnodes having only attribute information. It is more challenging since the new\nnodes do not have structure information and cannot be seen during the model\ntraining. To solve this problem, we propose a model called DEAL, which consists\nof three components: two node embedding encoders and one alignment mechanism.\nThe two encoders aim to output the attribute-oriented node embedding and the\nstructure-oriented node embedding, and the alignment mechanism aligns the two\ntypes of embeddings to build the connections between the attributes and links.\nOur model DEAL is versatile in the sense that it works for both inductive and\ntransductive link prediction. Extensive experiments on several benchmark\ndatasets show that our proposed model significantly outperforms existing\ninductive link prediction methods, and also outperforms the state-of-the-art\nmethods on transductive link prediction.",
        "This work investigates fundamental questions related to learning features in\nconvolutional neural networks (CNN). Empirical findings across multiple\narchitectures such as VGG, ResNet, Inception, DenseNet and MobileNet indicate\nthat weights near the center of a filter are larger than weights on the\noutside. Current regularization schemes violate this principle. Thus, we\nintroduce Locality-promoting Regularization (LOCO-Reg), which yields accuracy\ngains across multiple architectures and datasets. We also show theoretically\nthat the empirical finding is a consequence of maximizing feature cohesion\nunder the assumption of spatial locality.",
        "A recently proposed methodology called the Horizontal Visibility Graph (HVG)\n[Luque {\\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes a\ngeometrical simplification of the well known Visibility Graph algorithm [Lacasa\n{\\it et al.\\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used to\nstudy the distinction between deterministic and stochastic components in time\nseries [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].\nSpecifically, the authors propose that the node degree distribution of these\nprocesses follows an exponential functional of the form $P(\\kappa)\\sim\n\\exp(-\\lambda~\\kappa)$, in which $\\kappa$ is the node degree and $\\lambda$ is a\npositive parameter able to distinguish between deterministic (chaotic) and\nstochastic (uncorrelated and correlated) dynamics. In this work, we investigate\nthe characteristics of the node degree distributions constructed by using HVG,\nfor time series corresponding to $28$ chaotic maps and $3$ different stochastic\nprocesses. We thoroughly study the methodology proposed by Lacasa and Toral\nfinding several cases for which their hypothesis is not valid. We propose a\nmethodology that uses the HVG together with Information Theory quantifiers. An\nextensive and careful analysis of the node degree distributions obtained by\napplying HVG allow us to conclude that the Fisher-Shannon information plane is\na remarkable tool able to graphically represent the different nature,\ndeterministic or stochastic, of the systems under study.",
        "Partial differential equations (PDEs) are indispensable for modeling many\nphysical phenomena and also commonly used for solving image processing tasks.\nIn the latter area, PDE-based approaches interpret image data as\ndiscretizations of multivariate functions and the output of image processing\nalgorithms as solutions to certain PDEs. Posing image processing problems in\nthe infinite dimensional setting provides powerful tools for their analysis and\nsolution. Over the last few decades, the reinterpretation of classical image\nprocessing problems through the PDE lens has been creating multiple celebrated\napproaches that benefit a vast area of tasks including image segmentation,\ndenoising, registration, and reconstruction.\n  In this paper, we establish a new PDE-interpretation of a class of deep\nconvolutional neural networks (CNN) that are commonly used to learn from\nspeech, image, and video data. Our interpretation includes convolution residual\nneural networks (ResNet), which are among the most promising approaches for\ntasks such as image classification having improved the state-of-the-art\nperformance in prestigious benchmark challenges. Despite their recent\nsuccesses, deep ResNets still face some critical challenges associated with\ntheir design, immense computational costs and memory requirements, and lack of\nunderstanding of their reasoning.\n  Guided by well-established PDE theory, we derive three new ResNet\narchitectures that fall into two new classes: parabolic and hyperbolic CNNs. We\ndemonstrate how PDE theory can provide new insights and algorithms for deep\nlearning and demonstrate the competitiveness of three new CNN architectures\nusing numerical experiments.",
        "Cross-network recommender systems use auxiliary information from multiple\nsource networks to create holistic user profiles and improve recommendations in\na target network. However, we find two major limitations in existing\ncross-network solutions that reduce overall recommender performance. Existing\nmodels (1) fail to capture complex non-linear relationships in user\ninteractions, and (2) are designed for offline settings hence, not updated\nonline with incoming interactions to capture the dynamics in the recommender\nenvironment. We propose a novel multi-layered Long Short-Term Memory (LSTM)\nnetwork based online solution to mitigate these issues. The proposed model\ncontains three main extensions to the standard LSTM: First, an attention gated\nmechanism to capture long-term user preference changes. Second, a higher order\ninteraction layer to alleviate data sparsity. Third, time aware LSTM cell gates\nto capture irregular time intervals between user interactions. We illustrate\nour solution using auxiliary information from Twitter and Google Plus to\nimprove recommendations on YouTube. Extensive experiments show that the\nproposed model consistently outperforms state-of-the-art in terms of accuracy,\ndiversity and novelty.",
        "The technique of distillation helps transform cumbersome neural network into\ncompact network so that the model can be deployed on alternative hardware\ndevices. The main advantages of distillation based approaches include simple\ntraining process, supported by most off-the-shelf deep learning softwares and\nno special requirement of hardwares. In this paper, we propose a guideline to\ndistill the architecture and knowledge of pre-trained standard CNNs\nsimultaneously. We first make a quantitative analysis of the baseline network,\nincluding computational cost and storage overhead in different components. And\nthen, according to the analysis results, optional strategies can be adopted to\nthe compression of fully-connected layers. For vanilla convolution layers, the\nproposed parsimonious convolution (ParConv) block only consisting of depthwise\nseparable convolution and pointwise convolution is used as a direct replacement\nwithout other adjustments such as the widths and depths in the network.\nFinally, the knowledge distillation with multiple losses is adopted to improve\nperformance of the compact CNN. The proposed algorithm is first verified on\noffline handwritten Chinese text recognition (HCTR) where the CNNs are\ncharacterized by tens of thousands of output nodes and trained by hundreds of\nmillions of training samples. Compared with the CNN in the state-of-the-art\nsystem, our proposed joint architecture and knowledge distillation can reduce\nthe computational cost by >10x and model size by >8x with negligible accuracy\nloss. And then, by conducting experiments on one of the most popular data sets:\nMNIST, we demonstrate the proposed approach can also be successfully applied on\nmainstream backbone networks.",
        "A cornerstone of geometric reconstruction, rotation averaging seeks the set\nof absolute rotations that optimally explains a set of measured relative\norientations between them. In spite of being an integral part of bundle\nadjustment and structure-from-motion, averaging rotations is both a non-convex\nand high-dimensional optimization problem. In this paper, we address it from a\nmaximum likelihood estimation standpoint and make a twofold contribution.\nFirstly, we set forth a novel initialization-free primal-dual method which we\nshow empirically to converge to the global optimum. Further, we derive what is\nto our knowledge, the first optimal closed-form solution for rotation averaging\nin cycle graphs and contextualize this result within spectral graph theory. Our\nproposed methods achieve a significant gain both in precision and performance.",
        "We present a novel deep generative model based on non i.i.d. variational\nautoencoders that captures global dependencies among observations in a fully\nunsupervised fashion. In contrast to the recent semi-supervised alternatives\nfor global modeling in deep generative models, our approach combines a mixture\nmodel in the local or data-dependent space and a global Gaussian latent\nvariable, which lead us to obtain three particular insights. First, the induced\nlatent global space captures interpretable disentangled representations with no\nuser-defined regularization in the evidence lower bound (as in $\\beta$-VAE and\nits generalizations). Second, we show that the model performs domain alignment\nto find correlations and interpolate between different databases. Finally, we\nstudy the ability of the global space to discriminate between groups of\nobservations with non-trivial underlying structures, such as face images with\nshared attributes or defined sequences of digits images.",
        "Differentiable image sampling in the form of backward warping has seen broad\nadoption in tasks like depth estimation and optical flow prediction. In\ncontrast, how to perform forward warping has seen less attention, partly due to\nadditional challenges such as resolving the conflict of mapping multiple pixels\nto the same target location in a differentiable way. We propose softmax\nsplatting to address this paradigm shift and show its effectiveness on the\napplication of frame interpolation. Specifically, given two input frames, we\nforward-warp the frames and their feature pyramid representations based on an\noptical flow estimate using softmax splatting. In doing so, the softmax\nsplatting seamlessly handles cases where multiple source pixels map to the same\ntarget location. We then use a synthesis network to predict the interpolation\nresult from the warped representations. Our softmax splatting allows us to not\nonly interpolate frames at an arbitrary time but also to fine tune the feature\npyramid and the optical flow. We show that our synthesis approach, empowered by\nsoftmax splatting, achieves new state-of-the-art results for video frame\ninterpolation.",
        "In this paper, we propose a novel model for time series prediction in which\ndifference-attention LSTM model and error-correction LSTM model are\nrespectively employed and combined in a cascade way. While difference-attention\nLSTM model introduces a difference feature to perform attention in traditional\nLSTM to focus on the obvious changes in time series. Error-correction LSTM\nmodel refines the prediction error of difference-attention LSTM model to\nfurther improve the prediction accuracy. Finally, we design a training strategy\nto jointly train the both models simultaneously. With additional difference\nfeatures and new principle learning framework, our model can improve the\nprediction accuracy in time series. Experiments on various time series are\nconducted to demonstrate the effectiveness of our method.",
        "Recently, 3D understanding research sheds light on extracting features from\npoint cloud directly, which requires effective shape pattern description of\npoint clouds. Inspired by the outstanding 2D shape descriptor SIFT, we design a\nmodule called PointSIFT that encodes information of different orientations and\nis adaptive to scale of shape. Specifically, an orientation-encoding unit is\ndesigned to describe eight crucial orientations, and multi-scale representation\nis achieved by stacking several orientation-encoding units. PointSIFT module\ncan be integrated into various PointNet-based architecture to improve the\nrepresentation ability. Extensive experiments show our PointSIFT-based\nframework outperforms state-of-the-art method on standard benchmark datasets.\nThe code and trained model will be published accompanied by this paper.",
        "When tasks change over time, meta-transfer learning seeks to improve the\nefficiency of learning a new task via both meta-learning and transfer-learning.\nWhile the standard attention has been effective in a variety of settings, we\nquestion its effectiveness in improving meta-transfer learning since the tasks\nbeing learned are dynamic and the amount of context can be substantially\nsmaller. In this paper, using a recently proposed meta-transfer learning model,\nSequential Neural Processes (SNP), we first empirically show that it suffers\nfrom a similar underfitting problem observed in the functions inferred by\nNeural Processes. However, we further demonstrate that unlike the meta-learning\nsetting, the standard attention mechanisms are not effective in meta-transfer\nsetting. To resolve, we propose a new attention mechanism, Recurrent Memory\nReconstruction (RMR), and demonstrate that providing an imaginary context that\nis recurrently updated and reconstructed with interaction is crucial in\nachieving effective attention for meta-transfer learning. Furthermore,\nincorporating RMR into SNP, we propose Attentive Sequential Neural\nProcesses-RMR (ASNP-RMR) and demonstrate in various tasks that ASNP-RMR\nsignificantly outperforms the baselines.",
        "This paper reports on a novel nonparametric rigid point cloud registration\nframework that jointly integrates geometric and semantic measurements such as\ncolor or semantic labels into the alignment process and does not require\nexplicit data association. The point clouds are represented as nonparametric\nfunctions in a reproducible kernel Hilbert space. The alignment problem is\nformulated as maximizing the inner product between two functions, essentially a\nsum of weighted kernels, each of which exploits the local geometric and\nsemantic features. As a result of the continuous models, analytical gradients\ncan be computed, and a local solution can be obtained by optimization over the\nrigid body transformation group. Besides, we present a new point cloud\nalignment metric that is intrinsic to the proposed framework and takes into\naccount geometric and semantic information. The evaluations using publicly\navailable stereo and RGB-D datasets show that the proposed method outperforms\nstate-of-the-art outdoor and indoor frame-to-frame registration methods. An\nopen-source GPU implementation is also provided.",
        "Remaining Useful Life (RUL) of an equipment or one of its components is\ndefined as the time left until the equipment or component reaches its end of\nuseful life. Accurate RUL estimation is exceptionally beneficial to Predictive\nMaintenance, and Prognostics and Health Management (PHM). Data driven\napproaches which leverage the power of algorithms for RUL estimation using\nsensor and operational time series data are gaining popularity. Existing\nalgorithms, such as linear regression, Convolutional Neural Network (CNN),\nHidden Markov Models (HMMs), and Long Short-Term Memory (LSTM), have their own\nlimitations for the RUL estimation task. In this work, we propose a novel\nFunctional Data Analysis (FDA) method called functional Multilayer Perceptron\n(functional MLP) for RUL estimation. Functional MLP treats time series data\nfrom multiple equipment as a sample of random continuous processes over time.\nFDA explicitly incorporates both the correlations within the same equipment and\nthe random variations across different equipment's sensor time series into the\nmodel. FDA also has the benefit of allowing the relationship between RUL and\nsensor variables to vary over time. We implement functional MLP on the\nbenchmark NASA C-MAPSS data and evaluate the performance using two\npopularly-used metrics. Results show the superiority of our algorithm over all\nthe other state-of-the-art methods.",
        "Deep convolutional neural networks have shown outstanding performance in\nmedical image segmentation tasks. The usual problem when training supervised\ndeep learning methods is the lack of labeled data which is time-consuming and\ncostly to obtain. In this paper, we propose a novel uncertainty-guided\nsemi-supervised learning based on a student-teacher approach for training the\nsegmentation network using limited labeled samples and a large number of\nunlabeled images. First, a teacher segmentation model is trained from the\nlabeled samples using Bayesian deep learning. The trained model is used to\ngenerate soft segmentation labels and uncertainty maps for the unlabeled set.\nThe student model is then updated using the softly segmented samples and the\ncorresponding pixel-wise confidence of the segmentation quality estimated from\nthe uncertainty of the teacher model using a newly designed loss function.\nExperimental results on a retinal layer segmentation task show that the\nproposed method improves the segmentation performance in comparison to the\nfully supervised approach and is on par with the expert annotator. The proposed\nsemi-supervised segmentation framework is a key contribution and applicable for\nbiomedical image segmentation across various imaging modalities where access to\nannotated medical images is challenging",
        "Restricted Boltzmann Machines (RBMs) are general unsupervised learning\ndevices to ascertain generative models of data distributions. RBMs are often\ntrained using the Contrastive Divergence learning algorithm (CD), an\napproximation to the gradient of the data log-likelihood. A simple\nreconstruction error is often used to decide whether the approximation provided\nby the CD algorithm is good enough, though several authors (Schulz et al.,\n2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of\nthis procedure. However, not many alternatives to the reconstruction error have\nbeen used in the literature. In this manuscript we investigate simple\nalternatives to the reconstruction error in order to detect as soon as possible\nthe decrease in the log-likelihood during learning.",
        "Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.",
        "Recent advances in machine learning (ML) and computer vision tools have\nenabled applications in a wide variety of arenas such as financial analytics,\nmedical diagnostics, and even within the Department of Defense. However, their\nwidespread implementation in real-world use cases poses several challenges: (1)\nmany applications are highly specialized, and hence operate in a \\emph{sparse\ndata} domain; (2) ML tools are sensitive to their training sets and typically\nrequire cumbersome, labor-intensive data collection and data labelling\nprocesses; and (3) ML tools can be extremely \"black box,\" offering users little\nto no insight into the decision-making process or how new data might affect\nprediction performance. To address these challenges, we have designed and\ndeveloped Data Augmentation from Proficient Pre-Training of Robust Generative\nAdversarial Networks (DAPPER GAN), an ML analytics support tool that\nautomatically generates novel views of training images in order to improve\ndownstream classifier performance. DAPPER GAN leverages high-fidelity\nembeddings generated by a StyleGAN2 model (trained on the LSUN cars dataset) to\ncreate novel imagery for previously unseen classes. We experimentally evaluate\nthis technique on the Stanford Cars dataset, demonstrating improved vehicle\nmake and model classification accuracy and reduced requirements for real data\nusing our GAN based data augmentation framework. The method's validity was\nsupported through an analysis of classifier performance on both augmented and\nnon-augmented datasets, achieving comparable or better accuracy with up to 30\\%\nless real data across visually similar classes. To support this method, we\ndeveloped a novel augmentation method that can manipulate semantically\nmeaningful dimensions (e.g., orientation) of the target object in the embedding\nspace.",
        "This paper proposes the Mesh Neural Network (MNN), a novel architecture which\nallows neurons to be connected in any topology, to efficiently route\ninformation. In MNNs, information is propagated between neurons throughout a\nstate transition function. State and error gradients are then directly computed\nfrom state updates without backward computation. The MNN architecture and the\nerror propagation schema is formalized and derived in tensor algebra. The\nproposed computational model can fully supply a gradient descent process, and\nis potentially suitable for very large scale sparse NNs, due to its\nexpressivity and training efficiency, with respect to NNs based on\nback-propagation and computational graphs.",
        "We consider the problem of object recognition in 3D using an ensemble of\nattribute-based classifiers. We propose two new concepts to improve\nclassification in practical situations, and show their implementation in an\napproach implemented for recognition from point-cloud data. First, the viewing\nconditions can have a strong influence on classification performance. We study\nthe impact of the distance between the camera and the object and propose an\napproach to fuse multiple attribute classifiers, which incorporates distance\ninto the decision making. Second, lack of representative training samples often\nmakes it difficult to learn the optimal threshold value for best positive and\nnegative detection rate. We address this issue, by setting in our attribute\nclassifiers instead of just one threshold value, two threshold values to\ndistinguish a positive, a negative and an uncertainty class, and we prove the\ntheoretical correctness of this approach. Empirical studies demonstrate the\neffectiveness and feasibility of the proposed concepts.",
        "It is well-known in image processing that computational cost increases\nrapidly with the number and dimensions of the images to be processed. Several\nfields, such as medical imaging, routinely use numerous very large images,\nwhich might also be 3D and/or captured at several frequency bands, all adding\nto the computational expense. Multiresolution analysis is a method of\nincreasing the efficiency of the segmentation process. One multiresolution\napproach is the coarse-to-fine segmentation strategy, whereby the segmentation\nstarts at a coarse resolution and is then fine-tuned during subsequent steps.\nThe starting resolution for segmentation is generally selected arbitrarily with\nno clear selection criteria. The research reported in this paper showed that\nstarting from different resolutions for image segmentation results in different\naccuracies and computational times, even for images of the same category\n(depicting similar scenes or objects). An automated method for resolution\nselection for an input image would thus be beneficial. This paper introduces a\nframework for the automated selection of the best resolution for image\nsegmentation. We propose a measure for defining the best resolution based on\nuser/system criteria, offering a trade-off between accuracy and computation\ntime. A learning approach is then introduced for the selection of the\nresolution, whereby extracted image features are mapped to the previously\ndetermined best resolution. In the learning process, class (i.e., resolution)\ndistribution is generally imbalanced, making effective learning from the data\ndifficult. Experiments conducted with three datasets using two different\nsegmentation algorithms show that the resolutions selected through learning\nenable much faster segmentation than the original ones, while retaining at\nleast the original accuracy.",
        "Most existing face image Super-Resolution (SR) methods assume that the\nLow-Resolution (LR) images were artificially downsampled from High-Resolution\n(HR) images with bicubic interpolation. This operation changes the natural\nimage characteristics and reduces noise. Hence, SR methods trained on such data\nmost often fail to produce good results when applied to real LR images. To\nsolve this problem, we propose a novel framework for generation of realistic\nLR/HR training pairs. Our framework estimates realistic blur kernels, noise\ndistributions, and JPEG compression artifacts to generate LR images with\nsimilar image characteristics as the ones in the source domain. This allows us\nto train a SR model using high quality face images as Ground-Truth (GT). For\nbetter perceptual quality we use a Generative Adversarial Network (GAN) based\nSR model where we have exchanged the commonly used VGG-loss [24] with\nLPIPS-loss [52]. Experimental results on both real and artificially corrupted\nface images show that our method results in more detailed reconstructions with\nless noise compared to existing State-of-the-Art (SoTA) methods. In addition,\nwe show that the traditional non-reference Image Quality Assessment (IQA)\nmethods fail to capture this improvement and demonstrate that the more recent\nNIMA metric [16] correlates better with human perception via Mean Opinion Rank\n(MOR).",
        "Video super-resolution (VSR) and frame interpolation (FI) are traditional\ncomputer vision problems, and the performance have been improving by\nincorporating deep learning recently. In this paper, we investigate the problem\nof jointly upsampling videos both in space and time, which is becoming more\nimportant with advances in display systems. One solution for this is to run VSR\nand FI, one by one, independently. This is highly inefficient as heavy deep\nneural networks (DNN) are involved in each solution. To this end, we propose an\nend-to-end DNN framework for the space-time video upsampling by efficiently\nmerging VSR and FI into a joint framework. In our framework, a novel weighting\nscheme is proposed to fuse input frames effectively without explicit motion\ncompensation for efficient processing of videos. The results show better\nresults both quantitatively and qualitatively, while reducing the computation\ntime (x7 faster) and the number of parameters (30%) compared to baselines.",
        "Despite recent advances, goal-directed generation of structured discrete data\nremains challenging. For problems such as program synthesis (generating source\ncode) and materials design (generating molecules), finding examples which\nsatisfy desired constraints or exhibit desired properties is difficult. In\npractice, expensive heuristic search or reinforcement learning algorithms are\noften employed. In this paper we investigate the use of conditional generative\nmodels which directly attack this inverse problem, by modeling the distribution\nof discrete structures given properties of interest. Unfortunately, maximum\nlikelihood training of such models often fails with the samples from the\ngenerative model inadequately respecting the input properties. To address this,\nwe introduce a novel approach to directly optimize a reinforcement learning\nobjective, maximizing an expected reward. We avoid high-variance score-function\nestimators that would otherwise be required by sampling from an approximation\nto the normalized rewards, allowing simple Monte Carlo estimation of model\ngradients. We test our methodology on two tasks: generating molecules with\nuser-defined properties and identifying short python expressions which evaluate\nto a given target value. In both cases, we find improvements over maximum\nlikelihood estimation and other baselines.",
        "We propose a new approach to value function approximation which combines\nlinear temporal difference reinforcement learning with subspace identification.\nIn practical applications, reinforcement learning (RL) is complicated by the\nfact that state is either high-dimensional or partially observable. Therefore,\nRL methods are designed to work with features of state rather than state\nitself, and the success or failure of learning is often determined by the\nsuitability of the selected features. By comparison, subspace identification\n(SSID) methods are designed to select a feature set which preserves as much\ninformation as possible about state. In this paper we connect the two\napproaches, looking at the problem of reinforcement learning with a large set\nof features, each of which may only be marginally useful for value function\napproximation. We introduce a new algorithm for this situation, called\nPredictive State Temporal Difference (PSTD) learning. As in SSID for predictive\nstate representations, PSTD finds a linear compression operator that projects a\nlarge set of features down to a small set that preserves the maximum amount of\npredictive information. As in RL, PSTD then uses a Bellman recursion to\nestimate a value function. We discuss the connection between PSTD and prior\napproaches in RL and SSID. We prove that PSTD is statistically consistent,\nperform several experiments that illustrate its properties, and demonstrate its\npotential on a difficult optimal stopping problem.",
        "Pedestrian attribute recognition has been an emerging research topic in the\narea of video surveillance. To predict the existence of a particular attribute,\nit is demanded to localize the regions related to the attribute. However, in\nthis task, the region annotations are not available. How to carve out these\nattribute-related regions remains challenging. Existing methods applied\nattribute-agnostic visual attention or heuristic body-part localization\nmechanisms to enhance the local feature representations, while neglecting to\nemploy attributes to define local feature areas. We propose a flexible\nAttribute Localization Module (ALM) to adaptively discover the most\ndiscriminative regions and learns the regional features for each attribute at\nmultiple levels. Moreover, a feature pyramid architecture is also introduced to\nenhance the attribute-specific localization at low-levels with high-level\nsemantic guidance. The proposed framework does not require additional region\nannotations and can be trained end-to-end with multi-level deep supervision.\nExtensive experiments show that the proposed method achieves state-of-the-art\nresults on three pedestrian attribute datasets, including PETA, RAP, and\nPA-100K.",
        "In this paper we test the use of a deep learning approach to automatically\ncount Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We\nuse a dataset of manually labelled imagery provided by the British Antarctic\nSurvey to train and develop our methods. We employ a U-Net architecture,\ndesigned for image segmentation, to simultaneously classify and localise\npotential albatrosses. We aid training with the use of the Focal Loss\ncriterion, to deal with extreme class imbalance in the dataset. Initial results\nachieve peak precision and recall values of approximately 80%. Finally we\nassess the model's performance in relation to inter-observer variation, by\ncomparing errors against an image labelled by multiple observers. We conclude\nmodel accuracy falls within the range of human counters. We hope that the\nmethods will streamline the analysis of VHR satellite images, enabling more\nfrequent monitoring of a species which is of high conservation concern.",
        "In this paper, we propose a similarity-aware fusion network (SAFNet) to\nadaptively fuse 2D images and 3D point clouds for 3D semantic segmentation.\nExisting fusion-based methods achieve remarkable performances by integrating\ninformation from multiple modalities. However, they heavily rely on the\ncorrespondence between 2D pixels and 3D points by projection and can only\nperform the information fusion in a fixed manner, and thus their performances\ncannot be easily migrated to a more realistic scenario where the collected data\noften lack strict pair-wise features for prediction. To address this, we employ\na late fusion strategy where we first learn the geometric and contextual\nsimilarities between the input and back-projected (from 2D pixels) point clouds\nand utilize them to guide the fusion of two modalities to further exploit\ncomplementary information. Specifically, we employ a geometric similarity\nmodule (GSM) to directly compare the spatial coordinate distributions of\npair-wise 3D neighborhoods, and a contextual similarity module (CSM) to\naggregate and compare spatial contextual information of corresponding central\npoints. The two proposed modules can effectively measure how much image\nfeatures can help predictions, enabling the network to adaptively adjust the\ncontributions of two modalities to the final prediction of each point.\nExperimental results on the ScanNetV2 benchmark demonstrate that SAFNet\nsignificantly outperforms existing state-of-the-art fusion-based approaches\nacross various data integrity.",
        "We revisit the structure learning problem for dynamic Bayesian networks and\npropose a method that simultaneously estimates contemporaneous (intra-slice)\nand time-lagged (inter-slice) relationships between variables in a time-series.\nOur approach is score-based, and revolves around minimizing a penalized loss\nsubject to an acyclicity constraint. To solve this problem, we leverage a\nrecent algebraic result characterizing the acyclicity constraint as a smooth\nequality constraint. The resulting algorithm, which we call DYNOTEARS,\noutperforms other methods on simulated data, especially in high-dimensions as\nthe number of variables increases. We also apply this algorithm on real\ndatasets from two different domains, finance and molecular biology, and analyze\nthe resulting output. Compared to state-of-the-art methods for learning dynamic\nBayesian networks, our method is both scalable and accurate on real data. The\nsimple formulation and competitive performance of our method make it suitable\nfor a variety of problems where one seeks to learn connections between\nvariables across time.",
        "Multivariate time series naturally exist in many fields, like energy,\nbioinformatics, signal processing, and finance. Most of these applications need\nto be able to compare these structured data. In this context, dynamic time\nwarping (DTW) is probably the most common comparison measure. However, not much\nresearch effort has been put into improving it by learning. In this paper, we\npropose a novel method for learning similarities based on DTW, in order to\nimprove time series classification. Making use of the uniform stability\nframework, we provide the first theoretical guarantees in the form of a\ngeneralization bound for linear classification. The experimental study shows\nthat the proposed approach is efficient, while yielding sparse classifiers.",
        "We present a transformation-grounded image generation network for novel 3D\nview synthesis from a single image. Instead of taking a 'blank slate' approach,\nwe first explicitly infer the parts of the geometry visible both in the input\nand novel views and then re-cast the remaining synthesis problem as image\ncompletion. Specifically, we both predict a flow to move the pixels from the\ninput to the novel view along with a novel visibility map that helps deal with\nocculsion/disocculsion. Next, conditioned on those intermediate results, we\nhallucinate (infer) parts of the object invisible in the input image. In\naddition to the new network structure, training with a combination of\nadversarial and perceptual loss results in a reduction in common artifacts of\nnovel view synthesis such as distortions and holes, while successfully\ngenerating high frequency details and preserving visual aspects of the input\nimage. We evaluate our approach on a wide range of synthetic and real examples.\nBoth qualitative and quantitative results show our method achieves\nsignificantly better results compared to existing methods.",
        "The seamless illumination integration between a foreground object and a\nbackground scene is an important but challenging task in computer vision and\naugmented reality community. However, to our knowledge, there is no publicly\navailable high-quality dataset that meets the illumination seamless integration\ntask, which greatly hinders the development of this research direction. To this\nend, we apply a physically-based rendering method to create a large-scale,\nhigh-quality dataset, named IH dataset, which provides rich illumination\ninformation for seamless illumination integration task. In addition, we propose\na deep learning-based SI-GAN method, a multi-task collaborative network, which\nmakes full use of the multi-scale attention mechanism and adversarial learning\nstrategy to directly infer mapping relationship between the inserted foreground\nobject and corresponding background environment, and edit object illumination\naccording to the proposed illumination exchange mechanism in parallel network.\nBy this means, we can achieve the seamless illumination integration without\nexplicit estimation of 3D geometric information. Comprehensive experiments on\nboth our dataset and real-world images collected from the Internet show that\nour proposed SI-GAN provides a practical and effective solution for image-based\nobject illumination editing, and validate the superiority of our method against\nstate-of-the-art methods.",
        "In Reinforcement Learning (RL), Convolutional Neural Networks(CNNs) have been\nsuccessfully applied as function approximators in Deep Q-Learning algorithms,\nwhich seek to learn action-value functions and policies in various\nenvironments. However, to date, there has been little work on the learning of\nsymmetry-transformation equivariant representations of the input environment\nstate. In this paper, we propose the use of Equivariant CNNs to train RL agents\nand study their inductive bias for transformation equivariant Q-value\napproximation. We demonstrate that equivariant architectures can dramatically\nenhance the performance and sample efficiency of RL agents in a highly\nsymmetric environment while requiring fewer parameters. Additionally, we show\nthat they are robust to changes in the environment caused by affine\ntransformations.",
        "In this paper, we present an efficient and high-performance neural\narchitecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which\ndeeply integrates both 3D voxel-based and point-based self-attention\ncomputation to learn more discriminative features from 3D data. Specifically,\nwe conduct multi-head self-attention (MSA) computation in voxels to obtain the\nefficient learning pattern and the coarse-grained local features while\nperforming self-attention in points to provide finer-grained information about\nthe global context. In addition, to reduce the cost of MSA computation with\nhigh efficiency, we design a cyclic shifted boxing scheme by limiting the MSA\ncomputation to non-overlapping local box and also preserving cross-box\nconnection. Evaluated on classification benchmark, our method not only achieves\nstate-of-the-art accuracy of 94.0% (no voting) but outperforms previous\nTransformer-based models with 7x measured speedup on average. On part and\nsemantic segmentation, our model also obtains strong performance(86.5% and\n68.2% mIoU, respectively). For 3D object detection task, we replace the\nprimitives in Frustrum PointNet with PVT block and achieve an improvement of\n8.6% AP.",
        "Current neural networks architectures are many times harder to train because\nof the increasing size and complexity of the used datasets. Our objective is to\ndesign more efficient training algorithms utilizing causal relationships\ninferred from neural networks. The transfer entropy (TE) was initially\nintroduced as an information transfer measure used to quantify the statistical\ncoherence between events (time series). Later, it was related to causality,\neven if they are not the same. There are only few papers reporting applications\nof causality or TE in neural networks. Our contribution is an\ninformation-theoretical method for analyzing information transfer between the\nnodes of feedforward neural networks. The information transfer is measured by\nthe TE of feedback neural connections. Intuitively, TE measures the relevance\nof a connection in the network and the feedback amplifies this connection. We\nintroduce a backpropagation type training algorithm that uses TE feedback\nconnections to improve its performance.",
        "This paper proposes a new optimization objective for value-based deep\nreinforcement learning. We extend conventional Deep Q-Networks (DQNs) by adding\na model-learning component yielding a transcoder network. The prediction errors\nfor the model are included in the basic DQN loss as additional regularizers.\nThis augmented objective leads to a richer training signal that provides\nfeedback at every time step. Moreover, because learning an environment model\nshares a common structure with the RL problem, we hypothesize that the\nresulting objective improves both sample efficiency and performance. We\nempirically confirm our hypothesis on a range of 20 games from the Atari\nbenchmark attaining superior results over vanilla DQN without model-based\nregularization.",
        "The balance of exploration and exploitation plays a crucial role in\naccelerating reinforcement learning (RL). To deploy an RL agent in human\nsociety, its explainability is also essential. However, basic RL approaches\nhave difficulties in deciding when to choose exploitation as well as in\nextracting useful points for a brief explanation of its operation. One reason\nfor the difficulties is that these approaches treat all states the same way.\nHere, we show that identifying critical states and treating them specially is\ncommonly beneficial to both problems. These critical states are the states at\nwhich the action selection changes the potential of success and failure\nsubstantially. We propose to identify the critical states using the variance in\nthe Q-function for the actions and to perform exploitation with high\nprobability on the identified states. These simple methods accelerate RL in a\ngrid world with cliffs and two baseline tasks of deep RL. Our results also\ndemonstrate that the identified critical states are intuitively interpretable\nregarding the crucial nature of the action selection. Furthermore, our analysis\nof the relationship between the timing of the identification of especially\ncritical states and the rapid progress of learning suggests there are a few\nespecially critical states that have important information for accelerating RL\nrapidly.",
        "Graph structured data provide two-fold information: graph structures and node\nattributes. Numerous graph-based algorithms rely on both information to achieve\nsuccess in supervised tasks, such as node classification and link prediction.\nHowever, node attributes could be missing or incomplete, which significantly\ndeteriorates the performance. The task of node attribute generation aims to\ngenerate attributes for those nodes whose attributes are completely unobserved.\nThis task benefits many real-world problems like profiling, node classification\nand graph data augmentation. To tackle this task, we propose a deep adversarial\nlearning based method to generate node attributes; called node attribute neural\ngenerator (NANG). NANG learns a unifying latent representation which is shared\nby both node attributes and graph structures and can be translated to different\nmodalities. We thus use this latent representation as a bridge to convert\ninformation from one modality to another. We further introduce practical\napplications to quantify the performance of node attribute generation.\nExtensive experiments are conducted on four real-world datasets and the\nempirical results show that node attributes generated by the proposed method\nare high-qualitative and beneficial to other applications. The datasets and\ncodes are available online.",
        "Deep reinforcement learning has achieved great successes in recent years, but\nthere are still open challenges, such as convergence to locally optimal\npolicies and sample inefficiency. In this paper, we contribute a novel\nself-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating\ntemporal closeness to terminal states for episodic tasks. The intuition is to\nhelp representation learning by letting the agent predict how close it is to a\nterminal state, while learning its control policy. Although TP could be\nintegrated with multiple algorithms, this paper focuses on Asynchronous\nAdvantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our\nextensive evaluation includes: a set of Atari games, the BipedalWalker domain,\nand a mini version of the recently proposed multi-agent Pommerman game. Our\nresults on Atari games and the BipedalWalker domain suggest that A3C-TP\noutperforms standard A3C in most of the tested domains and in others it has\nsimilar performance. In Pommerman, our proposed method provides significant\nimprovement both in learning efficiency and converging to better policies\nagainst different opponents.",
        "Transformers have recently gained increasing attention in computer vision.\nHowever, existing studies mostly use Transformers for feature representation\nlearning, e.g. for image classification and dense predictions. In this work, we\nfurther investigate the possibility of applying Transformers for image matching\nand metric learning given pairs of images. We find that the Vision Transformer\n(ViT) and the vanilla Transformer with decoders are not adequate for image\nmatching due to their lack of image-to-image attention. Thus, we further design\ntwo naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery\ncross-attention in the vanilla Transformer. The latter improves the\nperformance, but it is still limited. This implies that the attention mechanism\nin Transformers is primarily designed for global feature aggregation, which is\nnot naturally suitable for image matching. Accordingly, we propose a new\nsimplified decoder, which drops the full attention implementation with the\nsoftmax weighting, keeping only the query-key similarity computation.\nAdditionally, global max pooling and a multilayer perceptron (MLP) head are\napplied to decode the matching result. This way, the simplified decoder is\ncomputationally more efficient, while at the same time more effective for image\nmatching. The proposed method, called TransMatcher, achieves state-of-the-art\nperformance in generalizable person re-identification, with up to 6.1% and 5.7%\nperformance gains in Rank-1 and mAP, respectively, on several popular datasets.\nThe source code of this study will be made publicly available.",
        "Most object detection methods use bounding boxes to encode and represent the\nobject shape and location. In this work, we explore a fuzzy representation of\nobject regions using Gaussian distributions, which provides an implicit binary\nrepresentation as (potentially rotated) ellipses. We also present a similarity\nmeasure for the Gaussian distributions based on the Hellinger Distance, which\ncan be viewed as a Probabilistic Intersection-over-Union (ProbIoU). Our\nexperimental results show that the proposed Gaussian representations are closer\nto annotated segmentation masks in publicly available datasets, and that loss\nfunctions based on ProbIoU can be successfully used to regress the parameters\nof the Gaussian representation. Furthermore, we present a simple mapping scheme\nfrom traditional (or rotated) bounding boxes to Gaussian representations,\nallowing the proposed ProbIoU-based losses to be seamlessly integrated into any\nobject detector.",
        "Reinforcement learning (RL) is a popular machine learning paradigm for game\nplaying, robotics control, and other sequential decision tasks. However, RL\nagents often have long learning times with high data requirements because they\nbegin by acting randomly. In order to better learn in complex tasks, this\narticle argues that an external teacher can often significantly help the RL\nagent learn.\n  OpenAI Gym is a common framework for RL research, including a large number of\nstandard environments and agents, making RL research significantly more\naccessible. This article introduces our new open-source RL framework, the Human\nInput Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions\nthat went into its creation. The goal of this platform is to facilitate\nhuman-RL research, again lowering the bar so that more researchers can quickly\ninvestigate different ways that human teachers could assist RL agents,\nincluding learning from demonstrations, learning from feedback, or curriculum\nlearning.",
        "Transferring existing image-based detectors to the video is non-trivial since\nthe quality of frames is always deteriorated by part occlusion, rare pose, and\nmotion blur. Previous approaches exploit to propagate and aggregate features\nacross video frames by using optical flow-warping. However, directly applying\nimage-level optical flow onto the high-level features might not establish\naccurate spatial correspondences. Therefore, a novel module called Learnable\nSpatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level\ncorrespondences among adjacent frame features accurately. The sampled locations\nare first randomly initialized, then updated iteratively to find better spatial\ncorrespondences guided by detection supervision progressively. Besides,\nSparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation\n(DFA) module are also introduced to model temporal relations and enhance\nper-frame features, respectively. Without bells and whistles, the proposed\nmethod achieves state-of-the-art performance on the ImageNet VID dataset with\nless computational complexity and real-time speed. Code will be made available\nat https://github.com/jiangzhengkai/LSTS.",
        "Despite the remarkable success of Deep RL in learning control policies from\nraw pixels, the resulting models do not generalize. We demonstrate that a\ntrained agent fails completely when facing small visual changes, and that\nfine-tuning---the common transfer learning paradigm---fails to adapt to these\nchanges, to the extent that it is faster to re-train the model from scratch. We\nshow that by separating the visual transfer task from the control policy we\nachieve substantially better sample efficiency and transfer behavior, allowing\nan agent trained on the source task to transfer well to the target tasks. The\nvisual mapping from the target to the source domain is performed using\nunaligned GANs, resulting in a control policy that can be further improved\nusing imitation learning from imperfect demonstrations. We demonstrate the\napproach on synthetic visual variants of the Breakout game, as well as on\ntransfer between subsequent levels of Road Fighter, a Nintendo car-driving\ngame. A visualization of our approach can be seen in\nhttps://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .",
        "Graphical causal inference as pioneered by Judea Pearl arose from research on\nartificial intelligence (AI), and for a long time had little connection to the\nfield of machine learning.\n  This article discusses where links have been and should be established,\nintroducing key concepts along the way. It argues that the hard open problems\nof machine learning and AI are intrinsically related to causality, and explains\nhow the field is beginning to understand them.",
        "Traffic signal control has long been considered as a critical topic in\nintelligent transportation systems. Most existing learning methods mainly focus\non isolated intersections and suffer from inefficient training. This paper aims\nat the cooperative control for large scale multi-intersection traffic signal,\nin which a novel end-to-end learning based model is established and the\nefficient training method is proposed correspondingly. In the proposed model,\nthe input traffic status in multi-intersections is represented by a tensor,\nwhich not only significantly reduces dimensionality than using a single matrix\nbut also avoids information loss. For the output, a multidimensional boolean\nvector is employed for the control policy to indicate whether the signal state\nchanges or not, which simplifies the representation and abides the practical\nphase changing rules. In the proposed model, a multi-task learning structure is\nused to get the cooperative policy by learning. Instead of only using the\nreinforcement learning to train the model, we employ imitation learning to\nintegrate a rule based model with neural networks to do the pre-training, which\nprovides a reliable and satisfactory stage solution and greatly accelerates the\nconvergence. Afterwards, the reinforcement learning method is adopted to\ncontinue the fine training, where proximal policy optimization algorithm is\nincorporated to solve the policy collapse problem in multi-dimensional output\nsituation. In numerical experiments, the advantages of the proposed model are\ndemonstrated with comparison to the related state-of-the-art methods.",
        "Graph neural networks have become a staple in problems addressing learning\nand analysis of data defined over graphs. However, several results suggest an\ninherent difficulty in extracting better performance by increasing the number\nof layers. Recent works attribute this to a phenomenon peculiar to the\nextraction of node features in graph-based tasks, i.e., the need to consider\nmultiple neighborhood sizes at the same time and adaptively tune them. In this\npaper, we investigate the recently proposed randomly wired architectures in the\ncontext of graph neural networks. Instead of building deeper networks by\nstacking many layers, we prove that employing a randomly-wired architecture can\nbe a more effective way to increase the capacity of the network and obtain\nricher representations. We show that such architectures behave like an ensemble\nof paths, which are able to merge contributions from receptive fields of varied\nsize. Moreover, these receptive fields can also be modulated to be wider or\nnarrower through the trainable weights over the paths. We also provide\nextensive experimental evidence of the superior performance of randomly wired\narchitectures over multiple tasks and four graph convolution definitions, using\nrecent benchmarking frameworks that addresses the reliability of previous\ntesting methodologies.",
        "This article proposes a data-driven methodology to achieve a fast design\nsupport, in order to generate or develop novel designs covering multiple object\ncategories. This methodology implements two state-of-the-art Variational\nAutoencoder dealing with 3D model data. Our methodology constructs a\nself-defined loss function. The loss function, containing the outputs of\ncertain layers in the autoencoder, obtains combination of different latent\nfeatures from different 3D model categories.\n  Additionally, this article provide detail explanation to utilize the\nPrinceton ModelNet40 database, a comprehensive clean collection of 3D CAD\nmodels for objects. After convert the original 3D mesh file to voxel and point\ncloud data type, we enable to feed our autoencoder with data of the same size\nof dimension. The novelty of this work is to leverage the power of deep\nlearning methods as an efficient latent feature extractor to explore unknown\ndesigning areas. Through this project, we expect the output can show a clear\nand smooth interpretation of model from different categories to develop a fast\ndesign support to generate novel shapes. This final report will explore 1) the\ntheoretical ideas, 2) the progresses to implement Variantional Autoencoder to\nattain implicit features from input shapes, 3) the results of output shapes\nduring training in selected domains of both 3D voxel data and 3D point cloud\ndata, and 4) our conclusion and future work to achieve the more outstanding\ngoal.",
        "Previous work on symmetric group equivariant neural networks generally only\nconsidered the case where the group acts by permuting the elements of a single\nvector. In this paper we derive formulae for general permutation equivariant\nlayers, including the case where the layer acts on matrices by permuting their\nrows and columns simultaneously. This case arises naturally in graph learning\nand relation learning applications. As a specific case of higher order\npermutation equivariant networks, we present a second order graph variational\nencoder, and show that the latent distribution of equivariant generative models\nmust be exchangeable. We demonstrate the efficacy of this architecture on the\ntasks of link prediction in citation graphs and molecular graph generation.",
        "Recently, deep neural networks have achieved impressive performance in terms\nof both reconstruction accuracy and efficiency for single image\nsuper-resolution (SISR). However, the network model of these methods is a fully\nconvolutional neural network, which is limit to exploit the differentiated\ncontextual information over the global region of the input image because of the\nweight sharing in convolution height and width extent. In this paper, we\ndiscuss a new SISR architecture where features are extracted in the\nlow-resolution (LR) space, and then we use a fully connected layer which learns\nan array of differentiated upsampling weights to reconstruct the desired\nhigh-resolution (HR) image from the final obtained LR features. By doing so, we\neffectively exploit the differentiated contextual information over the whole\ninput image region, whilst maintaining the low computational complexity for the\noverall SR operations. In addition, we introduce an edge difference constraint\ninto our loss function to preserve edges and texture structures. Extensive\nexperiments validate that our SISR method outperforms the existing\nstate-of-the-art methods.",
        "We study the invariance characteristics of pre-trained predictive models by\nempirically learning transformations on the input that leave the prediction\nfunction approximately unchanged. To learn invariant transformations, we\nminimize the Wasserstein distance between the predictive distribution\nconditioned on the data instances and the predictive distribution conditioned\non the transformed data instances. To avoid finding degenerate or perturbative\ntransformations, we add a similarity regularization to discourage similarity\nbetween the data and its transformed values. We theoretically analyze the\ncorrectness of the algorithm and the structure of the solutions. Applying the\nproposed technique to clinical time series data, we discover variables that\ncommonly-used LSTM models do not rely on for their prediction, especially when\nthe LSTM is trained to be adversarially robust. We also analyze the invariances\nof BioBERT on clinical notes and discover words that it is invariant to.",
        "The Transformer architecture has become a dominant choice in many domains,\nsuch as natural language processing and computer vision. Yet, it has not\nachieved competitive performance on popular leaderboards of graph-level\nprediction compared to mainstream GNN variants. Therefore, it remains a mystery\nhow Transformers could perform well for graph representation learning. In this\npaper, we solve this mystery by presenting Graphormer, which is built upon the\nstandard Transformer architecture, and could attain excellent results on a\nbroad range of graph representation learning tasks, especially on the recent\nOGB Large-Scale Challenge. Our key insight to utilizing Transformer in the\ngraph is the necessity of effectively encoding the structural information of a\ngraph into the model. To this end, we propose several simple yet effective\nstructural encoding methods to help Graphormer better model graph-structured\ndata. Besides, we mathematically characterize the expressive power of\nGraphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the\nspecial cases of Graphormer.",
        "Weakly Supervised Object Detection (WSOD), aiming to train detectors with\nonly image-level dataset, has arisen increasing attention for researchers. In\nthis project, we focus on two-phase WSOD architecture which integrates a\npowerful detector with a pure WSOD model. We explore the effectiveness of some\nrepresentative detectors utilized as the second-phase detector in two-phase\nWSOD and propose a two-phase WSOD architecture. In addition, we present a\nstrategy to establish the pseudo ground truth (PGT) used to train the\nsecond-phase detector. Unlike previous works that regard top one bounding boxes\nas PGT, we consider more bounding boxes to establish the PGT annotations. This\nalleviates the insufficient learning problem caused by the low recall of PGT.\nWe also propose some strategies to refine the PGT during the training of the\nsecond detector. Our strategies suspend the training in specific epoch, then\nrefine the PGT by the outputs of the second-phase detector. After that, the\nalgorithm continues the training with the same gradients and weights as those\nbefore suspending. Elaborate experiments are conduceted on the PASCAL VOC 2007\ndataset to verify the effectiveness of our methods. As results demonstrate, our\ntwo-phase architecture improves the mAP from 49.17% to 53.21% compared with the\nsingle PCL model. Additionally, the best PGT generation strategy obtains a 0.7%\nmAP increment. Our best refinement strategy boosts the performance by 1.74%\nmAP. The best results adopting all of our methods achieve 55.231% mAP which is\nthe state-of-the-art performance.",
        "An agent in a non-stationary contextual bandit problem should balance between\nexploration and the exploitation of (periodic or structured) patterns present\nin its previous experiences. Handcrafting an appropriate historical context is\nan attractive alternative to transform a non-stationary problem into a\nstationary problem that can be solved efficiently. However, even a carefully\ndesigned historical context may introduce spurious relationships or lack a\nconvenient representation of crucial information. In order to address these\nissues, we propose an approach that learns to represent the relevant context\nfor a decision based solely on the raw history of interactions between the\nagent and the environment. This approach relies on a combination of features\nextracted by recurrent neural networks with a contextual linear bandit\nalgorithm based on posterior sampling. Our experiments on a diverse selection\nof contextual and non-contextual non-stationary problems show that our\nrecurrent approach consistently outperforms its feedforward counterpart, which\nrequires handcrafted historical contexts, while being more widely applicable\nthan conventional non-stationary bandit algorithms.",
        "We propose a method of improving detection precision (mAP) with the help of\nthe prior knowledge about the scene geometry: we assume the scene to be a plane\nwith objects placed on it. We focus our attention on autonomous robots, so\ngiven the robot's dimensions and the inclination angles of the camera, it is\npossible to predict the spatial scale for each pixel of the input frame. With\nslightly modified YOLOv3-tiny we demonstrate that the detection supplemented by\nthe scale channel, further referred as S, outperforms standard RGB-based\ndetection with small computational overhead.",
        "In this paper, we propose a novel learning framework for the problem of\ndomain transfer learning. We map the data of two domains to one single common\nspace, and learn a classifier in this common space. Then we adapt the common\nclassifier to the two domains by adding two adaptive functions to it\nrespectively. In the common space, the target domain data points are weighted\nand matched to the target domain in term of distributions. The weighting terms\nof source domain data points and the target domain classification responses are\nalso regularized by the local reconstruction coefficients. The novel transfer\nlearning framework is evaluated over some benchmark cross-domain data sets, and\nit outperforms the existing state-of-the-art transfer learning methods.",
        "Scalable sensor simulation is an important yet challenging open problem for\nsafety-critical domains such as self-driving. Current works in image simulation\neither fail to be photorealistic or do not model the 3D environment and the\ndynamic objects within, losing high-level control and physical realism. In this\npaper, we present GeoSim, a geometry-aware image composition process which\nsynthesizes novel urban driving scenarios by augmenting existing images with\ndynamic objects extracted from other scenes and rendered at novel poses.\nTowards this goal, we first build a diverse bank of 3D objects with both\nrealistic geometry and appearance from sensor data. During simulation, we\nperform a novel geometry-aware simulation-by-composition procedure which 1)\nproposes plausible and realistic object placements into a given scene, 2)\nrender novel views of dynamic objects from the asset bank, and 3) composes and\nblends the rendered image segments. The resulting synthetic images are\nrealistic, traffic-aware, and geometrically consistent, allowing our approach\nto scale to complex use cases. We demonstrate two such important applications:\nlong-range realistic video simulation across multiple camera sensors, and\nsynthetic data generation for data augmentation on downstream segmentation\ntasks. Please check https://tmux.top/publication/geosim/ for high-resolution\nvideo results.",
        "Data augmentation has been an indispensable tool to improve the performance\nof deep neural networks, however the augmentation can hardly transfer among\ndifferent tasks and datasets. Consequently, a recent trend is to adopt AutoML\ntechnique to learn proper augmentation policy without extensive hand-crafted\ntuning. In this paper, we propose an efficient differentiable search algorithm\ncalled Direct Differentiable Augmentation Search (DDAS). It exploits\nmeta-learning with one-step gradient update and continuous relaxation to the\nexpected training loss for efficient search. Our DDAS can achieve efficient\naugmentation search without relying on approximations such as Gumbel Softmax or\nsecond order gradient approximation. To further reduce the adverse effect of\nimproper augmentations, we organize the search space into a two level\nhierarchy, in which we first decide whether to apply augmentation, and then\ndetermine the specific augmentation policy. On standard image classification\nbenchmarks, our DDAS achieves state-of-the-art performance and efficiency\ntradeoff while reducing the search cost dramatically, e.g. 0.15 GPU hours for\nCIFAR-10. In addition, we also use DDAS to search augmentation for object\ndetection task and achieve comparable performance with AutoAugment, while being\n1000x faster.",
        "In this paper we present a large-scale visual object detection and tracking\nbenchmark, named VisDrone2018, aiming at advancing visual understanding tasks\non the drone platform. The images and video sequences in the benchmark were\ncaptured over various urban/suburban areas of 14 different cities across China\nfrom north to south. Specifically, VisDrone2018 consists of 263 video clips and\n10,209 images (no overlap with video clips) with rich annotations, including\nobject bounding boxes, object categories, occlusion, truncation ratios, etc.\nWith intensive amount of effort, our benchmark has more than 2.5 million\nannotated instances in 179,264 images/video frames. Being the largest such\ndataset ever published, the benchmark enables extensive evaluation and\ninvestigation of visual analysis algorithms on the drone platform. In\nparticular, we design four popular tasks with the benchmark, including object\ndetection in images, object detection in videos, single object tracking, and\nmulti-object tracking. All these tasks are extremely challenging in the\nproposed dataset due to factors such as occlusion, large scale and pose\nvariation, and fast motion. We hope the benchmark largely boost the research\nand development in visual analysis on drone platforms.",
        "We propose a novel framework for structured prediction via adversarial\nlearning. Existing adversarial learning methods involve two separate networks,\ni.e., the structured prediction models and the discriminative models, in the\ntraining. The information captured by discriminative models complements that in\nthe structured prediction models, but few existing researches have studied on\nutilizing such information to improve structured prediction models at the\ninference stage. In this work, we propose to refine the predictions of\nstructured prediction models by effectively integrating discriminative models\ninto the prediction. Discriminative models are treated as energy-based models.\nSimilar to the adversarial learning, discriminative models are trained to\nestimate scores which measure the quality of predicted outputs, while\nstructured prediction models are trained to predict contrastive outputs with\nmaximal energy scores. In this way, the gradient vanishing problem is\nameliorated, and thus we are able to perform inference by following the ascent\ngradient directions of discriminative models to refine structured prediction\nmodels. The proposed method is able to handle a range of tasks, e.g.,\nmulti-label classification and image segmentation. Empirical results on these\ntwo tasks validate the effectiveness of our learning method.",
        "Amodal recognition is the ability of the system to detect occluded objects.\nMost state-of-the-art Visual Recognition systems lack the ability to perform\namodal recognition. Few studies have achieved amodal recognition through\npassive prediction or embodied recognition approaches. However, these\napproaches suffer from challenges in real-world applications, such as dynamic\nobjects. We propose SeekNet, an improved optimization method for amodal\nrecognition through embodied visual recognition. Additionally, we implement\nSeekNet for social robots, where there are multiple interactions with crowded\nhumans. Hence, we focus on occluded human detection & tracking and showcase the\nsuperiority of our algorithm over other baselines. We also experiment with\nSeekNet to improve the confidence of COVID-19 symptoms pre-screening algorithms\nusing our efficient embodied recognition system.",
        "After generalizing the Archimedean property of real numbers in such a way as\nto make it adaptable to non-numeric structures, we demonstrate that the real\nnumbers cannot be used to accurately measure non-Archimedean structures. We\nargue that, since an agent with Artificial General Intelligence (AGI) should\nhave no problem engaging in tasks that inherently involve non-Archimedean\nrewards, and since traditional reinforcement learning rewards are real numbers,\ntherefore traditional reinforcement learning probably will not lead to AGI. We\nindicate two possible ways traditional reinforcement learning could be altered\nto remove this roadblock.",
        "Knowledge Distillation (KD) methods are capable of transferring the knowledge\nencoded in a large and complex teacher into a smaller and faster student. Early\nmethods were usually limited to transferring the knowledge only between the\nlast layers of the networks, while latter approaches were capable of performing\nmulti-layer KD, further increasing the accuracy of the student. However,\ndespite their improved performance, these methods still suffer from several\nlimitations that restrict both their efficiency and flexibility. First,\nexisting KD methods typically ignore that neural networks undergo through\ndifferent learning phases during the training process, which often requires\ndifferent types of supervision for each one. Furthermore, existing multi-layer\nKD methods are usually unable to effectively handle networks with significantly\ndifferent architectures (heterogeneous KD). In this paper we propose a novel KD\nmethod that works by modeling the information flow through the various layers\nof the teacher model and then train a student model to mimic this information\nflow. The proposed method is capable of overcoming the aforementioned\nlimitations by using an appropriate supervision scheme during the different\nphases of the training process, as well as by designing and training an\nappropriate auxiliary teacher model that acts as a proxy model capable of\n\"explaining\" the way the teacher works to the student. The effectiveness of the\nproposed method is demonstrated using four image datasets and several different\nevaluation setups.",
        "An important goal of medical imaging is to be able to precisely detect\npatterns of disease specific to individual scans; however, this is challenged\nin brain imaging by the degree of heterogeneity of shape and appearance.\nTraditional methods, based on image registration to a global template,\nhistorically fail to detect variable features of disease, as they utilise\npopulation-based analyses, suited primarily to studying group-average effects.\nIn this paper we therefore take advantage of recent developments in generative\ndeep learning to develop a method for simultaneous classification, or\nregression, and feature attribution (FA). Specifically, we explore the use of a\nVAE-GAN translation network called ICAM, to explicitly disentangle class\nrelevant features from background confounds for improved interpretability and\nregression of neurological phenotypes. We validate our method on the tasks of\nMini-Mental State Examination (MMSE) cognitive test score prediction for the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) cohort, as well as brain age\nprediction, for both neurodevelopment and neurodegeneration, using the\ndeveloping Human Connectome Project (dHCP) and UK Biobank datasets. We show\nthat the generated FA maps can be used to explain outlier predictions and\ndemonstrate that the inclusion of a regression module improves the\ndisentanglement of the latent space. Our code is freely available on Github\nhttps://github.com/CherBass/ICAM.",
        "Scene graph generation aims to provide a semantic and structural description\nof an image, denoting the objects (with nodes) and their relationships (with\nedges). The best performing works to date are based on exploiting the context\nsurrounding objects or relations,e.g., by passing information among objects. In\nthese approaches, to transform the representation of source objects is a\ncritical process for extracting information for the use by target objects. In\nthis work, we argue that a source object should give what tar-get object needs\nand give different objects different information rather than contributing\ncommon information to all targets. To achieve this goal, we propose a\nTarget-TailoredSource-Transformation (TTST) method to efficiently propagate\ninformation among object proposals and relations. Particularly, for a source\nobject proposal which will contribute information to other target objects, we\ntransform the source object feature to the target object feature domain by\nsimultaneously taking both the source and target into account. We further\nexplore more powerful representations by integrating language prior with the\nvisual context in the transformation for the scene graph generation. By doing\nso the target object is able to extract target-specific information from the\nsource object and source relation accordingly to refine its representation. Our\nframework is validated on the Visual Genome bench-mark and demonstrated its\nstate-of-the-art performance for the scene graph generation. The experimental\nresults show that the performance of object detection and visual relation-ship\ndetection are promoted mutually by our method.",
        "There is a prevailing trend to study urban morphology quantitatively thanks\nto the growing accessibility to various forms of spatial big data, increasing\ncomputing power, and use cases benefiting from such information. The methods\ndeveloped up to now measure urban morphology with numerical indices describing\ndensity, proportion, and mixture, but they do not directly represent\nmorphological features from the human's visual and intuitive perspective. We\ntake the first step to bridge the gap by proposing a deep learning-based\ntechnique to automatically classify road networks into four classes on a visual\nbasis. The method is implemented by generating an image of the street network\n(Colored Road Hierarchy Diagram), which we introduce in this paper, and\nclassifying it using a deep convolutional neural network (ResNet-34). The model\nachieves an overall classification accuracy of 0.875. Nine cities around the\nworld are selected as the study areas with their road networks acquired from\nOpenStreetMap. Latent subgroups among the cities are uncovered through\nclustering on the percentage of each road network category. In the subsequent\npart of the paper, we focus on the usability of such classification: we apply\nour method in a case study of urban vitality prediction. An advanced tree-based\nregression model (LightGBM) is for the first time designated to establish the\nrelationship between morphological indices and vitality indicators. The effect\nof road network classification is found to be small but positively associated\nwith urban vitality. This work expands the toolkit of quantitative urban\nmorphology study with new techniques, supporting further studies in the future.",
        "This paper concerns the statistical analysis of a weighted graph through\nspectral embedding. Under a latent position model in which the expected\nadjacency matrix has low rank, we prove uniform consistency and a central limit\ntheorem for the embedded nodes, treated as latent position estimates. In the\nspecial case of a weighted stochastic block model, this result implies that the\nembedding follows a Gaussian mixture model with each component representing a\ncommunity. We exploit this to formally evaluate different weight\nrepresentations of the graph using Chernoff information. For example, in a\nnetwork anomaly detection problem where we observe a p-value on each edge, we\nrecommend against directly embedding the matrix of p-values, and instead using\nthreshold or log p-values, depending on network sparsity and signal strength.",
        "We propose a filtering feature selection framework that considers subsets of\nfeatures as paths in a graph, where a node is a feature and an edge indicates\npairwise (customizable) relations among features, dealing with relevance and\nredundancy principles. By two different interpretations (exploiting properties\nof power series of matrices and relying on Markov chains fundamentals) we can\nevaluate the values of paths (i.e., feature subsets) of arbitrary lengths,\neventually go to infinite, from which we dub our framework Infinite Feature\nSelection (Inf-FS). Going to infinite allows to constrain the computational\ncomplexity of the selection process, and to rank the features in an elegant\nway, that is, considering the value of any path (subset) containing a\nparticular feature. We also propose a simple unsupervised strategy to cut the\nranking, so providing the subset of features to keep. In the experiments, we\nanalyze diverse settings with heterogeneous features, for a total of 11\nbenchmarks, comparing against 18 widely-known comparative approaches. The\nresults show that Inf-FS behaves better in almost any situation, that is, when\nthe number of features to keep are fixed a priori, or when the decision of the\nsubset cardinality is part of the process.",
        "Object detection in road scenes is necessary to develop both autonomous\nvehicles and driving assistance systems. Even if deep neural networks for\nrecognition task have shown great performances using conventional images, they\nfail to detect objects in road scenes in complex acquisition situations. In\ncontrast, polarization images, characterizing the light wave, can robustly\ndescribe important physical properties of the object even under poor\nillumination or strong reflections. This paper shows how non-conventional\npolarimetric imaging modality overcomes the classical methods for object\ndetection especially in adverse weather conditions. The efficiency of the\nproposed method is mostly due to the high power of the polarimetry to\ndiscriminate any object by its reflective properties and on the use of deep\nneural networks for object detection. Our goal by this work, is to prove that\npolarimetry brings a real added value compared with RGB images for object\ndetection. Experimental results on our own dataset composed of road scene\nimages taken during adverse weather conditions show that polarimetry together\nwith deep learning can improve the state-of-the-art by about 20% to 50% on\ndifferent detection tasks.",
        "Recently, maximizing mutual information has emerged as a powerful method for\nunsupervised graph representation learning. The existing methods are typically\neffective to capture information from the topology view but ignore the feature\nview. To circumvent this issue, we propose a novel approach by exploiting\nmutual information maximization across feature and topology views.\nSpecifically, we first utilize a multi-view representation learning module to\nbetter capture both local and global information content across feature and\ntopology views on graphs. To model the information shared by the feature and\ntopology spaces, we then develop a common representation learning module using\nmutual information maximization and reconstruction loss minimization. To\nexplicitly encourage diversity between graph representations from the same\nview, we also introduce a disagreement regularization to enlarge the distance\nbetween representations from the same view. Experiments on synthetic and\nreal-world datasets demonstrate the effectiveness of integrating feature and\ntopology views. In particular, compared with the previous supervised methods,\nour proposed method can achieve comparable or even better performance under the\nunsupervised representation and linear evaluation protocol.",
        "The nonlinear vector autoregressive (NVAR) model provides an appealing\nframework to analyze multivariate time series obtained from a nonlinear\ndynamical system. However, the innovation (or error), which plays a key role by\ndriving the dynamics, is almost always assumed to be additive. Additivity\ngreatly limits the generality of the model, hindering analysis of general NVAR\nprocesses which have nonlinear interactions between the innovations. Here, we\npropose a new general framework called independent innovation analysis (IIA),\nwhich estimates the innovations from completely general NVAR. We assume mutual\nindependence of the innovations as well as their modulation by an auxiliary\nvariable (which is often taken as the time index and simply interpreted as\nnonstationarity). We show that IIA guarantees the identifiability of the\ninnovations with arbitrary nonlinearities, up to a permutation and\ncomponent-wise invertible nonlinearities. We also propose three estimation\nframeworks depending on the type of the auxiliary variable. We thus provide the\nfirst rigorous identifiability result for general NVAR, as well as very general\ntools for learning such models.",
        "Deep CCA is a recently proposed deep neural network extension to the\ntraditional canonical correlation analysis (CCA), and has been successful for\nmulti-view representation learning in several domains. However, stochastic\noptimization of the deep CCA objective is not straightforward, because it does\nnot decouple over training examples. Previous optimizers for deep CCA are\neither batch-based algorithms or stochastic optimization using large\nminibatches, which can have high memory consumption. In this paper, we tackle\nthe problem of stochastic optimization for deep CCA with small minibatches,\nbased on an iterative solution to the CCA objective, and show that we can\nachieve as good performance as previous optimizers and thus alleviate the\nmemory requirement.",
        "In this paper, we present a novel approach to perform deep neural networks\nlayer-wise weight initialization using Linear Discriminant Analysis (LDA).\nTypically, the weights of a deep neural network are initialized with: random\nvalues, greedy layer-wise pre-training (usually as Deep Belief Network or as\nauto-encoder) or by re-using the layers from another network (transfer\nlearning). Hence, many training epochs are needed before meaningful weights are\nlearned, or a rather similar dataset is required for seeding a fine-tuning of\ntransfer learning. In this paper, we describe how to turn an LDA into either a\nneural layer or a classification layer. We analyze the initialization technique\non historical documents. First, we show that an LDA-based initialization is\nquick and leads to a very stable initialization. Furthermore, for the task of\nlayout analysis at pixel level, we investigate the effectiveness of LDA-based\ninitialization and show that it outperforms state-of-the-art random weight\ninitialization methods.",
        "Even though Deep Neural Networks (DNNs) are widely celebrated for their\npractical performance, they possess many intriguing properties related to depth\nthat are difficult to explain both theoretically and intuitively. Understanding\nhow weights in deep networks coordinate together across layers to form useful\nlearners has proven challenging, in part because the repeated composition of\nnonlinearities has proved intractable. This paper presents a reparameterization\nof DNNs as a linear function of a feature map that is locally independent of\nthe weights. This feature map transforms depth-dependencies into simple tensor\nproducts and maps each input to a discrete subset of the feature space. Then,\nusing a max-margin assumption, the paper develops a sample compression\nrepresentation of the neural network in terms of the discrete activation state\nof neurons induced by s ``support vectors\". The paper shows that the number of\nsupport vectors s relates with learning guarantees for neural networks through\nsample compression bounds, yielding a sample complexity of O(ns/epsilon) for\nnetworks with n neurons. Finally, the number of support vectors s is found to\nmonotonically increase with width and label noise but decrease with depth.",
        "In this paper we introduce the Perception for Autonomous Systems (PAZ)\nsoftware library. PAZ is a hierarchical perception library that allow users to\nmanipulate multiple levels of abstraction in accordance to their requirements\nor skill level. More specifically, PAZ is divided into three hierarchical\nlevels which we refer to as pipelines, processors, and backends. These\nabstractions allows users to compose functions in a hierarchical modular scheme\nthat can be applied for preprocessing, data-augmentation, prediction and\npostprocessing of inputs and outputs of machine learning (ML) models. PAZ uses\nthese abstractions to build reusable training and prediction pipelines for\nmultiple robot perception tasks such as: 2D keypoint estimation, 2D object\ndetection, 3D keypoint discovery, 6D pose estimation, emotion classification,\nface recognition, instance segmentation, and attention mechanisms.",
        "In this thesis, we present new schemes which leverage a constrained\nclustering method to solve several computer vision tasks ranging from image\nretrieval, image segmentation and co-segmentation, to person re-identification.\nIn the last decades clustering methods have played a vital role in computer\nvision applications; herein, we focus on the extension, reformulation, and\nintegration of a well-known graph and game theoretic clustering method known as\nDominant Sets. Thus, we have demonstrated the validity of the proposed methods\nwith extensive experiments which are conducted on several benchmark datasets.",
        "Advanced methods of applying deep learning to structured data such as graphs\nhave been proposed in recent years. In particular, studies have focused on\ngeneralizing convolutional neural networks to graph data, which includes\nredefining the convolution and the downsampling (pooling) operations for\ngraphs. The method of generalizing the convolution operation to graphs has been\nproven to improve performance and is widely used. However, the method of\napplying downsampling to graphs is still difficult to perform and has room for\nimprovement. In this paper, we propose a graph pooling method based on\nself-attention. Self-attention using graph convolution allows our pooling\nmethod to consider both node features and graph topology. To ensure a fair\ncomparison, the same training procedures and model architectures were used for\nthe existing pooling methods and our method. The experimental results\ndemonstrate that our method achieves superior graph classification performance\non the benchmark datasets using a reasonable number of parameters.",
        "Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.",
        "To improve patient survival and treatment outcomes, early diagnosis of brain\ntumors is an essential task. It is a difficult task to evaluate the magnetic\nresonance imaging (MRI) images manually. Thus, there is a need for digital\nmethods for tumor diagnosis with better accuracy. However, it is still a very\nchallenging task in assessing their shape, volume, boundaries, tumor detection,\nsize, segmentation, and classification. In this proposed work, we propose a\nhybrid ensemble method using Random Forest (RF), K-Nearest Neighbour, and\nDecision Tree (DT) (KNN-RF-DT) based on Majority Voting Method. It aims to\ncalculate the area of the tumor region and classify brain tumors as benign and\nmalignant. In the beginning, segmentation is done by using Otsu's Threshold\nmethod. Feature Extraction is done by using Stationary Wavelet Transform (SWT),\nPrinciple Component Analysis (PCA), and Gray Level Co-occurrence Matrix (GLCM),\nwhich gives thirteen features for classification. The classification is done by\nhybrid ensemble classifier (KNN-RF-DT) based on the Majority Voting method.\nOverall it aimed at improving the performance by traditional classifiers\ninstead of going to deep learning. Traditional classifiers have an advantage\nover deep learning algorithms because they require small datasets for training\nand have low computational time complexity, low cost to the users, and can be\neasily adopted by less skilled people. Overall, our proposed method is tested\nupon dataset of 2556 images, which are used in 85:15 for training and testing\nrespectively and gives good accuracy of 97.305%.",
        "The rise of deep learning in today's applications entailed an increasing need\nin explaining the model's decisions beyond prediction performances in order to\nfoster trust and accountability. Recently, the field of explainable AI (XAI)\nhas developed methods that provide such explanations for already trained neural\nnetworks. In computer vision tasks such explanations, termed heatmaps,\nvisualize the contributions of individual pixels to the prediction. So far XAI\nmethods along with their heatmaps were mainly validated qualitatively via\nhuman-based assessment, or evaluated through auxiliary proxy tasks such as\npixel perturbation, weak object localization or randomization tests. Due to the\nlack of an objective and commonly accepted quality measure for heatmaps, it was\ndebatable which XAI method performs best and whether explanations can be\ntrusted at all. In the present work, we tackle the problem by proposing a\nground truth based evaluation framework for XAI methods based on the CLEVR\nvisual question answering task. Our framework provides a (1) selective, (2)\ncontrolled and (3) realistic testbed for the evaluation of neural network\nexplanations. We compare ten different explanation methods, resulting in new\ninsights about the quality and properties of XAI methods, sometimes\ncontradicting with conclusions from previous comparative studies. The CLEVR-XAI\ndataset and the benchmarking code can be found at\nhttps://github.com/ahmedmagdiosman/clevr-xai.",
        "Object detection and classification in 3D is a key task in Automated Driving\n(AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction\nof the surrounding environment, while the task of 3D object bounding box\ndetection in real time remains a strong algorithmic challenge. In this paper,\nwe build on the success of the one-shot regression meta-architecture in the 2D\nperspective image space and extend it to generate oriented 3D object bounding\nboxes from LiDAR point cloud. Our main contribution is in extending the loss\nfunction of YOLO v2 to include the yaw angle, the 3D box center in Cartesian\ncoordinates and the height of the box as a direct regression problem. This\nformulation enables real-time performance, which is essential for automated\ndriving. Our results are showing promising figures on KITTI benchmark,\nachieving real-time performance (40 fps) on Titan X GPU.",
        "Transfer in reinforcement learning is usually achieved through generalisation\nacross tasks. Whilst many studies have investigated transferring knowledge when\nthe reward function changes, they have assumed that the dynamics of the\nenvironments remain consistent. Many real-world RL problems require transfer\namong environments with different dynamics. To address this problem, we propose\nan approach based on successor features in which we model successor feature\nfunctions with Gaussian Processes permitting the source successor features to\nbe treated as noisy measurements of the target successor feature function. Our\ntheoretical analysis proves the convergence of this approach as well as the\nbounded error on modelling successor feature functions with Gaussian Processes\nin environments with both different dynamics and rewards. We demonstrate our\nmethod on benchmark datasets and show that it outperforms current baselines.",
        "Humans can abstract prior knowledge from very little data and use it to boost\nskill learning. In this paper, we propose routine-augmented policy learning\n(RAPL), which discovers routines composed of primitive actions from a single\ndemonstration and uses discovered routines to augment policy learning. To\ndiscover routines from the demonstration, we first abstract routine candidates\nby identifying grammar over the demonstrated action trajectory. Then, the best\nroutines measured by length and frequency are selected to form a routine\nlibrary. We propose to learn policy simultaneously at primitive-level and\nroutine-level with discovered routines, leveraging the temporal structure of\nroutines. Our approach enables imitating expert behavior at multiple temporal\nscales for imitation learning and promotes reinforcement learning exploration.\nExtensive experiments on Atari games demonstrate that RAPL improves the\nstate-of-the-art imitation learning method SQIL and reinforcement learning\nmethod A2C. Further, we show that discovered routines can generalize to unseen\nlevels and difficulties on the CoinRun benchmark.",
        "We present AVOD, an Aggregate View Object Detection network for autonomous\ndriving scenarios. The proposed neural network architecture uses LIDAR point\nclouds and RGB images to generate features that are shared by two subnetworks:\na region proposal network (RPN) and a second stage detector network. The\nproposed RPN uses a novel architecture capable of performing multimodal feature\nfusion on high resolution feature maps to generate reliable 3D object proposals\nfor multiple object classes in road scenes. Using these proposals, the second\nstage detection network performs accurate oriented 3D bounding box regression\nand category classification to predict the extents, orientation, and\nclassification of objects in 3D space. Our proposed architecture is shown to\nproduce state of the art results on the KITTI 3D object detection benchmark\nwhile running in real time with a low memory footprint, making it a suitable\ncandidate for deployment on autonomous vehicles. Code is at:\nhttps://github.com/kujason/avod",
        "Recent machine learning models have shown that including attention as a\ncomponent results in improved model accuracy and interpretability, despite the\nconcept of attention in these approaches only loosely approximating the brain's\nattention mechanism. Here we extend this work by building a more brain-inspired\ndeep network model of the primate ATTention Network (ATTNet) that learns to\nshift its attention so as to maximize the reward. Using deep reinforcement\nlearning, ATTNet learned to shift its attention to the visual features of a\ntarget category in the context of a search task. ATTNet's dorsal layers also\nlearned to prioritize these shifts of attention so as to maximize success of\nthe ventral pathway classification and receive greater reward. Model behavior\nwas tested against the fixations made by subjects searching images for the same\ncued category. Both subjects and ATTNet showed evidence for attention being\npreferentially directed to target goals, behaviorally measured as oculomotor\nguidance to targets. More fundamentally, ATTNet learned to shift its attention\nto target like objects and spatially route its visual inputs to accomplish the\ntask. This work makes a step toward a better understanding of the role of\nattention in the brain and other computational systems.",
        "This paper presents a novel method for structural data recognition using a\nlarge number of graph models. In general, prevalent methods for structural data\nrecognition have two shortcomings: 1) Only a single model is used to capture\nstructural variation. 2) Naive recognition methods are used, such as the\nnearest neighbor method. In this paper, we propose strengthening the\nrecognition performance of these models as well as their ability to capture\nstructural variation. The proposed method constructs a large number of graph\nmodels and trains decision trees using the models. This paper makes two main\ncontributions. The first is a novel graph model that can quickly perform\ncalculations, which allows us to construct several models in a feasible amount\nof time. The second contribution is a novel approach to structural data\nrecognition: graph model boosting. Comprehensive structural variations can be\ncaptured with a large number of graph models constructed in a boosting\nframework, and a sophisticated classifier can be formed by aggregating the\ndecision trees. Consequently, we can carry out structural data recognition with\npowerful recognition capability in the face of comprehensive structural\nvariation. The experiments shows that the proposed method achieves impressive\nresults and outperforms existing methods on datasets of IAM graph database\nrepository.",
        "In this paper, we propose an inverse reinforcement learning method for\narchitecture search (IRLAS), which trains an agent to learn to search network\nstructures that are topologically inspired by human-designed network. Most\nexisting architecture search approaches totally neglect the topological\ncharacteristics of architectures, which results in complicated architecture\nwith a high inference latency. Motivated by the fact that human-designed\nnetworks are elegant in topology with a fast inference speed, we propose a\nmirror stimuli function inspired by biological cognition theory to extract the\nabstract topological knowledge of an expert human-design network (ResNeXt). To\navoid raising a too strong prior over the search space, we introduce inverse\nreinforcement learning to train the mirror stimuli function and exploit it as a\nheuristic guidance for architecture search, easily generalized to different\narchitecture search algorithms. On CIFAR-10, the best architecture searched by\nour proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our\nmodel achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x\nfaster than most auto-generated architectures. A fast version of this model\nachieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",
        "In this paper, we study \\emph{Federated Bandit}, a decentralized Multi-Armed\nBandit problem with a set of $N$ agents, who can only communicate their local\ndata with neighbors described by a connected graph $G$. Each agent makes a\nsequence of decisions on selecting an arm from $M$ candidates, yet they only\nhave access to local and potentially biased feedback/evaluation of the true\nreward for each action taken. Learning only locally will lead agents to\nsub-optimal actions while converging to a no-regret strategy requires a\ncollection of distributed data. Motivated by the proposal of federated\nlearning, we aim for a solution with which agents will never share their local\nobservations with a central entity, and will be allowed to only share a private\ncopy of his/her own information with their neighbors. We first propose a\ndecentralized bandit algorithm Gossip_UCB, which is a coupling of variants of\nboth the classical gossiping algorithm and the celebrated Upper Confidence\nBound (UCB) bandit algorithm. We show that Gossip_UCB successfully adapts local\nbandit learning into a global gossiping process for sharing information among\nconnected agents, and achieves guaranteed regret at the order of $O(\\max\\{\n\\texttt{poly}(N,M) \\log T, \\texttt{poly}(N,M)\\log_{\\lambda_2^{-1}} N\\})$ for\nall $N$ agents, where $\\lambda_2\\in(0,1)$ is the second largest eigenvalue of\nthe expected gossip matrix, which is a function of $G$. We then propose\nFed_UCB, a differentially private version of Gossip_UCB, in which the agents\npreserve $\\epsilon$-differential privacy of their local data while achieving\n$O(\\max \\{\\frac{\\texttt{poly}(N,M)}{\\epsilon}\\log^{2.5} T, \\texttt{poly}(N,M)\n(\\log_{\\lambda_2^{-1}} N + \\log T) \\})$ regret.",
        "Person re-identification is a basic subject in the field of computer vision.\nThe traditional methods have several limitations in solving the problems of\nperson illumination like occlusion, pose variation and feature variation under\ncomplex background. Fortunately, deep learning paradigm opens new ways of the\nperson re-identification research and becomes a hot spot in this field.\nGenerative Adversarial Nets (GANs) in the past few years attracted lots of\nattention in solving these problems. This paper reviews the GAN based methods\nfor person re-identification focuses on the related papers about different GAN\nbased frameworks and discusses their advantages and disadvantages. Finally, it\nproposes the direction of future research, especially the prospect of person\nre-identification methods based on GANs.",
        "Graph neural networks~(GNNs) apply deep learning techniques to\ngraph-structured data and have achieved promising performance in graph\nrepresentation learning. However, existing GNNs rely heavily on enough labels\nor well-designed negative samples. To address these issues, we propose a new\nself-supervised graph representation method: deep graph bootstrapping~(DGB).\nDGB consists of two neural networks: online and target networks, and the input\nof them are different augmented views of the initial graph. The online network\nis trained to predict the target network while the target network is updated\nwith a slow-moving average of the online network, which means the online and\ntarget networks can learn from each other. As a result, the proposed DGB can\nlearn graph representation without negative examples in an unsupervised manner.\nIn addition, we summarize three kinds of augmentation methods for\ngraph-structured data and apply them to the DGB. Experiments on the benchmark\ndatasets show the DGB performs better than the current state-of-the-art methods\nand how the augmentation methods affect the performances.",
        "Despite the great advances made in the field of image super-resolution (ISR)\nduring the last years, the performance has merely been evaluated perceptually.\nThus, it is still unclear whether ISR is helpful for other vision tasks. In\nthis paper, we present the first comprehensive study and analysis of the\nusefulness of ISR for other vision applications. In particular, six ISR methods\nare evaluated on four popular vision tasks, namely edge detection, semantic\nimage segmentation, digit recognition, and scene recognition. We show that\napplying ISR to input images of other vision systems does improve their\nperformance when the input images are of low-resolution. We also study the\ncorrelation between four standard perceptual evaluation criteria (namely PSNR,\nSSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments\nshow that they correlate well with each other in general, but perceptual\ncriteria are still not accurate enough to be used as full proxies for the\nusefulness. We hope this work will inspire the community to evaluate ISR\nmethods also in real vision applications, and to adopt ISR as a pre-processing\nstep of other vision tasks if the resolution of their input images is low.",
        "In recent years, there has been a rapidly expanding focus on explaining the\npredictions made by black-box AI systems that handle image and tabular data.\nHowever, considerably less attention has been paid to explaining the\npredictions of opaque AI systems handling time series data. In this paper, we\nadvance a novel model-agnostic, case-based technique -- Native Guide -- that\ngenerates counterfactual explanations for time series classifiers. Given a\nquery time series, $T_{q}$, for which a black-box classification system\npredicts class, $c$, a counterfactual time series explanation shows how $T_{q}$\ncould change, such that the system predicts an alternative class, $c'$. The\nproposed instance-based technique adapts existing counterfactual instances in\nthe case-base by highlighting and modifying discriminative areas of the time\nseries that underlie the classification. Quantitative and qualitative results\nfrom two comparative experiments indicate that Native Guide generates\nplausible, proximal, sparse and diverse explanations that are better than those\nproduced by key benchmark counterfactual methods.",
        "Self-supervised learning of graph neural networks (GNN) is in great need\nbecause of the widespread label scarcity issue in real-world graph/network\ndata. Graph contrastive learning (GCL), by training GNNs to maximize the\ncorrespondence between the representations of the same graph in its different\naugmented forms, may yield robust and transferable GNNs even without using\nlabels. However, GNNs trained by traditional GCL often risk capturing redundant\ngraph features and thus may be brittle and provide sub-par performance in\ndownstream tasks. Here, we propose a novel principle, termed adversarial-GCL\n(AD-GCL), which enables GNNs to avoid capturing redundant information during\nthe training by optimizing adversarial graph augmentation strategies used in\nGCL. We pair AD-GCL with theoretical explanations and design a practical\ninstantiation based on trainable edge-dropping graph augmentation. We\nexperimentally validate AD-GCL by comparing with the state-of-the-art GCL\nmethods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in\ntransfer, and $3\\%$ in semi-supervised learning settings overall with 18\ndifferent benchmark datasets for the tasks of molecule property regression and\nclassification, and social network classification.",
        "Automated machine learning (AutoML) can produce complex model ensembles by\nstacking, bagging, and boosting many individual models like trees, deep\nnetworks, and nearest neighbor estimators. While highly accurate, the resulting\npredictors are large, slow, and opaque as compared to their constituents. To\nimprove the deployment of AutoML on tabular data, we propose FAST-DAD to\ndistill arbitrarily complex ensemble predictors into individual models like\nboosted trees, random forests, and deep networks. At the heart of our approach\nis a data augmentation strategy based on Gibbs sampling from a self-attention\npseudolikelihood estimator. Across 30 datasets spanning regression and\nbinary/multiclass classification tasks, FAST-DAD distillation produces\nsignificantly better individual models than one obtains through standard\ntraining on the original data. Our individual distilled models are over 10x\nfaster and more accurate than ensemble predictors produced by AutoML tools like\nH2O/AutoSklearn.",
        "Small area change detection from synthetic aperture radar (SAR) is a highly\nchallenging task. In this paper, a robust unsupervised approach is proposed for\nsmall area change detection from multi-temporal SAR images using deep learning.\nFirst, a multi-scale superpixel reconstruction method is developed to generate\na difference image (DI), which can suppress the speckle noise effectively and\nenhance edges by exploiting local, spatially homogeneous information. Second, a\ntwo-stage centre-constrained fuzzy c-means clustering algorithm is proposed to\ndivide the pixels of the DI into changed, unchanged and intermediate classes\nwith a parallel clustering strategy. Image patches belonging to the first two\nclasses are then constructed as pseudo-label training samples, and image\npatches of the intermediate class are treated as testing samples. Finally, a\nconvolutional wavelet neural network (CWNN) is designed and trained to classify\ntesting samples into changed or unchanged classes, coupled with a deep\nconvolutional generative adversarial network (DCGAN) to increase the number of\nchanged class within the pseudo-label training samples. Numerical experiments\non four real SAR datasets demonstrate the validity and robustness of the\nproposed approach, achieving up to 99.61% accuracy for small area change\ndetection.",
        "A reinforcement-learning-based non-uniform compressed sensing (NCS) framework\nfor time-varying signals is introduced. The proposed scheme, referred to as\nRL-NCS, aims to boost the performance of signal recovery through an optimal and\nadaptive distribution of sensing energy among two groups of coefficients of the\nsignal, referred to as the region of interest (ROI) coefficients and non-ROI\ncoefficients. The coefficients in ROI usually have greater importance and need\nto be reconstructed with higher accuracy compared to non-ROI coefficients. In\norder to accomplish this task, the ROI is predicted at each time step using two\nspecific approaches. One of these approaches incorporates a long short-term\nmemory (LSTM) network for the prediction. The other approach employs the\nprevious ROI information for predicting the next step ROI. Using the\nexploration-exploitation technique, a Q-network learns to choose the best\napproach for designing the measurement matrix. Furthermore, a joint loss\nfunction is introduced for the efficient training of the Q-network as well as\nthe LSTM network. The result indicates a significant performance gain for our\nproposed method, even for rapidly varying signals and a reduced number of\nmeasurements.",
        "Ensembles of decision trees perform well on many problems, but are not\ninterpretable. In contrast to existing approaches in interpretability that\nfocus on explaining relationships between features and predictions, we propose\nan alternative approach to interpret tree ensemble classifiers by surfacing\nrepresentative points for each class -- prototypes. We introduce a new distance\nfor Gradient Boosted Tree models, and propose new, adaptive prototype selection\nmethods with theoretical guarantees, with the flexibility to choose a different\nnumber of prototypes in each class. We demonstrate our methods on random\nforests and gradient boosted trees, showing that the prototypes can perform as\nwell as or even better than the original tree ensemble when used as a\nnearest-prototype classifier. In a user study, humans were better at predicting\nthe output of a tree ensemble classifier when using prototypes than when using\nShapley values, a popular feature attribution method. Hence, prototypes present\na viable alternative to feature-based explanations for tree ensembles.",
        "Dynamic dispatching is one of the core problems for operation optimization in\ntraditional industries such as mining, as it is about how to smartly allocate\nthe right resources to the right place at the right time. Conventionally, the\nindustry relies on heuristics or even human intuitions which are often\nshort-sighted and sub-optimal solutions. Leveraging the power of AI and\nInternet of Things (IoT), data-driven automation is reshaping this area.\nHowever, facing its own challenges such as large-scale and heterogenous trucks\nrunning in a highly dynamic environment, it can barely adopt methods developed\nin other domains (e.g., ride-sharing). In this paper, we propose a novel Deep\nReinforcement Learning approach to solve the dynamic dispatching problem in\nmining. We first develop an event-based mining simulator with parameters\ncalibrated in real mines. Then we propose an experience-sharing Deep Q Network\nwith a novel abstract state/action representation to learn memories from\nheterogeneous agents altogether and realizes learning in a centralized way. We\ndemonstrate that the proposed methods significantly outperform the most widely\nadopted approaches in the industry by $5.56\\%$ in terms of productivity. The\nproposed approach has great potential in a broader range of industries (e.g.,\nmanufacturing, logistics) which have a large-scale of heterogenous equipment\nworking in a highly dynamic environment, as a general framework for dynamic\nresource allocation.",
        "Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases\non longitudinal data has drawn great interest from computer vision researchers.\nThe current state-of-the-art models for many image classification tasks are\nbased on the Convolutional Neural Networks (CNN). However, a key challenge in\napplying CNN to biological problems is that the available labeled training\nsamples are very limited. Another issue for CNN to be applied in computer aided\ndiagnosis applications is that to achieve better diagnosis and prognosis\naccuracy, one usually has to deal with the longitudinal dataset, i.e., the\ndataset of images scanned at different time points. Here we argue that an\nenhanced CNN model with transfer learning for the joint analysis of tasks from\nmultiple time points or regions of interests may have a potential to improve\nthe accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN\nbased deep learning multi-task dictionary learning framework to address the\nabove challenges. Firstly, we pre-train CNN on the ImageNet dataset and\ntransfer the knowledge from the pre-trained model to the medical imaging\nprogression representation, generating the features for different tasks. Then,\nwe propose a novel unsupervised learning method, termed Multi-task Stochastic\nCoordinate Coding (MSCC), for learning different tasks by using shared and\nindividual dictionaries and generating the sparse features required to predict\nthe future cognitive clinical scores. We apply our new model in a publicly\navailable neuroimaging cohort to predict clinical measures with two different\nfeature sets and compare them with seven other state-of-the-art methods. The\nexperimental results show our proposed method achieved superior results.",
        "A graph is a powerful concept for representation of relations between pairs\nof entities. Data with underlying graph structure can be found across many\ndisciplines and there is a natural desire for understanding such data better.\nDeep learning (DL) has achieved significant breakthroughs in a variety of\nmachine learning tasks in recent years, especially where data is structured on\na grid, such as in text, speech, or image understanding. However, surprisingly\nlittle has been done to explore the applicability of DL on arbitrary\ngraph-structured data directly.\n  The goal of this thesis is to investigate architectures for DL on graphs and\nstudy how to transfer, adapt or generalize concepts that work well on\nsequential and image data to this domain. We concentrate on two important\nprimitives: embedding graphs or their nodes into a continuous vector space\nrepresentation (encoding) and, conversely, generating graphs from such vectors\nback (decoding). To that end, we make the following contributions.\n  First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like\noperation on graphs performed in the spatial domain where filters are\ndynamically generated based on edge attributes. The method is used to encode\ngraphs with arbitrary and varying structure.\n  Second, we propose SuperPoint Graph, an intermediate point cloud\nrepresentation with rich edge attributes encoding the contextual relationship\nbetween object parts. Based on this representation, ECC is employed to segment\nlarge-scale point clouds without major sacrifice in fine details.\n  Third, we present GraphVAE, a graph generator allowing us to decode graphs\nwith variable but upper-bounded number of nodes making use of approximate graph\nmatching for aligning the predictions of an autoencoder with its inputs. The\nmethod is applied to the task of molecule generation.",
        "Graph neural networks (GNNs) have achieved outstanding performance in\nlearning graph-structured data and various tasks. However, many current GNNs\nsuffer from three common problems when facing large-size graphs or using a\ndeeper structure: neighbors explosion, node dependence, and oversmoothing. Such\nproblems attribute to the data structures of the graph itself or the designing\nof the multi-layers GNNs framework, and can lead to low training efficiency and\nhigh space complexity. To deal with these problems, in this paper, we propose a\ngeneral subgraph-based training framework, namely Ripple Walk Training (RWT),\nfor deep and large graph neural networks. RWT samples subgraphs from the full\ngraph to constitute a mini-batch, and the full GNN is updated based on the\nmini-batch gradient. We analyze the high-quality subgraphs to train GNNs in a\ntheoretical way. A novel sampling method Ripple Walk Sampler works for sampling\nthese high-quality subgraphs to constitute the mini-batch, which considers both\nthe randomness and connectivity of the graph-structured data. Extensive\nexperiments on different sizes of graphs demonstrate the effectiveness and\nefficiency of RWT in training various GNNs (GCN & GAT).",
        "Gradient-based algorithms are effective for many machine learning tasks, but\ndespite ample recent effort and some progress, it often remains unclear why\nthey work in practice in optimising high-dimensional non-convex functions and\nwhy they find good minima instead of being trapped in spurious ones.\n  Here we present a quantitative theory explaining this behaviour in a spiked\nmatrix-tensor model.\n  Our framework is based on the Kac-Rice analysis of stationary points and a\nclosed-form analysis of gradient-flow originating from statistical physics. We\nshow that there is a well defined region of parameters where the gradient-flow\nalgorithm finds a good global minimum despite the presence of exponentially\nmany spurious local minima.\n  We show that this is achieved by surfing on saddles that have strong negative\ndirection towards the global minima, a phenomenon that is connected to a\nBBP-type threshold in the Hessian describing the critical points of the\nlandscapes.",
        "An \"elephant in the room\" for most current object detection and localization\nmethods is the lack of explicit modelling of partial visibility due to\nocclusion by other objects or truncation by the image boundary. Based on a\nsliding window approach, we propose a detection method which explicitly models\npartial visibility by treating it as a latent variable. A novel non-maximum\nsuppression scheme is proposed which takes into account the inferred partial\nvisibility of objects while providing a globally optimal solution. The method\ngives more detailed scene interpretations than conventional detectors in that\nwe are able to identify the visible parts of an object. We report improved\naverage precision on the PASCAL VOC 2010 dataset compared to a baseline\ndetector.",
        "Although Transformer has made breakthrough success in widespread domains\nespecially in Natural Language Processing (NLP), applying it to time series\nforecasting is still a great challenge. In time series forecasting, the\nautoregressive decoding of canonical Transformer models could introduce huge\naccumulative errors inevitably. Besides, utilizing Transformer to deal with\nspatial-temporal dependencies in the problem still faces tough difficulties.~To\ntackle these limitations, this work is the first attempt to propose a\nNon-Autoregressive Transformer architecture for time series forecasting, aiming\nat overcoming the time delay and accumulative error issues in the canonical\nTransformer. Moreover, we present a novel spatial-temporal attention mechanism,\nbuilding a bridge by a learned temporal influence map to fill the gaps between\nthe spatial and temporal attention, so that spatial and temporal dependencies\ncan be processed integrally. Empirically, we evaluate our model on diversified\nego-centric future localization datasets and demonstrate state-of-the-art\nperformance on both real-time and accuracy.",
        "Temporal Point Processes (TPPs) are often used to represent the sequence of\nevents ordered as per the time of occurrence. Owing to their flexible nature,\nTPPs have been used to model different scenarios and have shown applicability\nin various real-world applications. While TPPs focus on modeling the event\noccurrence, Marked Temporal Point Process (MTPP) focuses on modeling the\ncategory/class of the event as well (termed as the marker). Research in MTPP\nhas garnered substantial attention over the past few years, with an extensive\nfocus on supervised algorithms. Despite the research focus, limited attention\nhas been given to the challenging problem of developing solutions in\nsemi-supervised settings, where algorithms have access to a mix of labeled and\nunlabeled data. This research proposes a novel algorithm for Semi-supervised\nLearning for Marked Temporal Point Processes (SSL-MTPP) applicable in such\nscenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled\nand unlabeled data for learning a robust marker prediction model. The proposed\nalgorithm utilizes an RNN-based Encoder-Decoder module for learning effective\nrepresentations of the time sequence. The efficacy of the proposed algorithm\nhas been demonstrated via multiple protocols on the Retweet dataset, where the\nproposed SSL-MTPP demonstrates improved performance in comparison to the\ntraditional supervised learning approach.",
        "Large-scale trademark retrieval is an important content-based image retrieval\ntask. A recent study shows that off-the-shelf deep features aggregated with\nRegional-Maximum Activation of Convolutions (R-MAC) achieve state-of-the-art\nresults. However, R-MAC suffers in the presence of background clutter/trivial\nregions and scale variance, and discards important spatial information. We\nintroduce three simple but effective modifications to R-MAC to overcome these\ndrawbacks. First, we propose the use of both sum and max pooling to minimise\nthe loss of spatial information. We also employ domain-specific unsupervised\nsoft-attention to eliminate background clutter and unimportant regions.\nFinally, we add multi-resolution inputs to enhance the scale-invariance of\nR-MAC. We evaluate these three modifications on the million-scale METU dataset.\nOur results show that all modifications bring non-trivial improvements, and\nsurpass previous state-of-the-art results.",
        "Markov networks are widely used in many Machine Learning applications\nincluding natural language processing, computer vision, and bioinformatics .\nLearning Markov networks have many complications ranging from intractable\ncomputations involved to the possibility of learning a model with a huge number\nof parameters. In this report, we provide a computationally tractable greedy\nheuristic for learning Markov networks structure. The proposed heuristic\nresults in a model with a limited predefined number of parameters. We ran our\nmethod on 3 fully-observed real datasets, and we observed that our method is\ndoing comparably good to the state of the art methods.",
        "In this paper, we study the problem of compressed sensing using binary\nmeasurement matrices and $\\ell_1$-norm minimization (basis pursuit) as the\nrecovery algorithm. We derive new upper and lower bounds on the number of\nmeasurements to achieve robust sparse recovery with binary matrices. We\nestablish sufficient conditions for a column-regular binary matrix to satisfy\nthe robust null space property (RNSP) and show that the associated sufficient\nconditions % sparsity bounds for robust sparse recovery obtained using the RNSP\nare better by a factor of $(3 \\sqrt{3})/2 \\approx 2.6$ compared to the\nsufficient conditions obtained using the restricted isometry property (RIP).\nNext we derive universal \\textit{lower} bounds on the number of measurements\nthat any binary matrix needs to have in order to satisfy the weaker sufficient\ncondition based on the RNSP and show that bipartite graphs of girth six are\noptimal. Then we display two classes of binary matrices, namely parity check\nmatrices of array codes and Euler squares, which have girth six and are nearly\noptimal in the sense of almost satisfying the lower bound. In principle,\nrandomly generated Gaussian measurement matrices are \"order-optimal\". So we\ncompare the phase transition behavior of the basis pursuit formulation using\nbinary array codes and Gaussian matrices and show that (i) there is essentially\nno difference between the phase transition boundaries in the two cases and (ii)\nthe CPU time of basis pursuit with binary matrices is hundreds of times faster\nthan with Gaussian matrices and the storage requirements are less. Therefore it\nis suggested that binary matrices are a viable alternative to Gaussian matrices\nfor compressed sensing using basis pursuit. \\end{abstract}",
        "Individual neurons in convolutional neural networks supervised for\nimage-level classification tasks have been shown to implicitly learn\nsemantically meaningful concepts ranging from simple textures and shapes to\nwhole or partial objects - forming a \"dictionary\" of concepts acquired through\nthe learning process. In this work we introduce a simple, efficient zero-shot\nlearning approach based on this observation. Our approach, which we call Neuron\nImportance-AwareWeight Transfer (NIWT), learns to map domain knowledge about\nnovel \"unseen\" classes onto this dictionary of learned concepts and then\noptimizes for network parameters that can effectively combine these concepts -\nessentially learning classifiers by discovering and composing learned semantic\nconcepts in deep networks. Our approach shows improvements over previous\napproaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.\nWe demonstrate our approach on a diverse set of semantic inputs as external\ndomain knowledge including attributes and natural language captions. Moreover\nby learning inverse mappings, NIWT can provide visual and textual explanations\nfor the predictions made by the newly learned classifiers and provide neuron\nnames. Our code is available at\nhttps://github.com/ramprs/neuron-importance-zsl.",
        "Many recent works on 3D object detection have focused on designing neural\nnetwork architectures that can consume point cloud data. While these approaches\ndemonstrate encouraging performance, they are typically based on a single\nmodality and are unable to leverage information from other modalities, such as\na camera. Although a few approaches fuse data from different modalities, these\nmethods either use a complicated pipeline to process the modalities\nsequentially, or perform late-fusion and are unable to learn interaction\nbetween different modalities at early stages. In this work, we present\nPointFusion and VoxelFusion: two simple yet effective early-fusion approaches\nto combine the RGB and point cloud modalities, by leveraging the recently\nintroduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates\nsignificant improvements in performance over approaches which only use point\ncloud data. Furthermore, the proposed method provides results competitive with\nthe state-of-the-art multimodal algorithms, achieving top-2 ranking in five of\nthe six bird's eye view and 3D detection categories on the KITTI benchmark, by\nusing a simple single stage network.",
        "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
        "Optical flow estimation is an essential step for many real-world computer\nvision tasks. Existing deep networks have achieved satisfactory results by\nmostly employing a pyramidal coarse-to-fine paradigm, where a key process is to\nadopt warped target feature based on previous flow prediction to correlate with\nsource feature for building 3D matching cost volume. However, the warping\noperation can lead to troublesome ghosting problem that results in ambiguity.\nMoreover, occluded areas are treated equally with non occluded regions in most\nexisting works, which may cause performance degradation. To deal with these\nchallenges, we propose a lightweight yet efficient optical flow network, named\nOAS-Net (occlusion aware sampling network) for accurate optical flow. First, a\nnew sampling based correlation layer is employed without noisy warping\noperation. Second, a novel occlusion aware module is presented to make raw cost\nvolume conscious of occluded regions. Third, a shared flow and occlusion\nawareness decoder is adopted for structure compactness. Experiments on Sintel\nand KITTI datasets demonstrate the effectiveness of proposed approaches.",
        "The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which improves on previous\nsimilar work.",
        "Automatic License Plate detection and Recognition (ALPR) is a quite popular\nand active research topic in the field of computer vision, image processing and\nintelligent transport systems. ALPR is used to make detection and recognition\nprocesses more robust and efficient in highly complicated environments and\nbackgrounds. Several research investigations are still necessary due to some\nconstraints such as: completeness of numbering systems of countries, different\ncolors, various languages, multiple sizes and varied fonts. For this, we\npresent in this paper an automatic framework for License Plate (LP) detection\nand recognition from complex scenes. Our framework is based on mask region\nconvolutional neural networks used for LP detection, segmentation and\nrecognition. Although some studies have focused on LP detection, LP\nrecognition, LP segmentation or just two of them, our study uses the maskr-cnn\nin the three stages. The evaluation of our framework is enhanced by four\ndatasets for different countries and consequently with various languages. In\nfact, it tested on four datasets including images captured from multiple scenes\nunder numerous conditions such as varied orientation, poor quality images,\nblurred images and complex environmental backgrounds. Extensive experiments\nshow the robustness and efficiency of our suggested framework in all datasets.",
        "This paper proposes a convolutional neural network that can fuse high-level\nprior for semantic image segmentation. Motivated by humans' vision recognition\nsystem, our key design is a three-layer generative structure consisting of\nhigh-level coding, middle-level segmentation and low-level image to introduce\nglobal prior for semantic segmentation. Based on this structure, we proposed a\ngenerative model called conditional variational auto-encoder (CVAE) that can\nbuild up the links behind these three layers. These important links include an\nimage encoder that extracts high level info from image, a segmentation encoder\nthat extracts high level info from segmentation, and a hybrid decoder that\noutputs semantic segmentation from the high level prior and input image. We\ntheoretically derive the semantic segmentation as an optimization problem\nparameterized by these links. Finally, the optimization problem enables us to\ntake advantage of state-of-the-art fully convolutional network structure for\nthe implementation of the above encoders and decoder. Experimental results on\nseveral representative datasets demonstrate our supreme performance for\nsemantic segmentation.",
        "We present HoHoNet, a versatile and efficient framework for holistic\nunderstanding of an indoor 360-degree panorama using a Latent Horizontal\nFeature (LHFeat). The compact LHFeat flattens the features along the vertical\ndirection and has shown success in modeling per-column modality for room layout\nreconstruction. HoHoNet advances in two important aspects. First, the deep\narchitecture is redesigned to run faster with improved accuracy. Second, we\npropose a novel horizon-to-dense module, which relaxes the per-column output\nshape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is\nfast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones\nrespectively, for modeling dense modalities from a high-resolution $512 \\times\n1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and\nsemantic segmentation, HoHoNet achieves results on par with current\nstate-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior\narts by a large margin.",
        "Automating molecular design using deep reinforcement learning (RL) holds the\npromise of accelerating the discovery of new chemical compounds. Existing\napproaches work with molecular graphs and thus ignore the location of atoms in\nspace, which restricts them to 1) generating single organic molecules and 2)\nheuristic reward functions. To address this, we present a novel RL formulation\nfor molecular design in Cartesian coordinates, thereby extending the class of\nmolecules that can be built. Our reward function is directly based on\nfundamental physical properties such as the energy, which we approximate via\nfast quantum-chemical methods. To enable progress towards de-novo molecular\ndesign, we introduce MolGym, an RL environment comprising several challenging\nmolecular design tasks along with baselines. In our experiments, we show that\nour agent can efficiently learn to solve these tasks from scratch by working in\na translation and rotation invariant state-action space.",
        "Current perception models in autonomous driving have become notorious for\ngreatly relying on a mass of annotated data to cover unseen cases and address\nthe long-tail problem. On the other hand, learning from unlabeled large-scale\ncollected data and incrementally self-training powerful recognition models have\nreceived increasing attention and may become the solutions of next-generation\nindustry-level powerful and robust perception models in autonomous driving.\nHowever, the research community generally suffered from data inadequacy of\nthose essential real-world scene data, which hampers the future exploration of\nfully/semi/self-supervised methods for 3D perception. In this paper, we\nintroduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the\nautonomous driving scenario. The ONCE dataset consists of 1 million LiDAR\nscenes and 7 million corresponding camera images. The data is selected from 144\ndriving hours, which is 20x longer than the largest 3D autonomous driving\ndataset available (e.g. nuScenes and Waymo), and it is collected across a range\nof different areas, periods and weather conditions. To facilitate future\nresearch on exploiting unlabeled data for 3D detection, we additionally provide\na benchmark in which we reproduce and evaluate a variety of self-supervised and\nsemi-supervised methods on the ONCE dataset. We conduct extensive analyses on\nthose methods and provide valuable observations on their performance related to\nthe scale of used data. Data, code, and more information are available at\nhttps://once-for-auto-driving.github.io/index.html.",
        "Reverse-engineering bar charts extracts textual and numeric information from\nthe visual representations of bar charts to support application scenarios that\nrequire the underlying information. In this paper, we propose a neural\nnetwork-based method for reverse-engineering bar charts. We adopt a neural\nnetwork-based object detection model to simultaneously localize and classify\ntextual information. This approach improves the efficiency of textual\ninformation extraction. We design an encoder-decoder framework that integrates\nconvolutional and recurrent neural networks to extract numeric information. We\nfurther introduce an attention mechanism into the framework to achieve high\naccuracy and robustness. Synthetic and real-world datasets are used to evaluate\nthe effectiveness of the method. To the best of our knowledge, this work takes\nthe lead in constructing a complete neural network-based method of\nreverse-engineering bar charts.",
        "To enable intelligent automated driving systems, a promising strategy is to\nunderstand how human drives and interacts with road users in complicated\ndriving situations. In this paper, we propose a 3D-aware egocentric\nspatial-temporal interaction framework for automated driving applications.\nGraph convolution networks (GCN) is devised for interaction modeling. We\nintroduce three novel concepts into GCN. First, we decompose egocentric\ninteractions into ego-thing and ego-stuff interaction, modeled by two GCNs. In\nboth GCNs, ego nodes are introduced to encode the interaction between thing\nobjects (e.g., car and pedestrian), and interaction between stuff objects\n(e.g., lane marking and traffic light). Second, objects' 3D locations are\nexplicitly incorporated into GCN to better model egocentric interactions.\nThird, to implement ego-stuff interaction in GCN, we propose a MaskAlign\noperation to extract features for irregular objects.\n  We validate the proposed framework on tactical driver behavior recognition.\nExtensive experiments are conducted using Honda Research Institute Driving\nDataset, the largest dataset with diverse tactical driver behavior annotations.\nOur framework demonstrates substantial performance boost over baselines on the\ntwo experimental settings by 3.9% and 6.0%, respectively. Furthermore, we\nvisualize the learned affinity matrices, which encode ego-thing and ego-stuff\ninteractions, to showcase the proposed framework can capture interactions\neffectively.",
        "Graph convolutional networks (GCNs) have been very successful in modeling\nnon-Euclidean data structures, like sequences of body skeletons forming actions\nmodeled as spatio-temporal graphs. Most GCN-based action recognition methods\nuse deep feed-forward networks with high computational complexity to process\nall skeletons in an action. This leads to a high number of floating point\noperations (ranging from 16G to 100G FLOPs) to process a single sample, making\ntheir adoption in restricted computation application scenarios infeasible. In\nthis paper, we propose a temporal attention module (TAM) for increasing the\nefficiency in skeleton-based action recognition by selecting the most\ninformative skeletons of an action at the early layers of the network. We\nincorporate the TAM in a light-weight GCN topology to further reduce the\noverall number of computations. Experimental results on two benchmark datasets\nshow that the proposed method outperforms with a large margin the baseline\nGCN-based method while having 2.9 times less number of computations. Moreover,\nit performs on par with the state-of-the-art with up to 9.6 times less number\nof computations.",
        "The question we answer with this work is: can we convert a text document into\nan image to exploit best image classification models to classify documents? To\nanswer this question we present a novel text classification method which\nconverts a text document into an encoded image, using word embedding and\ncapabilities of Convolutional Neural Networks (CNNs), successfully employed in\nimage classification. We evaluate our approach by obtaining promising results\non some well-known benchmark datasets for text classification. This work allows\nthe application of many of the advanced CNN architectures developed for\nComputer Vision to Natural Language Processing. We test the proposed approach\non a multi-modal dataset, proving that it is possible to use a single deep\nmodel to represent text and image in the same feature space.",
        "Resonant Beam Charging (RBC) is a wireless charging technology which supports\nmulti-watt power transfer over meter-level distance. The features of safety,\nmobility and simultaneous charging capability enable RBC to charge multiple\nmobile devices safely at the same time. To detect the devices that need to be\ncharged, a Mask R-CNN based dection model is proposed in previous work.\nHowever, considering the constraints of the RBC system, it's not easy to apply\nMask R-CNN in lightweight hardware-embedded devices because of its heavy model\nand huge computation. Thus, we propose a machine learning detection approach\nwhich provides a lighter and faster model based on traditional Mask R-CNN. The\nproposed approach makes the object detection much easier to be transplanted on\nmobile devices and reduce the burden of hardware computation. By adjusting the\nstructure of the backbone and the head part of Mask R-CNN, we reduce the\naverage detection time from $1.02\\mbox{s}$ per image to $0.6132\\mbox{s}$, and\nreduce the model size from $245\\mbox{MB}$ to $47.1\\mbox{MB}$. The improved\nmodel is much more suitable for the application in the RBC system.",
        "We propose an approach for unsupervised adaptation of object detectors from\nlabel-rich to label-poor domains which can significantly reduce annotation\ncosts associated with detection. Recently, approaches that align distributions\nof source and target images using an adversarial loss have been proven\neffective for adapting object classifiers. However, for object detection, fully\nmatching the entire distributions of source and target images to each other at\nthe global image level may fail, as domains could have distinct scene layouts\nand different combinations of objects. On the other hand, strong matching of\nlocal features such as texture and color makes sense, as it does not change\ncategory level semantics. This motivates us to propose a novel method for\ndetector adaptation based on strong local alignment and weak global alignment.\nOur key contribution is the weak alignment model, which focuses the adversarial\nalignment loss on images that are globally similar and puts less emphasis on\naligning images that are globally dissimilar. Additionally, we design the\nstrong domain alignment model to only look at local receptive fields of the\nfeature map. We empirically verify the effectiveness of our method on four\ndatasets comprising both large and small domain shifts. Our code is available\nat \\url{https://github.com/VisionLearningGroup/DA_Detection}",
        "Machine learning classifiers are often trained to recognize a set of\npre-defined classes. However, in many applications, it is often desirable to\nhave the flexibility of learning additional concepts, with limited data and\nwithout re-training on the full training set. This paper addresses this\nproblem, incremental few-shot learning, where a regular classification network\nhas already been trained to recognize a set of base classes, and several extra\nnovel classes are being considered, each with only a few labeled examples.\nAfter learning the novel classes, the model is then evaluated on the overall\nclassification performance on both base and novel classes. To this end, we\npropose a meta-learning model, the Attention Attractor Network, which\nregularizes the learning of novel classes. In each episode, we train a set of\nnew weights to recognize novel classes until they converge, and we show that\nthe technique of recurrent back-propagation can back-propagate through the\noptimization process and facilitate the learning of these parameters. We\ndemonstrate that the learned attractor network can help recognize novel classes\nwhile remembering old classes without the need to review the original training\nset, outperforming various baselines.",
        "The joint understanding of vision and language has been recently gaining a\nlot of attention in both the Computer Vision and Natural Language Processing\ncommunities, with the emergence of tasks such as image captioning, image-text\nmatching, and visual question answering. As both images and text can be encoded\nas sets or sequences of elements -- like regions and words -- proper reduction\nfunctions are needed to transform a set of encoded elements into a single\nresponse, like a classification or similarity score. In this paper, we propose\na novel fully-attentive reduction method for vision and language. Specifically,\nour approach computes a set of scores for each element of each modality\nemploying a novel variant of cross-attention, and performs a learnable and\ncross-modal reduction, which can be used for both classification and ranking.\nWe test our approach on image-text matching and visual question answering,\nbuilding fair comparisons with other reduction choices, on both COCO and VQA\n2.0 datasets. Experimentally, we demonstrate that our approach leads to a\nperformance increase on both tasks. Further, we conduct ablation studies to\nvalidate the role of each component of the approach.",
        "Remarkable results have been achieved by DCNN based self-supervised depth\nestimation approaches. However, most of these approaches can only handle either\nday-time or night-time images, while their performance degrades for all-day\nimages due to large domain shift and the variation of illumination between day\nand night images. To relieve these limitations, we propose a domain-separated\nnetwork for self-supervised depth estimation of all-day images. Specifically,\nto relieve the negative influence of disturbing terms (illumination, etc.), we\npartition the information of day and night image pairs into two complementary\nsub-spaces: private and invariant domains, where the former contains the unique\ninformation (illumination, etc.) of day and night images and the latter\ncontains essential shared information (texture, etc.). Meanwhile, to guarantee\nthat the day and night images contain the same information, the\ndomain-separated network takes the day-time images and corresponding night-time\nimages (generated by GAN) as input, and the private and invariant feature\nextractors are learned by orthogonality and similarity loss, where the domain\ngap can be alleviated, thus better depth maps can be expected. Meanwhile, the\nreconstruction and photometric losses are utilized to estimate complementary\ninformation and depth maps effectively. Experimental results demonstrate that\nour approach achieves state-of-the-art depth estimation results for all-day\nimages on the challenging Oxford RobotCar dataset, proving the superiority of\nour proposed approach.",
        "In this paper, we revisit the Image-to-Image (I2I) translation problem with\ntransition consistency, namely the consistency defined on the conditional data\nmapping between each data pairs. Explicitly parameterizing each data mappings\nwith a transition variable $t$, i.e., $x \\overset{t(x,y)}{\\mapsto}y$, we\ndiscover that existing I2I translation models mainly focus on maintaining\nconsistency on results, e.g., image reconstruction or attribute prediction,\nnamed result consistency in our paper. This restricts their generalization\nability to generate satisfactory results with unseen transitions in the test\nphase. Consequently, we propose to enforce both result consistency and\ntransition consistency for I2I translation, to benefit the problem with a\ncloser consistency between the input and output. To benefit the generalization\nability of the translation model, we propose transition encoding to facilitate\nexplicit regularization of these two {kinds} of consistencies on unseen\ntransitions. We further generalize such explicitly regularized consistencies to\ndistribution-level, thus facilitating a generalized overall consistency for I2I\ntranslation problems. With the above design, our proposed model, named\nTransition Encoding GAN (TEGAN), can poss superb generalization ability to\ngenerate realistic and semantically consistent translation results with unseen\ntransitions in the test phase. It also provides a unified understanding of the\nexisting GAN-based I2I transition models with our explicitly modeling of the\ndata mapping, i.e., transition. Experiments on four different I2I translation\ntasks demonstrate the efficacy and generality of TEGAN.",
        "Point cloud-based large scale place recognition is fundamental for many\napplications like Simultaneous Localization and Mapping (SLAM). Although many\nmodels have been proposed and have achieved good performance by learning\nshort-range local features, long-range contextual properties have often been\nneglected. Moreover, the model size has also become a bottleneck for their wide\napplications. To overcome these challenges, we propose a super light-weight\nnetwork model termed SVT-Net for large scale place recognition. Specifically,\non top of the highly efficient 3D Sparse Convolution (SP-Conv), an Atom-based\nSparse Voxel Transformer (ASVT) and a Cluster-based Sparse Voxel Transformer\n(CSVT) are proposed to learn both short-range local features and long-range\ncontextual features in this model. Consisting of ASVT and CSVT, SVT-Net can\nachieve state-of-the-art on benchmark datasets in terms of both accuracy and\nspeed with a super-light model size (0.9M). Meanwhile, two simplified versions\nof SVT-Net are introduced, which also achieve state-of-the-art and further\nreduce the model size to 0.8M and 0.4M respectively.",
        "In recent years, deep learning has rapidly become a method of choice for the\nsegmentation of medical images. Deep Neural Network (DNN) architectures such as\nUNet have achieved state-of-the-art results on many medical datasets. To\nfurther improve the performance in the segmentation task, we develop an\nensemble system which combines various deep learning architectures. We propose\na two-layer ensemble of deep learning models for the segmentation of medical\nimages. The prediction for each training image pixel made by each model in the\nfirst layer is used as the augmented data of the training image for the second\nlayer of the ensemble. The prediction of the second layer is then combined by\nusing a weights-based scheme in which each model contributes differently to the\ncombined result. The weights are found by solving linear regression problems.\nExperiments conducted on two popular medical datasets namely CAMUS and\nKvasir-SEG show that the proposed method achieves better results concerning two\nperformance metrics (Dice Coefficient and Hausdorff distance) compared to some\nwell-known benchmark algorithms.",
        "Deep convolutional neural networks (CNNs) have been successful in many tasks\nin machine vision, however, millions of weights in the form of thousands of\nconvolutional filters in CNNs makes them difficult for human intepretation or\nunderstanding in science. In this article, we introduce CAR, a greedy\nstructural compression scheme to obtain smaller and more interpretable CNNs,\nwhile achieving close to original accuracy. The compression is based on pruning\nfilters with the least contribution to the classification accuracy. We\ndemonstrate the interpretability of CAR-compressed CNNs by showing that our\nalgorithm prunes filters with visually redundant functionalities such as color\nfilters. These compressed networks are easier to interpret because they retain\nthe filter diversity of uncompressed networks with order of magnitude less\nfilters. Finally, a variant of CAR is introduced to quantify the importance of\neach image category to each CNN filter. Specifically, the most and the least\nimportant class labels are shown to be meaningful interpretations of each\nfilter.",
        "Pose guided person image generation means to generate a photo-realistic\nperson image conditioned on an input person image and a desired pose. This task\nrequires spatial manipulation of the source image according to the target pose.\nHowever, the generative adversarial networks (GANs) widely used for image\ngeneration and translation rely on spatially local and translation equivariant\noperators, i.e., convolution, pooling and unpooling, which cannot handle large\nimage deformation. This paper introduces a novel two-stream appearance transfer\nnetwork (2s-ATN) to address this challenge. It is a multi-stage architecture\nconsisting of a source stream and a target stream. Each stage features an\nappearance transfer module and several two-stream feature fusion modules. The\nformer finds the dense correspondence between the two-stream feature maps and\nthen transfers the appearance information from the source stream to the target\nstream. The latter exchange local information between the two streams and\nsupplement the non-local appearance transfer. Both quantitative and qualitative\nresults indicate the proposed 2s-ATN can effectively handle large spatial\ndeformation and occlusion while retaining the appearance details. It\noutperforms prior states of the art on two widely used benchmarks.",
        "A large fraction of the electronic health records (EHRs) consists of clinical\nmeasurements collected over time, such as lab tests and vital signs, which\nprovide important information about a patient's health status. These sequences\nof clinical measurements are naturally represented as time series,\ncharacterized by multiple variables and large amounts of missing data, which\ncomplicate the analysis. In this work, we propose a novel kernel which is\ncapable of exploiting both the information from the observed values as well the\ninformation hidden in the missing patterns in multivariate time series (MTS)\noriginating e.g. from EHRs. The kernel, called TCK$_{IM}$, is designed using an\nensemble learning strategy in which the base models are novel mixed mode\nBayesian mixture models which can effectively exploit informative missingness\nwithout having to resort to imputation methods. Moreover, the ensemble approach\nensures robustness to hyperparameters and therefore TCK$_{IM}$ is particularly\nwell suited if there is a lack of labels - a known challenge in medical\napplications. Experiments on three real-world clinical datasets demonstrate the\neffectiveness of the proposed kernel.",
        "One of the main challenges in real-world reinforcement learning is to learn\nsuccessfully from limited training samples. We show that in certain settings,\nthe available data can be dramatically increased through a form of multi-task\nlearning, by exploiting an invariance property in the tasks. We provide a\ntheoretical performance bound for the gain in sample efficiency under this\nsetting. This motivates a new approach to multi-task learning, which involves\nthe design of an appropriate neural network architecture and a prioritized\ntask-sampling strategy. We demonstrate empirically the effectiveness of the\nproposed approach on two real-world sequential resource allocation tasks where\nthis invariance property occurs: financial portfolio optimization and meta\nfederated learning.",
        "Attribution methods calculate attributions that visually explain the\npredictions of deep neural networks (DNNs) by highlighting important parts of\nthe input features. In particular, gradient-based attribution (GBA) methods are\nwidely used because they can be easily implemented through automatic\ndifferentiation. In this study, we use the attributions that filter out\nirrelevant parts of the input features and then verify the effectiveness of\nthis approach by measuring the classification accuracy of a pre-trained DNN.\nThis is achieved by calculating and applying an \\textit{attribution mask} to\nthe input features and subsequently introducing the masked features to the DNN,\nfor which the mask is designed to recursively focus attention on the parts of\nthe input related to the target label. The accuracy is enhanced under a certain\ncondition, i.e., \\textit{no implicit bias}, which can be derived based on our\ntheoretical insight into compressing the DNN into a single-layer neural\nnetwork. We also provide Gradient\\,*\\,Sign-of-Input (GxSI) to obtain the\nattribution mask that further improves the accuracy. As an example, on CIFAR-10\nthat is modified using the attribution mask obtained from GxSI, we achieve the\naccuracy ranging from 99.8\\% to 99.9\\% without additional training.",
        "The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI\n(rs-fMRI) records the temporal dynamics of intrinsic functional networks in the\nbrain. However, existing deep learning methods applied to rs-fMRI either\nneglect the functional dependency between different brain regions in a network\nor discard the information in the temporal dynamics of brain activity. To\novercome those shortcomings, we propose to formulate functional connectivity\nnetworks within the context of spatio-temporal graphs. We train a\nspatio-temporal graph convolutional network (ST-GCN) on short sub-sequences of\nthe BOLD time series to model the non-stationary nature of functional\nconnectivity. Simultaneously, the model learns the importance of graph edges\nwithin ST-GCN to gain insight into the functional connectivities contributing\nto the prediction. In analyzing the rs-fMRI of the Human Connectome Project\n(HCP, N=1,091) and the National Consortium on Alcohol and Neurodevelopment in\nAdolescence (NCANDA, N=773), ST-GCN is significantly more accurate than common\napproaches in predicting gender and age based on BOLD signals. Furthermore, the\nbrain regions and functional connections significantly contributing to the\npredictions of our model are important markers according to the neuroscience\nliterature.",
        "Pricing a rental property on Airbnb is a challenging task for the owner as it\ndetermines the number of customers for the place. On the other hand, customers\nhave to evaluate an offered price with minimal knowledge of an optimal value\nfor the property. This paper aims to develop a reliable price prediction model\nusing machine learning, deep learning, and natural language processing\ntechniques to aid both the property owners and the customers with price\nevaluation given minimal available information about the property. Features of\nthe rentals, owner characteristics, and the customer reviews will comprise the\npredictors, and a range of methods from linear regression to tree-based models,\nsupport-vector regression (SVR), K-means Clustering (KMC), and neural networks\n(NNs) will be used for creating the prediction model.",
        "Graphs and networks are a key research tool for a variety of science fields,\nmost notably chemistry, biology, engineering and social sciences. Modeling and\ngeneration of graphs with efficient sampling is a key challenge for graphs. In\nparticular, the non-uniqueness, high dimensionality of the vertices and local\ndependencies of the edges may render the task challenging. We apply our\nrecently introduced method, Generative Examination Networks (GENs) to create\nthe first text-based generative graph models using one-line text formats as\ngraph representation. In our GEN, a RNN-generative model for a one-line text\nformat learns autonomously to predict the next available character. The\ntraining is stopped by an examination mechanism checking validating the\npercentage of valid graphs generated. We achieved moderate to high validity\nusing dense g6 strings (random 67.8 +/- 0.6, canonical 99.1 +/- 0.2). Based on\nthese results we have adapted the widely used SMILES representation for\nmolecules to a new input format, which we call linear graph input (LGI). Apart\nfrom the benefits of a short compressible text-format, a major advantage\ninclude the possibility to randomize and augment the format. The generative\nmodels are evaluated for overall performance and for reconstruction of the\nproperty space. The results show that LGI strings are very well suited for\nmachine-learning and that augmentation is essential for the performance of the\nmodel in terms of validity, uniqueness and novelty. Lastly, the format can\naddress smaller and larger dataset of graphs and the format can be easily\nadapted to define another meaning of the characters used in the LGI-string and\ncan address sparse graph problems in used in other fields of science.",
        "Deeper convolutional neural networks provide more capacity to approximate\ncomplex mapping functions. However, increasing network depth imposes\ndifficulties on training and increases model complexity. This paper presents a\nnew nonlinear computational layer of considerably high capacity to the deep\nconvolutional neural network architectures. This layer performs a set of\ncomprehensive convolution operations that mimics the overall function of the\nhuman visual system (HVS) via focusing on learning structural information in\nits input. The core of its computations is evaluating the components of the\nstructural similarity metric (SSIM) in a setting that allows the kernels to\nlearn to match structural information. The proposed SSIMLayer is inherently\nnonlinear and hence, it does not require subsequent nonlinear transformations.\nExperiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer\nprovides better convergence than the traditional convolutional layer, bypasses\nthe need for nonlinear transformations and shows more robustness against noise\nperturbations and adversarial attacks.",
        "Several papers argue that wide minima generalize better than narrow minima.\nIn this paper, through detailed experiments that not only corroborate the\ngeneralization properties of wide minima, we also provide empirical evidence\nfor a new hypothesis that the density of wide minima is likely lower than the\ndensity of narrow minima. Further, motivated by this hypothesis, we design a\nnovel explore-exploit learning rate schedule. On a variety of image and natural\nlanguage datasets, compared to their original hand-tuned learning rate\nbaselines, we show that our explore-exploit schedule can result in either up to\n0.84% higher absolute accuracy using the original training budget or up to 57%\nreduced training time while achieving the original reported accuracy. For\nexample, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN)\ndataset by just modifying the learning rate schedule of a high performing\nmodel.",
        "Supervised semantic segmentation normally assumes the test data being in a\nsimilar data domain as the training data. However, in practice, the domain\nmismatch between the training and unseen data could lead to a significant\nperformance drop. Obtaining accurate pixel-wise label for images in different\ndomains is tedious and labor intensive, especially for histopathology images.\nIn this paper, we propose a dual adaptive pyramid network (DAPNet) for\nhistopathological gland segmentation adapting from one stain domain to another.\nWe tackle the domain adaptation problem on two levels: 1) the image-level\nconsiders the differences of image color and style; 2) the feature-level\naddresses the spatial inconsistency between two domains. The two components are\nimplemented as domain classifiers with adversarial training. We evaluate our\nnew approach using two gland segmentation datasets with H&E and DAB-H stains\nrespectively. The extensive experiments and ablation study demonstrate the\neffectiveness of our approach on the domain adaptive segmentation task. We show\nthat the proposed approach performs favorably against other state-of-the-art\nmethods.",
        "Convolutional neural networks have been applied to a wide variety of computer\nvision tasks. Recent advances in semantic segmentation have enabled their\napplication to medical image segmentation. While most CNNs use two-dimensional\nkernels, recent CNN-based publications on medical image segmentation featured\nthree-dimensional kernels, allowing full access to the three-dimensional\nstructure of medical images. Though closely related to semantic segmentation,\nmedical image segmentation includes specific challenges that need to be\naddressed, such as the scarcity of labelled data, the high class imbalance\nfound in the ground truth and the high memory demand of three-dimensional\nimages. In this work, a CNN-based method with three-dimensional filters is\ndemonstrated and applied to hand and brain MRI. Two modifications to an\nexisting CNN architecture are discussed, along with methods on addressing the\naforementioned challenges. While most of the existing literature on medical\nimage segmentation focuses on soft tissue and the major organs, this work is\nvalidated on data both from the central nervous system as well as the bones of\nthe hand.",
        "In many applications, a dataset can be considered as a set of observed\nsignals that live on an unknown underlying graph structure. Some of these\nsignals may be seen as white noise that has been filtered on the graph topology\nby a graph filter. Hence, the knowledge of the filter and the graph provides\nvaluable information about the underlying data generation process and the\ncomplex interactions that arise in the dataset. We hence introduce a novel\ngraph signal processing framework for jointly learning the graph and its\ngenerating filter from signal observations. We cast a new optimisation problem\nthat minimises the Wasserstein distance between the distribution of the signal\nobservations and the filtered signal distribution model. Our proposed method\noutperforms state-of-the-art graph learning frameworks on synthetic data. We\nthen apply our method to a temperature anomaly dataset, and further show how\nthis framework can be used to infer missing values if only very little\ninformation is available.",
        "In optical coherence tomography (OCT) volumes of retina, the sequential\nacquisition of the individual slices makes this modality prone to motion\nartifacts, misalignments between adjacent slices being the most noticeable. Any\ndistortion in OCT volumes can bias structural analysis and influence the\noutcome of longitudinal studies. On the other hand, presence of speckle noise\nthat is characteristic of this imaging modality, leads to inaccuracies when\ntraditional registration techniques are employed. Also, the lack of a\nwell-defined ground truth makes supervised deep-learning techniques ill-posed\nto tackle the problem. In this paper, we tackle these issues by using deep\nreinforcement learning to correct inter-frame movements in an unsupervised\nmanner. Specifically, we use dueling deep Q-network to train an artificial\nagent to find the optimal policy, i.e. a sequence of actions, that best\nimproves the alignment by maximizing the sum of reward signals. Instead of\nrelying on the ground-truth of transformation parameters to guide the rewarding\nsystem, for the first time, we use a combination of intensity based image\nsimilarity metrics. Further, to avoid the agent bias towards speckle noise, we\nensure the agent can see retinal layers as part of the interacting environment.\nFor quantitative evaluation, we simulate the eye movement artifacts by applying\n2D rigid transformations on individual B-scans. The proposed model achieves an\naverage of 0.985 and 0.914 for normalized mutual information and correlation\ncoefficient, respectively. We also compare our model with elastix intensity\nbased medical image registration approach, where significant improvement is\nachieved by our model for both noisy and denoised volumes.",
        "The remarkable performance of deep neural networks depends on the\navailability of massive labeled data. To alleviate the load of data annotation,\nactive deep learning aims to select a minimal set of training points to be\nlabelled which yields maximal model accuracy. Most existing approaches\nimplement either an `exploration'-type selection criterion, which aims at\nexploring the joint distribution of data and labels, or a `refinement'-type\ncriterion which aims at localizing the detected decision boundaries. We propose\na versatile and efficient criterion that automatically switches from\nexploration to refinement when the distribution has been sufficiently mapped.\nOur criterion relies on a process of diffusing the existing label information\nover a graph constructed from the hidden representation of the data set as\nprovided by the neural network. This graph representation captures the\nintrinsic geometry of the approximated labeling function. The diffusion-based\ncriterion is shown to be advantageous as it outperforms existing criteria for\ndeep active learning.",
        "This paper presents a new method for automatic quantification of ellipse-like\ncells in images, an important and challenging problem that has been studied by\nthe computer vision community. The proposed method can be described by two main\nsteps. Initially, image segmentation based on the k-means algorithm is\nperformed to separate different types of cells from the background. Then, a\nrobust and efficient strategy is performed on the blob contour for touching\ncells splitting. Due to the contour processing, the method achieves excellent\nresults of detection compared to manual detection performed by specialists.",
        "About 8% of the male population of the world are affected by a determined\ntype of color vision disturbance, which varies from the partial to complete\nreduction of the ability to distinguish certain colors. A considerable amount\nof color blind people are able to live all life long without knowing they have\ncolor vision disabilities and abnormalities. Nowadays the evolution of\ninformation technology and computer science, specifically image processing\ntechniques and computer graphics, can be fundamental to aid at the development\nof adaptive color blindness correction tools. This paper presents a software\ntool based on Fuzzy Logic to evaluate the type and the degree of color\nblindness a person suffer from. In order to model several degrees of color\nblindness, herein this work we modified the classical linear transform-based\nsimulation method by the use of fuzzy parameters. We also proposed four new\nmethods to correct color blindness based on a fuzzy approach: Methods A and B,\nwith and without histogram equalization. All the methods are based on\ncombinations of linear transforms and histogram operations. In order to\nevaluate the results we implemented a web-based survey to get the best results\naccording to optimize to distinguish different elements in an image. Results\nobtained from 40 volunteers proved that the Method B with histogram\nequalization got the best results for about 47% of volunteers.",
        "Removing rain streaks from a single image continues to draw attentions today\nin outdoor vision systems. In this paper, we present an efficient method to\nremove rain streaks. First, the location map of rain pixels needs to be known\nas precisely as possible, to which we implement a relatively accurate detection\nof rain streaks by utilizing two characteristics of rain streaks.The key\ncomponent of our method is to represent the intensity of each detected rain\npixel using a linear model: $p=\\alpha s + \\beta$, where $p$ is the observed\nintensity of a rain pixel and $s$ represents the intensity of the background\n(i.e., before rain-affected). To solve $\\alpha$ and $\\beta$ for each detected\nrain pixel, we concentrate on a window centered around it and form an\n$L_2$-norm cost function by considering all detected rain pixels within the\nwindow, where the corresponding rain-removed intensity of each detected rain\npixel is estimated by some neighboring non-rain pixels. By minimizing this cost\nfunction, we determine $\\alpha$ and $\\beta$ so as to construct the final\nrain-removed pixel intensity. Compared with several state-of-the-art works, our\nproposed method can remove rain streaks from a single color image much more\nefficiently - it offers not only a better visual quality but also a speed-up of\nseveral times to one degree of magnitude.",
        "Quantification of uncertainty is one of the most promising approaches to\nestablish safe machine learning. Despite its importance, it is far from being\ngenerally solved, especially for neural networks. One of the most commonly used\napproaches so far is Monte Carlo dropout, which is computationally cheap and\neasy to apply in practice. However, it can underestimate the uncertainty. We\npropose a new objective, referred to as second-moment loss (SML), to address\nthis issue. While the full network is encouraged to model the mean, the dropout\nnetworks are explicitly used to optimize the model variance. We analyze the\nperformance of the new objective on various toy and UCI regression datasets.\nComparing to the state-of-the-art of deep ensembles, SML leads to comparable\nprediction accuracies and uncertainty estimates while only requiring a single\nmodel. Under distribution shift, we observe moderate improvements. From a\nsafety perspective also the study of worst-case uncertainties is crucial. In\nthis regard we improve considerably. Finally, we show that SML can be\nsuccessfully applied to SqueezeDet, a modern object detection network. We\nimprove on its uncertainty-related scores while not deteriorating regression\nquality. As a side result, we introduce an intuitive Wasserstein distance-based\nuncertainty measure that is non-saturating and thus allows to resolve quality\ndifferences between any two uncertainty estimates.",
        "The temporal and spatial resolution of rainfall data is crucial for climate\nchange modeling studies in which its variability in space and time is\nconsidered as a primary factor. Rainfall products from different remote sensing\ninstruments (e.g., radar or satellite) provide different space-time resolutions\nbecause of the differences in their sensing capabilities. We developed an\napproach that augments rainfall data with increased time resolutions to\ncomplement relatively lower resolution products. This study proposes a neural\nnetwork architecture based on Convolutional Neural Networks (CNNs) to improve\ntemporal resolution of radar-based rainfall products and compares the proposed\nmodel with an optical flow-based interpolation method.",
        "Graph generation techniques are increasingly being adopted for drug\ndiscovery. Previous graph generation approaches have utilized relatively small\nmolecular building blocks such as atoms or simple cycles, limiting their\neffectiveness to smaller molecules. Indeed, as we demonstrate, their\nperformance degrades significantly for larger molecules. In this paper, we\npropose a new hierarchical graph encoder-decoder that employs significantly\nlarger and more flexible graph motifs as basic building blocks. Our encoder\nproduces a multi-resolution representation for each molecule in a\nfine-to-coarse fashion, from atoms to connected motifs. Each level integrates\nthe encoding of constituents below with the graph at that level. Our\nautoregressive coarse-to-fine decoder adds one motif at a time, interleaving\nthe decision of selecting a new motif with the process of resolving its\nattachments to the emerging molecule. We evaluate our model on multiple\nmolecule generation tasks, including polymers, and show that our model\nsignificantly outperforms previous state-of-the-art baselines.",
        "With an aim to increase the capture range and accelerate the performance of\nstate-of-the-art inter-subject and subject-to-template 3D registration, we\npropose deep learning-based methods that are trained to find the 3D position of\narbitrarily oriented subjects or anatomy based on slices or volumes of medical\nimages. For this, we propose regression CNNs that learn to predict the\nangle-axis representation of 3D rotations and translations using image\nfeatures. We use and compare mean square error and geodesic loss to train\nregression CNNs for 3D pose estimation used in two different scenarios:\nslice-to-volume registration and volume-to-volume registration. Our results\nshow that in such registration applications that are amendable to learning, the\nproposed deep learning methods with geodesic loss minimization can achieve\naccurate results with a wide capture range in real-time (<100ms). We also\ntested the generalization capability of the trained CNNs on an expanded age\nrange and on images of newborn subjects with similar and different MR image\ncontrasts. We trained our models on T2-weighted fetal brain MRI scans and used\nthem to predict the 3D pose of newborn brains based on T1-weighted MRI scans.\nWe showed that the trained models generalized well for the new domain when we\nperformed image contrast transfer through a conditional generative adversarial\nnetwork. This indicates that the domain of application of the trained deep\nregression CNNs can be further expanded to image modalities and contrasts other\nthan those used in training. A combination of our proposed methods with\naccelerated optimization-based registration algorithms can dramatically enhance\nthe performance of automatic imaging devices and image processing methods of\nthe future.",
        "Given a natural language expression and an image/video, the goal of referring\nsegmentation is to produce the pixel-level masks of the entities described by\nthe subject of the expression. Previous approaches tackle this problem by\nimplicit feature interaction and fusion between visual and linguistic\nmodalities in a one-stage manner. However, human tends to solve the referring\nproblem in a progressive manner based on informative words in the expression,\ni.e., first roughly locating candidate entities and then distinguishing the\ntarget one. In this paper, we propose a Cross-Modal Progressive Comprehension\n(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I\n(Image) module and a CMPC-V (Video) module to improve referring image and video\nsegmentation models. For image data, our CMPC-I module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ntarget entity as well as suppress other irrelevant ones by spatial graph\nreasoning. For video data, our CMPC-V module further exploits action words\nbased on CMPC-I to highlight the correct entity matched with the action cues by\ntemporal graph reasoning. In addition to the CMPC, we also introduce a simple\nyet effective Text-Guided Feature Exchange (TGFE) module to integrate the\nreasoned multimodal features corresponding to different levels in the visual\nbackbone under the guidance of textual information. In this way, multi-level\nfeatures can communicate with each other and be mutually refined based on the\ntextual context. Combining CMPC-I or CMPC-V with TGFE can form our image or\nvideo version referring segmentation frameworks and our frameworks achieve new\nstate-of-the-art performances on four referring image segmentation benchmarks\nand three referring video segmentation benchmarks respectively.",
        "Deep Learning has become exceptionally popular in the last few years due to\nits success in computer vision and other fields of AI. However, deep neural\nnetworks are computationally expensive, which limits their application in low\npower embedded systems, such as mobile robots. In this paper, an efficient\nneural network architecture is proposed for the problem of detecting relevant\nobjects in robot soccer environments. The ROBO model's increase in efficiency\nis achieved by exploiting the peculiarities of the environment. Compared to the\nstate-of-the-art Tiny YOLO model, the proposed network provides approximately\n35 times decrease in run time, while achieving superior average precision,\nalthough at the cost of slightly worse localization accuracy.",
        "This paper presents a new approach for synthesizing a novel street-view\npanorama given an overhead satellite image. Taking a small satellite image\npatch as input, our method generates a Google's omnidirectional street-view\ntype panorama, as if it is captured from the same geographical location as the\ncenter of the satellite patch. Existing works tackle this task as an image\ngeneration problem which adopts generative adversarial networks to implicitly\nlearn the cross-view transformations, while ignoring the domain relevance. In\nthis paper, we propose to explicitly establish the geometric correspondences\nbetween the two-view images so as to facilitate the cross-view transformation\nlearning. Specifically, we observe that when a 3D point in the real world is\nvisible in both views, there is a deterministic mapping between the projected\npoints in the two-view images given the height information of this 3D point.\nMotivated by this, we develop a novel Satellite to Street-view image Projection\n(S2SP) module which explicitly establishes such geometric correspondences and\nprojects the satellite images to the street viewpoint. With these projected\nsatellite images as network input, we next employ a generator to synthesize\nrealistic street-view panoramas that are geometrically consistent with the\nsatellite images. Our S2SP module is differentiable and the whole framework is\ntrained in an end-to-end manner. Extensive experimental results on two\ncross-view benchmark datasets demonstrate that our method generates images that\nbetter respect the scene geometry than existing approaches.",
        "Confusing classes that are ubiquitous in real world often degrade performance\nfor many vision related applications like object detection, classification, and\nsegmentation. The confusion errors are not only caused by similar visual\npatterns but also amplified by various factors during the training of our\ndesigned models, such as reduced feature resolution in the encoding process or\nimbalanced data distributions. A large amount of deep learning based network\nstructures has been proposed in recent years to deal with these individual\nfactors and improve network performance. However, to our knowledge, no existing\nwork in semantic image segmentation is designed to tackle confusion errors\nexplicitly. In this paper, we present a novel and general network structure\nthat reduces confusion errors in more direct manner and apply the network for\nsemantic segmentation. There are two major contributions in our network\nstructure: 1) We ensemble subnets with heterogeneous output spaces based on the\ndiscriminative confusing groups. The training for each subnet can distinguish\nconfusing classes within the group without affecting unrelated classes outside\nthe group. 2) We propose an improved cross-entropy loss function that maximizes\nthe probability assigned to the correct class and penalizes the probabilities\nassigned to the confusing classes at the same time. Our network structure is a\ngeneral structure and can be easily adapted to any other networks to further\nreduce confusion errors. Without any changes in the feature encoder and\npost-processing steps, our experiments demonstrate consistent and significant\nimprovements on different baseline models on Cityscapes and PASCAL VOC datasets\n(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).",
        "Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently. Previous methods have shown their efficiency in\nface parsing, which however overlook the correlation among different face\nregions. The correlation is a critical clue about the facial appearance, pose,\nexpression etc., and should be taken into account for face parsing. To this\nend, we propose to model and reason the region-wise relations by learning graph\nrepresentations, and leverage the edge information between regions for\noptimized abstraction. Specifically, we encode a facial image onto a global\ngraph representation where a collection of pixels (\"regions\") with similar\nfeatures are projected to each vertex. Our model learns and reasons over\nrelations between the regions by propagating information across vertices on the\ngraph. Furthermore, we incorporate the edge information to aggregate the\npixel-wise features onto vertices, which emphasizes on the features around\nedges for fine segmentation along edges. The finally learned graph\nrepresentation is projected back to pixel grids for parsing. Experiments\ndemonstrate that our model outperforms state-of-the-art methods on the widely\nused Helen dataset, and also exhibits the superior performance on the\nlarge-scale CelebAMask-HQ and LaPa dataset. The code is available at\nhttps://github.com/tegusi/EAGRNet.",
        "We introduce a network that directly predicts the 3D layout of lanes in a\nroad scene from a single image. This work marks a first attempt to address this\ntask with on-board sensing without assuming a known constant lane width or\nrelying on pre-mapped environments. Our network architecture, 3D-LaneNet,\napplies two new concepts: intra-network inverse-perspective mapping (IPM) and\nanchor-based lane representation. The intra-network IPM projection facilitates\na dual-representation information flow in both regular image-view and top-view.\nAn anchor-per-column output representation enables our end-to-end approach\nwhich replaces common heuristics such as clustering and outlier rejection,\ncasting lane estimation as an object detection problem. In addition, our\napproach explicitly handles complex situations such as lane merges and splits.\nResults are shown on two new 3D lane datasets, a synthetic and a real one. For\ncomparison with existing methods, we test our approach on the image-only\ntuSimple lane detection benchmark, achieving performance competitive with\nstate-of-the-art.",
        "Ad hoc teamwork is the challenging problem of designing an autonomous agent\nwhich can adapt quickly to collaborate with teammates without prior\ncoordination mechanisms, including joint training. Prior work in this area has\nfocused on closed teams in which the number of agents is fixed. In this work,\nwe consider open teams by allowing agents with different fixed policies to\nenter and leave the environment without prior notification. Our solution builds\non graph neural networks to learn agent models and joint-action value models\nunder varying team compositions. We contribute a novel action-value computation\nthat integrates the agent model and joint-action value model to produce\naction-value estimates. We empirically demonstrate that our approach\nsuccessfully models the effects other agents have on the learner, leading to\npolicies that robustly adapt to dynamic team compositions and significantly\noutperform several alternative methods.",
        "The medical image is characterized by the inter-class indistinction, high\nvariability, and noise, where the recognition of pixels is challenging. Unlike\nprevious self-attention based methods that capture context information from one\nlevel, we reformulate the self-attention mechanism from the view of the\nhigh-order graph and propose a novel method, namely Hierarchical Attention\nNetwork (HANet), to address the problem of medical image segmentation.\nConcretely, an HA module embedded in the HANet captures context information\nfrom neighbors of multiple levels, where these neighbors are extracted from the\nhigh-order graph. In the high-order graph, there will be an edge between two\nnodes only if the correlation between them is high enough, which naturally\nreduces the noisy attention information caused by the inter-class\nindistinction. The proposed HA module is robust to the variance of input and\ncan be flexibly inserted into the existing convolution neural networks. We\nconduct experiments on three medical image segmentation tasks including optic\ndisc/cup segmentation, blood vessel segmentation, and lung segmentation.\nExtensive results show our method is more effective and robust than the\nexisting state-of-the-art methods.",
        "We consider how image super resolution (SR) can contribute to an object\ndetection task in low-resolution images. Intuitively, SR gives a positive\nimpact on the object detection task. While several previous works demonstrated\nthat this intuition is correct, SR and detector are optimized independently in\nthese works. This paper proposes a novel framework to train a deep neural\nnetwork where the SR sub-network explicitly incorporates a detection loss in\nits training objective, via a tradeoff with a traditional detection loss. This\nend-to-end training procedure allows us to train SR preprocessing for any\ndifferentiable detector. We demonstrate that our task-driven SR consistently\nand significantly improves accuracy of an object detector on low-resolution\nimages for a variety of conditions and scaling factors.",
        "Due to difficulties in acquiring ground truth depth of equirectangular (360)\nimages, the quality and quantity of equirectangular depth data today is\ninsufficient to represent the various scenes in the world. Therefore, 360 depth\nestimation studies, which relied solely on supervised learning, are destined to\nproduce unsatisfactory results. Although self-supervised learning methods\nfocusing on equirectangular images (EIs) are introduced, they often have\nincorrect or non-unique solutions, causing unstable performance. In this paper,\nwe propose 360 monocular depth estimation methods which improve on the areas\nthat limited previous studies. First, we introduce a self-supervised 360 depth\nlearning method that only utilizes gravity-aligned videos, which has the\npotential to eliminate the needs for depth data during the training procedure.\nSecond, we propose a joint learning scheme realized by combining supervised and\nself-supervised learning. The weakness of each learning is compensated, thus\nleading to more accurate depth estimation. Third, we propose a non-local fusion\nblock, which retains global information encoded by vision transformer when\nreconstructing the depths. With the proposed methods, we successfully apply the\ntransformer to 360 depth estimations, to the best of our knowledge, which has\nnot been tried before. On several benchmarks, our approach achieves significant\nimprovements over previous works and establishes a state of the art.",
        "Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.",
        "Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients.",
        "Understanding generalization and estimation error of estimators for simple\nmodels such as linear and generalized linear models has attracted a lot of\nattention recently. This is in part due to an interesting observation made in\nmachine learning community that highly over-parameterized neural networks\nachieve zero training error, and yet they are able to generalize well over the\ntest samples. This phenomenon is captured by the so called double descent\ncurve, where the generalization error starts decreasing again after the\ninterpolation threshold. A series of recent works tried to explain such\nphenomenon for simple models. In this work, we analyze the asymptotics of\nestimation error in ridge estimators for convolutional linear models. These\nconvolutional inverse problems, also known as deconvolution, naturally arise in\ndifferent fields such as seismology, imaging, and acoustics among others. Our\nresults hold for a large class of input distributions that include i.i.d.\nfeatures as a special case. We derive exact formulae for estimation error of\nridge estimators that hold in a certain high-dimensional regime. We show the\ndouble descent phenomenon in our experiments for convolutional models and show\nthat our theoretical results match the experiments.",
        "Node embedding learns a low-dimensional representation for each node in the\ngraph. Recent progress on node embedding shows that proximity matrix\nfactorization methods gain superb performance and scale to large graphs with\nmillions of nodes. Existing approaches first define a proximity matrix and then\nlearn the embeddings that fit the proximity by matrix factorization. Most\nexisting matrix factorization methods adopt the same proximity for different\ntasks, while it is observed that different tasks and datasets may require\ndifferent proximity, limiting their representation power.\n  Motivated by this, we propose {\\em Lemane}, a framework with trainable\nproximity measures, which can be learned to best suit the datasets and tasks at\nhand automatically. Our method is end-to-end, which incorporates differentiable\nSVD in the pipeline so that the parameters can be trained via backpropagation.\nHowever, this learning process is still expensive on large graphs. To improve\nthe scalability, we train proximity measures only on carefully subsampled\ngraphs, and then apply standard proximity matrix factorization on the original\ngraph using the learned proximity. Note that, computing the learned proximities\nfor each pair is still expensive for large graphs, and existing techniques for\ncomputing proximities are not applicable to the learned proximities. Thus, we\npresent generalized push techniques to make our solution scalable to large\ngraphs with millions of nodes. Extensive experiments show that our proposed\nsolution outperforms existing solutions on both link prediction and node\nclassification tasks on almost all datasets.",
        "For many large undirected models that arise in real-world applications, exact\nmaximumlikelihood training is intractable, because it requires computing\nmarginal distributions of the model. Conditional training is even more\ndifficult, because the partition function depends not only on the parameters,\nbut also on the observed input, requiring repeated inference over each training\nexample. An appealing idea for such models is to independently train a local\nundirected classifier over each clique, afterwards combining the learned\nweights into a single global model. In this paper, we show that this piecewise\nmethod can be justified as minimizing a new family of upper bounds on the log\npartition function. On three natural-language data sets, piecewise training is\nmore accurate than pseudolikelihood, and often performs comparably to global\ntraining using belief propagation.",
        "Graph Neural Networks (GNNs), a generalization of deep neural networks on\ngraph data have been widely used in various domains, ranging from drug\ndiscovery to recommender systems. However, GNNs on such applications are\nlimited when there are few available samples. Meta-learning has been an\nimportant framework to address the lack of samples in machine learning, and in\nrecent years, researchers have started to apply meta-learning to GNNs. In this\nwork, we provide a comprehensive survey of different meta-learning approaches\ninvolving GNNs on various graph problems showing the power of using these two\napproaches together. We categorize the literature based on proposed\narchitectures, shared representations, and applications. Finally, we discuss\nseveral exciting future research directions and open problems.",
        "City Logistics is characterized by multiple stakeholders that often have\ndifferent views of such a complex system. From a public policy perspective,\nidentifying stakeholders, issues and trends is a daunting challenge, only\npartially addressed by traditional observation systems. Nowadays, social media\nis one of the biggest channels of public expression and is often used to\ncommunicate opinions and content related to City Logistics. The idea of this\nresearch is that analysing social media content could help in understanding the\npublic perception of City logistics. This paper offers a methodology for\ncollecting content from Twitter and implementing Machine Learning techniques\n(Unsupervised Learning and Natural Language Processing), to perform content and\nsentiment analysis. The proposed methodology is applied to more than 110 000\ntweets containing City Logistics key-terms. Results allowed the building of an\nInterest Map of concepts and a Sentiment Analysis to determine if City\nLogistics entries are positive, negative or neutral.",
        "Single image view synthesis allows for the generation of new views of a scene\ngiven a single input image. This is challenging, as it requires comprehensively\nunderstanding the 3D scene from a single image. As a result, current methods\ntypically use multiple images, train on ground-truth depth, or are limited to\nsynthetic data. We propose a novel end-to-end model for this task; it is\ntrained on real images without any ground-truth 3D information. To this end, we\nintroduce a novel differentiable point cloud renderer that is used to transform\na latent 3D point cloud of features into the target view. The projected\nfeatures are decoded by our refinement network to inpaint missing regions and\ngenerate a realistic output image. The 3D component inside of our generative\nmodel allows for interpretable manipulation of the latent feature space at test\ntime, e.g. we can animate trajectories from a single image. Unlike prior work,\nwe can generate high resolution images and generalise to other input\nresolutions. We outperform baselines and prior work on the Matterport, Replica,\nand RealEstate10K datasets.",
        "Deep neural networks (DNNs) have recently achieved state-of-the-art\nperformance and provide significant progress in many machine learning tasks,\nsuch as image classification, speech processing, natural language processing,\netc. However, recent studies have shown that DNNs are vulnerable to adversarial\nattacks. For instance, in the image classification domain, adding small\nimperceptible perturbations to the input image is sufficient to fool the DNN\nand to cause misclassification. The perturbed image, called \\textit{adversarial\nexample}, should be visually as close as possible to the original image.\nHowever, all the works proposed in the literature for generating adversarial\nexamples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\\infty}$) as\ndistance metrics to quantify the similarity between the original image and the\nadversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human\njudgment, making them not suitable to reliably assess the perceptual\nsimilarity/fidelity of adversarial examples. In this paper, we present a\ndatabase for visual fidelity assessment of adversarial examples. We describe\nthe creation of the database and evaluate the performance of fifteen\nstate-of-the-art full-reference (FR) image fidelity assessment metrics that\ncould substitute $L_{p}$ norms. The database as well as subjective scores are\npublicly available to help designing new metrics for adversarial examples and\nto facilitate future research works.",
        "Bayesian inference is used extensively to quantify the uncertainty in an\ninferred field given the measurement of a related field when the two are linked\nby a mathematical model. Despite its many applications, Bayesian inference\nfaces challenges when inferring fields that have discrete representations of\nlarge dimension, and/or have prior distributions that are difficult to\ncharacterize mathematically. In this work we demonstrate how the approximate\ndistribution learned by a deep generative adversarial network (GAN) may be used\nas a prior in a Bayesian update to address both these challenges. We\ndemonstrate the efficacy of this approach on two distinct, and remarkably\nbroad, classes of problems. The first class leads to supervised learning\nalgorithms for image classification with superior out of distribution detection\nand accuracy, and for image inpainting with built-in variance estimation. The\nsecond class leads to unsupervised learning algorithms for image denoising and\nfor solving physics-driven inverse problems.",
        "State-of-the-art approaches for semantic segmentation rely on deep\nconvolutional neural networks trained on fully annotated datasets, that have\nbeen shown to be notoriously expensive to collect, both in terms of time and\nmoney. To remedy this situation, weakly supervised methods leverage other forms\nof supervision that require substantially less annotation effort, but they\ntypically present an inability to predict precise object boundaries due to\napproximate nature of the supervisory signals in those regions. While great\nprogress has been made in improving the performance, many of these weakly\nsupervised methods are highly tailored to their own specific settings. This\nraises challenges in reusing algorithms and making steady progress. In this\npaper, we intentionally avoid such practices when tackling weakly supervised\nsemantic segmentation. In particular, we train standard neural networks with\npartial cross-entropy loss function for the labeled pixels and our proposed\nGated CRF loss for the unlabeled pixels. The Gated CRF loss is designed to\ndeliver several important assets: 1) it enables flexibility in the kernel\nconstruction to mask out influence from undesired pixel positions; 2) it\noffloads learning contextual relations to CNN and concentrates on semantic\nboundaries; 3) it does not rely on high-dimensional filtering and thus has a\nsimple implementation. Throughout the paper we present the advantages of the\nloss function, analyze several aspects of weakly supervised training, and show\nthat our `purist' approach achieves state-of-the-art performance for both\nclick-based and scribble-based annotations.",
        "We propose a novel hybrid stochastic policy gradient estimator by combining\nan unbiased policy gradient estimator, the REINFORCE estimator, with another\nbiased one, an adapted SARAH estimator for policy optimization. The hybrid\npolicy gradient estimator is shown to be biased, but has variance reduced\nproperty. Using this estimator, we develop a new Proximal Hybrid Stochastic\nPolicy Gradient Algorithm (ProxHSPGA) to solve a composite policy optimization\nproblem that allows us to handle constraints or regularizers on the policy\nparameters. We first propose a single-looped algorithm then introduce a more\npractical restarting variant. We prove that both algorithms can achieve the\nbest-known trajectory complexity $\\mathcal{O}\\left(\\varepsilon^{-3}\\right)$ to\nattain a first-order stationary point for the composite problem which is better\nthan existing REINFORCE/GPOMDP $\\mathcal{O}\\left(\\varepsilon^{-4}\\right)$ and\nSVRPG $\\mathcal{O}\\left(\\varepsilon^{-10/3}\\right)$ in the non-composite\nsetting. We evaluate the performance of our algorithm on several well-known\nexamples in reinforcement learning. Numerical results show that our algorithm\noutperforms two existing methods on these examples. Moreover, the composite\nsettings indeed have some advantages compared to the non-composite ones on\ncertain problems.",
        "Most of the successful deep neural network architectures are structured,\noften consisting of elements like convolutional neural networks and gated\nrecurrent neural networks. Recently, graph neural networks have been\nsuccessfully applied to graph structured data such as point cloud and molecular\ndata. These networks often only consider pairwise dependencies, as they operate\non a graph structure. We generalize the graph neural network into a factor\ngraph neural network (FGNN) in order to capture higher order dependencies. We\nshow that FGNN is able to represent Max-Product Belief Propagation, an\napproximate inference algorithm on probabilistic graphical models; hence it is\nable to do well when Max-Product does well. Promising results on both synthetic\nand real datasets demonstrate the effectiveness of the proposed model.",
        "Increasing numbers of software vulnerabilities are discovered every year\nwhether they are reported publicly or discovered internally in proprietary\ncode. These vulnerabilities can pose serious risk of exploit and result in\nsystem compromise, information leaks, or denial of service. We leveraged the\nwealth of C and C++ open-source code available to develop a large-scale\nfunction-level vulnerability detection system using machine learning. To\nsupplement existing labeled vulnerability datasets, we compiled a vast dataset\nof millions of open-source functions and labeled it with carefully-selected\nfindings from three different static analyzers that indicate potential\nexploits. The labeled dataset is available at: https://osf.io/d45bw/. Using\nthese datasets, we developed a fast and scalable vulnerability detection tool\nbased on deep feature representation learning that directly interprets lexed\nsource code. We evaluated our tool on code from both real software packages and\nthe NIST SATE IV benchmark dataset. Our results demonstrate that deep feature\nrepresentation learning on source code is a promising approach for automated\nsoftware vulnerability detection.",
        "To get more accurate saliency maps, recent methods mainly focus on\naggregating multi-level features from fully convolutional network (FCN) and\nintroducing edge information as auxiliary supervision. Though remarkable\nprogress has been achieved, we observe that the closer the pixel is to the\nedge, the more difficult it is to be predicted, because edge pixels have a very\nimbalance distribution. To address this problem, we propose a label decoupling\nframework (LDF) which consists of a label decoupling (LD) procedure and a\nfeature interaction network (FIN). LD explicitly decomposes the original\nsaliency map into body map and detail map, where body map concentrates on\ncenter areas of objects and detail map focuses on regions around edges. Detail\nmap works better because it involves much more pixels than traditional edge\nsupervision. Different from saliency map, body map discards edge pixels and\nonly pays attention to center areas. This successfully avoids the distraction\nfrom edge pixels during training. Therefore, we employ two branches in FIN to\ndeal with body map and detail map respectively. Feature interaction (FI) is\ndesigned to fuse the two complementary branches to predict the saliency map,\nwhich is then used to refine the two branches again. This iterative refinement\nis helpful for learning better representations and more precise saliency maps.\nComprehensive experiments on six benchmark datasets demonstrate that LDF\noutperforms state-of-the-art approaches on different evaluation metrics.",
        "Robust multi-object tracking (MOT) is a prerequisite fora safe deployment of\nself-driving cars. Tracking objects, however, remains a highly challenging\nproblem, especially in cluttered autonomous driving scenes in which objects\ntend to interact with each other in complex ways and frequently get occluded.\nWe propose a novel approach to MOT that uses attention to compute track\nembeddings that encode the spatiotemporal dependencies between observed\nobjects. This attention measurement encoding allows our model to relax hard\ndata associations, which may lead to unrecoverable errors. Instead, our model\naggregates information from all object detections via soft data associations.\nThe resulting latent space representation allows our model to learn to reason\nabout occlusions in a holistic data-driven way and maintain track estimates for\nobjects even when they are occluded. Our experimental results on the Waymo\nOpenDataset suggest that our approach leverages modern large-scale datasets and\nperforms favorably compared to the state of the art in visual multi-object\ntracking.",
        "In medical applications, the same anatomical structures may be observed in\nmultiple modalities despite the different image characteristics. Currently,\nmost deep models for multimodal segmentation rely on paired registered images.\nHowever, multimodal paired registered images are difficult to obtain in many\ncases. Therefore, developing a model that can segment the target objects from\ndifferent modalities with unpaired images is significant for many clinical\napplications. In this work, we propose a novel two-stream translation and\nsegmentation unified attentional generative adversarial network (UAGAN), which\ncan perform any-to-any image modality translation and segment the target\nobjects simultaneously in the case where two or more modalities are available.\nThe translation stream is used to capture modality-invariant features of the\ntarget anatomical structures. In addition, to focus on segmentation-related\nfeatures, we add attentional blocks to extract valuable features from the\ntranslation stream. Experiments on three-modality brain tumor segmentation\nindicate that UAGAN outperforms the existing methods in most cases.",
        "Semantic segmentation is an established while rapidly evolving field in\nmedical imaging. In this paper we focus on the segmentation of brain Magnetic\nResonance Images (MRI) into cerebral structures using convolutional neural\nnetworks (CNN). CNNs achieve good performance by finding effective high\ndimensional image features describing the patch content only. In this work, we\npropose different ways to introduce spatial constraints into the network to\nfurther reduce prediction inconsistencies.\n  A patch based CNN architecture was trained, making use of multiple scales to\ngather contextual information. Spatial constraints were introduced within the\nCNN through a distance to landmarks feature or through the integration of a\nprobability atlas. We demonstrate experimentally that using spatial information\nhelps to reduce segmentation inconsistencies.",
        "This paper studies semi-supervised object classification in relational data,\nwhich is a fundamental problem in relational data modeling. The problem has\nbeen extensively studied in the literature of both statistical relational\nlearning (e.g. relational Markov networks) and graph neural networks (e.g.\ngraph convolutional networks). Statistical relational learning methods can\neffectively model the dependency of object labels through conditional random\nfields for collective classification, whereas graph neural networks learn\neffective object representations for classification through end-to-end\ntraining. In this paper, we propose the Graph Markov Neural Network (GMNN) that\ncombines the advantages of both worlds. A GMNN models the joint distribution of\nobject labels with a conditional random field, which can be effectively trained\nwith the variational EM algorithm. In the E-step, one graph neural network\nlearns effective object representations for approximating the posterior\ndistributions of object labels. In the M-step, another graph neural network is\nused to model the local label dependency. Experiments on object classification,\nlink classification, and unsupervised node representation learning show that\nGMNN achieves state-of-the-art results.",
        "The problem of selecting the right state-representation in a reinforcement\nlearning problem is considered. Several models (functions mapping past\nobservations to a finite set) of the observations are given, and it is known\nthat for at least one of these models the resulting state dynamics are indeed\nMarkovian. Without knowing neither which of the models is the correct one, nor\nwhat are the probabilistic characteristics of the resulting MDP, it is required\nto obtain as much reward as the optimal policy for the correct model (or for\nthe best of the correct models, if there are several). We propose an algorithm\nthat achieves that, with a regret of order T^{2/3} where T is the horizon time.",
        "Task-specific scores are often used to optimize for and evaluate the\nperformance of conditional text generation systems. However, such scores are\nnon-differentiable and cannot be used in the standard supervised learning\nparadigm. Hence, policy gradient methods are used since the gradient can be\ncomputed without requiring a differentiable objective.\n  However, we argue that current n-gram overlap based measures that are used as\nrewards can be improved by using model-based rewards transferred from tasks\nthat directly compare the similarity of sentence pairs. These reward models\neither output a score of sentence-level syntactic and semantic similarity\nbetween entire predicted and target sentences as the expected return, or for\nintermediate phrases as segmented accumulative rewards.\n  We demonstrate that using a \\textit{Transferable Reward Learner} leads to\nimproved results on semantical evaluation measures in policy-gradient models\nfor image captioning tasks. Our InferSent actor-critic model improves over a\nBLEU trained actor-critic model on MSCOCO when evaluated on a Word Mover's\nDistance similarity measure by 6.97 points, also improving on a Sliding Window\nCosine Similarity measure by 10.48 points. Similar performance improvements are\nalso obtained on the smaller Flickr-30k dataset, demonstrating the general\napplicability of the proposed transfer learning method.",
        "Learning with complete or partial supervision is powerful but relies on\never-growing human annotation efforts. As a way to mitigate this serious\nproblem, as well as to serve specific applications, unsupervised learning has\nemerged as an important field of research. In computer vision, unsupervised\nlearning comes in various guises. We focus here on the unsupervised discovery\nand matching of object categories among images in a collection, following the\nwork of Cho et al. 2015. We show that the original approach can be reformulated\nand solved as a proper optimization problem. Experiments on several benchmarks\nestablish the merit of our approach.",
        "We propose ALFA - a novel late fusion algorithm for object detection. ALFA is\nbased on agglomerative clustering of object detector predictions taking into\nconsideration both the bounding box locations and the class scores. Each\ncluster represents a single object hypothesis whose location is a weighted\ncombination of the clustered bounding boxes.\n  ALFA was evaluated using combinations of a pair (SSD and DeNet) and a triplet\n(SSD, DeNet and Faster R-CNN) of recent object detectors that are close to the\nstate-of-the-art. ALFA achieves state of the art results on PASCAL VOC 2007 and\nPASCAL VOC 2012, outperforming the individual detectors as well as baseline\ncombination strategies, achieving up to 32% lower error than the best\nindividual detectors and up to 6% lower error than the reference fusion\nalgorithm DBF - Dynamic Belief Fusion.",
        "In this paper, we consider the important problem of safe exploration in\nreinforcement learning. While reinforcement learning is well-suited to domains\nwith complex transition dynamics and high-dimensional state-action spaces, an\nadditional challenge is posed by the need for safe and efficient exploration.\nTraditional exploration techniques are not particularly useful for solving\ndangerous tasks, where the trial and error process may lead to the selection of\nactions whose execution in some states may result in damage to the learning\nsystem (or any other system). Consequently, when an agent begins an interaction\nwith a dangerous and high-dimensional state-action space, an important question\narises; namely, that of how to avoid (or at least minimize) damage caused by\nthe exploration of the state-action space. We introduce the PI-SRL algorithm\nwhich safely improves suboptimal albeit robust behaviors for continuous state\nand action control tasks and which efficiently learns from the experience\ngained from the environment. We evaluate the proposed method in four complex\ntasks: automatic car parking, pole-balancing, helicopter hovering, and business\nmanagement.",
        "Semi-supervised domain adaptation (SSDA) methods have demonstrated great\npotential in large-scale image classification tasks when massive labeled data\nare available in the source domain but very few labeled samples are provided in\nthe target domain. Existing solutions usually focus on feature alignment\nbetween the two domains while paying little attention to the discrimination\ncapability of learned representations in the target domain. In this paper, we\npresent a novel and effective method, namely Effective Label Propagation (ELP),\nto tackle this problem by using effective inter-domain and intra-domain\nsemantic information propagation. For inter-domain propagation, we propose a\nnew cycle discrepancy loss to encourage consistency of semantic information\nbetween the two domains. For intra-domain propagation, we propose an effective\nself-training strategy to mitigate the noises in pseudo-labeled target domain\ndata and improve the feature discriminability in the target domain. As a\ngeneral method, our ELP can be easily applied to various domain adaptation\napproaches and can facilitate their feature discrimination in the target\ndomain. Experiments on Office-Home and DomainNet benchmarks show ELP\nconsistently improves the classification accuracy of mainstream SSDA methods by\n2%~3%. Additionally, ELP also improves the performance of UDA methods as well\n(81.5% vs 86.1%), based on UDA experiments on the VisDA-2017 benchmark. Our\nsource code and pre-trained models will be released soon.",
        "In recent years, deep learning (DL) methods have become powerful tools for\nbiomedical image segmentation. However, high annotation efforts and costs are\ncommonly needed to acquire sufficient biomedical training data for DL models.\nTo alleviate the burden of manual annotation, in this paper, we propose a new\nweakly supervised DL approach for biomedical image segmentation using boxes\nonly annotation. First, we develop a method to combine graph search (GS) and DL\nto generate fine object masks from box annotation, in which DL uses box\nannotation to compute a rough segmentation for GS and then GS is applied to\nlocate the optimal object boundaries. During the mask generation process, we\ncarefully utilize information from box annotation to filter out potential\nerrors, and then use the generated masks to train an accurate DL segmentation\nnetwork. Extensive experiments on gland segmentation in histology images, lymph\nnode segmentation in ultrasound images, and fungus segmentation in electron\nmicroscopy images show that our approach attains superior performance over the\nbest known state-of-the-art weakly supervised DL method and is able to achieve\n(1) nearly the same accuracy compared to fully supervised DL methods with far\nless annotation effort, (2) significantly better results with similar\nannotation time, and (3) robust performance in various applications.",
        "Explainable AI (XAI) is a research area whose objective is to increase\ntrustworthiness and to enlighten the hidden mechanism of opaque machine\nlearning techniques. This becomes increasingly important in case such models\nare applied to the chemistry domain, for its potential impact on humans'\nhealth, e.g, toxicity analysis in pharmacology. In this paper, we present a\nnovel approach to tackle explainability of deep graph networks in the context\nof molecule property prediction t asks, named MEG (Molecular Explanation\nGenerator). We generate informative counterfactual explanations for a specific\nprediction under the form of (valid) compounds with high structural similarity\nand different predicted properties. Given a trained DGN, we train a\nreinforcement learning based generator to output counterfactual explanations.\nAt each step, MEG feeds the current candidate counterfactual into the DGN,\ncollects the prediction and uses it to reward the RL agent to guide the\nexploration. Furthermore, we restrict the action space of the agent in order to\nonly keep actions that maintain the molecule in a valid state. We discuss the\nresults showing how the model can convey non-ML experts with key insights into\nthe learning model focus in the neighbourhood of a molecule.",
        "State-of-the-art techniques in Generative Adversarial Networks (GANs) have\nshown remarkable success in image-to-image translation from peer domain X to\ndomain Y using paired image data. However, obtaining abundant paired data is a\nnon-trivial and expensive process in the majority of applications. When there\nis a need to translate images across n domains, if the training is performed\nbetween every two domains, the complexity of the training will increase\nquadratically. Moreover, training with data from two domains only at a time\ncannot benefit from data of other domains, which prevents the extraction of\nmore useful features and hinders the progress of this research area. In this\nwork, we propose a general framework for unsupervised image-to-image\ntranslation across multiple domains, which can translate images from domain X\nto any a domain without requiring direct training between the two domains\ninvolved in image translation. A byproduct of the framework is the reduction of\ncomputing time and computing resources, since it needs less time than training\nthe domains in pairs as is done in state-of-the-art works. Our proposed\nframework consists of a pair of encoders along with a pair of GANs which learns\nhigh-level features across different domains to generate diverse and realistic\nsamples from. Our framework shows competing results on many image-to-image\ntasks compared with state-of-the-art techniques.",
        "This is a machine learning application paper involving big data. We present\nhigh-accuracy prediction methods of rare events in semi-structured machine log\nfiles, which are produced at high velocity and high volume by NORC's\ncomputer-assisted telephone interviewing (CATI) network for conducting surveys.\nWe judiciously apply natural language processing (NLP) techniques and\ndata-mining strategies to train effective learning and prediction models for\nclassifying uncommon error messages in the log---without access to source code,\nupdated documentation or dictionaries. In particular, our simple but effective\napproach of features preallocation for learning from imbalanced data coupled\nwith naive Bayes classifiers can be conceivably generalized to supervised or\nsemi-supervised learning and prediction methods for other critical events such\nas cyberattack detection.",
        "This paper presents Deep Retinal Image Understanding (DRIU), a unified\nframework of retinal image analysis that provides both retinal vessel and optic\ndisc segmentation. We make use of deep Convolutional Neural Networks (CNNs),\nwhich have proven revolutionary in other fields of computer vision such as\nobject detection and image classification, and we bring their power to the\nstudy of eye fundus images. DRIU uses a base network architecture on which two\nset of specialized layers are trained to solve both the retinal vessel and\noptic disc segmentation. We present experimental validation, both qualitative\nand quantitative, in four public datasets for these tasks. In all of them, DRIU\npresents super-human performance, that is, it shows results more consistent\nwith a gold standard than a second human annotator used as control.",
        "We study image segmentation from an information-theoretic perspective,\nproposing a novel adversarial method that performs unsupervised segmentation by\npartitioning images into maximally independent sets. More specifically, we\ngroup image pixels into foreground and background, with the goal of minimizing\npredictability of one set from the other. An easily computed loss drives a\ngreedy search process to maximize inpainting error over these partitions. Our\nmethod does not involve training deep networks, is computationally cheap,\nclass-agnostic, and even applicable in isolation to a single unlabeled image.\nExperiments demonstrate that it achieves a new state-of-the-art in unsupervised\nsegmentation quality, while being substantially faster and more general than\ncompeting approaches.",
        "This work proposes RaNNC (Rapid Neural Network Connector) as middleware for\nautomatic hybrid parallelism. In recent deep learning research, as exemplified\nby T5 and GPT-3, the size of neural network models continues to grow. Since\nsuch models do not fit into the memory of accelerator devices, they need to be\npartitioned by model parallelism techniques. Moreover, to accelerate training\nfor huge training data, we need a combination of model and data parallelisms,\ni.e., hybrid parallelism. Given a model description for PyTorch without any\nspecification for model parallelism, RaNNC automatically partitions the model\ninto a set of subcomponents so that (1) each subcomponent fits a device memory\nand (2) a high training throughput for pipeline parallelism is achieved by\nbalancing the computation times of the subcomponents. In our experiments, we\ncompared RaNNC with two popular frameworks, Megatron-LM (hybrid parallelism)\nand GPipe (originally proposed for model parallelism, but a version allowing\nhybrid parallelism also exists), for training models with increasingly greater\nnumbers of parameters. In the pre-training of enlarged BERT models, RaNNC\nsuccessfully trained models five times larger than those Megatron-LM could, and\nRaNNC's training throughputs were comparable to Megatron-LM's when pre-training\nthe same models. RaNNC also achieved better training throughputs than GPipe on\nboth the enlarged BERT model pre-training (GPipe with hybrid parallelism) and\nthe enlarged ResNet models (GPipe with model parallelism) in all of the\nsettings we tried. These results are remarkable, since RaNNC automatically\npartitions models without any modification to their descriptions; Megatron-LM\nand GPipe require users to manually rewrite the models' descriptions.",
        "Mitotic figure count is an important marker of tumor proliferation and has\nbeen shown to be associated with patients' prognosis. Deep learning based\nmitotic figure detection methods have been utilized to automatically locate the\ncell in mitosis using hematoxylin \\& eosin (H\\&E) stained images. However, the\nmodel performance deteriorates due to the large variation of color tone and\nintensity in H\\&E images. In this work, we proposed a two stage mitotic figure\ndetection framework by fusing a detector and a deep ensemble classification\nmodel. To alleviate the impact of color variation in H\\&E images, we utilize\nboth stain normalization and data augmentation, aiding model to learn color\nirrelevant features. The proposed model obtains an F1 score of 0.7550 on the\npreliminary testing set released by the MIDOG challenge.",
        "Lots of neural network architectures have been proposed to deal with learning\ntasks on graph-structured data. However, most of these models concentrate on\nonly node features during the learning process. The edge features, which\nusually play a similarly important role as the nodes, are often ignored or\nsimplified by these models. In this paper, we present edge-featured graph\nattention networks, namely EGATs, to extend the use of graph neural networks to\nthose tasks learning on graphs with both node and edge features. These models\ncan be regarded as extensions of graph attention networks (GATs). By reforming\nthe model structure and the learning process, the new models can accept node\nand edge features as inputs, incorporate the edge information into feature\nrepresentations, and iterate both node and edge features in a parallel but\nmutual way. The results demonstrate that our work is highly competitive against\nother node classification approaches, and can be well applied in edge-featured\ngraph learning tasks.",
        "Multi Task Learning (MTL) efficiently leverages useful information contained\nin multiple related tasks to help improve the generalization performance of all\ntasks. This article conducts a large dimensional analysis of a simple but, as\nwe shall see, extremely powerful when carefully tuned, Least Square Support\nVector Machine (LSSVM) version of MTL, in the regime where the dimension $p$ of\nthe data and their number $n$ grow large at the same rate.\n  Under mild assumptions on the input data, the theoretical analysis of the\nMTL-LSSVM algorithm first reveals the \"sufficient statistics\" exploited by the\nalgorithm and their interaction at work. These results demonstrate, as a\nstriking consequence, that the standard approach to MTL-LSSVM is largely\nsuboptimal, can lead to severe effects of negative transfer but that these\nimpairments are easily corrected. These corrections are turned into an improved\nMTL-LSSVM algorithm which can only benefit from additional data, and the\ntheoretical performance of which is also analyzed.\n  As evidenced and theoretically sustained in numerous recent works, these\nlarge dimensional results are robust to broad ranges of data distributions,\nwhich our present experiments corroborate. Specifically, the article reports a\nsystematically close behavior between theoretical and empirical performances on\npopular datasets, which is strongly suggestive of the applicability of the\nproposed carefully tuned MTL-LSSVM method to real data. This fine-tuning is\nfully based on the theoretical analysis and does not in particular require any\ncross validation procedure. Besides, the reported performances on real datasets\nalmost systematically outperform much more elaborate and less intuitive\nstate-of-the-art multi-task and transfer learning methods.",
        "Transformers are increasingly dominating multi-modal reasoning tasks, such as\nvisual question answering, achieving state-of-the-art results thanks to their\nability to contextualize information using the self-attention and co-attention\nmechanisms. These attention modules also play a role in other computer vision\ntasks including object detection and image segmentation. Unlike Transformers\nthat only use self-attention, Transformers with co-attention require to\nconsider multiple attention maps in parallel in order to highlight the\ninformation that is relevant to the prediction in the model's input. In this\nwork, we propose the first method to explain prediction by any\nTransformer-based architecture, including bi-modal Transformers and\nTransformers with co-attentions. We provide generic solutions and apply these\nto the three most commonly used of these architectures: (i) pure\nself-attention, (ii) self-attention combined with co-attention, and (iii)\nencoder-decoder attention. We show that our method is superior to all existing\nmethods which are adapted from single modality explainability.",
        "Contextual information can have a substantial impact on the performance of\nvisual tasks such as semantic segmentation, object detection, and geometric\nestimation. Data stored in Geographic Information Systems (GIS) offers a rich\nsource of contextual information that has been largely untapped by computer\nvision. We propose to leverage such information for scene understanding by\ncombining GIS resources with large sets of unorganized photographs using\nStructure from Motion (SfM) techniques. We present a pipeline to quickly\ngenerate strong 3D geometric priors from 2D GIS data using SfM models aligned\nwith minimal user input. Given an image resectioned against this model, we\ngenerate robust predictions of depth, surface normals, and semantic labels. We\nshow that the precision of the predicted geometry is substantially more\naccurate other single-image depth estimation methods. We then demonstrate the\nutility of these contextual constraints for re-scoring pedestrian detections,\nand use these GIS contextual features alongside object detection score maps to\nimprove a CRF-based semantic segmentation framework, boosting accuracy over\nbaseline models.",
        "Generative Adversarial Networks (GAN) have attracted much research attention\nrecently, leading to impressive results for natural image generation. However,\nto date little success was observed in using GAN generated images for improving\nclassification tasks. Here we attempt to explore, in the context of car license\nplate recognition, whether it is possible to generate synthetic training data\nusing GAN to improve recognition accuracy. With a carefully-designed pipeline,\nwe show that the answer is affirmative. First, a large-scale image set is\ngenerated using the generator of GAN, without manual annotation. Then, these\nimages are fed to a deep convolutional neural network (DCNN) followed by a\nbidirectional recurrent neural network (BRNN) with long short-term memory\n(LSTM), which performs the feature learning and sequence labelling. Finally,\nthe pre-trained model is fine-tuned on real images. Our experimental results on\na few data sets demonstrate the effectiveness of using GAN images: an\nimprovement of 7.5% over a strong baseline with moderate-sized real data being\navailable. We show that the proposed framework achieves competitive recognition\naccuracy on challenging test datasets. We also leverage the depthwise separate\nconvolution to construct a lightweight convolutional RNN, which is about half\nsize and 2x faster on CPU. Combining this framework and the proposed pipeline,\nwe make progress in performing accurate recognition on mobile and embedded\ndevices.",
        "Learning to compare two objects are essential in applications, such as\ndigital forensics, face recognition, and brain network analysis, especially\nwhen labeled data is scarce and imbalanced. As these applications make\nhigh-stake decisions and involve societal values like fairness and\ntransparency, it is critical to explain the learned models. We aim to study\npost-hoc explanations of Siamese networks (SN) widely used in learning to\ncompare. We characterize the instability of gradient-based explanations due to\nthe additional compared object in SN, in contrast to architectures with a\nsingle input instance. We propose an optimization framework that derives global\ninvariance from unlabeled data using self-learning to promote the stability of\nlocal explanations tailored for specific query-reference pairs. The\noptimization problems can be solved using gradient descent-ascent (GDA) for\nconstrained optimization, or SGD for KL-divergence regularized unconstrained\noptimization, with convergence proofs, especially when the objective functions\nare nonconvex due to the Siamese architecture. Quantitative results and case\nstudies on tabular and graph data from neuroscience and chemical engineering\nshow that the framework respects the self-learned invariance while robustly\noptimizing the faithfulness and simplicity of the explanation. We further\ndemonstrate the convergence of GDA experimentally.",
        "As autonomous vehicles become an every-day reality, high-accuracy pedestrian\ndetection is of paramount practical importance. Pedestrian detection is a\nhighly researched topic with mature methods, but most datasets focus on common\nscenes of people engaged in typical walking poses on sidewalks. But performance\nis most crucial for dangerous scenarios, such as children playing in the street\nor people using bicycles/skateboards in unexpected ways. Such \"in-the-tail\"\ndata is notoriously hard to observe, making both training and testing\ndifficult. To analyze this problem, we have collected a novel annotated dataset\nof dangerous scenarios called the Precarious Pedestrian dataset. Even given a\ndedicated collection effort, it is relatively small by contemporary standards\n(around 1000 images). To allow for large-scale data-driven learning, we explore\nthe use of synthetic data generated by a game engine. A significant challenge\nis selected the right \"priors\" or parameters for synthesis: we would like\nrealistic data with poses and object configurations that mimic true Precarious\nPedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a\nmassive amount of synthetic data and train a discriminative classifier to\nselect a realistic subset, which we deem the Adversarial Imposters. We\ndemonstrate that this simple pipeline allows one to synthesize realistic\ntraining data by making use of rendering/animation engines within a GAN\nframework. Interestingly, we also demonstrate that such data can be used to\nrank algorithms, suggesting that Adversarial Imposters can also be used for\n\"in-the-tail\" validation at test-time, a notoriously difficult challenge for\nreal-world deployment.",
        "Detecting action units (AUs) on human faces is challenging because various\nAUs make subtle facial appearance change over various regions at different\nscales. Current works have attempted to recognize AUs by emphasizing important\nregions. However, the incorporation of expert prior knowledge into region\ndefinition remains under-exploited, and current AU detection approaches do not\nuse regional convolutional neural networks (R-CNN) with expert prior knowledge\nto directly focus on AU-related regions adaptively. By incorporating expert\nprior knowledge, we propose a novel R-CNN based model named AU R-CNN. The\nproposed solution offers two main contributions: (1) AU R-CNN directly observes\ndifferent facial regions, where various AUs are located. Specifically, we\ndefine an AU partition rule which encodes the expert prior knowledge into the\nregion definition and RoI-level label definition. This design produces\nconsiderably better detection performance than existing approaches. (2) We\nintegrate various dynamic models (including convolutional long short-term\nmemory, two stream network, conditional random field, and temporal action\nlocalization network) into AU R-CNN and then investigate and analyze the reason\nbehind the performance of dynamic models. Experiment results demonstrate that\n\\textit{only} static RGB image information and no optical flow-based AU R-CNN\nsurpasses the one fused with dynamic models. AU R-CNN is also superior to\ntraditional CNNs that use the same backbone on varying image resolutions.\nState-of-the-art recognition performance of AU detection is achieved. The\ncomplete network is end-to-end trainable. Experiments on BP4D and DISFA\ndatasets show the effectiveness of our approach. The implementation code is\navailable online.",
        "Recent work on exploration in reinforcement learning (RL) has led to a series\nof increasingly complex solutions to the problem. This increase in complexity\noften comes at the expense of generality. Recent empirical studies suggest\nthat, when applied to a broader set of domains, some sophisticated exploration\nmethods are outperformed by simpler counterparts, such as {\\epsilon}-greedy. In\nthis paper we propose an exploration algorithm that retains the simplicity of\n{\\epsilon}-greedy while reducing dithering. We build on a simple hypothesis:\nthe main limitation of {\\epsilon}-greedy exploration is its lack of temporal\npersistence, which limits its ability to escape local optima. We propose a\ntemporally extended form of {\\epsilon}-greedy that simply repeats the sampled\naction for a random duration. It turns out that, for many duration\ndistributions, this suffices to improve exploration on a large set of domains.\nInterestingly, a class of distributions inspired by ecological models of animal\nforaging behaviour yields particularly strong performance.",
        "State-of-the-art semantic image segmentation methods are mostly based on\ntraining deep convolutional neural networks (CNNs). In this work, we proffer to\nimprove semantic segmentation with the use of contextual information. In\nparticular, we explore `patch-patch' context and `patch-background' context in\ndeep CNNs. We formulate deep structured models by combining CNNs and\nConditional Random Fields (CRFs) for learning the patch-patch context between\nimage regions. Specifically, we formulate CNN-based pairwise potential\nfunctions to capture semantic correlations between neighboring patches.\nEfficient piecewise training of the proposed deep structured model is then\napplied in order to avoid repeated expensive CRF inference during the course of\nback propagation. For capturing the patch-background context, we show that a\nnetwork design with traditional multi-scale image inputs and sliding pyramid\npooling is very effective for improving performance. We perform comprehensive\nevaluation of the proposed method. We achieve new state-of-the-art performance\non a number of challenging semantic segmentation datasets including $NYUDv2$,\n$PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$,\n$SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an\nintersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.",
        "Transfer learning is widely used in deep neural network models when there are\nfew labeled examples available. The common approach is to take a pre-trained\nnetwork in a similar task and finetune the model parameters. This is usually\ndone blindly without a pre-selection from a set of pre-trained models, or by\nfinetuning a set of models trained on different tasks and selecting the best\nperforming one by cross-validation. We address this problem by proposing an\napproach to assess the relationship between visual tasks and their\ntask-specific models. Our method uses Representation Similarity Analysis (RSA),\nwhich is commonly used to find a correlation between neuronal responses from\nbrain data and models. With RSA we obtain a similarity score among tasks by\ncomputing correlations between models trained on different tasks. Our method is\nefficient as it requires only pre-trained models, and a few images with no\nfurther training. We demonstrate the effectiveness and efficiency of our method\nfor generating task taxonomy on Taskonomy dataset. We next evaluate the\nrelationship of RSA with the transfer learning performance on Taskonomy tasks\nand a new task: Pascal VOC semantic segmentation. Our results reveal that\nmodels trained on tasks with higher similarity score show higher transfer\nlearning performance. Surprisingly, the best transfer learning result for\nPascal VOC semantic segmentation is not obtained from the pre-trained model on\nsemantic segmentation, probably due to the domain differences, and our method\nsuccessfully selects the high performing models.",
        "Hidden Markov Models (HMMs) are a ubiquitous tool to model time series data,\nand have been widely used in two main tasks of Automatic Music Transcription\n(AMT): note segmentation, i.e. identifying the played notes after a multi-pitch\nestimation, and sequential post-processing, i.e. correcting note segmentation\nusing training data. In this paper, we employ the multi-pitch estimation method\ncalled Probabilistic Latent Component Analysis (PLCA), and develop AMT systems\nby integrating different HMM-based modules in this framework. For note\nsegmentation, we use two different twostate on/o? HMMs, including a\nhigher-order one for duration modeling. For sequential post-processing, we\nfocused on a musicological modeling of polyphonic harmonic transitions, using a\nfirst- and second-order HMMs whose states are defined through candidate note\nmixtures. These different PLCA plus HMM systems have been evaluated\ncomparatively on two different instrument repertoires, namely the piano (using\nthe MAPS database) and the marovany zither. Our results show that the use of\nHMMs could bring noticeable improvements to transcription results, depending on\nthe instrument repertoire.",
        "Generative Adversarial Networks (GANs) are a class of generative algorithms\nthat have been shown to produce state-of-the art samples, especially in the\ndomain of image creation. The fundamental principle of GANs is to approximate\nthe unknown distribution of a given data set by optimizing an objective\nfunction through an adversarial game between a family of generators and a\nfamily of discriminators. In this paper, we offer a better theoretical\nunderstanding of GANs by analyzing some of their mathematical and statistical\nproperties. We study the deep connection between the adversarial principle\nunderlying GANs and the Jensen-Shannon divergence, together with some\noptimality characteristics of the problem. An analysis of the role of the\ndiscriminator family via approximation arguments is also provided. In addition,\ntaking a statistical point of view, we study the large sample properties of the\nestimated distribution and prove in particular a central limit theorem. Some of\nour results are illustrated with simulated examples.",
        "Video question answering (VideoQA) is challenging given its multimodal\ncombination of visual understanding and natural language understanding. While\nexisting approaches seldom leverage the appearance-motion information in the\nvideo at multiple temporal scales, the interaction between the question and the\nvisual information for textual semantics extraction is frequently ignored.\nTargeting these issues, this paper proposes a novel Temporal Pyramid\nTransformer (TPT) model with multimodal interaction for VideoQA. The TPT model\ncomprises two modules, namely Question-specific Transformer (QT) and Visual\nInference (VI). Given the temporal pyramid constructed from a video, QT builds\nthe question semantics from the coarse-to-fine multimodal co-occurrence between\neach word and the visual content. Under the guidance of such question-specific\nsemantics, VI infers the visual clues from the local-to-global multi-level\ninteractions between the question and the video. Within each module, we\nintroduce a multimodal attention mechanism to aid the extraction of\nquestion-video interactions, with residual connections adopted for the\ninformation passing across different levels. Through extensive experiments on\nthree VideoQA datasets, we demonstrate better performances of the proposed\nmethod in comparison with the state-of-the-arts.",
        "We present a powerful method to extract per-point semantic class labels from\naerialphotogrammetry data. Labeling this kind of data is important for tasks\nsuch as environmental modelling, object classification and scene understanding.\nUnlike previous point cloud classification methods that rely exclusively on\ngeometric features, we show that incorporating color information yields a\nsignificant increase in accuracy in detecting semantic classes. We test our\nclassification method on three real-world photogrammetry datasets that were\ngenerated with Pix4Dmapper Pro, and with varying point densities. We show that\noff-the-shelf machine learning techniques coupled with our new features allow\nus to train highly accurate classifiers that generalize well to unseen data,\nprocessing point clouds containing 10 million points in less than 3 minutes on\na desktop computer.",
        "Objective: This paper targets a major challenge in developing practical\nEEG-based brain-computer interfaces (BCIs): how to cope with individual\ndifferences so that better learning performance can be obtained for a new\nsubject, with minimum or even no subject-specific data? Methods: We propose a\nnovel approach to align EEG trials from different subjects in the Euclidean\nspace to make them more similar, and hence improve the learning performance for\na new subject. Our approach has three desirable properties: 1) it aligns the\nEEG trials directly in the Euclidean space, and any signal processing, feature\nextraction and machine learning algorithms can then be applied to the aligned\ntrials; 2) its computational cost is very low; and, 3) it is unsupervised and\ndoes not need any label information from the new subject. Results: Both offline\nand simulated online experiments on motor imagery classification and\nevent-related potential classification verified that our proposed approach\noutperformed a state-of-the-art Riemannian space data alignment approach, and\nseveral approaches without data alignment. Conclusion: The proposed Euclidean\nspace EEG data alignment approach can greatly facilitate transfer learning in\nBCIs. Significance: Our proposed approach is effective, efficient, and easy to\nimplement. It could be an essential pre-processing step for EEG-based BCIs.",
        "Vision-and-language pre-training has achieved impressive success in learning\nmultimodal representations between vision and language. To generalize this\nsuccess to non-English languages, we introduce UC2, the first machine\ntranslation-augmented framework for cross-lingual cross-modal representation\nlearning. To tackle the scarcity problem of multilingual captions for image\ndatasets, we first augment existing English-only datasets with other languages\nvia machine translation (MT). Then we extend the standard Masked Language\nModeling and Image-Text Matching training objectives to multilingual setting,\nwhere alignment between different languages is captured through shared visual\ncontext (i.e, using image as pivot). To facilitate the learning of a joint\nembedding space of images and all languages of interest, we further propose two\nnovel pre-training tasks, namely Masked Region-to-Token Modeling (MRTM) and\nVisual Translation Language Modeling (VTLM), leveraging MT-enhanced translated\ndata. Evaluation on multilingual image-text retrieval and multilingual visual\nquestion answering benchmarks demonstrates that our proposed framework achieves\nnew state-of-the-art on diverse non-English benchmarks while maintaining\ncomparable performance to monolingual pre-trained models on English tasks.",
        "In neural architecture search, the structure of the neural network to best\nmodel a given dataset is determined by an automated search process. Efficient\nNeural Architecture Search (ENAS), proposed by Pham et al. (2018), has recently\nreceived considerable attention due to its ability to find excellent\narchitectures within a comparably short search time. In this work, which is\nmotivated by the quest to further improve the learning speed of architecture\nsearch, we evaluate the learning progress of the controller which generates the\narchitectures in ENAS. We measure the progress by comparing the architectures\ngenerated by it at different controller training epochs, where architectures\nare evaluated after having re-trained them from scratch. As a surprising\nresult, we find that the learning curves are completely flat, i.e., there is no\nobservable progress of the controller in terms of the performance of its\ngenerated architectures. This observation is consistent across the CIFAR-10 and\nCIFAR-100 datasets and two different search spaces. We conclude that the high\nquality of the models generated by ENAS is a result of the search space design\nrather than the controller training, and our results indicate that one-shot\narchitecture design is an efficient alternative to architecture search by ENAS.",
        "Human attention mechanisms often work in a top-down manner, yet it is not\nwell explored in vision research. Here, we propose the Top-Down Attention\nFramework (TDAF) to capture top-down attentions, which can be easily adopted in\nmost existing models. The designed Recursive Dual-Directional Nested Structure\nin it forms two sets of orthogonal paths, recursive and structural ones, where\nbottom-up spatial features and top-down attention features are extracted\nrespectively. Such spatial and attention features are nested deeply, therefore,\nthe proposed framework works in a mixed top-down and bottom-up manner.\nEmpirical evidence shows that our TDAF can capture effective stratified\nattention information and boost performance. ResNet with TDAF achieves 2.0%\nimprovements on ImageNet. For object detection, the performance is improved by\n2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And\nfor action recognition, the 3D-ResNet adopting TDAF achieves improvements of\n1.7% accuracy.",
        "Stochastic variational inference is an established way to carry out\napproximate Bayesian inference for deep models. While there have been effective\nproposals for good initializations for loss minimization in deep learning, far\nless attention has been devoted to the issue of initialization of stochastic\nvariational inference. We address this by proposing a novel layer-wise\ninitialization strategy based on Bayesian linear models. The proposed method is\nextensively validated on regression and classification tasks, including\nBayesian DeepNets and ConvNets, showing faster and better convergence compared\nto alternatives inspired by the literature on initializations for loss\nminimization.",
        "Automatic License Plate Recognition (ALPR) is a challenging problem to the\nresearch community due to its potential applicability in the diverse\ngeographical condition over the globe with varying license plate parameters.\nAny ALPR system includes three main modules, viz. localization of the license\nplate, segmentation of the characters therein and recognition of the segmented\ncharacters. In real life applications where the images are captured over days\nand nights in an outdoor environment with varying lighting and weather\nconditions, varying pollution level and wind turbulences, localization,\nsegmentation and recognition become challenging tasks. The tasks become more\ncomplex if the license plate is not in conformity with the standards laid by\ncorresponding Motor Vehicles Department in terms of various features, e.g. area\nand aspect ratio of the license plate, background color, foreground color,\nshape, number of lines, font face/ size of characters, spacing between\ncharacters etc. Besides, license plates are often dirty or broken or having\nscratches or bent or tilted at its position. All these add to the challenges in\ndeveloping an effective ALPR system.",
        "Modeling the probability distribution of rows in tabular data and generating\nrealistic synthetic data is a non-trivial task. Tabular data usually contains a\nmix of discrete and continuous columns. Continuous columns may have multiple\nmodes whereas discrete columns are sometimes imbalanced making the modeling\ndifficult. Existing statistical and deep neural network models fail to properly\nmodel this type of data. We design TGAN, which uses a conditional generative\nadversarial network to address these challenges. To aid in a fair and thorough\ncomparison, we design a benchmark with 7 simulated and 8 real datasets and\nseveral Bayesian network baselines. TGAN outperforms Bayesian methods on most\nof the real datasets whereas other deep learning methods could not.",
        "Data-driven graph learning models a network by determining the strength of\nconnections between its nodes. The data refers to a graph signal which\nassociates a value with each graph node. Existing graph learning methods either\nuse simplified models for the graph signal, or they are prohibitively expensive\nin terms of computational and memory requirements. This is particularly true\nwhen the number of nodes is high or there are temporal changes in the network.\nIn order to consider richer models with a reasonable computational\ntractability, we introduce a graph learning method based on representation\nlearning on graphs. Representation learning generates an embedding for each\ngraph node, taking the information from neighbouring nodes into account. Our\ngraph learning method further modifies the embeddings to compute the graph\nsimilarity matrix. In this work, graph learning is used to examine brain\nnetworks for brain state identification. We infer time-varying brain graphs\nfrom an extensive dataset of intracranial electroencephalographic (iEEG)\nsignals from ten patients. We then apply the graphs as input to a classifier to\ndistinguish seizure vs. non-seizure brain states. Using the binary\nclassification metric of area under the receiver operating characteristic curve\n(AUC), this approach yields an average of 9.13 percent improvement when\ncompared to two widely used brain network modeling methods.",
        "In this paper we put the visibility transformation on a clear theoretical\nfooting and show that this transform is able to embed the effect of the\nabsolute position of the data stream into signature features in a unified and\nefficient way. The generated feature set is particularly useful in pattern\nrecognition tasks, for its simplifying role in allowing the signature feature\nset to accommodate nonlinear functions of absolute and relative values.",
        "The success of neural networks in image classification has inspired various\nhardware implementations on embedded platforms such as Field Programmable Gate\nArrays, embedded processors and Graphical Processing Units. These embedded\nplatforms are constrained in terms of power, which is mainly consumed by the\nMultiply Accumulate operations and the memory accesses for weight fetching.\nQuantization and pruning have been proposed to address this issue. Though\neffective, these techniques do not take into account the underlying\narchitecture of the embedded hardware. In this work, we propose PoET-BiN, a\nLook-Up Table based power efficient implementation on resource constrained\nembedded devices. A modified Decision Tree approach forms the backbone of the\nproposed implementation in the binary domain. A LUT access consumes far less\npower than the equivalent Multiply Accumulate operation it replaces, and the\nmodified Decision Tree algorithm eliminates the need for memory accesses. We\napplied the PoET-BiN architecture to implement the classification layers of\nnetworks trained on MNIST, SVHN and CIFAR-10 datasets, with near state-of-the\nart results. The energy reduction for the classifier portion reaches up to six\norders of magnitude compared to a floating point implementations and up to\nthree orders of magnitude when compared to recent binary quantized neural\nnetworks.",
        "Online advertising is a huge, rapidly growing advertising market in today's\nworld. One common form of online advertising is using image ads. A decision is\nmade (often in real time) every time a user sees an ad, and the advertiser is\neager to determine the best ad to display. Consequently, many algorithms have\nbeen developed that calculate the optimal ad to show to the current user at the\npresent time. Typically, these algorithms focus on variations of the ad,\noptimizing among different properties such as background color, image size, or\nset of images. However, there is a more fundamental layer. Our study looks at\nnew qualities of ads that can be determined before an ad is shown (rather than\nonline optimization) and defines which ads are most likely to be successful.\n  We present a set of novel algorithms that utilize deep-learning image\nprocessing, machine learning, and graph theory to investigate online\nadvertising and to construct prediction models which can foresee an image ad's\nsuccess. We evaluated our algorithms on a dataset with over 260,000 ad images,\nas well as a smaller dataset specifically related to the automotive industry,\nand we succeeded in constructing regression models for ad image click rate\nprediction. The obtained results emphasize the great potential of using\ndeep-learning algorithms to effectively and efficiently analyze image ads and\nto create better and more innovative online ads. Moreover, the algorithms\npresented in this paper can help predict ad success and can be applied to\nanalyze other large-scale image corpora.",
        "Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.",
        "Spectral clustering is a popular method for community detection in network\ngraphs: starting from a matrix representation of the graph, the nodes are\nclustered on a low dimensional projection obtained from a truncated spectral\ndecomposition of the matrix. Estimating correctly the number of communities and\nthe dimension of the reduced latent space is critical for good performance of\nspectral clustering algorithms. Furthermore, many real-world graphs, such as\nenterprise computer networks studied in cyber-security applications, often\ndisplay heterogeneous within-community degree distributions. Such heterogeneous\ndegree distributions are usually not well captured by standard spectral\nclustering algorithms. In this article, a novel spectral clustering algorithm\nis proposed for community detection under the degree-corrected stochastic\nblockmodel. The proposed method is based on a transformation of the spectral\nembedding to spherical coordinates, and a novel modelling assumption in the\ntransformed space. The method allows for simultaneous and automated selection\nof the number of communities and the latent dimension for spectral embeddings\nof graphs with uneven node degrees. Results show improved performance over\ncompeting methods in representing computer networks.",
        "Recent works demonstrate the texture bias in Convolutional Neural Networks\n(CNNs), conflicting with early works claiming that networks identify objects\nusing shape. It is commonly believed that the cost function forces the network\nto take a greedy route to increase accuracy using texture, failing to explore\nany global statistics. We propose a novel intuitive architecture, namely\nCognitiveCNN, inspired from feature integration theory in psychology to utilise\nhuman-interpretable feature like shape, texture, edges etc. to reconstruct, and\nclassify the image. We define two metrics, namely TIC and RIC to quantify the\nimportance of each stream using attention maps. We introduce a regulariser\nwhich ensures that the contribution of each feature is same for any task, as it\nis for reconstruction; and perform experiments to show the resulting boost in\naccuracy and robustness besides imparting explainability. Lastly, we adapt\nthese ideas to conventional CNNs and propose Augmented Cognitive CNN to achieve\nsuperior performance in object recognition.",
        "Interpreting the behaviors of Deep Neural Networks (usually considered as a\nblack box) is critical especially when they are now being widely adopted over\ndiverse aspects of human life. Taking the advancements from Explainable\nArtificial Intelligent, this paper proposes a novel technique called Auto\nDeepVis to dissect catastrophic forgetting in continual learning. A new method\nto deal with catastrophic forgetting named critical freezing is also introduced\nupon investigating the dilemma by Auto DeepVis. Experiments on a captioning\nmodel meticulously present how catastrophic forgetting happens, particularly\nshowing which components are forgetting or changing. The effectiveness of our\ntechnique is then assessed; and more precisely, critical freezing claims the\nbest performance on both previous and coming tasks over baselines, proving the\ncapability of the investigation. Our techniques could not only be supplementary\nto existing solutions for completely eradicating catastrophic forgetting for\nlife-long learning but also explainable.",
        "Recommendation problems with large numbers of discrete items, such as\nproducts, webpages, or videos, are ubiquitous in the technology industry. Deep\nneural networks are being increasingly used for these recommendation problems.\nThese models use embeddings to represent discrete items as continuous vectors,\nand the vocabulary sizes and embedding dimensions, although heavily influence\nthe model's accuracy, are often manually selected in a heuristical manner. We\npresent Neural Input Search (NIS), a technique for learning the optimal\nvocabulary sizes and embedding dimensions for categorical features. The goal is\nto maximize prediction accuracy subject to a constraint on the total memory\nused by all embeddings. Moreover, we argue that the traditional Single-size\nEmbedding (SE), which uses the same embedding dimension for all values of a\nfeature, suffers from inefficient usage of model capacity and training data. We\npropose a novel type of embedding, namely Multi-size Embedding (ME), which\nallows the embedding dimension to vary for different values of the feature.\nDuring training we use reinforcement learning to find the optimal vocabulary\nsize for each feature and embedding dimension for each value of the feature. In\nexperiments on two common types of large scale recommendation problems, i.e.\nretrieval and ranking problems, NIS automatically found better vocabulary and\nembedding sizes that result in $6.8\\%$ and $1.8\\%$ relative improvements on\nRecall@1 and ROC-AUC over manually optimized ones.",
        "The ability to generate natural language sequences from source code snippets\nhas a variety of applications such as code summarization, documentation, and\nretrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine\ntranslation (NMT), have achieved state-of-the-art performance on these tasks by\ntreating source code as a sequence of tokens. We present ${\\rm {\\scriptsize\nCODE2SEQ}}$: an alternative approach that leverages the syntactic structure of\nprogramming languages to better encode source code. Our model represents a code\nsnippet as the set of compositional paths in its abstract syntax tree (AST) and\nuses attention to select the relevant paths while decoding. We demonstrate the\neffectiveness of our approach for two tasks, two programming languages, and\nfour datasets of up to $16$M examples. Our model significantly outperforms\nprevious models that were specifically designed for programming languages, as\nwell as state-of-the-art NMT models. An interactive online demo of our model is\navailable at http://code2seq.org. Our code, data and trained models are\navailable at http://github.com/tech-srl/code2seq.",
        "As the field of machine learning for combinatorial optimization advances,\ntraditional problems are resurfaced and readdressed through this new\nperspective. The overwhelming majority of the literature focuses on small graph\nproblems, while several real-world problems are devoted to large graphs. Here,\nwe focus on two such problems that are related: influence estimation, a\n\\#P-hard counting problem, and influence maximization, an NP-hard problem. We\ndevelop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an\nupper bound of influence estimation and train it on small simulated graphs.\nExperiments show that GLIE can provide accurate predictions faster than the\nalternatives for graphs 10 times larger than the train set. More importantly,\nit can be used on arbitrary large graphs for influence maximization, as the\npredictions can rank effectively seed sets even when the accuracy deteriorates.\nTo showcase this, we propose a version of a standard Influence Maximization\n(IM) algorithm where we substitute traditional influence estimation with the\npredictions of GLIE.We also transfer GLIE into a reinforcement learning model\nthat learns how to choose seeds to maximize influence sequentially using GLIE's\nhidden representations and predictions. The final results show that the\nproposed methods surpasses a previous GNN-RL approach and perform on par with a\nstate-of-the-art IM algorithm.",
        "We first study the suitability of behavioral biometrics to distinguish\nbetween computers and humans, commonly named as bot detection. We then present\nBeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse\ndynamics to obtain a novel feature set for the classification of human and bot\nsamples; and ii) a learning framework involving real and synthetically\ngenerated mouse trajectories. We propose two new mouse trajectory synthesis\nmethods for generating realistic data: a) a function-based method based on\nheuristic functions, and b) a data-driven method based on Generative\nAdversarial Networks (GANs) in which a Generator synthesizes human-like\ntrajectories from a Gaussian noise input. Experiments are conducted on a new\ntestbed also introduced here and available in GitHub: BeCAPTCHA-Mouse\nBenchmark; useful for research in bot detection and other mouse-based HCI\napplications. Our benchmark data consists of 15,000 mouse trajectories\nincluding real data from 58 users and bot data with various levels of realism.\nOur experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of\nhigh realism with 93% of accuracy in average using only one mouse trajectory.\nWhen our approach is fused with state-of-the-art mouse dynamic features, the\nbot detection accuracy increases relatively by more than 36%, proving that\nmouse-based bot detection is a fast, easy, and reliable tool to complement\ntraditional CAPTCHA systems.",
        "The task of Video Question Answering (VideoQA) consists in answering natural\nlanguage questions about a video and serves as a proxy to evaluate the\nperformance of a model in scene sequence understanding. Most methods designed\nfor VideoQA up-to-date are end-to-end deep learning architectures which\nstruggle at complex temporal and causal reasoning and provide limited\ntransparency in reasoning steps. We present the HySTER: a Hybrid\nSpatio-Temporal Event Reasoner to reason over physical events in videos. Our\nmodel leverages the strength of deep learning methods to extract information\nfrom video frames with the reasoning capabilities and explainability of\nsymbolic artificial intelligence in an answer set programming framework. We\ndefine a method based on general temporal, causal and physics rules which can\nbe transferred across tasks. We apply our model to the CLEVRER dataset and\ndemonstrate state-of-the-art results in question answering accuracy. This work\nsets the foundations for the incorporation of inductive logic programming in\nthe field of VideoQA.",
        "Unsupervised machine learning methods can be of great help in many\ntraditional engineering disciplines, where huge amount of labeled data is not\nreadily available or is extremely difficult or costly to generate. Two specific\nexamples include the structure of granular materials and atomic structure of\nmetallic glasses. While the former is critically important for several hundreds\nof billion dollars global industries, the latter is still a big puzzle in\nfundamental science. One thing is common in both the examples is that the\nparticles are the elements of the ensembles that are embedded in Euclidean\nspace and one can create a spatially embedded network to represent their key\nfeatures. Some recent studies show that clustering, which generically refers to\nunsupervised learning, holds great promise in partitioning these networks. In\nmany complex networks, the spatial information of nodes play very important\nrole in determining the network properties. So understanding the structure of\nsuch networks is very crucial. We have compared the performance of our newly\ndeveloped modularity function with some of the well-known modularity functions.\nWe performed this comparison by finding the best partition in 2D and 3D\ngranular assemblies. We show that for the class of networks considered in this\narticle, our method produce much better results than the competing methods.",
        "We identify a phenomenon, which we refer to as multi-model forgetting, that\noccurs when sequentially training multiple deep networks with partially-shared\nparameters; the performance of previously-trained models degrades as one\noptimizes a subsequent one, due to the overwriting of shared parameters. To\novercome this, we introduce a statistically-justified weight plasticity loss\nthat regularizes the learning of a model's shared parameters according to their\nimportance for the previous models, and demonstrate its effectiveness when\ntraining two models sequentially and for neural architecture search. Adding\nweight plasticity in neural architecture search preserves the best models to\nthe end of the search and yields improved results in both natural language\nprocessing and computer vision tasks.",
        "We present a holistic data-driven approach to the problem of productivity\nincrease on the example of a metallurgical pickling line. The proposed approach\ncombines mathematical modeling as a base algorithm and a cooperative\nMulti-Agent Reinforcement Learning (MARL) system implemented such as to enhance\nthe performance by multiple criteria while also meeting safety and reliability\nrequirements and taking into account the unexpected volatility of certain\ntechnological processes. We demonstrate how Deep Q-Learning can be applied to a\nreal-life task in a heavy industry, resulting in significant improvement of\npreviously existing automation systems.The problem of input data scarcity is\nsolved by a two-step combination of LSTM and CGAN, which helps to embrace both\nthe tabular representation of the data and its sequential properties. Offline\nRL training, a necessity in this setting, has become possible through the\nsophisticated probabilistic kinematic environment.",
        "The distributional reinforcement learning (RL) approach advocates for\nrepresenting the complete probability distribution of the random return instead\nof only modelling its expectation. A distributional RL algorithm may be\ncharacterised by two main components, namely the representation and\nparameterisation of the distribution and the probability metric defining the\nloss. This research considers the unconstrained monotonic neural network (UMNN)\narchitecture, a universal approximator of continuous monotonic functions which\nis particularly well suited for modelling different representations of a\ndistribution (PDF, CDF, quantile function). This property enables the\ndecoupling of the effect of the function approximator class from that of the\nprobability metric. The paper firstly introduces a methodology for learning\ndifferent representations of the random return distribution. Secondly, a novel\ndistributional RL algorithm named unconstrained monotonic deep Q-network\n(UMDQN) is presented. Lastly, in light of this new algorithm, an empirical\ncomparison is performed between three probability quasimetrics, namely the\nKullback-Leibler divergence, Cramer distance and Wasserstein distance. The\nresults call for a reconsideration of all probability metrics in distributional\nRL, which contrasts with the dominance of the Wasserstein distance in recent\npublications.",
        "Graph neural networks (GNNs) are a popular class of machine learning models\nwhose major advantage is their ability to incorporate a sparse and discrete\ndependency structure between data points. Unfortunately, GNNs can only be used\nwhen such a graph-structure is available. In practice, however, real-world\ngraphs are often noisy and incomplete or might not be available at all. With\nthis work, we propose to jointly learn the graph structure and the parameters\nof graph convolutional networks (GCNs) by approximately solving a bilevel\nprogram that learns a discrete probability distribution on the edges of the\ngraph. This allows one to apply GCNs not only in scenarios where the given\ngraph is incomplete or corrupted but also in those where a graph is not\navailable. We conduct a series of experiments that analyze the behavior of the\nproposed method and demonstrate that it outperforms related methods by a\nsignificant margin.",
        "Localized Narratives is a dataset with detailed natural language descriptions\nof images paired with mouse traces that provide a sparse, fine-grained visual\ngrounding for phrases. We propose TReCS, a sequential model that exploits this\ngrounding to generate images. TReCS uses descriptions to retrieve segmentation\nmasks and predict object labels aligned with mouse traces. These alignments are\nused to select and position masks to generate a fully covered segmentation\ncanvas; the final image is produced by a segmentation-to-image generator using\nthis canvas. This multi-step, retrieval-based approach outperforms existing\ndirect text-to-image generation models on both automatic metrics and human\nevaluations: overall, its generated images are more photo-realistic and better\nmatch descriptions.",
        "There is an emerging trend in the reinforcement learning for healthcare\nliterature. In order to prepare longitudinal, irregularly sampled, clinical\ndatasets for reinforcement learning algorithms, many researchers will resample\nthe time series data to short, regular intervals and use\nlast-observation-carried-forward (LOCF) imputation to fill in these gaps.\nTypically, they will not maintain any explicit information about which values\nwere imputed. In this work, we (1) call attention to this practice and discuss\nits potential implications; (2) propose an alternative representation of the\npatient state that addresses some of these issues; and (3) demonstrate in a\nnovel but representative clinical dataset that our alternative representation\nyields consistently better results for achieving optimal control, as measured\nby off-policy policy evaluation, compared to representations that do not\nincorporate missingness information.",
        "Much data with graph structures satisfy the principle of homophily, meaning\nthat connected nodes tend to be similar with respect to a specific attribute.\nAs such, ubiquitous datasets for graph machine learning tasks have generally\nbeen highly homophilous, rewarding methods that leverage homophily as an\ninductive bias. Recent work has pointed out this particular focus, as new\nnon-homophilous datasets have been introduced and graph representation learning\nmodels better suited for low-homophily settings have been developed. However,\nthese datasets are small and poorly suited to truly testing the effectiveness\nof new methods in non-homophilous settings. We present a series of improved\ngraph datasets with node label relationships that do not satisfy the homophily\nprinciple. Along with this, we introduce a new measure of the presence or\nabsence of homophily that is better suited than existing measures in different\nregimes. We benchmark a range of simple methods and graph neural networks\nacross our proposed datasets, drawing new insights for further research. Data\nand codes can be found at https://github.com/CUAI/Non-Homophily-Benchmarks.",
        "We present a new unsupervised learning algorithm, \"FAIM\", for 3D medical\nimage registration. With a different architecture than the popular \"U-net\", the\nnetwork takes a pair of full image volumes and predicts the displacement fields\nneeded to register source to target. Compared with \"U-net\" based registration\nnetworks such as VoxelMorph, FAIM has fewer trainable parameters but can\nachieve higher registration accuracy as judged by Dice score on region labels\nin the Mindboggle-101 dataset. Moreover, with the proposed penalty loss on\nnegative Jacobian determinants, FAIM produces deformations with many fewer\n\"foldings\", i.e. regions of non-invertibility where the surface folds over\nitself. In our experiment, we varied the strength of this penalty and\ninvestigated changes in registration accuracy and non-invertibility in terms of\nnumber of \"folding\" locations. We found that FAIM is able to maintain both the\nadvantages of higher accuracy and fewer \"folding\" locations over VoxelMorph,\nover a range of hyper-parameters (with the same values used for both networks).\nFurther, when trading off registration accuracy for better invertibility, FAIM\nrequired less sacrifice of registration accuracy. Codes for this paper will be\nreleased upon publication.",
        "On an artist's profile page, music streaming services frequently recommend a\nranked list of \"similar artists\" that fans also liked. However, implementing\nsuch a feature is challenging for new artists, for which usage data on the\nservice (e.g. streams or likes) is not yet available. In this paper, we model\nthis cold start similar artists ranking problem as a link prediction task in a\ndirected and attributed graph, connecting artists to their top-k most similar\nneighbors and incorporating side musical information. Then, we leverage a graph\nautoencoder architecture to learn node embedding representations from this\ngraph, and to automatically rank the top-k most similar neighbors of new\nartists using a gravity-inspired mechanism. We empirically show the flexibility\nand the effectiveness of our framework, by addressing a real-world cold start\nsimilar artists ranking problem on a global music streaming service. Along with\nthis paper, we also publicly release our source code as well as the industrial\ngraph data from our experiments.",
        "While most neural generative models generate outputs in a single pass, the\nhuman creative process is usually one of iterative building and refinement.\nRecent work has proposed models of editing processes, but these mostly focus on\nediting sequential data and/or only model a single editing pass. In this paper,\nwe present a generic model for incremental editing of structured data (i.e.,\n\"structural edits\"). Particularly, we focus on tree-structured data, taking\nabstract syntax trees of computer programs as our canonical example. Our editor\nlearns to iteratively generate tree edits (e.g., deleting or adding a subtree)\nand applies them to the partially edited data, thereby the entire editing\nprocess can be formulated as consecutive, incremental tree transformations. To\nshow the unique benefits of modeling tree edits directly, we further propose a\nnovel edit encoder for learning to represent edits, as well as an imitation\nlearning method that allows the editor to be more robust. We evaluate our\nproposed editor on two source code edit datasets, where results show that, with\nthe proposed edit encoder, our editor significantly improves accuracy over\nprevious approaches that generate the edited program directly in one pass.\nFinally, we demonstrate that training our editor to imitate experts and correct\nits mistakes dynamically can further improve its performance.",
        "In this paper, we propose enhancing actor-critic reinforcement learning\nagents by parameterising the final actor layer which produces the actions in\norder to accommodate the behaviour discrepancy of different actuators, under\ndifferent load conditions during interaction with the environment. We propose\nbranching the action producing layer in the actor to learn the tuning parameter\ncontrolling the activation layer (e.g. Tanh and Sigmoid). The learned\nparameters are then used to create tailored activation functions for each\nactuator. We ran experiments on three OpenAI Gym environments, i.e.\nPendulum-v0, LunarLanderContinuous-v2 and BipedalWalker-v2. Results have shown\nan average of 23.15% and 33.80% increase in total episode reward of the\nLunarLanderContinuous-v2 and BipedalWalker-v2 environments, respectively. There\nwas no significant improvement in Pendulum-v0 environment but the proposed\nmethod produces a more stable actuation signal compared to the state-of-the-art\nmethod. The proposed method allows the reinforcement learning actor to produce\nmore robust actions that accommodate the discrepancy in the actuators' response\nfunctions. This is particularly useful for real life scenarios where actuators\nexhibit different response functions depending on the load and the interaction\nwith the environment. This also simplifies the transfer learning problem by\nfine tuning the parameterised activation layers instead of retraining the\nentire policy every time an actuator is replaced. Finally, the proposed method\nwould allow better accommodation to biological actuators (e.g. muscles) in\nbiomechanical systems.",
        "This paper studies the problem of learning semantic segmentation from\nimage-level supervision only. Current popular solutions leverage object\nlocalization maps from classifiers as supervision signals, and struggle to make\nthe localization maps capture more complete object content. Rather than\nprevious efforts that primarily focus on intra-image information, we address\nthe value of cross-image semantic relations for comprehensive object pattern\nmining. To achieve this, two neural co-attentions are incorporated into the\nclassifier to complimentarily capture cross-image semantic similarities and\ndifferences. In particular, given a pair of training images, one co-attention\nenforces the classifier to recognize the common semantics from co-attentive\nobjects, while the other one, called contrastive co-attention, drives the\nclassifier to identify the unshared semantics from the rest, uncommon objects.\nThis helps the classifier discover more object patterns and better ground\nsemantics in image regions. In addition to boosting object pattern learning,\nthe co-attention can leverage context from other related images to improve\nlocalization map inference, hence eventually benefiting semantic segmentation\nlearning. More essentially, our algorithm provides a unified framework that\nhandles well different WSSS settings, i.e., learning WSSS with (1) precise\nimage-level supervision only, (2) extra simple single-label data, and (3) extra\nnoisy web data. It sets new state-of-the-arts on all these settings,\ndemonstrating well its efficacy and generalizability. Moreover, our approach\nranked 1st place in the Weakly-Supervised Semantic Segmentation Track of\nCVPR2020 Learning from Imperfect Data Challenge.",
        "The cross-subject application of EEG-based brain-computer interface (BCI) has\nalways been limited by large individual difference and complex characteristics\nthat are difficult to perceive. Therefore, it takes a long time to collect the\ntraining data of each user for calibration. Even transfer learning method\npre-training with amounts of subject-independent data cannot decode different\nEEG signal categories without enough subject-specific data. Hence, we proposed\na cross-subject EEG classification framework with a generative adversarial\nnetworks (GANs) based method named common spatial GAN (CS-GAN), which used\nadversarial training between a generator and a discriminator to obtain\nhigh-quality data for augmentation. A particular module in the discriminator\nwas employed to maintain the spatial features of the EEG signals and increase\nthe difference between different categories, with two losses for further\nenhancement. Through adaptive training with sufficient augmentation data, our\ncross-subject classification accuracy yielded a significant improvement of\n15.85% than leave-one subject-out (LOO) test and 8.57% than just adapting 100\noriginal samples on the dataset 2a of BCI competition IV. Moreover, We designed\na convolutional neural networks (CNNs) based classification method as a\nbenchmark with a similar spatial enhancement idea, which achieved remarkable\nresults to classify motor imagery EEG data. In summary, our framework provides\na promising way to deal with the cross-subject problem and promote the\npractical application of BCI.",
        "The black-box nature of machine learning models hinders the deployment of\nsome high-accuracy models in medical diagnosis. It is risky to put one's life\nin the hands of models that medical researchers do not fully understand.\nHowever, through model interpretation, black-box models can promptly reveal\nsignificant biomarkers that medical practitioners may have overlooked due to\nthe surge of infected patients in the COVID-19 pandemic.\n  This research leverages a database of 92 patients with confirmed SARS-CoV-2\nlaboratory tests between 18th Jan. 2020 and 5th Mar. 2020, in Zhuhai, China, to\nidentify biomarkers indicative of severity prediction. Through the\ninterpretation of four machine learning models, decision tree, random forests,\ngradient boosted trees, and neural networks using permutation feature\nimportance, Partial Dependence Plot (PDP), Individual Conditional Expectation\n(ICE), Accumulated Local Effects (ALE), Local Interpretable Model-agnostic\nExplanations (LIME), and Shapley Additive Explanation (SHAP), we identify an\nincrease in N-Terminal pro-Brain Natriuretic Peptide (NTproBNP), C-Reaction\nProtein (CRP), and lactic dehydrogenase (LDH), a decrease in lymphocyte (LYM)\nis associated with severe infection and an increased risk of death, which is\nconsistent with recent medical research on COVID-19 and other research using\ndedicated models. We further validate our methods on a large open dataset with\n5644 confirmed patients from the Hospital Israelita Albert Einstein, at S\\~ao\nPaulo, Brazil from Kaggle, and unveil leukocytes, eosinophils, and platelets as\nthree indicative biomarkers for COVID-19.",
        "Stochastic optimization naturally arises in machine learning. Efficient\nalgorithms with provable guarantees, however, are still largely missing, when\nthe objective function is nonconvex and the data points are dependent. This\npaper studies this fundamental challenge through a streaming PCA problem for\nstationary time series data. Specifically, our goal is to estimate the\nprinciple component of time series data with respect to the covariance matrix\nof the stationary distribution. Computationally, we propose a variant of Oja's\nalgorithm combined with downsampling to control the bias of the stochastic\ngradient caused by the data dependency. Theoretically, we quantify the\nuncertainty of our proposed stochastic algorithm based on diffusion\napproximations. This allows us to prove the asymptotic rate of convergence and\nfurther implies near optimal asymptotic sample complexity. Numerical\nexperiments are provided to support our analysis.",
        "Relevance has significant impact on user experience and business profit for\ne-commerce search platform. In this work, we propose a data-driven framework\nfor search relevance prediction, by distilling knowledge from BERT and related\nmulti-layer Transformer teacher models into simple feed-forward networks with\nlarge amount of unlabeled data. The distillation process produces a student\nmodel that recovers more than 97\\% test accuracy of teacher models on new\nqueries, at a serving cost that's several magnitude lower (latency 150x lower\nthan BERT-Base and 15x lower than the most efficient BERT variant, TinyBERT).\nThe applications of temperature rescaling and teacher model stacking further\nboost model accuracy, without increasing the student model complexity.\n  We present experimental results on both in-house e-commerce search relevance\ndata as well as a public data set on sentiment analysis from the GLUE\nbenchmark. The latter takes advantage of another related public data set of\nmuch larger scale, while disregarding its potentially noisy labels. Embedding\nanalysis and case study on the in-house data further highlight the strength of\nthe resulting model. By making the data processing and model training source\ncode public, we hope the techniques presented here can help reduce energy\nconsumption of the state of the art Transformer models and also level the\nplaying field for small organizations lacking access to cutting edge machine\nlearning hardwares.",
        "In this technical report, we present our solution to localize a\nspatio-temporal person in an untrimmed video based on a sentence. We achieve\nthe second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC)\nChallenge. Our solution contains three parts: 1) human attributes information\nis extracted from the sentence, it is helpful to filter out tube proposals in\nthe testing phase and supervise our classifier to learn appearance information\nin the training phase. 2) we detect humans with YoloV5 and track humans based\non the DeepSort framework but replace the original ReID network with FastReID.\n3) a visual transformer is used to extract cross-modal representations for\nlocalizing a spatio-temporal tube of the target person.",
        "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.",
        "Off-policy Reinforcement Learning (RL) holds the promise of better data\nefficiency as it allows sample reuse and potentially enables safe interaction\nwith the environment. Current off-policy policy gradient methods either suffer\nfrom high bias or high variance, delivering often unreliable estimates. The\nprice of inefficiency becomes evident in real-world scenarios such as\ninteraction-driven robot learning, where the success of RL has been rather\nlimited, and a very high sample cost hinders straightforward application. In\nthis paper, we propose a nonparametric Bellman equation, which can be solved in\nclosed form. The solution is differentiable w.r.t the policy parameters and\ngives access to an estimation of the policy gradient. In this way, we avoid the\nhigh variance of importance sampling approaches, and the high bias of\nsemi-gradient methods. We empirically analyze the quality of our gradient\nestimate against state-of-the-art methods, and show that it outperforms the\nbaselines in terms of sample efficiency on classical control tasks.",
        "This paper introduces constrained mixtures for continuous distributions,\ncharacterized by a mixture of distributions where each distribution has a shape\nsimilar to the base distribution and disjoint domains. This new concept is used\nto create generalized asymmetric versions of the Laplace and normal\ndistributions, which are shown to define exponential families, with known\nconjugate priors, and to have maximum likelihood estimates for the original\nparameters, with known closed-form expressions. The asymmetric and symmetric\nnormal distributions are compared in a linear regression example, showing that\nthe asymmetric version performs at least as well as the symmetric one, and in a\nreal world time-series problem, where a hidden Markov model is used to fit a\nstock index, indicating that the asymmetric version provides higher likelihood\nand may learn distribution models over states and transition distributions with\nconsiderably less entropy.",
        "The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions.",
        "Memory is an important aspect of intelligence and plays a role in many deep\nreinforcement learning models. However, little progress has been made in\nunderstanding when specific memory systems help more than others and how well\nthey generalize. The field also has yet to see a prevalent consistent and\nrigorous approach for evaluating agent performance on holdout data. In this\npaper, we aim to develop a comprehensive methodology to test different kinds of\nmemory in an agent and assess how well the agent can apply what it learns in\ntraining to a holdout set that differs from the training set along dimensions\nthat we suggest are relevant for evaluating memory-specific generalization. To\nthat end, we first construct a diverse set of memory tasks that allow us to\nevaluate test-time generalization across multiple dimensions. Second, we\ndevelop and perform multiple ablations on an agent architecture that combines\nmultiple memory systems, observe its baseline models, and investigate its\nperformance against the task suite.",
        "The goal of few-shot classification is to classify new categories with few\nlabeled examples within each class. Nowadays, the excellent performance in\nhandling few-shot classification problems is shown by metric-based\nmeta-learning methods. However, it is very hard for previous methods to\ndiscriminate the fine-grained sub-categories in the embedding space without\nfine-grained labels. This may lead to unsatisfactory generalization to\nfine-grained subcategories, and thus affects model interpretation. To tackle\nthis problem, we introduce the contrastive loss into few-shot classification\nfor learning latent fine-grained structure in the embedding space. Furthermore,\nto overcome the drawbacks of random image transformation used in current\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\nviews), we develop a learning-to-learn algorithm to automatically generate\ndifferent views of the same image. Extensive experiments on standard few-shot\nlearning benchmarks demonstrate the superiority of our method.",
        "The analysis of human motion as a clinical tool can bring many benefits such\nas the early detection of disease and the monitoring of recovery, so in turn\nhelping people to lead independent lives. However, it is currently under used.\nDevelopments in depth cameras, such as Kinect, have opened up the use of motion\nanalysis in settings such as GP surgeries, care homes and private homes. To\nprovide an insight into the use of Kinect in the healthcare domain, we present\na review of the current state of the art. We then propose a method that can\nrepresent human motions from time-series data of arbitrary length, as a single\nvector. Finally, we demonstrate the utility of this method by extracting a set\nof clinically significant features and using them to detect the age related\nchanges in the motions of a set of 54 individuals, with a high degree of\ncertainty (F1- score between 0.9 - 1.0). Indicating its potential application\nin the detection of a range of age-related motion impairments.",
        "Markov Decision Processes (MDPs), the mathematical framework underlying most\nalgorithms in Reinforcement Learning (RL), are often used in a way that\nwrongfully assumes that the state of an agent's environment does not change\nduring action selection. As RL systems based on MDPs begin to find application\nin real-world safety critical situations, this mismatch between the assumptions\nunderlying classical MDPs and the reality of real-time computation may lead to\nundesirable outcomes. In this paper, we introduce a new framework, in which\nstates and actions evolve simultaneously and show how it is related to the\nclassical MDP formulation. We analyze existing algorithms under the new\nreal-time formulation and show why they are suboptimal when used in real-time.\nWe then use those insights to create a new algorithm Real-Time Actor-Critic\n(RTAC) that outperforms the existing state-of-the-art continuous control\nalgorithm Soft Actor-Critic both in real-time and non-real-time settings. Code\nand videos can be found at https://github.com/rmst/rtrl.",
        "Context enhancement is critical for night vision (NV) applications,\nespecially for the dark night situation without any artificial lights. In this\npaper, we present the infrared-to-visual (IR2VI) algorithm, a novel\nunsupervised thermal-to-visible image translation framework based on generative\nadversarial networks (GANs). IR2VI is able to learn the intrinsic\ncharacteristics from VI images and integrate them into IR images. Since the\nexisting unsupervised GAN-based image translation approaches face several\nchallenges, such as incorrect mapping and lack of fine details, we propose a\nstructure connection module and a region-of-interest (ROI) focal loss method to\naddress the current limitations. Experimental results show the superiority of\nthe IR2VI algorithm over baseline methods.",
        "Domain adaptation is critical for success when confronting with the lack of\nannotations in a new domain. As the huge time consumption of labeling process\non 3D point cloud, domain adaptation for 3D semantic segmentation is of great\nexpectation. With the rise of multi-modal datasets, large amount of 2D images\nare accessible besides 3D point clouds. In light of this, we propose to further\nleverage 2D data for 3D domain adaptation by intra and inter domain cross modal\nlearning. As for intra-domain cross modal learning, most existing works sample\nthe dense 2D pixel-wise features into the same size with sparse 3D point-wise\nfeatures, resulting in the abandon of numerous useful 2D features. To address\nthis problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)\nto increase the sufficiency of multi-modality information interaction for\ndomain adaptation. For inter-domain cross modal learning, we further advance\nCross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains\ndifferent semantic content aiming to promote high-level modal complementarity.\nWe evaluate our model under various multi-modality domain adaptation settings\nincluding day-to-night, country-to-country and dataset-to-dataset, brings large\nimprovements over both uni-modal and multi-modal domain adaptation methods on\nall settings.",
        "Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach.",
        "Acquiring sufficient ground-truth supervision to train deep visual models has\nbeen a bottleneck over the years due to the data-hungry nature of deep\nlearning. This is exacerbated in some structured prediction tasks, such as\nsemantic segmentation, which requires pixel-level annotations. This work\naddresses weakly supervised semantic segmentation (WSSS), with the goal of\nbridging the gap between image-level annotations and pixel-level segmentation.\nWe formulate WSSS as a novel group-wise learning task that explicitly models\nsemantic dependencies in a group of images to estimate more reliable pseudo\nground-truths, which can be used for training more accurate segmentation\nmodels. In particular, we devise a graph neural network (GNN) for group-wise\nsemantic mining, wherein input images are represented as graph nodes, and the\nunderlying relations between a pair of images are characterized by an efficient\nco-attention mechanism. Moreover, in order to prevent the model from paying\nexcessive attention to common semantics only, we further propose a graph\ndropout layer, encouraging the model to learn more accurate and complete object\nresponses. The whole network is end-to-end trainable by iterative message\npassing, which propagates interaction cues over the images to progressively\nimprove the performance. We conduct experiments on the popular PASCAL VOC 2012\nand COCO benchmarks, and our model yields state-of-the-art performance. Our\ncode is available at: https://github.com/Lixy1997/Group-WSSS.",
        "For people with chronic pain, the assessment of protective behavior during\nphysical functioning is essential to understand their subjective pain-related\nexperiences (e.g., fear and anxiety toward pain and injury) and how they deal\nwith such experiences (avoidance or reliance on specific body joints), with the\nultimate goal of guiding intervention. Advances in deep learning (DL) can\nenable the development of such intervention. Using the EmoPain MoCap dataset,\nwe investigate how attention-based DL architectures can be used to improve the\ndetection of protective behavior by capturing the most informative temporal and\nbody configurational cues characterizing specific movements and the strategies\nused to perform them. We propose an end-to-end deep learning architecture named\nBodyAttentionNet (BANet). BANet is designed to learn temporal and bodily parts\nthat are more informative to the detection of protective behavior. The approach\naddresses the variety of ways people execute a movement (including healthy\npeople) independently of the type of movement analyzed. Through extensive\ncomparison experiments with other state-of-the-art machine learning techniques\nused with motion capture data, we show statistically significant improvements\nachieved by using these attention mechanisms. In addition, the BANet\narchitecture requires a much lower number of parameters than the state of the\nart for comparable if not higher performances.",
        "Arbitrary-oriented object detection has been a building block for rotation\nsensitive tasks. We first show that the problem of discontinuous boundaries\nsuffered in existing dominant regression-based rotation detectors, is caused by\nangular periodicity or corner ordering, according to the parameterization\nprotocol. We also show that the root cause is that the ideal predictions can be\nout of the defined range. Accordingly, we transform the angular prediction task\nfrom a regression problem to a classification one. For the resulting circularly\ndistributed angle classification problem, we first devise a Circular Smooth\nLabel (CSL) technique to handle the periodicity of angle and increase the error\ntolerance to adjacent angles. To reduce the excessive model parameters by CSL,\nwe further design a Gray Coded Label (GCL), which greatly reduces the length of\nthe encoding. Finally, we further develop an object heading detection module,\nwhich can be useful when the exact heading orientation information is needed\ne.g. for ship and plane heading detection. We release our OHD-SJTU dataset and\nOHDet detector for heading detection. Results on three large-scale public\ndatasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU, as well as scene text\ndataset ICDAR2015 and MLT, show the effectiveness of our approach.",
        "Automatic forensic image analysis assists criminal investigation experts in\nthe search for suspicious persons, abnormal behaviors detection and identity\nmatching in images. In this paper we propose a person retrieval system that\nuses textual queries (e.g., \"black trousers and green shirt\") as descriptions\nand a one-class generative color model with outlier filtering to represent the\nimages both to train the models and to perform the search. The method is\nevaluated in terms of its efficiency in fulfilling the needs of a forensic\nretrieval system: limited annotation, robustness, extensibility, adaptability\nand computational cost. The proposed generative method is compared to a\ncorresponding discriminative approach. Experiments are carried out using a\nrange of queries in three different databases. The experiments show that the\ntwo evaluated algorithms provide average retrieval performance and adaptable to\nnew datasets. The proposed generative algorithm has some advantages over the\ndiscriminative one, specifically its capability to work with very few training\nsamples and its much lower computational requirements when the number of\ntraining examples increases.",
        "We exam various geometric active contour methods for radar image\nsegmentation. Due to special properties of radar images, we propose our new\nmodel based on modified Chan-Vese functional. Our method is efficient in\nseparating non-meteorological noises from meteorological images.",
        "For relocalization in large-scale point clouds, we propose the first approach\nthat unifies global place recognition and local 6DoF pose refinement. To this\nend, we design a Siamese network that jointly learns 3D local feature detection\nand description directly from raw 3D points. It integrates FlexConv and\nSqueeze-and-Excitation (SE) to assure that the learned local descriptor\ncaptures multi-level geometric information and channel-wise relations. For\ndetecting 3D keypoints we predict the discriminativeness of the local\ndescriptors in an unsupervised manner. We generate the global descriptor by\ndirectly aggregating the learned local descriptors with an effective attention\nmechanism. In this way, local and global 3D descriptors are inferred in one\nsingle forward pass. Experiments on various benchmarks demonstrate that our\nmethod achieves competitive results for both global point cloud retrieval and\nlocal point cloud registration in comparison to state-of-the-art approaches. To\nvalidate the generalizability and robustness of our 3D keypoints, we\ndemonstrate that our method also performs favorably without fine-tuning on the\nregistration of point clouds that were generated by a visual SLAM system. Code\nand related materials are available at\nhttps://vision.in.tum.de/research/vslam/dh3d.",
        "Graph neural networks (GNN) have been ubiquitous in graph learning tasks such\nas node classification. Most of GNN methods update the node embedding\niteratively by aggregating its neighbors' information. However, they often\nsuffer from negative disturbance, due to edges connecting nodes with different\nlabels. One approach to alleviate this negative disturbance is to use\nattention, but current attention always considers feature similarity and\nsuffers from the lack of supervision. In this paper, we consider the label\ndependency of graph nodes and propose a decoupling attention mechanism to learn\nboth hard and soft attention. The hard attention is learned on labels for a\nrefined graph structure with fewer inter-class edges. Its purpose is to reduce\nthe aggregation's negative disturbance. The soft attention is learned on\nfeatures maximizing the information gain by message passing over better graph\nstructures. Moreover, the learned attention guides the label propagation and\nthe feature propagation. Extensive experiments are performed on five well-known\nbenchmark graph datasets to verify the effectiveness of the proposed method.",
        "As the will to deploy neural networks models on embedded systems grows, and\nconsidering the related memory footprint and energy consumption issues, finding\nlighter solutions to store neural networks such as weight quantization and more\nefficient inference methods become major research topics. Parallel to that,\nadversarial machine learning has risen recently with an impressive and\nsignificant attention, unveiling some critical flaws of machine learning\nmodels, especially neural networks. In particular, perturbed inputs called\nadversarial examples have been shown to fool a model into making incorrect\npredictions. In this article, we investigate the adversarial robustness of\nquantized neural networks under different threat models for a classical\nsupervised image classification task. We show that quantization does not offer\nany robust protection, results in severe form of gradient masking and advance\nsome hypotheses to explain it. However, we experimentally observe poor\ntransferability capacities which we explain by quantization value shift\nphenomenon and gradient misalignment and explore how these results can be\nexploited with an ensemble-based defense.",
        "Lifting is an efficient technique to scale up graphical models generalized to\nrelational domains by exploiting the underlying symmetries. Concurrently,\nneural models are continuously expanding from grid-like tensor data into\nstructured representations, such as various attributed graphs and relational\ndatabases. To address the irregular structure of the data, the models typically\nextrapolate on the idea of convolution, effectively introducing parameter\nsharing in their, dynamically unfolded, computation graphs. The computation\ngraphs themselves then reflect the symmetries of the underlying data, similarly\nto the lifted graphical models. Inspired by lifting, we introduce a simple and\nefficient technique to detect the symmetries and compress the neural models\nwithout loss of any information. We demonstrate through experiments that such\ncompression can lead to significant speedups of structured convolutional\nmodels, such as various Graph Neural Networks, across various tasks, such as\nmolecule classification and knowledge-base completion.",
        "We train an object detector built from convolutional neural networks to count\ninterference fringes in elliptical antinode regions in frames of high-speed\nvideo recordings of transient oscillations in Caribbean steelpan drums\nilluminated by electronic speckle pattern interferometry (ESPI). The\nannotations provided by our model aim to contribute to the understanding of\ntime-dependent behavior in such drums by tracking the development of\nsympathetic vibration modes. The system is trained on a dataset of crowdsourced\nhuman-annotated images obtained from the Zooniverse Steelpan Vibrations\nProject. Due to the small number of human-annotated images and the ambiguity of\nthe annotation task, we also evaluate the model on a large corpus of synthetic\nimages whose properties have been matched to the real images by style transfer\nusing a Generative Adversarial Network. Applying the model to thousands of\nunlabeled video frames, we measure oscillations consistent with audio\nrecordings of these drum strikes. One unanticipated result is that sympathetic\noscillations of higher-octave notes significantly precede the rise in sound\nintensity of the corresponding second harmonic tones; the mechanism responsible\nfor this remains unidentified. This paper primarily concerns the development of\nthe predictive model; further exploration of the steelpan images and deeper\nphysical insights await its further application.",
        "Automated Vehicle License Plate (VLP) detection and recognition have ended up\nbeing a significant research issue as of late. VLP localization and recognition\nare some of the most essential techniques for managing traffic using digital\ntechniques. In this paper, four smart systems are developed to recognize\nEgyptian vehicles license plates. Two systems are based on character\nrecognition, which are (System1, Characters Recognition with Classical Machine\nLearning) and (System2, Characters Recognition with Deep Machine Learning). The\nother two systems are based on the whole plate recognition which are (System3,\nWhole License Plate Recognition with Classical Machine Learning) and (System4,\nWhole License Plate Recognition with Deep Machine Learning). We use object\ndetection algorithms, and machine learning based object recognition algorithms.\nThe performance of the developed systems has been tested on real images, and\nthe experimental results demonstrate that the best detection accuracy rate for\nVLP is provided by using the deep learning method. Where the VLP detection\naccuracy rate is better than the classical system by 32%. However, the best\ndetection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is\nprovided by using the classical method. Where VLPAC detection accuracy rate is\nbetter than the deep learning-based system by 6%. Also, the results show that\ndeep learning is better than the classical technique used in VLP recognition\nprocesses. Where the recognition accuracy rate is better than the classical\nsystem by 8%. Finally, the paper output recommends a robust VLP recognition\nsystem based on both statistical and deep machine learning.",
        "Background modeling techniques are used for moving object detection in video.\nMany algorithms exist in the field of object detection with different purposes.\nIn this paper, we propose an improvement of moving object detection based on\ncodebook segmentation. We associate the original codebook algorithm with an\nedge detection algorithm. Our goal is to prove the efficiency of using an edge\ndetection algorithm with a background modeling algorithm. Throughout our study,\nwe compared the quality of the moving object detection when codebook\nsegmentation algorithm is associated with some standard edge detectors. In each\ncase, we use frame-based metrics for the evaluation of the detection. The\ndifferent results are presented and analyzed.",
        "This paper investigates the problem of pseudo-healthy synthesis that is\ndefined as synthesizing a subject-specific pathology-free image from a\npathological one. Recent approaches based on Generative Adversarial Network\n(GAN) have been developed for this task. However, these methods will inevitably\nfall into the trade-off between preserving the subject-specific identity and\ngenerating healthy-like appearances. To overcome this challenge, we propose a\nnovel adversarial training regime, Generator versus Segmentor (GVS), to\nalleviate this trade-off by a divide-and-conquer strategy. We further consider\nthe deteriorating generalization performance of the segmentor throughout the\ntraining and develop a pixel-wise weighted loss by muting the well-transformed\npixels to promote it. Moreover, we propose a new metric to measure how healthy\nthe synthetic images look. The qualitative and quantitative experiments on the\npublic dataset BraTS demonstrate that the proposed method outperforms the\nexisting methods. Besides, we also certify the effectiveness of our method on\ndatasets LiTS. Our implementation and pre-trained networks are publicly\navailable at https://github.com/Au3C2/Generator-Versus-Segmentor.",
        "Deep neural networks (DNNs) have been employed for designing wireless\nnetworks in many aspects, such as transceiver optimization, resource\nallocation, and information prediction. Existing works either use\nfully-connected DNN or the DNNs with specific structures that are designed in\nother domains. In this paper, we show that a priori information widely existed\nin wireless tasks is permutation invariant. For these tasks, we propose a DNN\nwith special structure, where the weight matrices between layers of the DNN\nonly consist of two smaller sub-matrices. By such way of parameter sharing, the\nnumber of model parameters reduces, giving rise to low sample and computational\ncomplexity for training a DNN. We take predictive resource allocation as an\nexample to show how the designed DNN can be applied for learning the optimal\npolicy with unsupervised learning. Simulations results validate our analysis\nand show dramatic gain of the proposed structure in terms of reducing training\ncomplexity.",
        "Deep learning models on graphs have achieved remarkable performance in\nvarious graph analysis tasks, e.g., node classification, link prediction and\ngraph clustering. However, they expose uncertainty and unreliability against\nthe well-designed inputs, i.e., adversarial examples. Accordingly, a line of\nstudies have emerged for both attack and defense addressed in different graph\nanalysis tasks, leading to the arms race in graph adversarial learning.\n  Despite the booming works, there still lacks a unified problem definition and\na comprehensive review. To bridge this gap, we investigate and summarize the\nexisting works on graph adversarial learning tasks systemically. Specifically,\nwe survey and unify the existing works w.r.t. attack and defense in graph\nanalysis tasks, and give appropriate definitions and taxonomies at the same\ntime. Besides, we emphasize the importance of related evaluation metrics,\ninvestigate and summarize them comprehensively. Hopefully, our works can\nprovide a comprehensive overview and offer insights for the relevant\nresearchers. More details of our works are available at\nhttps://github.com/gitgiter/Graph-Adversarial-Learning.",
        "Graph neural networks (GNNs) have emerged as a powerful tool for learning\nsoftware engineering tasks including code completion, bug finding, and program\nrepair. They benefit from leveraging program structure like control flow\ngraphs, but they are not well-suited to tasks like program execution that\nrequire far more sequential reasoning steps than number of GNN propagation\nsteps. Recurrent neural networks (RNNs), on the other hand, are well-suited to\nlong sequential chains of reasoning, but they do not naturally incorporate\nprogram structure and generally perform worse on the above tasks. Our aim is to\nachieve the best of both worlds, and we do so by introducing a novel GNN\narchitecture, the Instruction Pointer Attention Graph Neural Networks\n(IPA-GNN), which achieves improved systematic generalization on the task of\nlearning to execute programs using control flow graphs. The model arises by\nconsidering RNNs operating on program traces with branch decisions as latent\nvariables. The IPA-GNN can be seen either as a continuous relaxation of the RNN\nmodel or as a GNN variant more tailored to execution. To test the models, we\npropose evaluating systematic generalization on learning to execute using\ncontrol flow graphs, which tests sequential reasoning and use of program\nstructure. More practically, we evaluate these models on the task of learning\nto execute partial programs, as might arise if using the model as a heuristic\nfunction in program synthesis. Results show that the IPA-GNN outperforms a\nvariety of RNN and GNN baselines on both tasks.",
        "We consider the problem of visually explaining similarity models, i.e.,\nexplaining why a model predicts two images to be similar in addition to\nproducing a scalar score. While much recent work in visual model\ninterpretability has focused on gradient-based attention, these methods rely on\na classification module to generate visual explanations. Consequently, they\ncannot readily explain other kinds of models that do not use or need\nclassification-like loss functions (e.g., similarity models trained with a\nmetric learning loss). In this work, we bridge this crucial gap, presenting a\nmethod to generate gradient-based visual attention for image similarity\npredictors. By relying solely on the learned feature embedding, we show that\nour approach can be applied to any kind of CNN-based similarity architecture,\nan important step towards generic visual explainability. We show that our\nresulting attention maps serve more than just interpretability; they can be\ninfused into the model learning process itself with new trainable constraints.\nWe show that the resulting similarity models perform, and can be visually\nexplained, better than the corresponding baseline models trained without these\nconstraints. We demonstrate our approach using extensive experiments on three\ndifferent kinds of tasks: generic image retrieval, person re-identification,\nand low-shot semantic segmentation.",
        "Conditional GANs are at the forefront of natural image synthesis. The main\ndrawback of such models is the necessity for labeled data. In this work we\nexploit two popular unsupervised learning techniques, adversarial training and\nself-supervision, and take a step towards bridging the gap between conditional\nand unconditional GANs. In particular, we allow the networks to collaborate on\nthe task of representation learning, while being adversarial with respect to\nthe classic GAN game. The role of self-supervision is to encourage the\ndiscriminator to learn meaningful feature representations which are not\nforgotten during training. We test empirically both the quality of the learned\nimage representations, and the quality of the synthesized images. Under the\nsame conditions, the self-supervised GAN attains a similar performance to\nstate-of-the-art conditional counterparts. Finally, we show that this approach\nto fully unsupervised learning can be scaled to attain an FID of 23.4 on\nunconditional ImageNet generation.",
        "Remote sensing image registration is valuable for image-based navigation\nsystem despite posing many challenges. As the search space of registration is\nusually non-convex, the optimization algorithm, which aims to search the best\ntransformation parameters, is a challenging step. Conventional optimization\nalgorithms can hardly reconcile the contradiction of simultaneous rapid\nconvergence and the global optimization. In this paper, a novel learning-based\noptimization algorithm named Image Registration Optimizer Network (IRON) is\nproposed, which can predict the global optimum after single iteration. The IRON\nis trained by a 3D tensor (9x9x9), which consists of similar metric values. The\nelements of the 3D tensor correspond to the 9x9x9 neighbors of the initial\nparameters in the search space. Then, the tensor's label is a vector that\npoints to the global optimal parameters from the initial parameters. Because of\nthe special architecture, the IRON could predict the global optimum directly\nfor any initialization. The experimental results demonstrate that the proposed\nalgorithm performs better than other classical optimization algorithms as it\nhas higher accuracy, lower root of mean square error (RMSE), and more\nefficiency. Our IRON codes are available for further\nstudy.https://www.github.com/jaxwangkd04/IRON",
        "This paper formulates and studies a novel algorithm for federated learning\nfrom large collections of local datasets. This algorithm capitalizes on an\nintrinsic network structure that relates the local datasets via an undirected\n\"empirical\" graph. We model such big data over networks using a networked\nlinear regression model. Each local dataset has individual regression weights.\nThe weights of close-knit sub-collections of local datasets are enforced to\ndeviate only little. This lends naturally to a network Lasso problem which we\nsolve using a primal-dual method. We obtain a distributed federated learning\nalgorithm via a message passing implementation of this primal-dual method. We\nprovide a detailed analysis of the statistical and computational properties of\nthe resulting federated learning algorithm.",
        "Automatic machine learning is an important problem in the forefront of\nmachine learning. The strongest AutoML systems are based on neural networks,\nevolutionary algorithms, and Bayesian optimization. Recently AlphaD3M reached\nstate-of-the-art results with an order of magnitude speedup using reinforcement\nlearning with self-play. In this work we extend AlphaD3M by using a pipeline\ngrammar and a pre-trained model which generalizes from many different datasets\nand similar tasks. Our results demonstrate improved performance compared with\nour earlier work and existing methods on AutoML benchmark datasets for\nclassification and regression tasks. In the spirit of reproducible research we\nmake our data, models, and code publicly available.",
        "We examine an analytic variational inference scheme for the Gaussian Process\nState Space Model (GPSSM) - a probabilistic model for system identification and\ntime-series modelling. Our approach performs variational inference over both\nthe system states and the transition function. We exploit Markov structure in\nthe true posterior, as well as an inducing point approximation to achieve\nlinear time complexity in the length of the time series. Contrary to previous\napproaches, no Monte Carlo sampling is required: inference is cast as a\ndeterministic optimisation problem. In a number of experiments, we demonstrate\nthe ability to model non-linear dynamics in the presence of both process and\nobservation noise as well as to impute missing information (e.g. velocities\nfrom raw positions through time), to de-noise, and to estimate the underlying\ndimensionality of the system. Finally, we also introduce a closed-form method\nfor multi-step prediction, and a novel criterion for assessing the quality of\nour approximate posterior.",
        "With the increasing deployment of diverse positioning devices and\nlocation-based services, a huge amount of spatial and temporal information has\nbeen collected and accumulated as trajectory data. Among many applications,\ntrajectory-based location prediction is gaining increasing attention because of\nits potential to improve the performance of many applications in multiple\ndomains. This research focuses on trajectory sequence prediction methods using\ntrajectory data obtained from the vehicles in urban traffic network. As\nRecurrent Neural Network(RNN) model is previously proposed, we propose an\nimproved method of Attention-based Recurrent Neural Network model(ARNN) for\nurban vehicle trajectory prediction. We introduce attention mechanism into\nurban vehicle trajectory prediction to explain the impact of network-level\ntraffic state information. The model is evaluated using the Bluetooth data of\nprivate vehicles collected in Brisbane, Australia with 5 metrics which are\nwidely used in the sequence modeling. The proposed ARNN model shows significant\nperformance improvement compared to the existing RNN models considering not\nonly the cells to be visited but also the alignment of the cells in sequence.",
        "This paper describes a novel approach to change-point detection when the\nobserved high-dimensional data may have missing elements. The performance of\nclassical methods for change-point detection typically scales poorly with the\ndimensionality of the data, so that a large number of observations are\ncollected after the true change-point before it can be reliably detected.\nFurthermore, missing components in the observed data handicap conventional\napproaches. The proposed method addresses these challenges by modeling the\ndynamic distribution underlying the data as lying close to a time-varying\nlow-dimensional submanifold embedded within the ambient observation space.\nSpecifically, streaming data is used to track a submanifold approximation,\nmeasure deviations from this approximation, and calculate a series of\nstatistics of the deviations for detecting when the underlying manifold has\nchanged in a sharp or unexpected manner. The approach described in this paper\nleverages several recent results in the field of high-dimensional data\nanalysis, including subspace tracking with missing data, multiscale analysis\ntechniques for point clouds, online optimization, and change-point detection\nperformance analysis. Simulations and experiments highlight the robustness and\nefficacy of the proposed approach in detecting an abrupt change in an otherwise\nslowly varying low-dimensional manifold.",
        "Motivated by recent advancements in Deep Reinforcement Learning (RL), we have\ndeveloped an RL agent to manage the operation of storage devices in a household\nand is designed to maximize demand-side cost savings. The proposed technique is\ndata-driven, and the RL agent learns from scratch how to efficiently use the\nenergy storage device given variable tariff structures. In most of the studies,\nthe RL agent is considered as a black box, and how the agent has learned is\noften ignored. We explain the learning progression of the RL agent, and the\nstrategies it follows based on the capacity of the storage device.",
        "Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.",
        "Unlabeled data is often abundant in the clinic, making machine learning\nmethods based on semi-supervised learning a good match for this setting.\nDespite this, they are currently receiving relatively little attention in\nmedical image analysis literature. Instead, most practitioners and researchers\nfocus on supervised or transfer learning approaches. The recently proposed\nMixMatch and FixMatch algorithms have demonstrated promising results in\nextracting useful representations while requiring very few labels. Motivated by\nthese recent successes, we apply MixMatch and FixMatch in an ophthalmological\ndiagnostic setting and investigate how they fare against standard transfer\nlearning. We find that both algorithms outperform the transfer learning\nbaseline on all fractions of labelled data. Furthermore, our experiments show\nthat exponential moving average (EMA) of model parameters, which is a component\nof both algorithms, is not needed for our classification problem, as disabling\nit leaves the outcome unchanged. Our code is available online:\nhttps://github.com/Valentyn1997/oct-diagn-semi-supervised",
        "Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. Applying the same methods on 3D\ndata still poses challenges due to the heavy memory requirements and the lack\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\nsemantic segmentation, which takes raw point clouds as input. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on multiple datasets where our method\nachieves state-of-the-art performance. We also extend and evaluate our network\nfor instance and dynamic object segmentation.",
        "Convolutional Networks (ConvNets) excel at semantic segmentation and have\nbecome a vital component for perception in autonomous driving. Enabling an\nall-encompassing view of street-scenes, omnidirectional cameras present\nthemselves as a perfect fit in such systems. Most segmentation models for\nparsing urban environments operate on common, narrow Field of View (FoV)\nimages. Transferring these models from the domain they were designed for to\n360-degree perception, their performance drops dramatically, e.g., by an\nabsolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of\nFoV and structural distribution between the imaging domains, we introduce\nEfficient Concurrent Attention Networks (ECANets), directly capturing the\ninherent long-range dependencies in omnidirectional imagery. In addition to the\nlearned attention-based contextual priors that can stretch across 360-degree\nimages, we upgrade model training by leveraging multi-source and\nomni-supervised learning, taking advantage of both: Densely labeled and\nunlabeled data originating from multiple datasets. To foster progress in\npanoramic image segmentation, we put forward and extensively evaluate models on\nWild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture\ndiverse scenes from all around the globe. Our novel model, training regimen and\nmulti-source prediction fusion elevate the performance (mIoU) to new\nstate-of-the-art results on the public PASS (60.2%) and the fresh WildPASS\n(69.0%) benchmarks.",
        "We introduce a novel self-supervised pretext task for learning\nrepresentations from audio-visual content. Prior work on audio-visual\nrepresentation learning leverages correspondences at the video level.\nApproaches based on audio-visual correspondence (AVC) predict whether audio and\nvideo clips originate from the same or different video instances. Audio-visual\ntemporal synchronization (AVTS) further discriminates negative pairs originated\nfrom the same video instance but at different moments in time. While these\napproaches learn high-quality representations for downstream tasks such as\naction recognition, their training objectives disregard spatial cues naturally\noccurring in audio and visual signals. To learn from these spatial cues, we\ntasked a network to perform contrastive audio-visual spatial alignment of\n360{\\deg} video and spatial audio. The ability to perform spatial alignment is\nenhanced by reasoning over the full spatial content of the 360{\\deg} video\nusing a transformer architecture to combine representations from multiple\nviewpoints. The advantages of the proposed pretext task are demonstrated on a\nvariety of audio and visual downstream tasks, including audio-visual\ncorrespondence, spatial alignment, action recognition, and video semantic\nsegmentation.",
        "3D object detection and dense depth estimation are one of the most vital\ntasks in autonomous driving. Multiple sensor modalities can jointly attribute\ntowards better robot perception, and to that end, we introduce a method for\njointly training 3D object detection and monocular dense depth reconstruction\nneural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB\nimage during inference and produces object pose predictions as well as a\ndensely reconstructed depth map. LiDAR point-cloud is converted into a set of\nvoxels, and its features are extracted using 3D convolution layers, from which\nwe regress object pose parameters. Corresponding RGB image features are\nextracted using another 2D convolutional neural network. We further use these\ncombined features to predict a dense depth map. While our object detection is\ntrained in a supervised manner, the depth prediction network is trained with\nboth self-supervised and supervised loss functions. We also introduce a loss\nfunction, edge-preserving smooth loss, and show that this results in better\ndepth estimation compared to the edge-aware smooth loss function, frequently\nused in depth prediction works.",
        "We introduce a framework that abstracts Reinforcement Learning (RL) as a\nsequence modeling problem. This allows us to draw upon the simplicity and\nscalability of the Transformer architecture, and associated advances in\nlanguage modeling such as GPT-x and BERT. In particular, we present Decision\nTransformer, an architecture that casts the problem of RL as conditional\nsequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal\nactions by leveraging a causally masked Transformer. By conditioning an\nautoregressive model on the desired return (reward), past states, and actions,\nour Decision Transformer model can generate future actions that achieve the\ndesired return. Despite its simplicity, Decision Transformer matches or exceeds\nthe performance of state-of-the-art model-free offline RL baselines on Atari,\nOpenAI Gym, and Key-to-Door tasks.",
        "The supervised learning paradigm is limited by the cost - and sometimes the\nimpracticality - of data collection and labeling in multiple domains.\nSelf-supervised learning, a paradigm which exploits the structure of unlabeled\ndata to create learning problems that can be solved with standard supervised\napproaches, has shown great promise as a pretraining or feature learning\napproach in fields like computer vision and time series processing. In this\nwork, we present self-supervision strategies that can be used to learn\ninformative representations from multivariate time series. One successful\napproach relies on predicting whether time windows are sampled from the same\ntemporal context or not. As demonstrated on a clinically relevant task (sleep\nscoring) and with two electroencephalography datasets, our approach outperforms\na purely supervised approach in low data regimes, while capturing important\nphysiological information without any access to labels.",
        "America has a massive railway system. As of 2006, U.S. freight railroads have\n140,490 route- miles of standard gauge, but maintaining such a huge system and\neliminating any dangers, like reduced track stability and poor drainage, caused\nby railway ballast degradation require huge amount of labor. The traditional\nway to quantify the degradation of ballast is to use an index called Fouling\nIndex (FI) through ballast sampling and sieve analysis. However, determining\nthe FI values in lab is very time-consuming and laborious, but with the help of\nrecent development in the field of computer vision, a novel method for a\npotential machine-vison based ballast inspection system can be employed that\ncan hopefully replace the traditional mechanical method. The new machine-vision\napproach analyses the images of the in-service ballasts, and then utilizes\nimage segmentation algorithm to get ballast segments. By comparing the segment\nresults and their corresponding FI values, this novel method produces a\nmachine-vision-based index that has the best-fit relation with FI. The\nimplementation details of how this algorithm works are discussed in this\nreport.",
        "This paper tries to give a gentle introduction to deep learning in medical\nimage processing, proceeding from theoretical foundations to applications. We\nfirst discuss general reasons for the popularity of deep learning, including\nseveral major breakthroughs in computer science. Next, we start reviewing the\nfundamental basics of the perceptron and neural networks, along with some\nfundamental theory that is often omitted. Doing so allows us to understand the\nreasons for the rise of deep learning in many application domains. Obviously\nmedical image processing is one of these areas which has been largely affected\nby this rapid progress, in particular in image detection and recognition, image\nsegmentation, image registration, and computer-aided diagnosis. There are also\nrecent trends in physical simulation, modelling, and reconstruction that have\nled to astonishing results. Yet, some of these approaches neglect prior\nknowledge and hence bear the risk of producing implausible results. These\napparent weaknesses highlight current limitations of deep learning. However, we\nalso briefly discuss promising approaches that might be able to resolve these\nproblems in the future.",
        "Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.",
        "Online tracking of multiple objects in videos requires strong capacity of\nmodeling and matching object appearances. Previous methods for learning\nappearance embedding mostly rely on instance-level matching without considering\nthe temporal continuity provided by videos. We design a new instance-to-track\nmatching objective to learn appearance embedding that compares a candidate\ndetection to the embedding of the tracks persisted in the tracker. It enables\nus to learn not only from videos labeled with complete tracks, but also\nunlabeled or partially labeled videos. We implement this learning objective in\na unified form following the spirit of constrastive loss. Experiments on\nmultiple object tracking datasets demonstrate that our method can effectively\nlearning discriminative appearance embeddings in a semi-supervised fashion and\noutperform state of the art methods on representative benchmarks.",
        "To train Variational Autoencoders (VAEs) to generate realistic imagery\nrequires a loss function that reflects human perception of image similarity. We\npropose such a loss function based on Watson's perceptual model, which computes\na weighted distance in frequency space and accounts for luminance and contrast\nmasking. We extend the model to color images, increase its robustness to\ntranslation by using the Fourier Transform, remove artifacts due to splitting\nthe image into blocks, and make it differentiable. In experiments, VAEs trained\nwith the new loss function generated realistic, high-quality image samples.\nCompared to using the Euclidean distance and the Structural Similarity Index,\nthe images were less blurry; compared to deep neural network based losses, the\nnew approach required less computational resources and generated images with\nless artifacts.",
        "Gradient-based algorithms are crucial to modern computer-vision and graphics\napplications, enabling learning-based optimization and inverse problems. For\nexample, photorealistic differentiable rendering pipelines for color images\nhave been proven highly valuable to applications aiming to map 2D and 3D\ndomains. However, to the best of our knowledge, no effort has been made so far\ntowards extending these gradient-based methods to the generation of depth\n(2.5D) images, as simulating structured-light depth sensors implies solving\ncomplex light transport and stereo-matching problems. In this paper, we\nintroduce a novel end-to-end differentiable simulation pipeline for the\ngeneration of realistic 2.5D scans, built on physics-based 3D rendering and\ncustom block-matching algorithms. Each module can be differentiated w.r.t\nsensor and scene parameters; e.g., to automatically tune the simulation for new\ndevices over some provided scans or to leverage the pipeline as a 3D-to-2.5D\ntransformer within larger computer-vision applications. Applied to the training\nof deep-learning methods for various depth-based recognition tasks\n(classification, pose estimation, semantic segmentation), our simulation\ngreatly improves the performance of the resulting models on real scans, thereby\ndemonstrating the fidelity and value of its synthetic depth data compared to\nprevious static simulations and learning-based domain adaptation schemes.",
        "Estimating 3D hand and object pose from a single image is an extremely\nchallenging problem: hands and objects are often self-occluded during\ninteractions, and the 3D annotations are scarce as even humans cannot directly\nlabel the ground-truths from a single image perfectly. To tackle these\nchallenges, we propose a unified framework for estimating the 3D hand and\nobject poses with semi-supervised learning. We build a joint learning framework\nwhere we perform explicit contextual reasoning between hand and object\nrepresentations by a Transformer. Going beyond limited 3D annotations in a\nsingle image, we leverage the spatial-temporal consistency in large-scale\nhand-object videos as a constraint for generating pseudo labels in\nsemi-supervised learning. Our method not only improves hand pose estimation in\nchallenging real-world dataset, but also substantially improve the object pose\nwhich has fewer ground-truths per instance. By training with large-scale\ndiverse videos, our model also generalizes better across multiple out-of-domain\ndatasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object",
        "Neural embedding-based machine learning models have shown promise for\npredicting novel links in biomedical knowledge graphs. Unfortunately, their\npractical utility is diminished by their lack of interpretability. Recently,\nthe fully interpretable, rule-based algorithm AnyBURL yielded highly\ncompetitive results on many general-purpose link prediction benchmarks.\nHowever, its applicability to large-scale prediction tasks on complex\nbiomedical knowledge bases is limited by long inference times and difficulties\nwith aggregating predictions made by multiple rules. We improve upon AnyBURL by\nintroducing the SAFRAN rule application framework which aggregates rules\nthrough a scalable clustering algorithm. SAFRAN yields new state-of-the-art\nresults for fully interpretable link prediction on the established\ngeneral-purpose benchmark FB15K-237 and the large-scale biomedical benchmark\nOpenBioLink. Furthermore, it exceeds the results of multiple established\nembedding-based algorithms on FB15K-237 and narrows the gap between rule-based\nand embedding-based algorithms on OpenBioLink. We also show that SAFRAN\nincreases inference speeds by up to two orders of magnitude.",
        "Generative adversarial networks (GANs) nowadays are capable of producing\nimages of incredible realism. One concern raised is whether the\nstate-of-the-art GAN's learned distribution still suffers from mode collapse,\nand what to do if so. Existing diversity tests of samples from GANs are usually\nconducted qualitatively on a small scale, and/or depends on the access to\noriginal training data as well as the trained model parameters. This paper\nexplores to diagnose GAN intra-mode collapse and calibrate that, in a novel\nblack-box setting: no access to training data, nor the trained model\nparameters, is assumed. The new setting is practically demanded, yet rarely\nexplored and significantly more challenging. As a first stab, we devise a set\nof statistical tools based on sampling, that can visualize, quantify, and\nrectify intra-mode collapse. We demonstrate the effectiveness of our proposed\ndiagnosis and calibration techniques, via extensive simulations and\nexperiments, on unconditional GAN image generation (e.g., face and vehicle).\nOur study reveals that the intra-mode collapse is still a prevailing problem in\nstate-of-the-art GANs and the mode collapse is diagnosable and calibratable in\nblack-box settings. Our codes are available at:\nhttps://github.com/VITA-Group/BlackBoxGANCollapse.",
        "We provide a theoretical framework for neural networks in terms of the\nrepresentation theory of quivers, thus revealing symmetries of the parameter\nspace of neural networks. An exploitation of these symmetries leads to a model\ncompression algorithm for radial neural networks based on an analogue of the QR\ndecomposition. A projected version of backpropogation on the original model\nmatches usual backpropogation on the compressed model.",
        "We present a benchmark suite for visual perception. The benchmark is based on\nmore than 250K high-resolution video frames, all annotated with ground-truth\ndata for both low-level and high-level vision tasks, including optical flow,\nsemantic instance segmentation, object detection and tracking, object-level 3D\nscene layout, and visual odometry. Ground-truth data for all tasks is available\nfor every frame. The data was collected while driving, riding, and walking a\ntotal of 184 kilometers in diverse ambient conditions in a realistic virtual\nworld. To create the benchmark, we have developed a new approach to collecting\nground-truth data from simulated worlds without access to their source code or\ncontent. We conduct statistical analyses that show that the composition of the\nscenes in the benchmark closely matches the composition of corresponding\nphysical environments. The realism of the collected data is further validated\nvia perceptual experiments. We analyze the performance of state-of-the-art\nmethods for multiple tasks, providing reference baselines and highlighting\nchallenges for future research. The supplementary video can be viewed at\nhttps://youtu.be/T9OybWv923Y",
        "Image-based age estimation aims to predict a person's age from facial images.\nIt is used in a variety of real-world applications. Although end-to-end deep\nmodels have achieved impressive results for age estimation on benchmark\ndatasets, their performance in-the-wild still leaves much room for improvement\ndue to the challenges caused by large variations in head pose, facial\nexpressions, and occlusions. To address this issue, we propose a simple yet\neffective method to explicitly incorporate facial semantics into age\nestimation, so that the model would learn to correctly focus on the most\ninformative facial components from unaligned facial images regardless of head\npose and non-rigid deformation. To this end, we design a face parsing-based\nnetwork to learn semantic information at different scales and a novel face\nparsing attention module to leverage these semantic features for age\nestimation. To evaluate our method on in-the-wild data, we also introduce a new\nchallenging large-scale benchmark called IMDB-Clean. This dataset is created by\nsemi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained\nclustering method. Through comprehensive experiment on IMDB-Clean and other\nbenchmark datasets, under both intra-dataset and cross-dataset evaluation\nprotocols, we show that our method consistently outperforms all existing age\nestimation methods and achieves a new state-of-the-art performance. To the best\nof our knowledge, our work presents the first attempt of leveraging face\nparsing attention to achieve semantic-aware age estimation, which may be\ninspiring to other high level facial analysis tasks.",
        "Human-in-the-loop aims to train an accurate prediction model with minimum\ncost by integrating human knowledge and experience. Humans can provide training\ndata for machine learning applications and directly accomplish some tasks that\nare hard for computers in the pipeline with the help of machine-based\napproaches. In this paper, we survey existing works on human-in-the-loop from a\ndata perspective and classify them into three categories with a progressive\nrelationship: (1) the work of improving model performance from data processing,\n(2) the work of improving model performance through interventional model\ntraining, and (3) the design of the system independent human-in-the-loop. Using\nthe above categorization, we summarize major approaches in the field, along\nwith their technical strengths/ weaknesses, we have simple classification and\ndiscussion in natural language processing, computer vision, and others.\nBesides, we provide some open challenges and opportunities. This survey intends\nto provide a high-level summarization for human-in-the-loop and motivates\ninterested readers to consider approaches for designing effective\nhuman-in-the-loop solutions.",
        "Through deep learning and computer vision techniques, driving manoeuvres can\nbe predicted accurately a few seconds in advance. Even though adapting a\nlearned model to new drivers and different vehicles is key for robust\ndriver-assistance systems, this problem has received little attention so far.\nThis work proposes to tackle this challenge through domain adaptation, a\ntechnique closely related to transfer learning. A proof of concept for the\napplication of a Domain-Adversarial Recurrent Neural Network (DA-RNN) to\nmulti-modal time series driving data is presented, in which domain-invariant\nfeatures are learned by maximizing the loss of an auxiliary domain classifier.\nOur implementation is evaluated using a leave-one-driver-out approach on\nindividual drivers from the Brain4Cars dataset, as well as using a new dataset\nacquired through driving simulations, yielding an average increase in\nperformance of 30% and 114% respectively compared to no adaptation. We also\nshow the importance of fine-tuning sections of the network to optimise the\nextraction of domain-independent features. The results demonstrate the\napplicability of the approach to driver-assistance systems as well as training\nand simulation environments.",
        "We propose a variational approach to obtain super-resolution images from\nmultiple low-resolution frames extracted from video clips. First the\ndisplacement between the low-resolution frames and the reference frame are\ncomputed by an optical flow algorithm. Then a low-rank model is used to\nconstruct the reference frame in high-resolution by incorporating the\ninformation of the low-resolution frames. The model has two terms: a 2-norm\ndata fidelity term and a nuclear-norm regularization term. Alternating\ndirection method of multipliers is used to solve the model. Comparison of our\nmethods with other models on synthetic and real video clips show that our\nresulting images are more accurate with less artifacts. It also provides much\nfiner and discernable details.",
        "While the Transformer architecture has become the de-facto standard for\nnatural language processing tasks, its applications to computer vision remain\nlimited. In vision, attention is either applied in conjunction with\nconvolutional networks, or used to replace certain components of convolutional\nnetworks while keeping their overall structure in place. We show that this\nreliance on CNNs is not necessary and a pure transformer applied directly to\nsequences of image patches can perform very well on image classification tasks.\nWhen pre-trained on large amounts of data and transferred to multiple mid-sized\nor small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision\nTransformer (ViT) attains excellent results compared to state-of-the-art\nconvolutional networks while requiring substantially fewer computational\nresources to train.",
        "A unified neural network structure is presented for joint 3D object detection\nand point cloud segmentation in this paper. We leverage rich supervision from\nboth detection and segmentation labels rather than using just one of them. In\naddition, an extension based on single-stage object detectors is proposed based\non the implicit function widely used in 3D scene and object understanding. The\nextension branch takes the final feature map from the object detection module\nas input, and produces an implicit function that generates semantic\ndistribution for each point for its corresponding voxel center. We demonstrated\nthe performance of our structure on nuScenes-lidarseg, a large-scale outdoor\ndataset. Our solution achieves competitive results against state-of-the-art\nmethods in both 3D object detection and point cloud segmentation with little\nadditional computation load compared with object detection solutions. The\ncapability of efficient weakly supervision semantic segmentation of the\nproposed method is also validated by experiments.",
        "Due to the large success in object detection and instance segmentation, Mask\nR-CNN attracts great attention and is widely adopted as a strong baseline for\narbitrary-shaped scene text detection and spotting. However, two issues remain\nto be settled. The first is dense text case, which is easy to be neglected but\nquite practical. There may exist multiple instances in one proposal, which\nmakes it difficult for the mask head to distinguish different instances and\ndegrades the performance. In this work, we argue that the performance\ndegradation results from the learning confusion issue in the mask head. We\npropose to use an MLP decoder instead of the \"deconv-conv\" decoder in the mask\nhead, which alleviates the issue and promotes robustness significantly. And we\npropose instance-aware mask learning in which the mask head learns to predict\nthe shape of the whole instance rather than classify each pixel to text or\nnon-text. With instance-aware mask learning, the mask branch can learn\nseparated and compact masks. The second is that due to large variations in\nscale and aspect ratio, RPN needs complicated anchor settings, making it hard\nto maintain and transfer across different datasets. To settle this issue, we\npropose an adaptive label assignment in which all instances especially those\nwith extreme aspect ratios are guaranteed to be associated with enough anchors.\nEquipped with these components, the proposed method named MAYOR achieves\nstate-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500,\nICDAR2015, CTW1500, and Total-Text.",
        "A classification technique incorporating a novel feature derivation method is\nproposed for predicting failure of a system or device with multivariate time\nseries sensor data. We treat the multivariate time series sensor data as images\nfor both visualization and computation. Failure follows various patterns which\nare closely related to the root causes. Different predefined transformations\nare applied on the original sensors data to better characterize the failure\npatterns. In addition to feature derivation, ensemble method is used to further\nimprove the performance. In addition, a general algorithm architecture of deep\nneural network is proposed to handle multiple types of data with less manual\nfeature engineering. We apply the proposed method on the early predict failure\nof computer disk drive in order to improve storage systems availability and\navoid data loss. The classification accuracy is largely improved with the\nenriched features, named smart features.",
        "The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector to generate domain-invariant features. We train and test our proposed\nmethod using popular datasets. Our experiments show significant improvements in\nobject detection performance when training YOLOv4 using the proposed MS-DAYOLO\nand when tested on target data representing challenging weather conditions for\nautonomous driving applications.",
        "Time series forecasting is an important problem across many domains, playing\na crucial role in multiple real-world applications. In this paper, we propose a\nforecasting architecture that combines deep autoregressive models with a\nSpectral Attention (SA) module, which merges global and local frequency domain\ninformation in the model's embedded space. By characterizing in the spectral\ndomain the embedding of the time series as occurrences of a random process, our\nmethod can identify global trends and seasonality patterns. Two spectral\nattention models, global and local to the time series, integrate this\ninformation within the forecast and perform spectral filtering to remove time\nseries's noise. The proposed architecture has a number of useful properties: it\ncan be effectively incorporated into well-know forecast architectures,\nrequiring a low number of parameters and producing interpretable results that\nimprove forecasting accuracy. We test the Spectral Attention Autoregressive\nModel (SAAM) on several well-know forecast datasets, consistently demonstrating\nthat our model compares favorably to state-of-the-art approaches.",
        "For many structured learning tasks, the data annotation process is complex\nand costly. Existing annotation schemes usually aim at acquiring completely\nannotated structures, under the common perception that partial structures are\nof low quality and could hurt the learning process. This paper questions this\ncommon perception, motivated by the fact that structures consist of\ninterdependent sets of variables. Thus, given a fixed budget, partly annotating\neach structure may provide the same level of supervision, while allowing for\nmore structures to be annotated. We provide an information theoretic\nformulation for this perspective and use it, in the context of three diverse\nstructured learning tasks, to show that learning from partial structures can\nsometimes outperform learning from complete ones. Our findings may provide\nimportant insights into structured data annotation schemes and could support\nprogress in learning protocols for structured tasks.",
        "In the past few years we have seen great advances in object perception\n(particularly in 4D space-time dimensions) thanks to deep learning methods.\nHowever, they typically rely on large amounts of high-quality labels to achieve\ngood performance, which often require time-consuming and expensive work by\nhuman annotators. To address this we propose an automatic annotation pipeline\nthat generates accurate object trajectories in 3D space (i.e., 4D labels) from\nLiDAR point clouds. The key idea is to decompose the 4D object label into two\nparts: the object size in 3D that's fixed through time for rigid objects, and\nthe motion path describing the evolution of the object's pose through time.\nInstead of generating a series of labels in one shot, we adopt an iterative\nrefinement process where online generated object detections are tracked through\ntime as the initialization. Given the cheap but noisy input, our model produces\nhigher quality 4D labels by re-estimating the object size and smoothing the\nmotion path, where the improvement is achieved by exploiting aggregated\nobservations and motion cues over the entire trajectory. We validate the\nproposed method on a large-scale driving dataset and show a 25% reduction of\nhuman annotation efforts. We also showcase the benefits of our approach in the\nannotator-in-the-loop setting.",
        "Message passing Graph Neural Networks (GNNs) provide a powerful modeling\nframework for relational data. However, the expressive power of existing GNNs\nis upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test,\nwhich means GNNs that are not able to predict node clustering coefficients and\nshortest path distances, and cannot differentiate between different d-regular\ngraphs. Here we develop a class of message passing GNNs, named Identity-aware\nGraph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL\ntest. ID-GNN offers a minimal but powerful solution to limitations of existing\nGNNs. ID-GNN extends existing GNN architectures by inductively considering\nnodes' identities during message passing. To embed a given node, ID-GNN first\nextracts the ego network centered at the node, then conducts rounds of\nheterogeneous message passing, where different sets of parameters are applied\nto the center node than to other surrounding nodes in the ego network. We\nfurther propose a simplified but faster version of ID-GNN that injects node\nidentity information as augmented node features. Altogether, both versions of\nID-GNN represent general extensions of message passing GNNs, where experiments\nshow that transforming existing GNNs to ID-GNNs yields on average 40% accuracy\nimprovement on challenging node, edge, and graph property prediction tasks; 3%\naccuracy improvement on node and graph classification benchmarks; and 15% ROC\nAUC improvement on real-world link prediction tasks. Additionally, ID-GNNs\ndemonstrate improved or comparable performance over other task-specific graph\nnetworks.",
        "IoT systems have been facing increasingly sophisticated technical problems\ndue to the growing complexity of these systems and their fast deployment\npractices. Consequently, IoT managers have to judiciously detect failures\n(anomalies) in order to reduce their cyber risk and operational cost. While\nthere is a rich literature on anomaly detection in many IoT-based systems,\nthere is no existing work that documents the use of ML models for anomaly\ndetection in digital agriculture and in smart manufacturing systems. These two\napplication domains pose certain salient technical challenges. In agriculture\nthe data is often sparse, due to the vast areas of farms and the requirement to\nkeep the cost of monitoring low. Second, in both domains, there are multiple\ntypes of sensors with varying capabilities and costs. The sensor data\ncharacteristics change with the operating point of the environment or machines,\nsuch as, the RPM of the motor. The inferencing and the anomaly detection\nprocesses therefore have to be calibrated for the operating point.\n  In this paper, we analyze data from sensors deployed in an agricultural farm\nwith data from seven different kinds of sensors, and from an advanced\nmanufacturing testbed with vibration sensors. We evaluate the performance of\nARIMA and LSTM models for predicting the time series of sensor data. Then,\nconsidering the sparse data from one kind of sensor, we perform transfer\nlearning from a high data rate sensor. We then perform anomaly detection using\nthe predicted sensor data. Taken together, we show how in these two application\ndomains, predictive failure classification can be achieved, thus paving the way\nfor predictive maintenance.",
        "This report presents the application of object detection on a database of\nunderwater images of different species of crabs, as well as aerial images of\nsea lions and finally the Pascal VOC dataset. The model is an end-to-end object\ndetection neural network based on a convolutional network base and a Long\nShort-Term Memory detector.",
        "Self-supervision as an emerging technique has been employed to train\nconvolutional neural networks (CNNs) for more transferrable, generalizable, and\nrobust representation learning of images. Its introduction to graph\nconvolutional networks (GCNs) operating on graph data is however rarely\nexplored. In this study, we report the first systematic exploration and\nassessment of incorporating self-supervision into GCNs. We first elaborate\nthree mechanisms to incorporate self-supervision into GCNs, analyze the\nlimitations of pretraining & finetuning and self-training, and proceed to focus\non multi-task learning. Moreover, we propose to investigate three novel\nself-supervised learning tasks for GCNs with theoretical rationales and\nnumerical comparisons. Lastly, we further integrate multi-task self-supervision\ninto graph adversarial training. Our results show that, with properly designed\ntask forms and incorporation mechanisms, self-supervision benefits GCNs in\ngaining more generalizability and robustness. Our codes are available at\nhttps://github.com/Shen-Lab/SS-GCNs.",
        "Moving object detection is a critical task for autonomous vehicles. As\ndynamic objects represent higher collision risk than static ones, our own\nego-trajectories have to be planned attending to the future states of the\nmoving elements of the scene. Motion can be perceived using temporal\ninformation such as optical flow. Conventional optical flow computation is\nbased on camera sensors only, which makes it prone to failure in conditions\nwith low illumination. On the other hand, LiDAR sensors are independent of\nillumination, as they measure the time-of-flight of their own emitted lasers.\nIn this work, we propose a robust and real-time CNN architecture for Moving\nObject Detection (MOD) under low-light conditions by capturing motion\ninformation from both camera and LiDAR sensors. We demonstrate the impact of\nour algorithm on KITTI dataset where we simulate a low-light environment\ncreating a novel dataset \"Dark KITTI\". We obtain a 10.1% relative improvement\non Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our\nbaselines. The proposed algorithm runs at 18 fps on a standard desktop GPU\nusing $256\\times1224$ resolution images.",
        "Verifying robustness of neural networks given a specified threat model is a\nfundamental yet challenging task. While current verification methods mainly\nfocus on the $\\ell_p$-norm threat model of the input instances, robustness\nverification against semantic adversarial attacks inducing large $\\ell_p$-norm\nperturbations, such as color shifting and lighting adjustment, are beyond their\ncapacity. To bridge this gap, we propose \\textit{Semantify-NN}, a\nmodel-agnostic and generic robustness verification approach against semantic\nperturbations for neural networks. By simply inserting our proposed\n\\textit{semantic perturbation layers} (SP-layers) to the input layer of any\ngiven model, \\textit{Semantify-NN} is model-agnostic, and any $\\ell_p$-norm\nbased verification tools can be used to verify the model robustness against\nsemantic perturbations. We illustrate the principles of designing the SP-layers\nand provide examples including semantic perturbations to image classification\nin the space of hue, saturation, lightness, brightness, contrast and rotation,\nrespectively. In addition, an efficient refinement technique is proposed to\nfurther significantly improve the semantic certificate. Experiments on various\nnetwork architectures and different datasets demonstrate the superior\nverification performance of \\textit{Semantify-NN} over $\\ell_p$-norm-based\nverification frameworks that naively convert semantic perturbation to\n$\\ell_p$-norm. The results show that \\textit{Semantify-NN} can support\nrobustness verification against a wide range of semantic perturbations.\n  Code available https://github.com/JeetMo/Semantify-NN",
        "Recent times have witnessed sharp improvements in reinforcement learning\ntasks using deep reinforcement learning techniques like Deep Q Networks, Policy\nGradients, Actor Critic methods which are based on deep learning based models\nand back-propagation of gradients to train such models. An active area of\nresearch in reinforcement learning is about training agents to play complex\nvideo games, which so far has been something accomplished only by human\nintelligence. Some state of the art performances in video game playing using\ndeep reinforcement learning are obtained by processing the sequence of frames\nfrom video games, passing them through a convolutional network to obtain\nfeatures and then using recurrent neural networks to figure out the action\nleading to optimal rewards. The recurrent neural network will learn to extract\nthe meaningful signal out of the sequence of such features. In this work, we\npropose a method utilizing a transformer network which have recently replaced\nRNNs in Natural Language Processing (NLP), and perform experiments to compare\nwith existing methods.",
        "We present a novel and hierarchical approach for supervised classification of\nsignals spanning over a fixed graph, reflecting shared properties of the\ndataset. To this end, we introduce a Convolutional Cluster Pooling layer\nexploiting a multi-scale clustering in order to highlight, at different\nresolutions, locally connected regions on the input graph. Our proposal\ngeneralises well-established neural models such as Convolutional Neural\nNetworks (CNNs) on irregular and complex domains, by means of the exploitation\nof the weight sharing property in a graph-oriented architecture. In this work,\nsuch property is based on the centrality of each vertex within its\nsoft-assigned cluster. Extensive experiments on NTU RGB+D, CIFAR-10 and 20NEWS\ndemonstrate the effectiveness of the proposed technique in capturing both local\nand global patterns in graph-structured data out of different domains.",
        "It has been well demonstrated that inverse reinforcement learning (IRL) is an\neffective technique for teaching machines to perform tasks at human skill\nlevels given human demonstrations (i.e., human to machine apprenticeship\nlearning). This paper seeks to show that a similar application can be\ndemonstrated with human learners. That is, given demonstrations from human\nexperts inverse reinforcement learning techniques can be used to teach other\nhumans to perform at higher skill levels (i.e., human to human apprenticeship\nlearning). To show this two experiments were conducted using a simple,\nreal-time web game where players were asked to touch targets in order to earn\nas many points as possible. For the experiment player performance was defined\nas the number of targets a player touched, irrespective of the points that a\nplayer actually earned. This allowed for in-game points to be modified and the\neffect of these alterations on performance measured. At no time were\nparticipants told the true performance metric. To determine the point\nmodifications IRL was applied on demonstrations of human experts playing the\ngame. The results of the experiment show with significance that performance\nimproved over the control for select treatment groups. Finally, in addition to\nthe experiment, we also detail the algorithmic challenges we faced when\nconducting the experiment and the techniques we used to overcome them.",
        "Japanese comics (called manga) are traditionally created in monochrome\nformat. In recent years, in addition to monochrome comics, full color comics, a\nmore attractive medium, have appeared. Unfortunately, color comics require\nmanual colorization, which incurs high labor costs. Although automatic\ncolorization methods have been recently proposed, most of them are designed for\nillustrations, not for comics. Unlike illustrations, since comics are composed\nof many consecutive images, the painting style must be consistent. To realize\nconsistent colorization, we propose here a semi-automatic colorization method\nbased on generative adversarial networks (GAN); the method learns the painting\nstyle of a specific comic from small amount of training data. The proposed\nmethod takes a pair of a screen tone image and a flat colored image as input,\nand outputs a colorized image. Experiments show that the proposed method\nachieves better performance than the existing alternatives.",
        "Semi-Supervised classification and segmentation methods have been widely\ninvestigated in medical image analysis. Both approaches can improve the\nperformance of fully-supervised methods with additional unlabeled data.\nHowever, as a fundamental task, semi-supervised object detection has not gained\nenough attention in the field of medical image analysis. In this paper, we\npropose a novel Semi-Supervised Medical image Detector (SSMD). The motivation\nbehind SSMD is to provide free yet effective supervision for unlabeled data, by\nregularizing the predictions at each position to be consistent. To achieve the\nabove idea, we develop a novel adaptive consistency cost function to regularize\ndifferent components in the predictions. Moreover, we introduce heterogeneous\nperturbation strategies that work in both feature space and image space, so\nthat the proposed detector is promising to produce powerful image\nrepresentations and robust predictions. Extensive experimental results show\nthat the proposed SSMD achieves the state-of-the-art performance at a wide\nrange of settings. We also demonstrate the strength of each proposed module\nwith comprehensive ablation studies.",
        "It is very common to face classification problems where the number of\navailable labeled samples is small compared to their dimension. These\nconditions are likely to cause underdetermined settings, with high risk of\noverfitting. To improve the generalization ability of trained classifiers,\ncommon solutions include using priors about the data distribution. Among many\noptions, data structure priors, often represented through graphs, are\nincreasingly popular in the field. In this paper, we introduce a generic model\nwhere observed class signals are supposed to be deteriorated with two sources\nof noise, one independent of the underlying graph structure and isotropic, and\nthe other colored by a known graph operator. Under this model, we derive an\noptimal methodology to classify such signals. Interestingly, this methodology\nincludes a single parameter, making it particularly suitable for cases where\navailable data is scarce. Using various real datasets, we showcase the ability\nof the proposed model to be implemented in real world scenarios, resulting in\nincreased generalization accuracy compared to popular alternatives.",
        "Learning powerful data embeddings has become a center piece in machine\nlearning, especially in natural language processing and computer vision\ndomains. The crux of these embeddings is that they are pretrained on huge\ncorpus of data in a unsupervised fashion, sometimes aided with transfer\nlearning. However currently in the graph learning domain, embeddings learned\nthrough existing graph neural networks (GNNs) are task dependent and thus\ncannot be shared across different datasets. In this paper, we present a first\npowerful and theoretically guaranteed graph neural network that is designed to\nlearn task-independent graph embeddings, thereafter referred to as deep\nuniversal graph embedding (DUGNN). Our DUGNN model incorporates a novel graph\nneural network (as a universal graph encoder) and leverages rich Graph Kernels\n(as a multi-task graph decoder) for both unsupervised learning and\n(task-specific) adaptive supervised learning. By learning task-independent\ngraph embeddings across diverse datasets, DUGNN also reaps the benefits of\ntransfer learning. Through extensive experiments and ablation studies, we show\nthat the proposed DUGNN model consistently outperforms both the existing\nstate-of-art GNN models and Graph Kernels by an increased accuracy of 3% - 8%\non graph classification benchmark datasets.",
        "Short-term traffic flow prediction is one of the crucial issues in\nintelligent transportation system, which is an important part of smart cities.\nAccurate predictions can enable both the drivers and the passengers to make\nbetter decisions about their travel route, departure time and travel origin\nselection, which can be helpful in traffic management. Multiple models and\nalgorithms based on time series prediction and machine learning were applied to\nthis issue and achieved acceptable results. Recently, the availability of\nsufficient data and computational power, motivates us to improve the prediction\naccuracy via deep-learning approaches. Recurrent neural networks have become\none of the most popular methods for time series forecasting, however, due to\nthe variety of these networks, the question that which type is the most\nappropriate one for this task remains unsolved. In this paper, we use three\nkinds of recurrent neural networks including simple RNN units, GRU and LSTM\nneural network to predict short-term traffic flow. The dataset from TAP30\nCorporation is used for building the models and comparing RNNs with several\nwell-known models, such as DEMA, LASSO and XGBoost. The results show that all\nthree types of RNNs outperform the others, however, more simple RNNs such as\nsimple recurrent units and GRU perform work better than LSTM in terms of\naccuracy and training time.",
        "Texture is a fundamental characteristic of many types of images, and texture\nrepresentation is one of the essential and challenging problems in computer\nvision and pattern recognition which has attracted extensive research\nattention. Since 2000, texture representations based on Bag of Words (BoW) and\non Convolutional Neural Networks (CNNs) have been extensively studied with\nimpressive performance. Given this period of remarkable evolution, this paper\naims to present a comprehensive survey of advances in texture representation\nover the last two decades. More than 200 major publications are cited in this\nsurvey covering different aspects of the research, which includes (i) problem\ndescription; (ii) recent advances in the broad categories of BoW-based,\nCNN-based and attribute-based methods; and (iii) evaluation issues,\nspecifically benchmark datasets and state of the art results. In retrospect of\nwhat has been achieved so far, the survey discusses open challenges and\ndirections for future research.",
        "This paper describes an end-to-end solution for the relationship prediction\ntask in heterogeneous, multi-relational graphs. We particularly address two\nbuilding blocks in the pipeline, namely heterogeneous graph representation\nlearning and negative sampling. Existing message passing-based graph neural\nnetworks use edges either for graph traversal and/or selection of message\nencoding functions. Ignoring the edge semantics could have severe repercussions\non the quality of embeddings, especially when dealing with two nodes having\nmultiple relations. Furthermore, the expressivity of the learned representation\ndepends on the quality of negative samples used during training. Although\nexisting hard negative sampling techniques can identify challenging negative\nrelationships for optimization, new techniques are required to control false\nnegatives during training as false negatives could corrupt the learning\nprocess. To address these issues, first, we propose RelGNN -- a message\npassing-based heterogeneous graph attention model. In particular, RelGNN\ngenerates the states of different relations and leverages them along with the\nnode states to weigh the messages. RelGNN also adopts a self-attention\nmechanism to balance the importance of attribute features and topological\nfeatures for generating the final entity embeddings. Second, we introduce a\nparameter-free negative sampling technique -- adaptive self-adversarial (ASA)\nnegative sampling. ASA reduces the false-negative rate by leveraging positive\nrelationships to effectively guide the identification of true negative samples.\nOur experimental evaluation demonstrates that RelGNN optimized by ASA for\nrelationship prediction improves state-of-the-art performance across\nestablished benchmarks as well as on a real industrial dataset.",
        "This paper considers a multi-armed bandit (MAB) problem in which multiple\nmobile agents receive rewards by sampling from a collection of spatially\ndispersed stochastic processes, called bandits. The goal is to formulate a\ndecentralized policy for each agent, in order to maximize the total cumulative\nreward over all agents, subject to option availability and inter-agent\ncommunication constraints. The problem formulation is motivated by applications\nin which a team of autonomous mobile robots cooperates to accomplish an\nexploration and exploitation task in an uncertain environment. Bandit locations\nare represented by vertices of the spatial graph. At any time, an agent's\noption consist of sampling the bandit at its current location, or traveling\nalong an edge of the spatial graph to a new bandit location. Communication\nconstraints are described by a directed, non-stationary, stochastic\ncommunication graph. At any time, agents may receive data only from their\ncommunication graph in-neighbors. For the case of a single agent on a fully\nconnected spatial graph, it is known that the expected regret for any optimal\npolicy is necessarily bounded below by a function that grows as the logarithm\nof time. A class of policies called upper confidence bound (UCB) algorithms\nasymptotically achieve logarithmic regret for the classical MAB problem. In\nthis paper, we propose a UCB-based decentralized motion and option selection\npolicy and a non-stationary stochastic communication protocol that guarantee\nlogarithmic regret. To our knowledge, this is the first such decentralized\npolicy for non-fully connected spatial graphs with communication constraints.\nWhen the spatial graph is fully connected and the communication graph is\nstationary, our decentralized algorithm matches or exceeds the best reported\nprior results from the literature.",
        "Our objective is to transform a video into a set of discrete audio-visual\nobjects using self-supervised learning. To this end, we introduce a model that\nuses attention to localize and group sound sources, and optical flow to\naggregate information over time. We demonstrate the effectiveness of the\naudio-visual object embeddings that our model learns by using them for four\ndownstream speech-oriented tasks: (a) multi-speaker sound source separation,\n(b) localizing and tracking speakers, (c) correcting misaligned audio-visual\ndata, and (d) active speaker detection. Using our representation, these tasks\ncan be solved entirely by training on unlabeled video, without the aid of\nobject detectors. We also demonstrate the generality of our method by applying\nit to non-human speakers, including cartoons and puppets.Our model\nsignificantly outperforms other self-supervised approaches, and obtains\nperformance competitive with methods that use supervised face detection.",
        "Deep learning is the current bet for image classification. Its greed for huge\namounts of annotated data limits its usage in medical imaging context. In this\nscenario transfer learning appears as a prominent solution. In this report we\naim to clarify how transfer learning schemes may influence classification\nresults. We are particularly focused in the automated melanoma screening\nproblem, a case of medical imaging in which transfer learning is still not\nwidely used. We explored transfer with and without fine-tuning, sequential\ntransfers and usage of pre-trained models in general and specific datasets.\nAlthough some issues remain open, our findings may drive future researches.",
        "Echo State Networks (ESNs) are known for their fast and precise one-shot\nlearning of time series. But they often need good hyper-parameter tuning for\nbest performance. For this good validation is key, but usually, a single\nvalidation split is used. In this rather practical contribution we suggest\nseveral schemes for cross-validating ESNs and introduce an efficient algorithm\nfor implementing them. The component that dominates the time complexity of the\nalready quite fast ESN training remains constant (does not scale up with $k$)\nin our proposed method of doing $k$-fold cross-validation. The component that\ndoes scale linearly with $k$ starts dominating only in some not very common\nsituations. Thus in many situations $k$-fold cross-validation of ESNs can be\ndone for virtually the same time complexity as a simple single split\nvalidation. Space complexity can also remain the same. We also discuss when the\nproposed validation schemes for ESNs could be beneficial and empirically\ninvestigate them on several different real-world datasets.",
        "Reinforcement learning (RL) is a promising field to enhance robotic autonomy\nand decision making capabilities for space robotics, something which is\nchallenging with traditional techniques due to stochasticity and uncertainty\nwithin the environment. RL can be used to enable lunar cave exploration with\ninfrequent human feedback, faster and safer lunar surface locomotion or the\ncoordination and collaboration of multi-robot systems. However, there are many\nhurdles making research challenging for space robotic applications using RL and\nmachine learning, particularly due to insufficient resources for traditional\nrobotics simulators like CoppeliaSim. Our solution to this is an open source\nmodular platform called Reinforcement Learning for Simulation based Training of\nRobots, or RL STaR, that helps to simplify and accelerate the application of RL\nto the space robotics research field. This paper introduces the RL STaR\nplatform, and how researchers can use it through a demonstration.",
        "Despite its success and popularity, machine learning is now recognized as\nvulnerable to evasion attacks, i.e., carefully crafted perturbations of test\ninputs designed to force prediction errors. In this paper we focus on evasion\nattacks against decision tree ensembles, which are among the most successful\npredictive models for dealing with non-perceptual problems. Even though they\nare powerful and interpretable, decision tree ensembles have received only\nlimited attention by the security and machine learning communities so far,\nleading to a sub-optimal state of the art for adversarial learning techniques.\nWe thus propose Treant, a novel decision tree learning algorithm that, on the\nbasis of a formal threat model, minimizes an evasion-aware loss function at\neach step of the tree construction. Treant is based on two key technical\ningredients: robust splitting and attack invariance, which jointly guarantee\nthe soundness of the learning process. Experimental results on three publicly\navailable datasets show that Treant is able to generate decision tree ensembles\nthat are at the same time accurate and nearly insensitive to evasion attacks,\noutperforming state-of-the-art adversarial learning techniques.",
        "Weakly supervised object detection (WSOD) aims to classify and locate objects\nwith only image-level supervision. Many WSOD approaches adopt multiple instance\nlearning as the initial model, which is prone to converge to the most\ndiscriminative object regions while ignoring the whole object, and therefore\nreduce the model detection performance. In this paper, a novel cascade\nattentive dropout strategy is proposed to alleviate the part domination\nproblem, together with an improved global context module. We purposely discard\nattentive elements in both channel and space dimensions, and capture the\ninter-pixel and inter-channel dependencies to induce the model to better\nunderstand the global context. Extensive experiments have been conducted on the\nchallenging PASCAL VOC 2007 benchmarks, which achieve 49.8% mAP and 66.0%\nCorLoc, outperforming state-of-the-arts.",
        "In this paper, we propose FedGP, a framework for privacy-preserving data\nrelease in the federated learning setting. We use generative adversarial\nnetworks, generator components of which are trained by FedAvg algorithm, to\ndraw privacy-preserving artificial data samples and empirically assess the risk\nof information disclosure. Our experiments show that FedGP is able to generate\nlabelled data of high quality to successfully train and validate supervised\nmodels. Finally, we demonstrate that our approach significantly reduces\nvulnerability of such models to model inversion attacks.",
        "Although reinforcement learning (RL) can provide reliable solutions in many\nsettings, practitioners are often wary of the discrepancies between the RL\nsolution and their status quo procedures. Therefore, they may be reluctant to\nadapt to the novel way of executing tasks proposed by RL. On the other hand,\nmany real-world problems require relatively small adjustments from the status\nquo policies to achieve improved performance. Therefore, we propose a\nstudent-teacher RL mechanism in which the RL (the \"student\") learns to maximize\nits reward, subject to a constraint that bounds the difference between the RL\npolicy and the \"teacher\" policy. The teacher can be another RL policy (e.g.,\ntrained under a slightly different setting), the status quo policy, or any\nother exogenous policy. We formulate this problem using a stochastic\noptimization model and solve it using a primal-dual policy gradient algorithm.\nWe prove that the policy is asymptotically optimal. However, a naive\nimplementation suffers from high variance and convergence to a stochastic\noptimal policy. With a few practical adjustments to address these issues, our\nnumerical experiments confirm the effectiveness of our proposed method in\nmultiple GridWorld scenarios.",
        "Reinforcement learning algorithms can be used to optimally solve dynamic\ndecision-making and control problems. With continuous-valued state and input\nvariables, reinforcement learning algorithms must rely on function\napproximators to represent the value function and policy mappings. Commonly\nused numerical approximators, such as neural networks or basis function\nexpansions, have two main drawbacks: they are black-box models offering no\ninsight in the mappings learned, and they require significant trial and error\ntuning of their meta-parameters. In this paper, we propose a new approach to\nconstructing smooth value functions by means of symbolic regression. We\nintroduce three off-line methods for finding value functions based on a state\ntransition model: symbolic value iteration, symbolic policy iteration, and a\ndirect solution of the Bellman equation. The methods are illustrated on four\nnonlinear control problems: velocity control under friction, one-link and\ntwo-link pendulum swing-up, and magnetic manipulation. The results show that\nthe value functions not only yield well-performing policies, but also are\ncompact, human-readable and mathematically tractable. This makes them\npotentially suitable for further analysis of the closed-loop system. A\ncomparison with alternative approaches using neural networks shows that our\nmethod constructs well-performing value functions with substantially fewer\nparameters.",
        "Hand pose estimation from a monocular RGB image is an important but\nchallenging task. The main factor affecting its performance is the lack of a\nsufficiently large training dataset with accurate hand-keypoint annotations. In\nthis work, we circumvent this problem by proposing an effective method for\ngenerating realistic hand poses and show that state-of-the-art algorithms for\nhand pose estimation can be greatly improved by utilizing the generated hand\nposes as training data. Specifically, we first adopt an augmented reality (AR)\nsimulator to synthesize hand poses with accurate hand-keypoint labels. Although\nthe synthetic hand poses come with precise joint labels, eliminating the need\nof manual annotations, they look unnatural and are not the ideal training data.\nTo produce more realistic hand poses, we propose to blend a synthetic hand pose\nwith a real background, such as arms and sleeves. To this end, we develop\ntonality-alignment generative adversarial networks (TAGANs), which align the\ntonality and color distributions between synthetic hand poses and real\nbackgrounds, and can generate high quality hand poses. We evaluate TAGAN on\nthree benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With\nthe aid of the synthesized poses, our method performs favorably against the\nstate-of-the-arts in both 2D and 3D hand pose estimations.",
        "Many camera sensors use a dual-pixel (DP) design that operates as a\nrudimentary light field providing two sub-aperture views of a scene in a single\ncapture. The DP sensor was developed to improve how cameras perform autofocus.\nSince the DP sensor's introduction, researchers have found additional uses for\nthe DP data, such as depth estimation, reflection removal, and defocus\ndeblurring. We are interested in the latter task of defocus deblurring. In\nparticular, we propose a single-image deblurring network that incorporates the\ntwo sub-aperture views into a multi-task framework. Specifically, we show that\njointly learning to predict the two DP views from a single blurry input image\nimproves the network's ability to learn to deblur the image. Our experiments\nshow this multi-task strategy achieves +1dB PSNR improvement over\nstate-of-the-art defocus deblurring methods. In addition, our multi-task\nframework allows accurate DP-view synthesis (e.g., ~ 39dB PSNR) from the single\ninput image. These high-quality DP views can be used for other DP-based\napplications, such as reflection removal. As part of this effort, we have\ncaptured a new dataset of 7,059 high-quality images to support our training for\nthe DP-view synthesis task. Our dataset, code, and trained models will be made\npublicly available at\nhttps://github.com/Abdullah-Abuolaim/multi-task-defocus-deblurring-dual-pixel-nimat",
        "The safety constraints commonly used by existing safe reinforcement learning\n(RL) methods are defined only on expectation of initial states, but allow each\ncertain state to be unsafe, which is unsatisfying for real-world\nsafety-critical tasks. In this paper, we introduce the feasible actor-critic\n(FAC) algorithm, which is the first model-free constrained RL method that\nconsiders statewise safety, e.g, safety for each initial state. We claim that\nsome states are inherently unsafe no matter what policy we choose, while for\nother states there exist policies ensuring safety, where we say such states and\npolicies are feasible. By constructing a statewise Lagrange function available\non RL sampling and adopting an additional neural network to approximate the\nstatewise Lagrange multiplier, we manage to obtain the optimal feasible policy\nwhich ensures safety for each feasible state and the safest possible policy for\ninfeasible states. Furthermore, the trained multiplier net can indicate whether\na given state is feasible or not through the statewise complementary slackness\ncondition. We provide theoretical guarantees that FAC outperforms previous\nexpectation-based constrained RL methods in terms of both constraint\nsatisfaction and reward optimization. Experimental results on both robot\nlocomotive tasks and safe exploration tasks verify the safety enhancement and\nfeasibility interpretation of the proposed method.",
        "In the last few years, several techniques for facial manipulation in videos\nhave been successfully developed and made available to the masses (i.e.,\nFaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in\nvideo sequences with incredibly realistic results and a very little effort.\nDespite the usefulness of these tools in many fields, if used maliciously, they\ncan have a significantly bad impact on society (e.g., fake news spreading,\ncyber bullying through fake revenge porn). The ability of objectively detecting\nwhether a face has been manipulated in a video sequence is then a task of\nutmost importance. In this paper, we tackle the problem of face manipulation\ndetection in video sequences targeting modern facial manipulation techniques.\nIn particular, we study the ensembling of different trained Convolutional\nNeural Network (CNN) models. In the proposed solution, different models are\nobtained starting from a base network (i.e., EfficientNetB4) making use of two\ndifferent concepts: (i) attention layers; (ii) siamese training. We show that\ncombining these networks leads to promising face manipulation detection results\non two publicly available datasets with more than 119000 videos.",
        "We present a generalization of the person-image generation task, in which a\nhuman image is generated conditioned on a target pose and a set X of source\nappearance images. In this way, we can exploit multiple, possibly complementary\nimages of the same person which are usually available at training and at\ntesting time. The solution we propose is mainly based on a local attention\nmechanism which selects relevant information from different source image\nregions, avoiding the necessity to build specific generators for each specific\ncardinality of X. The empirical evaluation of our method shows the practical\ninterest of addressing the person-image generation problem in a multi-source\nsetting.",
        "Finding synthesis routes for molecules of interest is an essential step in\nthe discovery of new drugs and materials. To find such routes,\ncomputer-assisted synthesis planning (CASP) methods are employed which rely on\na model of chemical reactivity. In this study, we model single-step\nretrosynthesis in a template-based approach using modern Hopfield networks\n(MHNs). We adapt MHNs to associate different modalities, reaction templates and\nmolecules, which allows the model to leverage structural information about\nreaction templates. This approach significantly improves the performance of\ntemplate relevance prediction, especially for templates with few or zero\ntraining examples. With inference speed several times faster than that of\nbaseline methods, we improve predictive performance for top-k exact match\naccuracy for $\\mathrm{k}\\geq5$ in the retrosynthesis benchmark USPTO-50k.",
        "In this paper, we propose a new approach for building cellular automata to\nsolve real-world segmentation problems. We design and train a cellular\nautomaton that can successfully segment high-resolution images. We consider a\ncolony that densely inhabits the pixel grid, and all cells are governed by a\nrandomized update that uses the current state, the color, and the state of the\n$3\\times 3$ neighborhood. The space of possible rules is defined by a small\nneural network. The update rule is applied repeatedly in parallel to a large\nrandom subset of cells and after convergence is used to produce segmentation\nmasks that are then back-propagated to learn the optimal update rules using\nstandard gradient descent methods. We demonstrate that such models can be\nlearned efficiently with only limited trajectory length and that they show\nremarkable ability to organize the information to produce a globally consistent\nsegmentation result, using only local information exchange. From a practical\nperspective, our approach allows us to build very efficient models -- our\nsmallest automaton uses less than 10,000 parameters to solve complex\nsegmentation tasks.",
        "Fueled by massive amounts of data, models produced by machine-learning (ML)\nalgorithms, especially deep neural networks, are being used in diverse domains\nwhere trustworthiness is a concern, including automotive systems, finance,\nhealth care, natural language processing, and malware detection. Of particular\nconcern is the use of ML algorithms in cyber-physical systems (CPS), such as\nself-driving cars and aviation, where an adversary can cause serious\nconsequences. However, existing approaches to generating adversarial examples\nand devising robust ML algorithms mostly ignore the semantics and context of\nthe overall system containing the ML component. For example, in an autonomous\nvehicle using deep learning for perception, not every adversarial example for\nthe neural network might lead to a harmful consequence. Moreover, one may want\nto prioritize the search for adversarial examples towards those that\nsignificantly modify the desired semantics of the overall system. Along the\nsame lines, existing algorithms for constructing robust ML algorithms ignore\nthe specification of the overall system. In this paper, we argue that the\nsemantics and specification of the overall system has a crucial role to play in\nthis line of research. We present preliminary research results that support\nthis claim.",
        "Sensitive medical data is often subject to strict usage constraints. In this\npaper, we trained a generative adversarial network (GAN) on real-world\nelectronic health records (EHR). It was then used to create a data-set of\n\"fake\" patients through synthetic data generation (SDG) to circumvent usage\nconstraints. This real-world data was tabular, binary, intensive care unit\n(ICU) patient diagnosis data. The entire data-set was split into separate data\nsilos to mimic real-world scenarios where multiple ICU units across different\nhospitals may have similarly structured data-sets within their own\norganisations but do not have access to each other's data-sets. We implemented\nfederated learning (FL) to train separate GANs locally at each organisation,\nusing their unique data silo and then combining the GANs into a single central\nGAN, without any siloed data ever being exposed. This global, central GAN was\nthen used to generate the synthetic patients data-set. We performed an\nevaluation of these synthetic patients with statistical measures and through a\nstructured review by a group of medical professionals. It was shown that there\nwas no significant reduction in the quality of the synthetic EHR when we moved\nbetween training a single central model and training on separate data silos\nwith individual models before combining them into a central model. This was\ntrue for both the statistical evaluation (Root Mean Square Error (RMSE) of\n0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also\nfor the medical professionals' evaluation (no quality difference between EHR\ngenerated from a single source and EHR generated from multiple sources).",
        "Detecting abnormal activities in real-world surveillance videos is an\nimportant yet challenging task as the prior knowledge about video anomalies is\nusually limited or unavailable. Despite that many approaches have been\ndeveloped to resolve this problem, few of them can capture the normal\nspatio-temporal patterns effectively and efficiently. Moreover, existing works\nseldom explicitly consider the local consistency at frame level and global\ncoherence of temporal dynamics in video sequences. To this end, we propose\nConvolutional Transformer based Dual Discriminator Generative Adversarial\nNetworks (CT-D2GAN) to perform unsupervised video anomaly detection.\nSpecifically, we first present a convolutional transformer to perform future\nframe prediction. It contains three key components, i.e., a convolutional\nencoder to capture the spatial information of the input video clips, a temporal\nself-attention module to encode the temporal dynamics, and a convolutional\ndecoder to integrate spatio-temporal features and predict the future frame.\nNext, a dual discriminator based adversarial training procedure, which jointly\nconsiders an image discriminator that can maintain the local consistency at\nframe-level and a video discriminator that can enforce the global coherence of\ntemporal dynamics, is employed to enhance the future frame prediction. Finally,\nthe prediction error is used to identify abnormal video frames. Thoroughly\nempirical studies on three public video anomaly detection datasets, i.e., UCSD\nPed2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of\nthe proposed adversarial spatio-temporal modeling framework.",
        "Deep neural networks have evolved to become power demanding and consequently\ndifficult to apply to small-size mobile platforms. Network parameter reduction\nmethods have been introduced to systematically deal with the computational and\nmemory complexity of deep networks. We propose to examine the ability of\nattentive connection pruning to deal with redundancy reduction in neural\nnetworks as a contribution to the reduction of computational demand. In this\nwork, we describe a Top-Down attention mechanism that is added to a Bottom-Up\nfeedforward network to select important connections and subsequently prune\nredundant ones at all parametric layers. Our method not only introduces a novel\nhierarchical selection mechanism as the basis of pruning but also remains\ncompetitive with previous baseline methods in the experimental evaluation. We\nconduct experiments using different network architectures on popular benchmark\ndatasets to show high compression ratio is achievable with negligible loss of\naccuracy.",
        "In this research we address the problem of capturing recurring concepts in a\ndata stream environment. Recurrence capture enables the re-use of previously\nlearned classifiers without the need for re-learning while providing for better\naccuracy during the concept recurrence interval. We capture concepts by\napplying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to\nobtain highly compressed versions of the trees at concept drift points in the\nstream and store such trees in a repository for future use. Our empirical\nresults on real world and synthetic data exhibiting varying degrees of\nrecurrence show that the Fourier compressed trees are more robust to noise and\nare able to capture recurring concepts with higher precision than a meta\nlearning approach that chooses to re-use classifiers in their originally\noccurring form.",
        "Empirical scoring functions based on either molecular force fields or\ncheminformatics descriptors are widely used, in conjunction with molecular\ndocking, during the early stages of drug discovery to predict potency and\nbinding affinity of a drug-like molecule to a given target. These models\nrequire expert-level knowledge of physical chemistry and biology to be encoded\nas hand-tuned parameters or features rather than allowing the underlying model\nto select features in a data-driven procedure. Here, we develop a general\n3-dimensional spatial convolution operation for learning atomic-level chemical\ninteractions directly from atomic coordinates and demonstrate its application\nto structure-based bioactivity prediction. The atomic convolutional neural\nnetwork is trained to predict the experimentally determined binding affinity of\na protein-ligand complex by direct calculation of the energy associated with\nthe complex, protein, and ligand given the crystal structure of the binding\npose. Non-covalent interactions present in the complex that are absent in the\nprotein-ligand sub-structures are identified and the model learns the\ninteraction strength associated with these features. We test our model by\npredicting the binding free energy of a subset of protein-ligand complexes\nfound in the PDBBind dataset and compare with state-of-the-art cheminformatics\nand machine learning-based approaches. We find that all methods achieve\nexperimental accuracy and that atomic convolutional networks either outperform\nor perform competitively with the cheminformatics based methods. Unlike all\nprevious protein-ligand prediction systems, atomic convolutional networks are\nend-to-end and fully-differentiable. They represent a new data-driven,\nphysics-based deep learning model paradigm that offers a strong foundation for\nfuture improvements in structure-based bioactivity prediction.",
        "Age progression and regression aim to synthesize photorealistic appearance of\na given face image with aging and rejuvenation effects, respectively. Existing\ngenerative adversarial networks (GANs) based methods suffer from the following\nthree major issues: 1) unstable training introducing strong ghost artifacts in\nthe generated faces, 2) unpaired training leading to unexpected changes in\nfacial attributes such as genders and races, and 3) non-bijective age mappings\nincreasing the uncertainty in the face transformation. To overcome these\nissues, this paper proposes a novel framework, termed AgeFlow, to integrate the\nadvantages of both flow-based models and GANs. The proposed AgeFlow contains\nthree parts: an encoder that maps a given face to a latent space through an\ninvertible neural network, a novel invertible conditional translation module\n(ICTM) that translates the source latent vector to target one, and a decoder\nthat reconstructs the generated face from the target latent vector using the\nsame encoder network; all parts are invertible achieving bijective age\nmappings. The novelties of ICTM are two-fold. First, we propose an\nattribute-aware knowledge distillation to learn the manipulation direction of\nage progression while keeping other unrelated attributes unchanged, alleviating\nunexpected changes in facial attributes. Second, we propose to use GANs in the\nlatent space to ensure the learned latent vector indistinguishable from the\nreal ones, which is much easier than traditional use of GANs in the image\ndomain. Experimental results demonstrate superior performance over existing\nGANs-based methods on two benchmarked datasets. The source code is available at\nhttps://github.com/Hzzone/AgeFlow.",
        "Time series classification has received great attention over the past decade\nwith a wide range of methods focusing on predictive performance by exploiting\nvarious types of temporal features. Nonetheless, little emphasis has been\nplaced on interpretability and explainability. In this paper, we formulate the\nnovel problem of explainable time series tweaking, where, given a time series\nand an opaque classifier that provides a particular classification decision for\nthe time series, we want to find the minimum number of changes to be performed\nto the given time series so that the classifier changes its decision to another\nclass. We show that the problem is NP-hard, and focus on two instantiations of\nthe problem, which we refer to as reversible and irreversible time series\ntweaking. The classifier under investigation is the random shapelet forest\nclassifier. Moreover, we propose two algorithmic solutions for the two problems\nalong with simple optimizations, as well as a baseline solution using the\nnearest neighbor classifier. An extensive experimental evaluation on a variety\nof real datasets demonstrates the usefulness and effectiveness of our problem\nformulation and solutions.",
        "Image demosaicing - one of the most important early stages in digital camera\npipelines - addressed the problem of reconstructing a full-resolution image\nfrom so-called color-filter-arrays. Despite tremendous progress made in the\npase decade, a fundamental issue that remains to be addressed is how to assure\nthe visual quality of reconstructed images especially in the presence of noise\ncorruption. Inspired by recent advances in generative adversarial networks\n(GAN), we present a novel deep learning approach toward joint demosaicing and\ndenoising (JDD) with perceptual optimization in order to ensure the visual\nquality of reconstructed images. The key contributions of this work include: 1)\nwe have developed a GAN-based approach toward image demosacing in which a\ndiscriminator network with both perceptual and adversarial loss functions are\nused for quality assurance; 2) we propose to optimize the perceptual quality of\nreconstructed images by the proposed GAN in an end-to-end manner. Such\nend-to-end optimization of GAN is particularly effective for jointly exploiting\nthe gain brought by each modular component (e.g., residue learning in the\ngenerative network and perceptual loss in the discriminator network). Our\nextensive experimental results have shown convincingly improved performance\nover existing state-of-the-art methods in terms of both subjective and\nobjective quality metrics with a comparable computational cost.",
        "Consequential decisions are increasingly informed by sophisticated\ndata-driven predictive models. However, to consistently learn accurate\npredictive models, one needs access to ground truth labels. Unfortunately, in\npractice, labels may only exist conditional on certain decisions---if a loan is\ndenied, there is not even an option for the individual to pay back the loan.\nHence, the observed data distribution depends on how decisions are being made.\nIn this paper, we show that in this selective labels setting, learning a\npredictor directly only from available labeled data is suboptimal in terms of\nboth fairness and utility. To avoid this undesirable behavior, we propose to\ndirectly learn decision policies that maximize utility under fairness\nconstraints and thereby take into account how decisions affect which data is\nobserved in the future. Our results suggest the need for a paradigm shift in\nthe context of fair machine learning from the currently prevalent idea of\nsimply building predictive models from a single static dataset via risk\nminimization, to a more interactive notion of \"learning to decide\". In\nparticular, such policies should not entirely neglect part of the input space,\ndrawing connections to explore/exploit tradeoffs in reinforcement learning,\ndata missingness, and potential outcomes in causal inference. Experiments on\nsynthetic and real-world data illustrate the favorable properties of learning\nto decide in terms of utility and fairness.",
        "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.",
        "The self-supervised learning (SSL) paradigm is an essential exploration area,\nwhich tries to eliminate the need for expensive data labeling. Despite the\ngreat success of SSL methods in computer vision and natural language\nprocessing, most of them employ contrastive learning objectives that require\nnegative samples, which are hard to define. This becomes even more challenging\nin the case of graphs and is a bottleneck for achieving robust representations.\nTo overcome such limitations, we propose a framework for self-supervised graph\nrepresentation learning -- Graph Barlow Twins, which utilizes a\ncross-correlation-based loss function instead of negative samples. Moreover, it\ndoes not rely on non-symmetric neural network architectures -- in contrast to\nstate-of-the-art self-supervised graph representation learning method BGRL. We\nshow that our method achieves as competitive results as BGRL, best\nself-supervised methods, and fully supervised ones while requiring\nsubstantially fewer hyperparameters and converging in an order of magnitude\ntraining steps earlier.",
        "Point clouds analysis has grasped researchers' eyes in recent years, while 3D\nsemantic segmentation remains a problem. Most deep point clouds models directly\nconduct learning on 3D point clouds, which will suffer from the severe sparsity\nand extreme data processing load in urban-scale data. To tackle the challenge,\nwe propose to transfer the 3D point clouds to dense bird's-eye-view projection.\nIn this case, the segmentation task is simplified because of class unbalance\nreduction and the feasibility of leveraging various 2D segmentation methods. We\nfurther design an attention-based fusion network that can conduct multi-modal\nlearning on the projected images. Finally, the 2D out are remapped to generate\n3D semantic segmentation results. To demonstrate the benefits of our method, we\nconduct various experiments on the SensatUrban dataset, in which our model\npresents competitive evaluation results (61.17% mIoU and 91.37%\nOverallAccuracy). We hope our work can inspire further exploration in point\ncloud analysis.",
        "Attribute extrapolation in sample generation is challenging for deep neural\nnetworks operating beyond the training distribution. We formulate a new task\nfor extrapolation in sequence generation, focusing on natural language and\nproteins, and propose GENhance, a generative framework that enhances attributes\nthrough a learned latent space. Trained on movie reviews and a computed protein\nstability dataset, GENhance can generate strongly-positive text reviews and\nhighly stable protein sequences without being exposed to similar data during\ntraining. We release our benchmark tasks and models to contribute to the study\nof generative modeling extrapolation and data-driven design in biology and\nchemistry.",
        "We introduce a convolutional neural network model for unsupervised learning\nof depth and ego-motion from cylindrical panoramic video. Panoramic depth\nestimation is an important technology for applications such as virtual reality,\n3D modeling, and autonomous robotic navigation. In contrast to previous\napproaches for applying convolutional neural networks to panoramic imagery, we\nuse the cylindrical panoramic projection which allows for the use of the\ntraditional CNN layers such as convolutional filters and max pooling without\nmodification. Our evaluation of synthetic and real data shows that unsupervised\nlearning of depth and ego-motion on cylindrical panoramic images can produce\nhigh-quality depth maps and that an increased field-of-view improves ego-motion\nestimation accuracy. We also introduce Headcam, a novel dataset of panoramic\nvideo collected from a helmet-mounted camera while biking in an urban setting.",
        "We propose an end-to-end deep learning architecture that produces a 3D shape\nin triangular mesh from a single color image. Limited by the nature of deep\nneural network, previous methods usually represent a 3D shape in volume or\npoint cloud, and it is non-trivial to convert them to the more ready-to-use\nmesh model. Unlike the existing methods, our network represents 3D mesh in a\ngraph-based convolutional neural network and produces correct geometry by\nprogressively deforming an ellipsoid, leveraging perceptual features extracted\nfrom the input image. We adopt a coarse-to-fine strategy to make the whole\ndeformation procedure stable, and define various of mesh related losses to\ncapture properties of different levels to guarantee visually appealing and\nphysically accurate 3D geometry. Extensive experiments show that our method not\nonly qualitatively produces mesh model with better details, but also achieves\nhigher 3D shape estimation accuracy compared to the state-of-the-art.",
        "In recent years, significant progress has been made in solving challenging\nproblems across various domains using deep reinforcement learning (RL).\nReproducing existing work and accurately judging the improvements offered by\nnovel methods is vital to sustaining this progress. Unfortunately, reproducing\nresults for state-of-the-art deep RL methods is seldom straightforward. In\nparticular, non-determinism in standard benchmark environments, combined with\nvariance intrinsic to the methods, can make reported results tough to\ninterpret. Without significance metrics and tighter standardization of\nexperimental reporting, it is difficult to determine whether improvements over\nthe prior state-of-the-art are meaningful. In this paper, we investigate\nchallenges posed by reproducibility, proper experimental techniques, and\nreporting procedures. We illustrate the variability in reported metrics and\nresults when comparing against common baselines and suggest guidelines to make\nfuture results in deep RL more reproducible. We aim to spur discussion about\nhow to ensure continued progress in the field by minimizing wasted effort\nstemming from results that are non-reproducible and easily misinterpreted.",
        "Revealing hidden features in unlabeled data is called unsupervised feature\nlearning, which plays an important role in pretraining a deep neural network.\nHere we provide a statistical mechanics analysis of the unsupervised learning\nin a restricted Boltzmann machine with binary synapses. A message passing\nequation to infer the hidden feature is derived, and furthermore, variants of\nthis equation are analyzed. A statistical analysis by replica theory describes\nthe thermodynamic properties of the model. Our analysis confirms an entropy\ncrisis preceding the non-convergence of the message passing equation,\nsuggesting a discontinuous phase transition as a key characteristic of the\nrestricted Boltzmann machine. Continuous phase transition is also confirmed\ndepending on the embedded feature strength in the data. The mean-field result\nunder the replica symmetric assumption agrees with that obtained by running\nmessage passing algorithms on single instances of finite sizes. Interestingly,\nin an approximate Hopfield model, the entropy crisis is absent, and a\ncontinuous phase transition is observed instead. We also develop an iterative\nequation to infer the hyper-parameter (temperature) hidden in the data, which\nin physics corresponds to iteratively imposing Nishimori condition. Our study\nprovides insights towards understanding the thermodynamic properties of the\nrestricted Boltzmann machine learning, and moreover important theoretical basis\nto build simplified deep networks.",
        "Person re-identification (reID) is an important task that requires to\nretrieve a person's images from an image dataset, given one image of the person\nof interest. For learning robust person features, the pose variation of person\nimages is one of the key challenges. Existing works targeting the problem\neither perform human alignment, or learn human-region-based representations.\nExtra pose information and computational cost is generally required for\ninference. To solve this issue, a Feature Distilling Generative Adversarial\nNetwork (FD-GAN) is proposed for learning identity-related and pose-unrelated\nrepresentations. It is a novel framework based on a Siamese structure with\nmultiple novel discriminators on human poses and identities. In addition to the\ndiscriminators, a novel same-pose loss is also integrated, which requires\nappearance of a same person's generated images to be similar. After learning\npose-unrelated person features with pose guidance, no auxiliary pose\ninformation and additional computational cost is required during testing. Our\nproposed FD-GAN achieves state-of-the-art performance on three person reID\ndatasets, which demonstrates that the effectiveness and robust feature\ndistilling capability of the proposed FD-GAN.",
        "During software front-end development, the work to convert Graphical User\nInterface(GUI) image to the corresponding front-end code is an inevitable\ntedious work. There have been some attempts to make this work to be automatic.\nHowever, the GUI code generated by these models is not accurate due to the lack\nof attention mechanism guidance. To solve this problem, we propose PixCoder\nbased on an artificially supervised attention mechanism. The approach is to\ntrain a neural network to predict the style sheets in the input GUI image and\nthen output a vector. PixCoder generate the GUI code targeting specific\nplatform according to the output vector. The experimental results have shown\nthe accuracy of the GUI code generated by PixCoder is over 95%.",
        "In this paper, we propose a new capsule network architecture called Attention\nRouting CapsuleNet (AR CapsNet). We replace the dynamic routing and squash\nactivation function of the capsule network with dynamic routing (CapsuleNet)\nwith the attention routing and capsule activation. The attention routing is a\nrouting between capsules through an attention module. The attention routing is\na fast forward-pass while keeping spatial information. On the other hand, the\nintuitive interpretation of the dynamic routing is finding a centroid of the\nprediction capsules. Thus, the squash activation function and its variant focus\non preserving a vector orientation. However, the capsule activation focuses on\nperforming a capsule-scale activation function.\n  We evaluate our proposed model on the MNIST, affNIST, and CIFAR-10\nclassification tasks. The proposed model achieves higher accuracy with fewer\nparameters (x0.65 in the MNIST, x0.82 in the CIFAR-10) and less training time\nthan CapsuleNet (x0.19 in the MNIST, x0.35 in the CIFAR-10). These results\nvalidate that designing a capsule-scale operation is a key factor to implement\nthe capsule concept.\n  Also, our experiment shows that our proposed model is transformation\nequivariant as CapsuleNet. As we perturb each element of the output capsule,\nthe decoder attached to the output capsules shows global variations. Further\nexperiments show that the difference in the capsule features caused by applying\naffine transformations on an input image is significantly aligned in one\ndirection.",
        "In addition, our work has text overlap with arXiv:1804.06242,\narXiv:1705.00938 by other authors. We want to rewrite this paper for avoiding\nthis fact.",
        "Efficient and easy segmentation of images and volumes is of great practical\nimportance. Segmentation problems that motivate our approach originate from\nmicroscopy imaging commonly used in materials science, medicine, and biology.\nWe formulate image segmentation as a probabilistic pixel classification\nproblem, and we apply segmentation as a step towards characterising image\ncontent. Our method allows the user to define structures of interest by\ninteractively marking a subset of pixels. Thanks to the real-time feedback, the\nuser can place new markings strategically, depending on the current outcome.\nThe final pixel classification may be obtained from a very modest user input.\nAn important ingredient of our method is a graph that encodes image content.\nThis graph is built in an unsupervised manner during initialisation and is\nbased on clustering of image features. Since we combine a limited amount of\nuser-labelled data with the clustering information obtained from the unlabelled\nparts of the image, our method fits in the general framework of semi-supervised\nlearning. We demonstrate how this can be a very efficient approach to\nsegmentation through pixel classification.",
        "Network representation learning (NRL) methods aim to map each vertex into a\nlow dimensional space by preserving the local and global structure of a given\nnetwork, and in recent years they have received a significant attention thanks\nto their success in several challenging problems. Although various approaches\nhave been proposed to compute node embeddings, many successful methods benefit\nfrom random walks in order to transform a given network into a collection of\nsequences of nodes and then they target to learn the representation of nodes by\npredicting the context of each vertex within the sequence. In this paper, we\nintroduce a general framework to enhance the embeddings of nodes acquired by\nmeans of the random walk-based approaches. Similar to the notion of topical\nword embeddings in NLP, the proposed method assigns each vertex to a topic with\nthe favor of various statistical models and community detection methods, and\nthen generates the enhanced community representations. We evaluate our method\non two downstream tasks: node classification and link prediction. The\nexperimental results demonstrate that the incorporation of vertex and topic\nembeddings outperform widely-known baseline NRL methods.",
        "Compared with object detection in static images, object detection in videos\nis more challenging due to degraded image qualities. An effective way to\naddress this problem is to exploit temporal contexts by linking the same object\nacross video to form tubelets and aggregating classification scores in the\ntubelets. In this paper, we focus on obtaining high quality object linking\nresults for better classification. Unlike previous methods that link objects by\nchecking boxes between neighboring frames, we propose to link in the same\nframe. To achieve this goal, we extend prior methods in following aspects: (1)\na cuboid proposal network that extracts spatio-temporal candidate cuboids which\nbound the movement of objects; (2) a short tubelet detection network that\ndetects short tubelets in short video segments; (3) a short tubelet linking\nalgorithm that links temporally-overlapping short tubelets to form long\ntubelets. Experiments on the ImageNet VID dataset show that our method\noutperforms both the static image detector and the previous state of the art.\nIn particular, our method improves results by 8.8% over the static image\ndetector for fast moving objects.",
        "What enables Stochastic Gradient Descent (SGD) to achieve better\ngeneralization than Gradient Descent (GD) in Neural Network training? This\nquestion has attracted much attention. In this paper, we study the distribution\nof the Stochastic Gradient Noise (SGN) vectors during the training. We observe\nthat for batch sizes 256 and above, the distribution is best described as\nGaussian at-least in the early phases of training. This holds across data-sets,\narchitectures, and other choices.",
        "A hallmark of graph neural networks is their ability to distinguish the\nisomorphism class of their inputs. This study derives hardness results for the\nclassification variant of graph isomorphism in the message-passing model\n(MPNN). MPNN encompasses the majority of graph neural networks used today and\nis universal when nodes are given unique features. The analysis relies on the\nintroduced measure of communication capacity. Capacity measures how much\ninformation the nodes of a network can exchange during the forward pass and\ndepends on the depth, message-size, global state, and width of the\narchitecture. It is shown that the capacity of MPNN needs to grow linearly with\nthe number of nodes so that a network can distinguish trees and quadratically\nfor general connected graphs. The derived bounds concern both worst- and\naverage-case behavior and apply to networks with/without unique features and\nadaptive architecture -- they are also up to two orders of magnitude tighter\nthan those given by simpler arguments. An empirical study involving 12 graph\nclassification tasks and 420 networks reveals strong alignment between actual\nperformance and theoretical predictions.",
        "Many interactive image segmentation techniques are based on semi-supervised\nlearning. The user may label some pixels from each object and the SSL algorithm\nwill propagate the labels from the labeled to the unlabeled pixels, finding\nobject boundaries. This paper proposes a new SSL graph-based interactive image\nsegmentation approach, using undirected and unweighted kNN graphs, from which\nthe unlabeled nodes receive contributions from other nodes (either labeled or\nunlabeled). It is simpler than many other techniques, but it still achieves\nsignificant classification accuracy in the image segmentation task. Computer\nsimulations are performed using some real-world images, extracted from the\nMicrosoft GrabCut dataset. The segmentation results show the effectiveness of\nthe proposed approach.",
        "Meta-learning becomes a practical approach towards few-shot image\nclassification, where a visual recognition system is constructed with limited\nannotated data. Inductive bias such as embedding is learned from a base class\nset with ample labeled examples and then generalizes to few-shot tasks with\nnovel classes. Surprisingly, we find that the base class set labels are not\nnecessary, and discriminative embeddings could be meta-learned in an\nunsupervised manner. Comprehensive analyses indicate two modifications -- the\nsemi-normalized distance metric and the sufficient sampling -- improves\nunsupervised meta-learning (UML) significantly. Based on the modified baseline,\nwe further amplify or compensate for the characteristic of tasks when training\na UML model. First, mixed embeddings are incorporated to increase the\ndifficulty of few-shot tasks. Next, we utilize a task-specific embedding\ntransformation to deal with the specific properties among tasks, maintaining\nthe generalization ability into the vanilla embeddings. Experiments on few-shot\nlearning benchmarks verify that our approaches outperform previous UML methods\nby a 4-10% performance gap, and embeddings learned with our UML achieve\ncomparable or even better performance than its supervised variants.",
        "In this work, we address the problem of multi-domain image-to-image\ntranslation with particular attention paid to computational cost. In\nparticular, current state of the art models require a large and deep model in\norder to handle the visual diversity of multiple domains. In a context of\nlimited computational resources, increasing the network size may not be\npossible. Therefore, we propose to increase the network capacity by using an\nadaptive graph structure. At inference time, the network estimates its own\ngraph by selecting specific sub-networks. Sub-network selection is implemented\nusing Gumbel-Softmax in order to allow end-to-end training. This approach leads\nto an adjustable increase in number of parameters while preserving an almost\nconstant computational cost. Our evaluation on two publicly available datasets\nof facial and painting images shows that our adaptive strategy generates better\nimages with fewer artifacts than literature methods",
        "Hand segmentation and detection in truly unconstrained RGB-based settings is\nimportant for many applications. However, existing datasets are far from\nsufficient both in terms of size and variety due to the infeasibility of manual\nannotation of large amounts of segmentation and detection data. As a result,\ncurrent methods are limited by many underlying assumptions such as constrained\nenvironment, consistent skin color and lighting. In this work, we present a\nlarge-scale RGB-based egocentric hand segmentation/detection dataset Ego2Hands\nthat is automatically annotated and a color-invariant compositing-based data\ngeneration technique capable of creating unlimited training data with variety.\nFor quantitative analysis, we manually annotated an evaluation set that\nsignificantly exceeds existing benchmarks in quantity, diversity and annotation\naccuracy. We provide cross-dataset evaluation as well as thorough analysis on\nthe performance of state-of-the-art models on Ego2Hands to show that our\ndataset and data generation technique can produce models that generalize to\nunseen environments without domain adaptation.",
        "Recent studies show a close connection between neural networks (NN) and\nkernel methods. However, most of these analyses (e.g., NTK) focus on the\ninfluence of (infinite) width instead of the depth of NN models. There remains\na gap between theory and practical network designs that benefit from the depth.\nThis paper first proposes a novel kernel family named Neural Optimization\nKernel (NOK). Our kernel is defined as the inner product between two $T$-step\nupdated functionals in RKHS w.r.t. a regularized optimization problem.\nTheoretically, we proved the monotonic descent property of our update rule for\nboth convex and non-convex problems, and a $O(1/T)$ convergence rate of our\nupdates for convex problems. Moreover, we propose a data-dependent structured\napproximation of our NOK, which builds the connection between training deep NNs\nand kernel methods associated with NOK. The resultant computational graph is a\nResNet-type finite width NN. Our structured approximation preserved the\nmonotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer\nNN performs $T$-step monotonic descent updates. Notably, we show our\n$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate\nw.r.t. a convex regularized problem, which explains the success of ReLU on\ntraining deep NN from a NN architecture optimization perspective. For the\nunsupervised learning and the shared parameter case, we show the equivalence of\ntraining structured NN with GD and performing functional gradient descent in\nRKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.\nFor finite NOKs, we prove generalization bounds. Remarkably, we show that\noverparameterized deep NN (NOK) can increase the expressive power to reduce\nempirical risk and reduce the generalization bound at the same time. Extensive\nexperiments verify the robustness of our structured NOK blocks.",
        "Purpose: To determine whether deep learning models can distinguish between\nbreast cancer molecular subtypes based on dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI). Materials and methods: In this institutional\nreview board-approved single-center study, we analyzed DCE-MR images of 270\npatients at our institution. Lesions of interest were identified by\nradiologists. The task was to automatically determine whether the tumor is of\nthe Luminal A subtype or of another subtype based on the MR image patches\nrepresenting the tumor. Three different deep learning approaches were used to\nclassify the tumor according to their molecular subtypes: learning from scratch\nwhere only tumor patches were used for training, transfer learning where\nnetworks pre-trained on natural images were fine-tuned using tumor patches, and\noff-the-shelf deep features where the features extracted by neural networks\ntrained on natural images were used for classification with a support vector\nmachine. Network architectures utilized in our experiments were GoogleNet, VGG,\nand CIFAR. We used 10-fold crossvalidation method for validation and area under\nthe receiver operating characteristic (AUC) as the measure of performance.\nResults: The best AUC performance for distinguishing molecular subtypes was\n0.65 (95% CI:[0.57,0.71]) and was achieved by the off-the-shelf deep features\napproach. The highest AUC performance for training from scratch was 0.58 (95%\nCI:[0.51,0.64]) and the best AUC performance for transfer learning was 0.60\n(95% CI:[0.52,0.65]) respectively. For the off-the-shelf approach, the features\nextracted from the fully connected layer performed the best. Conclusion: Deep\nlearning may play a role in discovering radiogenomic associations in breast\ncancer.",
        "We present an approach to training neural networks to generate sequences\nusing actor-critic methods from reinforcement learning (RL). Current\nlog-likelihood training methods are limited by the discrepancy between their\ntraining and testing modes, as models must generate tokens conditioned on their\nprevious guesses rather than the ground-truth tokens. We address this problem\nby introducing a \\textit{critic} network that is trained to predict the value\nof an output token, given the policy of an \\textit{actor} network. This results\nin a training procedure that is much closer to the test phase, and allows us to\ndirectly optimize for a task-specific score such as BLEU. Crucially, since we\nleverage these techniques in the supervised learning setting rather than the\ntraditional RL setting, we condition the critic network on the ground-truth\noutput. We show that our method leads to improved performance on both a\nsynthetic task, and for German-English machine translation. Our analysis paves\nthe way for such methods to be applied in natural language generation tasks,\nsuch as machine translation, caption generation, and dialogue modelling.",
        "Time series are widely used as signals in many classification/regression\ntasks. It is ubiquitous that time series contains many missing values. Given\nmultiple correlated time series data, how to fill in missing values and to\npredict their class labels? Existing imputation methods often impose strong\nassumptions of the underlying data generating process, such as linear dynamics\nin the state space. In this paper, we propose BRITS, a novel method based on\nrecurrent neural networks for missing value imputation in time series data. Our\nproposed method directly learns the missing values in a bidirectional recurrent\ndynamical system, without any specific assumption. The imputed values are\ntreated as variables of RNN graph and can be effectively updated during the\nbackpropagation.BRITS has three advantages: (a) it can handle multiple\ncorrelated missing values in time series; (b) it generalizes to time series\nwith nonlinear dynamics underlying; (c) it provides a data-driven imputation\nprocedure and applies to general settings with missing data.We evaluate our\nmodel on three real-world datasets, including an air quality dataset, a\nhealth-care data, and a localization data for human activity. Experiments show\nthat our model outperforms the state-of-the-art methods in both imputation and\nclassification/regression accuracies.",
        "Offline reinforcement learning (RL) enables learning control policies by\nutilizing only prior experience, without any online interaction. This can allow\nrobots to acquire generalizable skills from large and diverse datasets, without\nany costly or unsafe online data collection. Despite recent algorithmic\nadvances in offline RL, applying these methods to real-world problems has\nproven challenging. Although offline RL methods can learn from prior data,\nthere is no clear and well-understood process for making various design\nchoices, from model architecture to algorithm hyperparameters, without actually\nevaluating the learned policies online. In this paper, our aim is to develop a\npractical workflow for using offline RL analogous to the relatively\nwell-understood workflows for supervised learning problems. To this end, we\ndevise a set of metrics and conditions that can be tracked over the course of\noffline training, and can inform the practitioner about how the algorithm and\nmodel architecture should be adjusted to improve final performance. Our\nworkflow is derived from a conceptual understanding of the behavior of\nconservative offline RL algorithms and cross-validation in supervised learning.\nWe demonstrate the efficacy of this workflow in producing effective policies\nwithout any online tuning, both in several simulated robotic learning scenarios\nand for three tasks on two distinct real robots, focusing on learning\nmanipulation skills with raw image observations with sparse binary rewards.\nExplanatory video and additional results can be found at\nsites.google.com/view/offline-rl-workflow",
        "Digital libraries store images which can be highly degraded and to index this\nkind of images we resort to word spot- ting as our information retrieval\nsystem. Information retrieval for handwritten document images is more\nchallenging due to the difficulties in complex layout analysis, large\nvariations of writing styles, and degradation or low quality of historical\nmanuscripts. This paper presents a simple innovative learning-free method for\nword spotting from large scale historical documents combining Local Binary\nPattern (LBP) and spatial sampling. This method offers three advantages:\nfirstly, it operates in completely learning free paradigm which is very\ndifferent from unsupervised learning methods, secondly, the computational time\nis significantly low because of the LBP features which are very fast to\ncompute, and thirdly, the method can be used in scenarios where annotations are\nnot available. Finally we compare the results of our proposed retrieval method\nwith the other methods in the literature.",
        "Growing concerns regarding the operational usage of AI models in the\nreal-world has caused a surge of interest in explaining AI models' decisions to\nhumans. Reinforcement Learning is not an exception in this regard. In this\nwork, we propose a method for offering local explanations on risk in\nreinforcement learning. Our method only requires a log of previous interactions\nbetween the agent and the environment to create a state-transition model. It is\ndesigned to work on RL environments with either continuous or discrete state\nand action spaces. After creating the model, actions of any agent can be\nexplained in terms of the features most influential in increasing or decreasing\nrisk or any other desirable objective function in the locality of the agent.\nThrough experiments, we demonstrate the effectiveness of the proposed method in\nproviding such explanations.",
        "Deep learning has become an area of interest in most scientific areas,\nincluding physical sciences. Modern networks apply real-valued transformations\non the data. Particularly, convolutions in convolutional neural networks\ndiscard phase information entirely. Many deterministic signals, such as seismic\ndata or electrical signals, contain significant information in the phase of the\nsignal. We explore complex-valued deep convolutional networks to leverage\nnon-linear feature maps. Seismic data commonly has a lowcut filter applied, to\nattenuate noise from ocean waves and similar long wavelength contributions.\nDiscarding the phase information leads to low-frequency aliasing analogous to\nthe Nyquist-Shannon theorem for high frequencies. In non-stationary data, the\nphase content can stabilize training and improve the generalizability of neural\nnetworks. While it has been shown that phase content can be restored in deep\nneural networks, we show how including phase information in feature maps\nimproves both training and inference from deterministic physical data.\nFurthermore, we show that the reduction of parameters in a complex network\noutperforms larger real-valued networks.",
        "Normalizing flows are exact-likelihood generative neural networks which\napproximately transform samples from a simple prior distribution to samples of\nthe probability distribution of interest. Recent work showed that such\ngenerative models can be utilized in statistical mechanics to sample\nequilibrium states of many-body systems in physics and chemistry. To scale and\ngeneralize these results, it is essential that the natural symmetries in the\nprobability density -- in physics defined by the invariances of the target\npotential -- are built into the flow. We provide a theoretical sufficient\ncriterion showing that the distribution generated by \\textit{equivariant}\nnormalizing flows is invariant with respect to these symmetries by design.\nFurthermore, we propose building blocks for flows which preserve symmetries\nwhich are usually found in physical/chemical many-body particle systems. Using\nbenchmark systems motivated from molecular physics, we demonstrate that those\nsymmetry preserving flows can provide better generalization capabilities and\nsampling efficiency.",
        "Solving the Goal-Conditioned Reward Sparse (GCRS) task is a challenging\nreinforcement learning problem due to the sparsity of reward signals. In this\nwork, we propose a new formulation of GCRS tasks from the perspective of the\ndrifted random walk on the state space, and design a novel method called\nEvolutionary Stochastic Policy Distillation (ESPD) to solve them based on the\ninsight of reducing the First Hitting Time of the stochastic process. As a\nself-imitate approach, ESPD enables a target policy to learn from a series of\nits stochastic variants through the technique of policy distillation (PD). The\nlearning mechanism of ESPD can be considered as an Evolution Strategy (ES) that\napplies perturbations upon policy directly on the action space, with a SELECT\nfunction to check the superiority of stochastic variants and then use PD to\nupdate the policy. The experiments based on the MuJoCo robotics control suite\nshow the high learning efficiency of the proposed method.",
        "Locating region of interest for breast cancer masses in the mammographic\nimage is a challenging problem in medical image processing. In this research\nwork, the keen idea is to efficiently extract suspected mass region for further\nexamination. In particular to this fact breast boundary segmentation on sliced\nrgb image using modified intensity based approach followed by quad tree based\ndivision to spot out suspicious area are proposed in the paper. To evaluate the\nperformance DDSM standard dataset are experimented and achieved acceptable\naccuracy.",
        "Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the\nattention of the computer vision community due to it's real-world applications,\nand the more realistic and challenging setting than found in SBIR. ZS-SBIR\ninherits the main challenges of multiple computer vision problems including\ncontent-based Image Retrieval (CBIR), zero-shot learning and domain adaptation.\nThe majority of previous studies using deep neural networks have achieved\nimproved results through either projecting sketch and images into a common\nlow-dimensional space or transferring knowledge from seen to unseen classes.\nHowever, those approaches are trained with complex frameworks composed of\nmultiple deep convolutional neural networks (CNNs) and are dependent on\ncategory-level word labels. This increases the requirements on training\nresources and datasets. In comparison, we propose a simple and efficient\nframework that does not require high computational training resources, and can\nbe trained on datasets without semantic categorical labels. Furthermore, at\ntraining and inference stages our method only uses a single CNN. In this work,\na pre-trained ImageNet CNN (e.g., ResNet50) is fine-tuned with three proposed\nlearning objects: domain-aware quadruplet loss, semantic classification loss,\nand semantic knowledge preservation loss. The domain-aware quadruplet and\nsemantic classification losses are introduced to learn discriminative, semantic\nand domain invariant features through considering ZS-SBIR as object detection\nand verification problem. ...",
        "Graph neural networks get significant attention for graph representation and\nclassification in machine learning community. Attention mechanism applied on\nthe neighborhood of a node improves the performance of graph neural networks.\nTypically, it helps to identify a neighbor node which plays more important role\nto determine the label of the node under consideration. But in real world\nscenarios, a particular subset of nodes together, but not the individual pairs\nin the subset, may be important to determine the label of the graph. To address\nthis problem, we introduce the concept of subgraph attention for graphs. On the\nother hand, hierarchical graph pooling has been shown to be promising in recent\nliterature. But due to noisy hierarchical structure of real world graphs, not\nall the hierarchies of a graph play equal role for graph classification.\nTowards this end, we propose a graph classification algorithm called\nSubGattPool which jointly learns the subgraph attention and employs two\ndifferent types of hierarchical attention mechanisms to find the important\nnodes in a hierarchy and the importance of individual hierarchies in a graph.\nExperimental evaluation with different types of graph classification algorithms\nshows that SubGattPool is able to improve the state-of-the-art or remains\ncompetitive on multiple publicly available graph classification datasets. We\nconduct further experiments on both synthetic and real world graph datasets to\njustify the usefulness of different components of SubGattPool and to show its\nconsistent performance on other downstream tasks.",
        "Forecasting accuracy is reliant on the quality of available past data. Data\ndisruptions can adversely affect the quality of the generated model (e.g.\nunexpected events such as out-of-stock products when forecasting demand). We\naddress this problem by pastcasting: predicting how data should have been in\nthe past to explain the future better. We propose Pastprop-LSTM, a data-centric\nbackpropagation algorithm that assigns part of the responsibility for errors to\nthe training data and changes it accordingly. We test three variants of\nPastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta\nAnomaly Benchmark. Empirical evaluation indicates that the proposed method can\nimprove forecasting accuracy, especially when the prediction errors of standard\nLSTM are high. It also demonstrates the potential of the algorithm on datasets\ncontaining anomalies.",
        "Humans are capable of learning new tasks without forgetting previous ones,\nwhile neural networks fail due to catastrophic forgetting between new and\npreviously-learned tasks. We consider a class-incremental setting which means\nthat the task-ID is unknown at inference time. The imbalance between old and\nnew classes typically results in a bias of the network towards the newest ones.\nThis imbalance problem can either be addressed by storing exemplars from\nprevious tasks, or by using image replay methods. However, the latter can only\nbe applied to toy datasets since image generation for complex datasets is a\nhard problem.\n  We propose a solution to the imbalance problem based on generative feature\nreplay which does not require any exemplars. To do this, we split the network\ninto two parts: a feature extractor and a classifier. To prevent forgetting, we\ncombine generative feature replay in the classifier with feature distillation\nin the feature extractor. Through feature generation, our method reduces the\ncomplexity of generative replay and prevents the imbalance problem. Our\napproach is computationally efficient and scalable to large datasets.\nExperiments confirm that our approach achieves state-of-the-art results on\nCIFAR-100 and ImageNet, while requiring only a fraction of the storage needed\nfor exemplar-based continual learning. Code available at\n\\url{https://github.com/xialeiliu/GFR-IL}.",
        "Graph Convolutional Networks (GCNs) are powerful models for node\nrepresentation learning tasks. However, the node representation in existing GCN\nmodels is usually generated by performing recursive neighborhood aggregation\nacross multiple graph convolutional layers with certain sampling methods, which\nmay lead to redundant feature mixing, needless information loss, and extensive\ncomputations. Therefore, in this paper, we propose a novel architecture named\nNon-Recursive Graph Convolutional Network (NRGCN) to improve both the training\nefficiency and the learning performance of GCNs in the context of node\nclassification. Specifically, NRGCN proposes to represent different hops of\nneighbors for each node based on inner-layer aggregation and layer-independent\nsampling. In this way, each node can be directly represented by concatenating\nthe information extracted independently from each hop of its neighbors thereby\navoiding the recursive neighborhood expansion across layers. Moreover, the\nlayer-independent sampling and aggregation can be precomputed before the model\ntraining, thus the training process can be accelerated considerably. Extensive\nexperiments on benchmark datasets verify that our NRGCN outperforms the\nstate-of-the-art GCN models, in terms of the node classification performance\nand reliability.",
        "Single-frame infrared small target (SIRST) detection aims at separating small\ntargets from clutter backgrounds. With the advances of deep learning, CNN-based\nmethods have yielded promising results in generic object detection due to their\npowerful modeling capability. However, existing CNN-based methods cannot be\ndirectly applied for infrared small targets since pooling layers in their\nnetworks could lead to the loss of targets in deep layers. To handle this\nproblem, we propose a dense nested attention network (DNANet) in this paper.\nSpecifically, we design a dense nested interactive module (DNIM) to achieve\nprogressive interaction among high-level and low-level features. With the\nrepeated interaction in DNIM, infrared small targets in deep layers can be\nmaintained. Based on DNIM, we further propose a cascaded channel and spatial\nattention module (CSAM) to adaptively enhance multi-level features. With our\nDNANet, contextual information of small targets can be well incorporated and\nfully exploited by repeated fusion and enhancement. Moreover, we develop an\ninfrared small target dataset (namely, NUDT-SIRST) and propose a set of\nevaluation metrics to conduct comprehensive performance evaluation. Experiments\non both public and our self-developed datasets demonstrate the effectiveness of\nour method. Compared to other state-of-the-art methods, our method achieves\nbetter performance in terms of probability of detection (Pd), false-alarm rate\n(Fa), and intersection of union (IoU).",
        "A large labeled dataset is a key to the success of supervised deep learning,\nbut for medical image segmentation, it is highly challenging to obtain\nsufficient annotated images for model training. In many scenarios, unannotated\nimages are abundant and easy to acquire. Self-supervised learning (SSL) has\nshown great potentials in exploiting raw data information and representation\nlearning. In this paper, we propose Hierarchical Self-Supervised Learning\n(HSSL), a new self-supervised framework that boosts medical image segmentation\nby making good use of unannotated data. Unlike the current literature on\ntask-specific self-supervised pretraining followed by supervised fine-tuning,\nwe utilize SSL to learn task-agnostic knowledge from heterogeneous data for\nvarious medical image segmentation tasks. Specifically, we first aggregate a\ndataset from several medical challenges, then pre-train the network in a\nself-supervised manner, and finally fine-tune on labeled data. We develop a new\nloss function by combining contrastive loss and classification loss and\npretrain an encoder-decoder architecture for segmentation tasks. Our extensive\nexperiments show that multi-domain joint pre-training benefits downstream\nsegmentation tasks and outperforms single-domain pre-training significantly.\nCompared to learning from scratch, our new method yields better performance on\nvarious tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated\ndata). With limited amounts of training data, our method can substantially\nbridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of\nannotated data).",
        "Training neural networks with binary weights and activations is a challenging\nproblem due to the lack of gradients and difficulty of optimization over\ndiscrete weights. Many successful experimental results have been achieved with\nempirical straight-through (ST) approaches, proposing a variety of ad-hoc rules\nfor propagating gradients through non-differentiable activations and updating\ndiscrete weights. At the same time, ST methods can be truly derived as\nestimators in the stochastic binary network (SBN) model with Bernoulli weights.\nWe advance these derivations to a more complete and systematic study. We\nanalyze properties, estimation accuracy, obtain different forms of correct ST\nestimators for activations and weights, explain existing empirical approaches\nand their shortcomings, explain how latent weights arise from the mirror\ndescent method when optimizing over probabilities. This allows to reintroduce,\nonce empirical, ST methods as sound approximations, apply them with clarity and\ndevelop further improvements.",
        "Recently, data-driven deep saliency models have achieved high performance and\nhave outperformed classical saliency models, as demonstrated by results on\ndatasets such as the MIT300 and SALICON. Yet, there remains a large gap between\nthe performance of these models and the inter-human baseline. Some outstanding\nquestions include what have these models learned, how and where they fail, and\nhow they can be improved. This article attempts to answer these questions by\nanalyzing the representations learned by individual neurons located at the\nintermediate layers of deep saliency models. To this end, we follow the steps\nof existing deep saliency models, that is borrowing a pre-trained model of\nobject recognition to encode the visual features and learning a decoder to\ninfer the saliency. We consider two cases when the encoder is used as a fixed\nfeature extractor and when it is fine-tuned, and compare the inner\nrepresentations of the network. To study how the learned representations depend\non the task, we fine-tune the same network using the same image set but for two\ndifferent tasks: saliency prediction versus scene classification. Our analyses\nreveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are\nalready encoded within various layers of the network pre-trained for object\nrecognition, 2) using modern datasets, we find that fine-tuning pre-trained\nmodels for saliency prediction makes them favor some categories (e.g. head)\nover some others (e.g. text), 3) although deep models of saliency outperform\nclassical models on natural images, the converse is true for synthetic stimuli\n(e.g. pop-out search arrays), an evidence of significant difference between\nhuman and data-driven saliency models, and 4) we confirm that, after-fine\ntuning, the change in inner-representations is mostly due to the task and not\nthe domain shift in the data.",
        "Deep learning (DL) has gained much attention and become increasingly popular\nin modern data science. Computer scientists led the way in developing deep\nlearning techniques, so the ideas and perspectives can seem alien to\nstatisticians. Nonetheless, it is important that statisticians become involved\n-- many of our students need this expertise for their careers. In this paper,\ndeveloped as part of a program on DL held at the Statistical and Applied\nMathematical Sciences Institute, we address this culture gap and provide tips\non how to teach deep learning to statistics graduate students. After some\nbackground, we list ways in which DL and statistical perspectives differ,\nprovide a recommended syllabus that evolved from teaching two iterations of a\nDL graduate course, offer examples of suggested homework assignments, give an\nannotated list of teaching resources, and discuss DL in the context of two\nresearch areas.",
        "Graph Neural Networks (GNNs) have achieved significant success in learning\nbetter representations by performing feature propagation and transformation\niteratively to leverage neighborhood information. Nevertheless, iterative\npropagation restricts the information of higher-layer neighborhoods to be\ntransported through and fused with the lower-layer neighborhoods', which\nunavoidably results in feature smoothing between neighborhoods in different\nlayers and can thus compromise the performance, especially on heterophily\nnetworks. Furthermore, most deep GNNs only recognize the importance of\nhigher-layer neighborhoods while yet to fully explore the importance of\nmulti-hop dependency within the context of different layer neighborhoods in\nlearning better representations. In this work, we first theoretically analyze\nthe feature smoothing between neighborhoods in different layers and empirically\ndemonstrate the variance of the homophily level across neighborhoods at\ndifferent layers. Motivated by these analyses, we further propose a tree\ndecomposition method to disentangle neighborhoods in different layers to\nalleviate feature smoothing among these layers. Moreover, we characterize the\nmulti-hop dependency via graph diffusion within our tree decomposition\nformulation to construct Tree Decomposed Graph Neural Network (TDGNN), which\ncan flexibly incorporate information from large receptive fields and aggregate\nthis information utilizing the multi-hop dependency. Comprehensive experiments\ndemonstrate the superior performance of TDGNN on both homophily and heterophily\nnetworks under a variety of node classification settings. Extensive parameter\nanalysis highlights the ability of TDGNN to prevent over-smoothing and\nincorporate features from shallow layers with deeper multi-hop dependencies,\nwhich provides new insights towards deeper graph neural networks. Code of\nTDGNN: http://github.com/YuWVandy/TDGNN",
        "Image segmentation refers to the process to divide an image into\nnonoverlapping meaningful regions according to human perception, which has\nbecome a classic topic since the early ages of computer vision. A lot of\nresearch has been conducted and has resulted in many applications. However,\nwhile many segmentation algorithms exist, yet there are only a few sparse and\noutdated summarizations available, an overview of the recent achievements and\nissues is lacking. We aim to provide a comprehensive review of the recent\nprogress in this field. Covering 180 publications, we give an overview of broad\nareas of segmentation topics including not only the classic bottom-up\napproaches, but also the recent development in superpixel, interactive methods,\nobject proposals, semantic image parsing and image cosegmentation. In addition,\nwe also review the existing influential datasets and evaluation metrics.\nFinally, we suggest some design flavors and research directions for future\nresearch in image segmentation.",
        "This paper presents an efficient annotation procedure and an application\nthereof to end-to-end, rich semantic segmentation of the sensed environment\nusing FMCW scanning radar. We advocate radar over the traditional sensors used\nfor this task as it operates at longer ranges and is substantially more robust\nto adverse weather and illumination conditions. We avoid laborious manual\nlabelling by exploiting the largest radar-focused urban autonomy dataset\ncollected to date, correlating radar scans with RGB cameras and LiDAR sensors,\nfor which semantic segmentation is an already consolidated procedure. The\ntraining procedure leverages a state-of-the-art natural image segmentation\nsystem which is publicly available and as such, in contrast to previous\napproaches, allows for the production of copious labels for the radar stream by\nincorporating four camera and two LiDAR streams. Additionally, the losses are\ncomputed taking into account labels to the radar sensor horizon by accumulating\nLiDAR returns along a pose-chain ahead and behind of the current vehicle\nposition. Finally, we present the network with multi-channel radar scan inputs\nin order to deal with ephemeral and dynamic scene objects.",
        "Haze removal is an extremely challenging task, and object detection in the\nhazy environment has recently gained much attention due to the popularity of\nautonomous driving and traffic surveillance. In this work, the authors propose\na multiple linear regression haze removal model based on a widely adopted\ndehazing algorithm named Dark Channel Prior. Training this model with a\nsynthetic hazy dataset, the proposed model can reduce the unanticipated\ndeviations generated from the rough estimations of transmission map and\natmospheric light in Dark Channel Prior. To increase object detection accuracy\nin the hazy environment, the authors further present an algorithm to build a\nsynthetic hazy COCO training dataset by generating the artificial haze to the\nMS COCO training dataset. The experimental results demonstrate that the\nproposed model obtains higher image quality and shares more similarity with\nground truth images than most conventional pixel-based dehazing algorithms and\nneural network based haze-removal models. The authors also evaluate the mean\naverage precision of Mask R-CNN when training the network with synthetic hazy\nCOCO training dataset and preprocessing test hazy dataset by removing the haze\nwith the proposed dehazing model. It turns out that both approaches can\nincrease the object detection accuracy significantly and outperform most\nexisting object detection models over hazy images.",
        "Nanoparticles occur in various environments as a consequence of man-made\nprocesses, which raises concerns about their impact on the environment and\nhuman health. To allow for proper risk assessment, a precise and statistically\nrelevant analysis of particle characteristics (such as e.g. size, shape and\ncomposition) is required that would greatly benefit from automated image\nanalysis procedures. While deep learning shows impressive results in object\ndetection tasks, its applicability is limited by the amount of representative,\nexperimentally collected and manually annotated training data. Here, we present\nan elegant, flexible and versatile method to bypass this costly and tedious\ndata acquisition process. We show that using a rendering software allows to\ngenerate realistic, synthetic training data to train a state-of-the art deep\nneural network. Using this approach, we derive a segmentation accuracy that is\ncomparable to man-made annotations for toxicologically relevant metal-oxide\nnanoparticle ensembles which we chose as examples. Our study paves the way\ntowards the use of deep learning for automated, high-throughput particle\ndetection in a variety of imaging techniques such as microscopies and\nspectroscopies, for a wide variety of studies and applications, including the\ndetection of plastic micro- and nanoparticles.",
        "We introduce the active audio-visual source separation problem, where an\nagent must move intelligently in order to better isolate the sounds coming from\nan object of interest in its environment. The agent hears multiple audio\nsources simultaneously (e.g., a person speaking down the hall in a noisy\nhousehold) and it must use its eyes and ears to automatically separate out the\nsounds originating from a target object within a limited time budget. Towards\nthis goal, we introduce a reinforcement learning approach that trains movement\npolicies controlling the agent's camera and microphone placement over time,\nguided by the improvement in predicted audio separation quality. We demonstrate\nour approach in scenarios motivated by both augmented reality (system is\nalready co-located with the target object) and mobile robotics (agent begins\narbitrarily far from the target object). Using state-of-the-art realistic\naudio-visual simulations in 3D environments, we demonstrate our model's ability\nto find minimal movement sequences with maximal payoff for audio source\nseparation. Project: http://vision.cs.utexas.edu/projects/move2hear.",
        "In image-based camera localization systems, information about the environment\nis usually stored in some representation, which can be referred to as a map.\nConventionally, most maps are built upon hand-crafted features. Recently,\nneural networks have attracted attention as a data-driven map representation,\nand have shown promising results in visual localization. However, these neural\nnetwork maps are generally hard to interpret by human. A readable map is not\nonly accessible to humans, but also provides a way to be verified when the\nground truth pose is unavailable. To tackle this problem, we propose Generative\nMap, a new framework for learning human-readable neural network maps, by\ncombining a generative model with the Kalman filter, which also allows it to\nincorporate additional sensor information such as stereo visual odometry. For\nevaluation, we use real world images from the 7-Scenes and Oxford RobotCar\ndatasets. We demonstrate that our Generative Map can be queried with a pose of\ninterest from the test sequence to predict an image, which closely resembles\nthe true scene. For localization, we show that Generative Map achieves\ncomparable performance with current regression models. Moreover, our framework\nis trained completely from scratch, unlike regression models which rely on\nlarge ImageNet pretrained networks.",
        "Multi-Task Learning (MTL) networks have emerged as a promising method for\ntransferring learned knowledge across different tasks. However, MTL must deal\nwith challenges such as: overfitting to low resource tasks, catastrophic\nforgetting, and negative task transfer, or learning interference. Often, in\nNatural Language Processing (NLP), a separate model per task is needed to\nobtain the best performance. However, many fine-tuning approaches are both\nparameter inefficient, i.e., potentially involving one new model per task, and\nhighly susceptible to losing knowledge acquired during pretraining. We propose\na novel Transformer architecture consisting of a new conditional attention\nmechanism as well as a set of task-conditioned modules that facilitate weight\nsharing. Through this construction, we achieve more efficient parameter sharing\nand mitigate forgetting by keeping half of the weights of a pretrained model\nfixed. We also use a new multi-task data sampling strategy to mitigate the\nnegative effects of data imbalance across tasks. Using this approach, we are\nable to surpass single task fine-tuning methods while being parameter and data\nefficient (using around 66% of the data for weight updates). Compared to other\nBERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by\n2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and\nsingle task fine-tuning. We show that a larger variant of our single multi-task\nmodel approach performs competitively across 26 NLP tasks and yields\nstate-of-the-art results on a number of test and development sets. Our code is\npublicly available at https://github.com/CAMTL/CA-MTL.",
        "Image colorization adds color to grayscale images. It not only increases the\nvisual appeal of grayscale images, but also enriches the information contained\nin scientific images that lack color information. Most existing methods of\ncolorization require laborious user interaction for scribbles or image\nsegmentation. To eliminate the need for human labor, we develop an automatic\nimage colorization method using epitome. Built upon a generative graphical\nmodel, epitome is a condensed image appearance and shape model which also\nproves to be an effective summary of color information for the colorization\ntask. We train the epitome from the reference images and perform inference in\nthe epitome to colorize grayscale images, rendering better colorization results\nthan previous method in our experiments.",
        "Recent work has studied the emergence of language among deep reinforcement\nlearning agents that must collaborate to solve a task. Of particular interest\nare the factors that cause language to be compositional -- i.e., express\nmeaning by combining words which themselves have meaning. Evolutionary\nlinguists have found that in addition to structural priors like those already\nstudied in deep learning, the dynamics of transmitting language from generation\nto generation contribute significantly to the emergence of compositionality. In\nthis paper, we introduce these cultural evolutionary dynamics into language\nemergence by periodically replacing agents in a population to create a\nknowledge gap, implicitly inducing cultural transmission of language. We show\nthat this implicit cultural transmission encourages the resulting languages to\nexhibit better compositional generalization.",
        "In machine learning and data mining, Cluster analysis is one of the most\nwidely used unsupervised learning technique. Philosophy of this algorithm is to\nfind similar data items and group them together based on any distance function\nin multidimensional space. These methods are suitable for finding groups of\ndata that behave in a coherent fashion. The perspective may vary for clustering\ni.e. the way we want to find similarity, some methods are based on distance\nsuch as K-Means technique and some are probability based, like GMM.\nUnderstanding prominent segment of data is always challenging as multidimension\nspace does not allow us to have a look and feel of the distance or any visual\ncontext on the health of the clustering.\n  While explaining data using clusters, the major problem is to tell how many\ncluster are good enough to explain the data. Generally basic descriptive\nstatistics are used to estimate cluster behaviour like scree plot, dendrogram\netc. We propose a novel method to understand the cluster behaviour which can be\nused not only to find right number of clusters but can also be used to access\nthe difference of health between different clustering methods on same data. Our\ntechnique would also help to also eliminate the noisy variables and optimize\nthe clustering result.\n  keywords - Clustering, Metric, K-means, hierarchical clustering, silhoutte,\nclustering index, measures",
        "Occluded person re-identification is a challenging task as the appearance\nvaries substantially with various obstacles, especially in the crowd scenario.\nTo address this issue, we propose a Pose-guided Visible Part Matching (PVPM)\nmethod that jointly learns the discriminative features with pose-guided\nattention and self-mines the part visibility in an end-to-end framework.\nSpecifically, the proposed PVPM includes two key components: 1) pose-guided\nattention (PGA) method for part feature pooling that exploits more\ndiscriminative local features; 2) pose-guided visibility predictor (PVP) that\nestimates whether a part suffers the occlusion or not. As there are no ground\ntruth training annotations for the occluded part, we turn to utilize the\ncharacteristic of part correspondence in positive pairs and self-mining the\ncorrespondence scores via graph matching. The generated correspondence scores\nare then utilized as pseudo-labels for visibility predictor (PVP). Experimental\nresults on three reported occluded benchmarks show that the proposed method\nachieves competitive performance to state-of-the-art methods. The source codes\nare available at https://github.com/hh23333/PVPM",
        "Image captioning is a challenging computer vision task, which aims to\ngenerate a natural language description of an image. Most recent researches\nfollow the encoder-decoder framework which depends heavily on the previous\ngenerated words for the current prediction. Such methods can not effectively\ntake advantage of the future predicted information to learn complete semantics.\nIn this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism\nthat can guide the captioning model to perceive global contexts. Upon the\ncaptioning model, CAAG performs semantic attention that selectively\nconcentrates on useful information of the global predictions to reproduce the\ncurrent generation. To validate the adaptability of the method, we apply CAAG\nto three popular captioners and our proposal achieves competitive performance\non the challenging Microsoft COCO image captioning benchmark, e.g. 132.2\nCIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official\nonline evaluation server.",
        "Many real-world applications involve data from multiple modalities and thus\nexhibit the view heterogeneity. For example, user modeling on social media\nmight leverage both the topology of the underlying social network and the\ncontent of the users' posts; in the medical domain, multiple views could be\nX-ray images taken at different poses. To date, various techniques have been\nproposed to achieve promising results, such as canonical correlation analysis\nbased methods, etc. In the meanwhile, it is critical for decision-makers to be\nable to understand the prediction results from these methods. For example,\ngiven the diagnostic result that a model provided based on the X-ray images of\na patient at different poses, the doctor needs to know why the model made such\na prediction. However, state-of-the-art techniques usually suffer from the\ninability to utilize the complementary information of each view and to explain\nthe predictions in an interpretable manner.\n  To address these issues, in this paper, we propose a deep co-attention\nnetwork for multi-view subspace learning, which aims to extract both the common\ninformation and the complementary information in an adversarial setting and\nprovide robust interpretations behind the prediction to the end-users via the\nco-attention mechanism. In particular, it uses a novel cross reconstruction\nloss and leverages the label information to guide the construction of the\nlatent representation by incorporating the classifier into our model. This\nimproves the quality of latent representation and accelerates the convergence\nspeed. Finally, we develop an efficient iterative algorithm to find the optimal\nencoders and discriminator, which are evaluated extensively on synthetic and\nreal-world data sets. We also conduct a case study to demonstrate how the\nproposed method robustly interprets the predictions on an image data set.",
        "With the rapidly increasing interest in machine learning based solutions for\nautomatic image annotation, the availability of reference annotations for\nalgorithm training is one of the major bottlenecks in the field. Crowdsourcing\nhas evolved as a valuable option for low-cost and large-scale data annotation;\nhowever, quality control remains a major issue which needs to be addressed. To\nour knowledge, we are the first to analyze the annotation process to improve\ncrowd-sourced image segmentation. Our method involves training a regressor to\nestimate the quality of a segmentation from the annotator's clickstream data.\nThe quality estimation can be used to identify spam and weight individual\nannotations by their (estimated) quality when merging multiple segmentations of\none image. Using a total of 29,000 crowd annotations performed on publicly\navailable data of different object classes, we show that (1) our method is\nhighly accurate in estimating the segmentation quality based on clickstream\ndata, (2) outperforms state-of-the-art methods for merging multiple\nannotations. As the regressor does not need to be trained on the object class\nthat it is applied to it can be regarded as a low-cost option for quality\ncontrol and confidence analysis in the context of crowd-based image annotation.",
        "In this work we consider the multi-image object matching problem, extend a\ncentralized solution of the problem to a distributed solution, and present an\nexperimental application of the centralized solution. Multi-image feature\nmatching is a keystone of many applications, including simultaneous\nlocalization and mapping, homography, object detection, and structure from\nmotion. We first review the QuickMatch algorithm for multi-image feature\nmatching. We then present a scheme for distributing sets of features across\ncomputational units (agents) that largely preserves feature match quality and\nminimizes communication between agents (avoiding, in particular, the need of\nflooding all data to all agents). Finally, we show how QuickMatch performs on\nan object matching test with low quality images. The centralized QuickMatch\nalgorithm is compared to other standard matching algorithms, while the\nDistributed QuickMatch algorithm is compared to the centralized algorithm in\nterms of preservation of match consistency. The presented experiment shows that\nQuickMatch matches features across a large number of images and features in\nlarger numbers and more accurately than standard techniques.",
        "Disentangled distributed representations of data are desirable for machine\nlearning, since they are more expressive and can generalize from fewer\nexamples. However, for complex data, the distributed representations of\nmultiple objects present in the same input can interfere and lead to\nambiguities, which is commonly referred to as the binding problem. We argue for\nthe importance of the binding problem to the field of representation learning,\nand develop a probabilistic framework that explicitly models inputs as a\ncomposition of multiple objects. We propose an unsupervised algorithm that uses\ndenoising autoencoders to dynamically bind features together in multi-object\ninputs through an Expectation-Maximization-like clustering process. The\neffectiveness of this method is demonstrated on artificially generated datasets\nof binary images, showing that it can even generalize to bind together new\nobjects never seen by the autoencoder during training.",
        "Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular\ndata, which produce state of the art results in many prediction tasks. Despite\nits popularity, the GBM framework suffers from a fundamental flaw in its base\nlearners. Specifically, most implementations utilize decision trees that are\ntypically biased towards categorical variables with large cardinalities. The\neffect of this bias was extensively studied over the years, mostly in terms of\npredictive performance. In this work, we extend the scope and study the effect\nof biased base learners on GBM feature importance (FI) measures. We show that\nalthough these implementation demonstrate highly competitive predictive\nperformance, they still, surprisingly, suffer from bias in FI. By utilizing\ncross-validated (CV) unbiased base learners, we fix this flaw at a relatively\nlow computational cost. We demonstrate the suggested framework in a variety of\nsynthetic and real-world setups, showing a significant improvement in all GBM\nFI measures while maintaining relatively the same level of prediction accuracy.",
        "Searching for objects in indoor organized environments such as homes or\noffices is part of our everyday activities. When looking for a target object,\nwe jointly reason about the rooms and containers the object is likely to be in;\nthe same type of container will have a different probability of having the\ntarget depending on the room it is in. We also combine geometric and semantic\ninformation to infer what container is best to search, or what other objects\nare best to move, if the target object is hidden from view. We propose to use a\n3D scene graph representation to capture the hierarchical, semantic, and\ngeometric aspects of this problem. To exploit this representation in a search\nprocess, we introduce Hierarchical Mechanical Search (HMS), a method that\nguides an agent's actions towards finding a target object specified with a\nnatural language description. HMS is based on a novel neural network\narchitecture that uses neural message passing of vectors with visual,\ngeometric, and linguistic information to allow HMS to reason across layers of\nthe graph while combining semantic and geometric cues. HMS is evaluated on a\nnovel dataset of 500 3D scene graphs with dense placements of semantically\nrelated objects in storage locations, and is shown to be significantly better\nthan several baselines at finding objects and close to the oracle policy in\nterms of the median number of actions required. Additional qualitative results\ncan be found at https://ai.stanford.edu/mech-search/hms.",
        "In this work we propose R-GPM, a parallel computing framework for graph\npattern mining (GPM) through a user-defined subgraph relation. More\nspecifically, we enable the computation of statistics of patterns through their\nsubgraph classes, generalizing traditional GPM methods. R-GPM provides\nefficient estimators for these statistics by employing a MCMC sampling\nalgorithm combined with several optimizations. We provide both theoretical\nguarantees and empirical evaluations of our estimators in application scenarios\nsuch as stochastic optimization of deep high-order graph neural network models\nand pattern (motif) counting. We also propose and evaluate optimizations that\nenable improvements of our estimators accuracy, while reducing their\ncomputational costs in up to 3-orders-of-magnitude. Finally,we show that R-GPM\nis scalable, providing near-linear speedups on 44 cores in all of our tests.",
        "We propose a light-weight video frame interpolation algorithm. Our key\ninnovation is an instance-level supervision that allows information to be\nlearned from the high-resolution version of similar objects. Our experiment\nshows that the proposed method can generate state-of-the-art results across\ndifferent datasets, with fractional computation resources (time and memory) of\ncompeting methods. Given two image frames, a cascade network creates an\nintermediate frame with 1) a flow-warping module that computes coarse\nbi-directional optical flow and creates an interpolated image via flow-based\nwarping, followed by 2) an image synthesis module to make fine-scale\ncorrections. In the learning stage, object detection proposals are generated on\nthe interpolated image.Lower resolution objects are zoomed into, and the\nlearning algorithms using an adversarial loss trained on high-resolution\nobjects to guide the system towards the instance-level refinement corrects\ndetails of object shape and boundaries.",
        "As one of the most well-known artificial feature sampler, the sliding window\nis widely used in scenarios where spatial and temporal information exists, such\nas computer vision, natural language process, data stream, and time series.\nAmong which time series is common in many scenarios like credit card payment,\nuser behavior, and sensors. General feature selection for features extracted by\nsliding window aggregate calls for time-consuming iteration to generate\nfeatures, and then traditional feature selection methods are employed to rank\nthem. The decision of key parameter, i.e. the period of sliding windows,\ndepends on the domain knowledge and calls for trivial. Currently, there is no\nautomatic method to handle the sliding window aggregate features selection. As\nthe time consumption of feature generation with different periods and sliding\nwindows is huge, it is very hard to enumerate them all and then select them.\n  In this paper, we propose a general framework using Markov Chain to solve\nthis problem. This framework is very efficient and has high accuracy, such that\nit is able to perform feature selection on a variety of features and period\noptions. We show the detail by 2 common sliding windows and 3 types of\naggregation operators. And it is easy to extend more sliding windows and\naggregation operators in this framework by employing existing theory about\nMarkov Chain.",
        "Graph neural networks have shown superior performance in a wide range of\napplications providing a powerful representation of graph-structured data.\nRecent works show that the representation can be further improved by auxiliary\ntasks. However, the auxiliary tasks for heterogeneous graphs, which contain\nrich semantic information with various types of nodes and edges, have less\nexplored in the literature. In this paper, to learn graph neural networks on\nheterogeneous graphs we propose a novel self-supervised auxiliary learning\nmethod using meta-paths, which are composite relations of multiple edge types.\nOur proposed method is learning to learn a primary task by predicting\nmeta-paths as auxiliary tasks. This can be viewed as a type of meta-learning.\nThe proposed method can identify an effective combination of auxiliary tasks\nand automatically balance them to improve the primary task. Our methods can be\napplied to any graph neural networks in a plug-in manner without manual\nlabeling or additional data. The experiments demonstrate that the proposed\nmethod consistently improves the performance of link prediction and node\nclassification on heterogeneous graphs.",
        "Graph convolutional neural networks (GCNs) embed nodes in a graph into\nEuclidean space, which has been shown to incur a large distortion when\nembedding real-world graphs with scale-free or hierarchical structure.\nHyperbolic geometry offers an exciting alternative, as it enables embeddings\nwith much smaller distortion. However, extending GCNs to hyperbolic geometry\npresents several unique challenges because it is not clear how to define neural\nnetwork operations, such as feature transformation and aggregation, in\nhyperbolic space. Furthermore, since input features are often Euclidean, it is\nunclear how to transform the features into hyperbolic embeddings with the right\namount of curvature. Here we propose Hyperbolic Graph Convolutional Neural\nNetwork (HGCN), the first inductive hyperbolic GCN that leverages both the\nexpressiveness of GCNs and hyperbolic geometry to learn inductive node\nrepresentations for hierarchical and scale-free graphs. We derive GCN\noperations in the hyperboloid model of hyperbolic space and map Euclidean input\nfeatures to embeddings in hyperbolic spaces with different trainable curvature\nat each layer. Experiments demonstrate that HGCN learns embeddings that\npreserve hierarchical structure, and leads to improved performance when\ncompared to Euclidean analogs, even with very low dimensional embeddings:\ncompared to state-of-the-art GCNs, HGCN achieves an error reduction of up to\n63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node\nclassification, also improving state-of-the art on the Pubmed dataset.",
        "Multi-hop knowledge based question answering (KBQA) is a complex task for\nnatural language understanding. Many KBQA approaches have been proposed in\nrecent years, and most of them are trained based on labeled reasoning path.\nThis hinders the system's performance as many correct reasoning paths are not\nlabeled as ground truth, and thus they cannot be learned. In this paper, we\nintroduce an end-to-end KBQA system which can leverage multiple reasoning\npaths' information and only requires labeled answer as supervision. We conduct\nexperiments on several benchmark datasets containing both single-hop simple\nquestions as well as muti-hop complex questions, including WebQuestionSP\n(WQSP), ComplexWebQuestion-1.1 (CWQ), and PathQuestion-Large (PQL), and\ndemonstrate strong performance.",
        "We propose a method to classify cardiac pathology based on a novel approach\nto extract image derived features to characterize the shape and motion of the\nheart. An original semi-supervised learning procedure, which makes efficient\nuse of a large amount of non-segmented images and a small amount of images\nsegmented manually by experts, is developed to generate pixel-wise apparent\nflow between two time points of a 2D+t cine MRI image sequence. Combining the\napparent flow maps and cardiac segmentation masks, we obtain a local apparent\nflow corresponding to the 2D motion of myocardium and ventricular cavities.\nThis leads to the generation of time series of the radius and thickness of\nmyocardial segments to represent cardiac motion. These time series of motion\nfeatures are reliable and explainable characteristics of pathological cardiac\nmotion. Furthermore, they are combined with shape-related features to classify\ncardiac pathologies. Using only nine feature values as input, we propose an\nexplainable, simple and flexible model for pathology classification. On ACDC\ntraining set and testing set, the model achieves 95% and 94% respectively as\nclassification accuracy. Its performance is hence comparable to that of the\nstate-of-the-art. Comparison with various other models is performed to outline\nsome advantages of our model.",
        "Deep convolutional neural networks (CNNs) based approaches are the\nstate-of-the-art in various computer vision tasks, including face recognition.\nConsiderable research effort is currently being directed towards further\nimproving deep CNNs by focusing on more powerful model architectures and better\nlearning techniques. However, studies systematically exploring the strengths\nand weaknesses of existing deep models for face recognition are still\nrelatively scarce in the literature. In this paper, we try to fill this gap and\nstudy the effects of different covariates on the verification performance of\nfour recent deep CNN models using the Labeled Faces in the Wild (LFW) dataset.\nSpecifically, we investigate the influence of covariates related to: image\nquality -- blur, JPEG compression, occlusion, noise, image brightness,\ncontrast, missing pixels; and model characteristics -- CNN architecture, color\ninformation, descriptor computation; and analyze their impact on the face\nverification performance of AlexNet, VGG-Face, GoogLeNet, and SqueezeNet. Based\non comprehensive and rigorous experimentation, we identify the strengths and\nweaknesses of the deep learning models, and present key areas for potential\nfuture research. Our results indicate that high levels of noise, blur, missing\npixels, and brightness have a detrimental effect on the verification\nperformance of all models, whereas the impact of contrast changes and\ncompression artifacts is limited. It has been found that the descriptor\ncomputation strategy and color information does not have a significant\ninfluence on performance.",
        "Being able to reach any desired location in the environment can be a valuable\nasset for an agent. Learning a policy to navigate between all pairs of states\nindividually is often not feasible. An all-goals updating algorithm uses each\ntransition to learn Q-values towards all goals simultaneously and off-policy.\nHowever the expensive numerous updates in parallel limited the approach to\nsmall tabular cases so far. To tackle this problem we propose to use\nconvolutional network architectures to generate Q-values and updates for a\nlarge number of goals at once. We demonstrate the accuracy and generalization\nqualities of the proposed method on randomly generated mazes and Sokoban\npuzzles. In the case of on-screen goal coordinates the resulting mapping from\nframes to distance-maps directly informs the agent about which places are\nreachable and in how many steps. As an example of application we show that\nreplacing the random actions in epsilon-greedy exploration by several actions\ntowards feasible goals generates better exploratory trajectories on Montezuma's\nRevenge and Super Mario All-Stars games.",
        "We utilize the dynamics involved in the imaging of a fingerprint on a\ntouch-based fingerprint reader, such as perspiration, changes in skin color\n(blanching), and skin distortion, to differentiate real fingers from spoof\n(fake) fingers. Specifically, we utilize a deep learning-based architecture\n(CNN-LSTM) trained end-to-end using sequences of minutiae-centered local\npatches extracted from ten color frames captured on a COTS fingerprint reader.\nA time-distributed CNN (MobileNet-v1) extracts spatial features from each local\npatch, while a bi-directional LSTM layer learns the temporal relationship\nbetween the patches in the sequence. Experimental results on a database of\n26,650 live frames from 685 subjects (1,333 unique fingers), and 32,910 spoof\nframes of 7 spoof materials (with 14 variants) shows the superiority of the\nproposed approach in both known-material and cross-material (generalization)\nscenarios. For instance, the proposed approach improves the state-of-the-art\ncross-material performance from TDR of 81.65% to 86.20% @ FDR = 0.2%.",
        "In the context of HCI, building an automatic system to recognize affect of\nhuman facial expression in real-world condition is very crucial to make machine\ninteract naturallisticaly with a man. However, existing facial emotion\ndatabases usually contain expression in the limited scenario under\nwell-controlled condition. Aff-Wild is currently the largest database\nconsisting of spontaneous facial expression in the wild annotated with valence\nand arousal. The first contribution of this project is the completion of\nextending Aff-Wild database which is fulfilled by collecting videos from\nYouTube on which the videos have spontaneous facial expressions in the wild,\nannotating videos with valence and arousal ranging in [-1,1], detecting faces\nin frames using FFLD2 detector and partitioning the whole data set into train,\nvalidate and test set, with 527056, 94223 and 135145 frames. The diversity is\nguaranteed regarding age, ethnicity and values of valence and arousal. The\nratio of male to female is close to 1. Regarding the techniques used to build\nthe automatic system, deep learning is outstanding since almost all winning\nmethods in emotion challenges adopt DNN techniques. The second contribution of\nthis project is that an end-to-end DNN is constructed to have joint CNN and RNN\nblock and gives the estimation on valence and arousal for each frame in\nsequential data. VGGFace, ResNet, DenseNet with the corresponding pre-trained\nmodel for CNN block and LSTM, GRU, IndRNN, Attention mechanism for RNN block\nare experimented aiming to find the best combination. Fine tuning and transfer\nlearning techniques are also tried out. By comparing the CCC evaluation value\non test data, the best model is found to be pre-trained VGGFace connected with\n2 layers GRU with attention mechanism. The models test performance is 0.555 CCC\nfor valence with sequence length 80 and 0.499 CCC for arousal with sequence\nlength 70.",
        "Decision trees and their ensembles are endowed with a rich set of diagnostic\ntools for ranking and screening variables in a predictive model. Despite the\nwidespread use of tree based variable importance measures, pinning down their\ntheoretical properties has been challenging and therefore largely unexplored.\nTo address this gap between theory and practice, we derive finite sample\nperformance guarantees for variable selection in nonparametric models using a\nsingle-level CART decision tree (a decision stump). Under standard operating\nassumptions in variable screening literature, we find that the marginal signal\nstrength of each variable and ambient dimensionality can be considerably weaker\nand higher, respectively, than state-of-the-art nonparametric variable\nselection methods. Furthermore, unlike previous marginal screening methods that\nattempt to directly estimate each marginal projection via a truncated basis\nexpansion, the fitted model used here is a simple, parsimonious decision stump,\nthereby eliminating the need for tuning the number of basis terms. Thus,\nsurprisingly, even though decision stumps are highly inaccurate for estimation\npurposes, they can still be used to perform consistent model selection.",
        "Mesh representation by random walks has been shown to benefit deep learning.\nRandomness is indeed a powerful concept. However, it comes with a price: some\nwalks might wander around non-characteristic regions of the mesh, which might\nbe harmful to shape analysis, especially when only a few walks are utilized. We\npropose a novel walk-attention mechanism that leverages the fact that multiple\nwalks are used. The key idea is that the walks may provide each other with\ninformation regarding the meaningful (attentive) features of the mesh. We\nutilize this mutual information to extract a single descriptor of the mesh.\nThis differs from common attention mechanisms that use attention to improve the\nrepresentation of each individual descriptor. Our approach achieves SOTA\nresults for two basic 3D shape analysis tasks: classification and retrieval.\nEven a handful of walks along a mesh suffice for learning.",
        "Analysis of over-parameterized neural networks has drawn significant\nattention in recentyears. It was shown that such systems behave like convex\nsystems under various restrictedsettings, such as for two-level neural\nnetworks, and when learning is only restricted locally inthe so-called neural\ntangent kernel space around specialized initializations. However, there areno\ntheoretical techniques that can analyze fully trained deep neural networks\nencountered inpractice. This paper solves this fundamental problem by\ninvestigating such overparameterizeddeep neural networks when fully trained. We\ngeneralize a new technique called neural feature repopulation, originally\nintroduced in (Fang et al., 2019a) for two-level neural networks, to analyze\ndeep neural networks. It is shown that under suitable representations,\noverparameterized deep neural networks are inherently convex, and when\noptimized, the system can learn effective features suitable for the underlying\nlearning task under mild conditions. This new analysis is consistent with\nempirical observations that deep neural networks are capable of learning\nefficient feature representations. Therefore, the highly unexpected result of\nthis paper can satisfactorily explain the practical success of deep neural\nnetworks. Empirical studies confirm that predictions of our theory are\nconsistent with results observed in practice.",
        "We present a weakly supervised model that jointly performs both semantic- and\ninstance-segmentation -- a particularly relevant problem given the substantial\ncost of obtaining pixel-perfect annotation for these tasks. In contrast to many\npopular instance segmentation approaches based on object detectors, our method\ndoes not predict any overlapping instances. Moreover, we are able to segment\nboth \"thing\" and \"stuff\" classes, and thus explain all the pixels in the image.\n\"Thing\" classes are weakly-supervised with bounding boxes, and \"stuff\" with\nimage-level tags. We obtain state-of-the-art results on Pascal VOC, for both\nfull and weak supervision (which achieves about 95% of fully-supervised\nperformance). Furthermore, we present the first weakly-supervised results on\nCityscapes for both semantic- and instance-segmentation. Finally, we use our\nweakly supervised framework to analyse the relationship between annotation\nquality and predictive performance, which is of interest to dataset creators.",
        "In this paper, we present a conditional generative adversarial network-based\nmodel for real-time underwater image enhancement. To supervise the adversarial\ntraining, we formulate an objective function that evaluates the perceptual\nimage quality based on its global content, color, local texture, and style\ninformation. We also present EUVP, a large-scale dataset of a paired and\nunpaired collection of underwater images (of `poor' and `good' quality) that\nare captured using seven different cameras over various visibility conditions\nduring oceanic explorations and human-robot collaborative experiments. In\naddition, we perform several qualitative and quantitative evaluations which\nsuggest that the proposed model can learn to enhance underwater image quality\nfrom both paired and unpaired training. More importantly, the enhanced images\nprovide improved performances of standard models for underwater object\ndetection, human pose estimation, and saliency prediction. These results\nvalidate that it is suitable for real-time preprocessing in the autonomy\npipeline by visually-guided underwater robots. The model and associated\ntraining pipelines are available at https://github.com/xahidbuffon/funie-gan.",
        "In this work, we compare the detection accuracy and speed of several\nstate-of-the-art models for the task of detecting oil and gas fracking wells\nand small cars in commercial electro-optical satellite imagery. Several models\nare studied from the single-stage, two-stage, and multi-stage object detection\nfamilies of techniques. For the detection of fracking well pads (50m - 250m),\nwe find single-stage detectors provide superior prediction speed while also\nmatching detection performance of their two and multi-stage counterparts.\nHowever, for detecting small cars, two-stage and multi-stage models provide\nsubstantially higher accuracies at the cost of some speed. We also measure\ntiming results of the sliding window object detection algorithm to provide a\nbaseline for comparison. Some of these models have been incorporated into the\nLockheed Martin Globally-Scalable Automated Target Recognition (GATR)\nframework.",
        "Several researches and evidence show the increasing likelihood of pandemics\n(large-scale outbreaks of infectious disease) which has far reaching sequels in\nall aspects of human lives ranging from rapid mortality rates to economic and\nsocial disruption across the world. In the recent time, COVID-19 (Coronavirus\nDisease 2019) pandemic disrupted normal human lives, and motivated by the\nurgent need of combating COVID-19, researchers have put significant efforts in\nmodelling and analysing the disease spread patterns for effective preventive\nmeasures (in addition to developing pharmaceutical solutions, like vaccine). In\nthis regards, it is absolutely necessary to develop an analytics framework by\nextracting and incorporating the knowledge of heterogeneous datasources to\ndeliver insights in improving administrative policy and enhance the\npreparedness to combat the pandemic. Specifically, human mobility, travel\nhistory and other transport statistics have significant impacts on the spread\nof any infectious disease. In this direction, this paper proposes a\nspatio-temporal knowledge mining framework, named STOPPAGE to model the impact\nof human mobility and other contextual information over large geographic area\nin different temporal scales. The framework has two major modules: (i)\nSpatio-temporal data and computing infrastructure using fog/edge based\narchitecture; and (ii) Spatio-temporal data analytics module to efficiently\nextract knowledge from heterogeneous data sources. Typically, we develop a\nPandemic-knowledge graph to discover correlations among mobility information\nand disease spread, a deep learning architecture to predict the next hot-spot\nzones; and provide necessary support in home-health monitoring utilizing\nFemtolet and fog/edge based solutions. The experimental evaluations on\nreal-life datasets related to COVID-19 in India illustrate the efficacy of the\nproposed methods.",
        "While data sharing is crucial for knowledge development, privacy concerns and\nstrict regulation (e.g., European General Data Protection Regulation (GDPR))\nunfortunately limit its full effectiveness. Synthetic tabular data emerges as\nan alternative to enable data sharing while fulfilling regulatory and privacy\nconstraints. The state-of-the-art tabular data synthesizers draw methodologies\nfrom generative Adversarial Networks (GAN) and address two main data types in\nthe industry, i.e., continuous and categorical. In this paper, we develop\nCTAB-GAN, a novel conditional table GAN architecture that can effectively model\ndiverse data types, including a mix of continuous and categorical variables.\nMoreover, we address data imbalance and long-tail issues, i.e., certain\nvariables have drastic frequency differences across large values. To achieve\nthose aims, we first introduce the information loss and classification loss to\nthe conditional GAN. Secondly, we design a novel conditional vector, which\nefficiently encodes the mixed data type and skewed distribution of data\nvariable. We extensively evaluate CTAB-GAN with the state of the art GANs that\ngenerate synthetic tables, in terms of data similarity and analysis utility.\nThe results on five datasets show that the synthetic data of CTAB-GAN\nremarkably resembles the real data for all three types of variables and results\ninto higher accuracy for five machine learning algorithms, by up to 17%.",
        "Deep neural nets achieve state-of-the-art performance on the problem of\noptical flow estimation. Since optical flow is used in several safety-critical\napplications like self-driving cars, it is important to gain insights into the\nrobustness of those techniques. Recently, it has been shown that adversarial\nattacks easily fool deep neural networks to misclassify objects. The robustness\nof optical flow networks to adversarial attacks, however, has not been studied\nso far. In this paper, we extend adversarial patch attacks to optical flow\nnetworks and show that such attacks can compromise their performance. We show\nthat corrupting a small patch of less than 1% of the image size can\nsignificantly affect optical flow estimates. Our attacks lead to noisy flow\nestimates that extend significantly beyond the region of the attack, in many\ncases even completely erasing the motion of objects in the scene. While\nnetworks using an encoder-decoder architecture are very sensitive to these\nattacks, we found that networks using a spatial pyramid architecture are less\naffected. We analyse the success and failure of attacking both architectures by\nvisualizing their feature maps and comparing them to classical optical flow\ntechniques which are robust to these attacks. We also demonstrate that such\nattacks are practical by placing a printed pattern into real scenes.",
        "Several classification methods assume that the underlying distributions\nfollow tree-structured graphical models. Indeed, trees capture statistical\ndependencies between pairs of variables, which may be crucial to attain low\nclassification errors. The resulting classifier is linear in the\nlog-transformed univariate and bivariate densities that correspond to the tree\nedges. In practice, however, observed data may not be well approximated by\ntrees. Yet, motivated by the importance of pairwise dependencies for accurate\nclassification, here we propose to approximate the optimal decision boundary by\na sparse linear combination of the univariate and bivariate log-transformed\ndensities. Our proposed approach is semi-parametric in nature: we\nnon-parametrically estimate the univariate and bivariate densities, remove\npairs of variables that are nearly independent using the Hilbert-Schmidt\nindependence criteria, and finally construct a linear SVM on the retained\nlog-transformed densities. We demonstrate using both synthetic and real data\nthat our resulting classifier, denoted SLB (Sparse Log-Bivariate density), is\ncompetitive with popular classification methods.",
        "Key features of mental illnesses are reflected in speech. Our research\nfocuses on designing a multimodal deep learning structure that automatically\nextracts salient features from recorded speech samples for predicting various\nmental disorders including depression, bipolar, and schizophrenia. We adopt a\nvariety of pre-trained models to extract embeddings from both audio and text\nsegments. We use several state-of-the-art embedding techniques including BERT,\nFastText, and Doc2VecC for the text representation learning and WaveNet and\nVGG-ish models for audio encoding. We also leverage huge auxiliary\nemotion-labeled text and audio corpora to train emotion-specific embeddings and\nuse transfer learning in order to address the problem of insufficient annotated\nmultimodal data available. All these embeddings are then combined into a joint\nrepresentation in a multimodal fusion layer and finally a recurrent neural\nnetwork is used to predict the mental disorder. Our results show that mental\ndisorders can be predicted with acceptable accuracy through multimodal analysis\nof clinical interviews.",
        "We propose a new segmentation model combining common regularization energies,\ne.g. Markov Random Field (MRF) potentials, and standard pairwise clustering\ncriteria like Normalized Cut (NC), average association (AA), etc. These\nclustering and regularization models are widely used in machine learning and\ncomputer vision, but they were not combined before due to significant\ndifferences in the corresponding optimization, e.g. spectral relaxation and\ncombinatorial max-flow techniques. On the one hand, we show that many common\napplications using MRF segmentation energies can benefit from a high-order NC\nterm, e.g. enforcing balanced clustering of arbitrary high-dimensional image\nfeatures combining color, texture, location, depth, motion, etc. On the other\nhand, standard clustering applications can benefit from an inclusion of common\npairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency,\nlabel cost, etc. To address joint energies like NC+MRF, we propose efficient\nKernel Cut algorithms based on bound optimization. While focusing on graph cut\nand move-making techniques, our new unary (linear) kernel and spectral bound\nformulations for common pairwise clustering criteria allow to integrate them\nwith any regularization functionals with existing discrete or continuous\nsolvers.",
        "The soaring demand for intelligent mobile applications calls for deploying\npowerful deep neural networks (DNNs) on mobile devices. However, the\noutstanding performance of DNNs notoriously relies on increasingly complex\nmodels, which in turn is associated with an increase in computational expense\nfar surpassing mobile devices' capacity. What is worse, app service providers\nneed to collect and utilize a large volume of users' data, which contain\nsensitive information, to build the sophisticated DNN models. Directly\ndeploying these models on public mobile devices presents prohibitive privacy\nrisk. To benefit from the on-device deep learning without the capacity and\nprivacy concerns, we design a private model compression framework RONA.\nFollowing the knowledge distillation paradigm, we jointly use hint learning,\ndistillation learning, and self learning to train a compact and fast neural\nnetwork. The knowledge distilled from the cumbersome model is adaptively\nbounded and carefully perturbed to enforce differential privacy. We further\npropose an elegant query sample selection method to reduce the number of\nqueries and control the privacy loss. A series of empirical evaluations as well\nas the implementation on an Android mobile device show that RONA can not only\ncompress cumbersome models efficiently but also provide a strong privacy\nguarantee. For example, on SVHN, when a meaningful\n$(9.83,10^{-6})$-differential privacy is guaranteed, the compact model trained\nby RONA can obtain 20$\\times$ compression ratio and 19$\\times$ speed-up with\nmerely 0.97% accuracy loss.",
        "Deep semi-supervised learning has been widely implemented in the real-world\ndue to the rapid development of deep learning. Recently, attention has shifted\nto the approaches such as Mean-Teacher to penalize the inconsistency between\ntwo perturbed input sets. Although these methods may achieve positive results,\nthey ignore the relationship information between data instances. To solve this\nproblem, we propose a novel method named Metric Learning by Similarity Network\n(MLSN), which aims to learn a distance metric adaptively on different domains.\nBy co-training with the classification network, similarity network can learn\nmore information about pairwise relationships and performs better on some\nempirical tasks than state-of-art methods.",
        "While multitask representation learning has become a popular approach in\nreinforcement learning (RL), theoretical understanding of why and when it works\nremains limited. This paper presents analyses for the statistical benefit of\nmultitask representation learning in linear Markov Decision Process (MDP) under\na generative model. In this paper, we consider an agent to learn a\nrepresentation function $\\phi$ out of a function class $\\Phi$ from $T$ source\ntasks with $N$ data per task, and then use the learned $\\hat{\\phi}$ to reduce\nthe required number of sample for a new task. We first discover a\n\\emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\\kappa$,\nwith which we prove that a straightforward least-square algorithm learns a\npolicy which is $\\tilde{O}(H^2\\sqrt{\\frac{\\mathcal{C}(\\Phi)^2 \\kappa\nd}{NT}+\\frac{\\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon,\n$\\mathcal{C}(\\Phi)$ is $\\Phi$'s complexity measure, $d$ is the dimension of the\nrepresentation (usually $d\\ll \\mathcal{C}(\\Phi)$) and $n$ is the number of\nsamples for the new task. Thus the required $n$ is $O(\\kappa d H^4)$ for the\nsub-optimality to be close to zero, which is much smaller than\n$O(\\mathcal{C}(\\Phi)^2\\kappa d H^4)$ in the setting without multitask\nrepresentation learning, whose sub-optimality gap is\n$\\tilde{O}(H^2\\sqrt{\\frac{\\kappa \\mathcal{C}(\\Phi)^2d}{n}})$. This\ntheoretically explains the power of multitask representation learning in\nreducing sample complexity. Further, we note that to ensure high sample\nefficiency, the LAFA criterion $\\kappa$ should be small. In fact, $\\kappa$\nvaries widely in magnitude depending on the different sampling distribution for\nnew task. This indicates adaptive sampling technique is important to make\n$\\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy\ngrid-world environment to corroborate our theoretical findings.",
        "Recent research has demonstrated the ability to estimate gaze on mobile\ndevices by performing inference on the image from the phone's front-facing\ncamera, and without requiring specialized hardware. While this offers wide\npotential applications such as in human-computer interaction, medical diagnosis\nand accessibility (e.g., hands free gaze as input for patients with motor\ndisorders), current methods are limited as they rely on collecting data from\nreal users, which is a tedious and expensive process that is hard to scale\nacross devices. There have been some attempts to synthesize eye region data\nusing 3D models that can simulate various head poses and camera settings,\nhowever these lack in realism.\n  In this paper, we improve upon a recently suggested method, and propose a\ngenerative adversarial framework to generate a large dataset of high resolution\ncolorful images with high diversity (e.g., in subjects, head pose, camera\nsettings) and realism, while simultaneously preserving the accuracy of gaze\nlabels. The proposed approach operates on extended regions of the eye, and even\ncompletes missing parts of the image. Using this rich synthesized dataset, and\nwithout using any additional training data from real users, we demonstrate\nimprovements over state-of-the-art for estimating 2D gaze position on mobile\ndevices. We further demonstrate cross-device generalization of model\nperformance, as well as improved robustness to diverse head pose, blur and\ndistance.",
        "Drones are enabling new forms of human actions surveillance due to their low\ncost and fast mobility. However, using deep neural networks for automatic\naerial action recognition is difficult due to the need for a large number of\ntraining aerial human action videos. Collecting a large number of human action\naerial videos is costly, time-consuming, and difficult. In this paper, we\nexplore two alternative data sources to improve aerial action classification\nwhen only a few training aerial examples are available. As a first data source,\nwe resort to video games. We collect plenty of aerial game action videos using\ntwo gaming engines. For the second data source, we leverage conditional\nWasserstein Generative Adversarial Networks to generate aerial features from\nground videos. Given that both data sources have some limitations, e.g. game\nvideos are biased towards specific actions categories (fighting, shooting,\netc.,), and it is not easy to generate good discriminative GAN-generated\nfeatures for all types of actions, we need to efficiently integrate two dataset\nsources with few available real aerial training videos. To address this\nchallenge of the heterogeneous nature of the data, we propose to use a disjoint\nmultitask learning framework. We feed the network with real and game, or real\nand GAN-generated data in an alternating fashion to obtain an improved action\nclassifier. We validate the proposed approach on two aerial action datasets and\ndemonstrate that features from aerial game videos and those generated from GAN\ncan be extremely useful for an improved action recognition in real aerial\nvideos when only a few real aerial training examples are available.",
        "Generative adversarial networks (GANs), famous for the capability of learning\ncomplex underlying data distribution, are however known to be tricky in the\ntraining process, which would probably result in mode collapse or performance\ndeterioration. Current approaches of dealing with GANs' issues almost utilize\nsome practical training techniques for the purpose of regularization, which on\nthe other hand undermines the convergence and theoretical soundness of GAN. In\nthis paper, we propose to stabilize GAN training via a novel particle-based\nvariational inference -- Langevin Stein variational gradient descent (LSVGD),\nwhich not only inherits the flexibility and efficiency of original SVGD but\naims to address its instability issues by incorporating an extra disturbance\ninto the update dynamics. We further demonstrate that by properly adjusting the\nnoise variance, LSVGD simulates a Langevin process whose stationary\ndistribution is exactly the target distribution. We also show that LSVGD\ndynamics has an implicit regularization which is able to enhance particles'\nspread-out and diversity. At last we present an efficient way of applying\nparticle-based variational inference on a general GAN training procedure no\nmatter what loss function is adopted. Experimental results on one synthetic\ndataset and three popular benchmark datasets -- Cifar-10, Tiny-ImageNet and\nCelebA validate that LSVGD can remarkably improve the performance and stability\nof various GAN models.",
        "Recent developments in image classification and natural language processing,\ncoupled with the rapid growth in social media usage, have enabled fundamental\nadvances in detecting breaking events around the world in real-time. Emergency\nresponse is one such area that stands to gain from these advances. By\nprocessing billions of texts and images a minute, events can be automatically\ndetected to enable emergency response workers to better assess rapidly evolving\nsituations and deploy resources accordingly. To date, most event detection\ntechniques in this area have focused on image-only or text-only approaches,\nlimiting detection performance and impacting the quality of information\ndelivered to crisis response teams. In this paper, we present a new multimodal\nfusion method that leverages both images and texts as input. In particular, we\nintroduce a cross-attention module that can filter uninformative and misleading\ncomponents from weak modalities on a sample by sample basis. In addition, we\nemploy a multimodal graph-based approach to stochastically transition between\nembeddings of different multimodal pairs during training to better regularize\nthe learning process as well as dealing with limited training data by\nconstructing new matched pairs from different samples. We show that our method\noutperforms the unimodal approaches and strong multimodal baselines by a large\nmargin on three crisis-related tasks.",
        "Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences. In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art.",
        "In-vehicle human object identification plays an important role in\nvision-based automated vehicle driving systems while objects such as\npedestrians and vehicles on roads or streets are the primary targets to protect\nfrom driverless vehicles. A challenge is the difficulty to detect objects in\nmoving under the wild conditions, while illumination and image quality could\ndrastically vary. In this work, to address this challenge, we exploit Deep\nConvolutional Generative Adversarial Networks (DCGANs) with Single Shot\nDetector (SSD) to handle with the wild conditions. In our work, a GAN was\ntrained with low-quality images to handle with the challenges arising from the\nwild conditions in smart cities, while a cascaded SSD is employed as the object\ndetector to perform with the GAN. We used tested our approach under wild\nconditions using taxi driver videos on London street in both daylight and night\ntimes, and the tests from in-vehicle videos demonstrate that this strategy can\ndrastically achieve a better detection rate under the wild conditions.",
        "Finding general evaluation metrics for unsupervised representation learning\ntechniques is a challenging open research question, which recently has become\nmore and more necessary due to the increasing interest in unsupervised methods.\nEven though these methods promise beneficial representation characteristics,\nmost approaches currently suffer from the objective function mismatch. This\nmismatch states that the performance on a desired target task can decrease when\nthe unsupervised pretext task is learned too long - especially when both tasks\nare ill-posed. In this work, we build upon the widely used linear evaluation\nprotocol and define new general evaluation metrics to quantitatively capture\nthe objective function mismatch and the more generic metrics mismatch. We\ndiscuss the usability and stability of our protocols on a variety of pretext\nand target tasks and study mismatches in a wide range of experiments. Thereby\nwe disclose dependencies of the objective function mismatch across several\npretext and target tasks with respect to the pretext model's representation\nsize, target model complexity, pretext and target augmentations as well as\npretext and target task types.",
        "Performing inference in graphs is a common task within several machine\nlearning problems, e.g., image segmentation, community detection, among others.\nFor a given undirected connected graph, we tackle the statistical problem of\nexactly recovering an unknown ground-truth binary labeling of the nodes from a\nsingle corrupted observation of each edge. Such problem can be formulated as a\nquadratic combinatorial optimization problem over the boolean hypercube, where\nit has been shown before that one can (with high probability and in polynomial\ntime) exactly recover the ground-truth labeling of graphs that have an\nisoperimetric number that grows with respect to the number of nodes (e.g.,\ncomplete graphs, regular expanders). In this work, we apply a powerful\nhierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the\ncombinatorial problem. Motivated by empirical evidence on the improvement in\nexact recoverability, we center our attention on the degree-4 SoS relaxation\nand set out to understand the origin of such improvement from a graph\ntheoretical perspective. We show that the solution of the dual of the relaxed\nproblem is related to finding edge weights of the Johnson and Kneser graphs,\nwhere the weights fulfill the SoS constraints and intuitively allow the input\ngraph to increase its algebraic connectivity. Finally, as byproduct of our\nanalysis, we derive a novel Cheeger-type lower bound for the algebraic\nconnectivity of graphs with signed edge weights.",
        "Real-world machine learning systems are achieving remarkable performance in\nterms of coarse-grained metrics like overall accuracy and F-1 score. However,\nmodel improvement and development often require fine-grained modeling on\nindividual data subsets or slices, for instance, the data slices where the\nmodels have unsatisfactory results. In practice, it gives tangible values for\ndeveloping such models that can pay extra attention to critical or interested\nslices while retaining the original overall performance. This work extends the\nrecent slice-based learning (SBL)~\\cite{chen2019slice} with a mixture of\nattentions (MoA) to learn slice-aware dual attentive representations. We\nempirically show that the MoA approach outperforms the baseline method as well\nas the original SBL approach on monitored slices with two natural language\nunderstanding (NLU) tasks.",
        "We propose OneFlow - a flow-based one-class classifier for anomaly (outliers)\ndetection that finds a minimal volume bounding region. Contrary to\ndensity-based methods, OneFlow is constructed in such a way that its result\ntypically does not depend on the structure of outliers. This is caused by the\nfact that during training the gradient of the cost function is propagated only\nover the points located near to the decision boundary (behavior similar to the\nsupport vectors in SVM). The combination of flow models and Bernstein quantile\nestimator allows OneFlow to find a parametric form of bounding region, which\ncan be useful in various applications including describing shapes from 3D point\nclouds. Experiments show that the proposed model outperforms related methods on\nreal-world anomaly detection problems.",
        "With the mass-market adoption of dual-camera mobile phones, leveraging stereo\ninformation in computer vision has become increasingly important. Current\nstate-of-the-art methods utilize learning-based algorithms, where the amount\nand quality of training samples heavily influence results. Existing stereo\nimage datasets are limited either in size or subject variety. Hence, algorithms\ntrained on such datasets do not generalize well to scenarios encountered in\nmobile photography. We present Holopix50k, a novel in-the-wild stereo image\ndataset, comprising 49,368 image pairs contributed by users of the Holopix\nmobile social platform. In this work, we describe our data collection process\nand statistically compare our dataset to other popular stereo datasets. We\nexperimentally show that using our dataset significantly improves results for\ntasks such as stereo super-resolution and self-supervised monocular depth\nestimation. Finally, we showcase practical applications of our dataset to\nmotivate novel works and use cases. The Holopix50k dataset is available at\nhttp://github.com/leiainc/holopix50k",
        "Graph self-supervised learning has gained increasing attention due to its\ncapacity to learn expressive node representations. Many pretext tasks, or loss\nfunctions have been designed from distinct perspectives. However, we observe\nthat different pretext tasks affect downstream tasks differently cross\ndatasets, which suggests that searching pretext tasks is crucial for graph\nself-supervised learning. Different from existing works focusing on designing\nsingle pretext tasks, this work aims to investigate how to automatically\nleverage multiple pretext tasks effectively. Nevertheless, evaluating\nrepresentations derived from multiple pretext tasks without direct access to\nground truth labels makes this problem challenging. To address this obstacle,\nwe make use of a key principle of many real-world graphs, i.e., homophily, or\nthe principle that ``like attracts like,'' as the guidance to effectively\nsearch various self-supervised pretext tasks. We provide theoretical\nunderstanding and empirical evidence to justify the flexibility of homophily in\nthis search task. Then we propose the AutoSSL framework which can automatically\nsearch over combinations of various self-supervised tasks. By evaluating the\nframework on 7 real-world datasets, our experimental results show that AutoSSL\ncan significantly boost the performance on downstream tasks including node\nclustering and node classification compared with training under individual\ntasks. Code will be released at https://github.com/ChandlerBang/AutoSSL.",
        "In this paper, it has attempted to use Reinforcement learning to model the\nproper dosage of Warfarin for patients.The paper first examines two baselines:\na fixed model of 35 mg/week dosages and a linear model that relies on patient\ndata. We implemented a LinUCB bandit that improved performance measured on\nregret and percent incorrect. On top of the LinUCB bandit, we experimented with\nonline supervised learning and reward reshaping to boost performance. Our\nresults clearly beat the baselines and show the promise of using multi-armed\nbandits and artificial intelligence to aid physicians in deciding proper\ndosages.",
        "We address the problem of estimating a high quality dense depth map from a\nsingle RGB input image. We start out with a baseline encoder-decoder\nconvolutional neural network architecture and pose the question of how the\nglobal processing of information can help improve overall depth estimation. To\nthis end, we propose a transformer-based architecture block that divides the\ndepth range into bins whose center value is estimated adaptively per image. The\nfinal depth values are estimated as linear combinations of the bin centers. We\ncall our new building block AdaBins. Our results show a decisive improvement\nover the state-of-the-art on several popular depth datasets across all metrics.\nWe also validate the effectiveness of the proposed block with an ablation study\nand provide the code and corresponding pre-trained weights of the new\nstate-of-the-art model.",
        "Recent research has seen numerous supervised learning-based methods for 3D\nshape segmentation and remarkable performance has been achieved on various\nbenchmark datasets. These supervised methods require a large amount of\nannotated data to train deep neural networks to ensure the generalization\nability on the unseen test set. In this paper, we introduce a\nmeta-learning-based method for few-shot 3D shape segmentation where only a few\nlabeled samples are provided for the unseen classes. To achieve this, we treat\nthe shape segmentation as a point labeling problem in the metric space.\nSpecifically, we first design a meta-metric learner to transform input shapes\ninto embedding space and our model learns to learn a proper metric space for\neach object class based on point embeddings. Then, for each class, we design a\nmetric learner to extract part-specific prototype representations from a few\nsupport shapes and our model performs per-point segmentation over the query\nshapes by matching each point to its nearest prototype in the learned metric\nspace. A metric-based loss function is used to dynamically modify distances\nbetween point embeddings thus maximizes in-part similarity while minimizing\ninter-part similarity. A dual segmentation branch is adopted to make full use\nof the support information and implicitly encourages consistency between the\nsupport and query prototypes. We demonstrate the superior performance of our\nproposed on the ShapeNet part dataset under the few-shot scenario, compared\nwith well-established baseline and state-of-the-art semi-supervised methods.",
        "Wavelet scattering networks, which are convolutional neural networks (CNNs)\nwith fixed filters and weights, are promising tools for image analysis.\nImposing symmetry on image statistics can improve human interpretability, aid\nin generalization, and provide dimension reduction. In this work, we introduce\na fast-to-compute, translationally invariant and rotationally equivariant\nwavelet scattering network (EqWS) and filter bank of wavelets (triglets). We\ndemonstrate the interpretability and quantify the invariance/equivariance of\nthe coefficients, briefly commenting on difficulties with implementing scale\nequivariance. On MNIST, we show that training on a rotationally invariant\nreduction of the coefficients maintains rotational invariance when generalized\nto test data and visualize residual symmetry breaking terms. Rotation\nequivariance is leveraged to estimate the rotation angle of digits and\nreconstruct the full rotation dependence of each coefficient from a single\nangle. We benchmark EqWS with linear classifiers on EMNIST and CIFAR-10/100,\nintroducing a new second-order, cross-color channel coupling for the color\nimages. We conclude by comparing the performance of an isotropic reduction of\nthe scattering coefficients and RWST, a previous coefficient reduction, on an\nisotropic classification of magnetohydrodynamic simulations with astrophysical\nrelevance.",
        "In this work, we move beyond the traditional complex-valued representations,\nintroducing more expressive hypercomplex representations to model entities and\nrelations for knowledge graph embeddings. More specifically, quaternion\nembeddings, hypercomplex-valued embeddings with three imaginary components, are\nutilized to represent entities. Relations are modelled as rotations in the\nquaternion space. The advantages of the proposed approach are: (1) Latent\ninter-dependencies (between all components) are aptly captured with Hamilton\nproduct, encouraging a more compact interaction between entities and relations;\n(2) Quaternions enable expressive rotation in four-dimensional space and have\nmore degree of freedom than rotation in complex plane; (3) The proposed\nframework is a generalization of ComplEx on hypercomplex space while offering\nbetter geometrical interpretations, concurrently satisfying the key desiderata\nof relational representation learning (i.e., modeling symmetry, anti-symmetry\nand inversion). Experimental results demonstrate that our method achieves\nstate-of-the-art performance on four well-established knowledge graph\ncompletion benchmarks.",
        "Although reinforcement learning has been successfully applied in many domains\nin recent years, we still lack agents that can systematically generalize. While\nrelational inductive biases that fit a task can improve generalization of RL\nagents, these biases are commonly hard-coded directly in the agent's neural\narchitecture. In this work, we show that we can incorporate relational\ninductive biases, encoded in the form of relational graphs, into agents. Based\non this insight, we propose Grid-to-Graph (GTG), a mapping from grid structures\nto relational graphs that carry useful spatial relational inductive biases when\nprocessed through a Relational Graph Convolution Network (R-GCN). We show that,\nwith GTG, R-GCNs generalize better both in terms of in-distribution and\nout-of-distribution compared to baselines based on Convolutional Neural\nNetworks and Neural Logic Machines on challenging procedurally generated\nenvironments and MinAtar. Furthermore, we show that GTG produces agents that\ncan jointly reason over observations and environment dynamics encoded in\nknowledge bases.",
        "In this work, we investigate whether state-of-the-art object detection\nsystems have equitable predictive performance on pedestrians with different\nskin tones. This work is motivated by many recent examples of ML and vision\nsystems displaying higher error rates for certain demographic groups than\nothers. We annotate an existing large scale dataset which contains pedestrians,\nBDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide\nan in-depth comparative analysis of performance between these two skin tone\ngroupings, finding that neither time of day nor occlusion explain this\nbehavior, suggesting this disparity is not merely the result of pedestrians in\nthe 4-6 range appearing in more difficult scenes for detection. We investigate\nto what extent time of day, occlusion, and reweighting the supervised loss\nduring training affect this predictive bias.",
        "Given an image, generating its natural language description (i.e., caption)\nis a well studied problem. Approaches proposed to address this problem usually\nrely on image features that are difficult to interpret. Particularly, these\nimage features are subdivided into global and local features, where global\nfeatures are extracted from the global representation of the image, while local\nfeatures are extracted from the objects detected locally in an image. Although,\nlocal features extract rich visual information from the image, existing models\ngenerate captions in a blackbox manner and humans have difficulty interpreting\nwhich local objects the caption is aimed to represent. Hence in this paper, we\npropose a novel framework for the image captioning with an explicit object\n(e.g., knowledge graph entity) selection process while still maintaining its\nend-to-end training ability. The model first explicitly selects which local\nentities to include in the caption according to a human-interpretable mask,\nthen generate proper captions by attending to selected entities. Experiments\nconducted on the MSCOCO dataset demonstrate that our method achieves good\nperformance in terms of the caption quality and diversity with a more\ninterpretable generating process than previous counterparts.",
        "Predicting motion of surrounding agents is critical to real-world\napplications of tactical path planning for autonomous driving. Due to the\ncomplex temporal dependencies and social interactions of agents, on-line\ntrajectory prediction is a challenging task. With the development of attention\nmechanism in recent years, transformer model has been applied in natural\nlanguage sequence processing first and then image processing. In this paper, we\npresent a Spatial-Channel Transformer Network for trajectory prediction with\nattention functions. Instead of RNN models, we employ transformer model to\ncapture the spatial-temporal features of agents. A channel-wise module is\ninserted to measure the social interaction between agents. We find that the\nSpatial-Channel Transformer Network achieves promising results on real-world\ntrajectory prediction datasets on the traffic scenes.",
        "Convolututional Neural Networks have achieved state of the art in image\nclassification, object detection and other image related tasks. In this paper I\npresent another use of CNNs i.e. if given a set of images and then giving a\nsingle test image the network identifies that the test image is part of which\nimage from the images given before. This is a task somehow similar to measuring\nimage similarity and can be done using a simple CNN. Doing this task manually\nby looping can be quite a time consuming problem and won't be a generalizable\nsolution. The task is quite similar to doing object detection but for that lots\ntraining data should be given or in the case of sliding window it takes lot of\ntime and my algorithm can work with much fewer examples, is totally\nunsupervised and works much efficiently. Also, I explain that how unsupervised\nalgorithm like K-Means or supervised algorithm like K-NN are not good enough to\nperform this task. The basic idea is that image encodings are collected for\neach image from a CNN, when a test image comes it is replaced by a part of\noriginal image, the encoding is generated using the same network, the frobenius\nnorm is calculated and if it comes under a tolerance level then the test image\nis said to be the part of the original image.",
        "Given sensor readings over time from a power grid, how can we accurately\ndetect when an anomaly occurs? A key part of achieving this goal is to use the\nnetwork of power grid sensors to quickly detect, in real-time, when any unusual\nevents, whether natural faults or malicious, occur on the power grid. Existing\nbad-data detectors in the industry lack the sophistication to robustly detect\nbroad types of anomalies, especially those due to emerging cyber-attacks, since\nthey operate on a single measurement snapshot of the grid at a time. New ML\nmethods are more widely applicable, but generally do not consider the impact of\ntopology change on sensor measurements and thus cannot accommodate regular\ntopology adjustments in historical data. Hence, we propose DYNWATCH, a domain\nknowledge based and topology-aware algorithm for anomaly detection using\nsensors placed on a dynamic grid. Our approach is accurate, outperforming\nexisting approaches by 20% or more (F-measure) in experiments; and fast,\nrunning in less than 1.7ms on average per time tick per sensor on a 60K+ branch\ncase using a laptop computer, and scaling linearly in the size of the graph.",
        "Vision transformers have attracted much attention from computer vision\nresearchers as they are not restricted to the spatial inductive bias of\nConvNets. However, although Transformer-based backbones have achieved much\nprogress on ImageNet classification, it is still unclear whether the learned\nrepresentations are as transferable as or even more transferable than ConvNets'\nfeatures. To address this point, we systematically investigate the transfer\nlearning ability of ConvNets and vision transformers in 15 single-task and\nmulti-task performance evaluations. Given the strong correlation between the\nperformance of pre-trained models and transfer learning, we include 2 residual\nConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones\n(i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that\nindicate similar transfer learning performance on downstream datasets.\n  We observe consistent advantages of Transformer-based backbones on 13\ndownstream tasks (out of 15), including but not limited to fine-grained\nclassification, scene recognition (classification, segmentation and depth\nestimation), open-domain classification, face recognition, etc. More\nspecifically, we find that two ViT models heavily rely on whole network\nfine-tuning to achieve performance gains while Swin Transformer does not have\nsuch a requirement. Moreover, vision transformers behave more robustly in\nmulti-task learning, i.e., bringing more improvements when managing mutually\nbeneficial tasks and reducing performance losses when tackling irrelevant\ntasks. We hope our discoveries can facilitate the exploration and exploitation\nof vision transformers in the future.",
        "Scene text detection and recognition have been well explored in the past few\nyears. Despite the progress, efficient and accurate end-to-end spotting of\narbitrarily-shaped text remains challenging. In this work, we propose an\nend-to-end text spotting framework, termed PAN++, which can efficiently detect\nand recognize text of arbitrary shapes in natural scenes. PAN++ is based on the\nkernel representation that reformulates a text line as a text kernel (central\nregion) surrounded by peripheral pixels. By systematically comparing with\nexisting scene text representations, we show that our kernel representation can\nnot only describe arbitrarily-shaped text but also well distinguish adjacent\ntext. Moreover, as a pixel-based representation, the kernel representation can\nbe predicted by a single fully convolutional network, which is very friendly to\nreal-time applications. Taking the advantages of the kernel representation, we\ndesign a series of components as follows: 1) a computationally efficient\nfeature enhancement network composed of stacked Feature Pyramid Enhancement\nModules (FPEMs); 2) a lightweight detection head cooperating with Pixel\nAggregation (PA); and 3) an efficient attention-based recognition head with\nMasked RoI. Benefiting from the kernel representation and the tailored\ncomponents, our method achieves high inference speed while maintaining\ncompetitive accuracy. Extensive experiments show the superiority of our method.\nFor example, the proposed PAN++ achieves an end-to-end text spotting F-measure\nof 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms\nthe previous best method. Code will be available at: https://git.io/PAN.",
        "Recent advancements in transfer learning have made it a promising approach\nfor domain adaptation via transfer of learned representations. This is\nespecially when relevant when alternate tasks have limited samples of\nwell-defined and labeled data, which is common in the molecule data domain.\nThis makes transfer learning an ideal approach to solve molecular learning\ntasks. While Adversarial reprogramming has proven to be a successful method to\nrepurpose neural networks for alternate tasks, most works consider source and\nalternate tasks within the same domain. In this work, we propose a new\nalgorithm, Representation Reprogramming via Dictionary Learning (R2DL), for\nadversarially reprogramming pretrained language models for molecular learning\ntasks, motivated by leveraging learned representations in massive state of the\nart language models. The adversarial program learns a linear transformation\nbetween a dense source model input space (language data) and a sparse target\nmodel input space (e.g., chemical and biological molecule data) using a k-SVD\nsolver to approximate a sparse representation of the encoded data, via\ndictionary learning. R2DL achieves the baseline established by state of the art\ntoxicity prediction models trained on domain-specific data and outperforms the\nbaseline in a limited training-data setting, thereby establishing avenues for\ndomain-agnostic transfer learning for tasks with molecule data.",
        "Data augmentation is becoming essential for improving regression accuracy in\ncritical applications including manufacturing and finance. Existing techniques\nfor data augmentation largely focus on classification tasks and do not readily\napply to regression tasks. In particular, the recent Mixup techniques for\nclassification rely on the key assumption that linearity holds among training\nexamples, which is reasonable if the label space is discrete, but has\nlimitations when the label space is continuous as in regression. We show that\nmixing examples that either have a large data or label distance may have an\nincreasingly-negative effect on model performance. Hence, we use the stricter\nassumption that linearity only holds within certain data or label distances for\nregression where the degree may vary by each example. We then propose MixRL, a\ndata augmentation meta learning framework for regression that learns for each\nexample how many nearest neighbors it should be mixed with for the best model\nperformance using a small validation set. MixRL achieves these objectives using\nMonte Carlo policy gradient reinforcement learning. Our experiments conducted\nboth on synthetic and real datasets show that MixRL significantly outperforms\nstate-of-the-art data augmentation baselines. MixRL can also be integrated with\nother classification Mixup techniques for better results.",
        "Pre-trained Transformer-based models have achieved state-of-the-art\nperformance for various Natural Language Processing (NLP) tasks. However, these\nmodels often have billions of parameters, and, thus, are too resource-hungry\nand computation-intensive to suit low-capability devices or applications with\nstrict latency requirements. One potential remedy for this is model\ncompression, which has attracted a lot of research attention. Here, we\nsummarize the research in compressing Transformers, focusing on the especially\npopular BERT model. In particular, we survey the state of the art in\ncompression for BERT, we clarify the current best practices for compressing\nlarge-scale Transformer models, and we provide insights into the workings of\nvarious methods. Our categorization and analysis also shed light on promising\nfuture research directions for achieving lightweight, accurate, and generic NLP\nmodels.",
        "PointGoal Navigation is an embodied task that requires agents to navigate to\na specified point in an unseen environment. Wijmans et al. showed that this\ntask is solvable but their method is computationally prohibitive, requiring 2.5\nbillion frames and 180 GPU-days. In this work, we develop a method to\nsignificantly increase sample and time efficiency in learning PointNav using\nself-supervised auxiliary tasks (e.g. predicting the action taken between two\negocentric observations, predicting the distance between two observations from\na trajectory,etc.).We find that naively combining multiple auxiliary tasks\nimproves sample efficiency,but only provides marginal gains beyond a point. To\novercome this, we use attention to combine representations learnt from\nindividual auxiliary tasks. Our best agent is 5.5x faster to reach the\nperformance of the previous state-of-the-art, DD-PPO, at 40M frames, and\nimproves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is\npublicly available at https://github.com/joel99/habitat-pointnav-aux.",
        "The vulnerability of deep neural networks (DNNs) to adversarial examples has\nattracted more attention. Many algorithms have been proposed to craft powerful\nadversarial examples. However, most of these algorithms modified the global or\nlocal region of pixels without taking network explanations into account. Hence,\nthe perturbations are redundant, which are easily detected by human eyes. In\nthis paper, we propose a novel method to generate local region perturbations.\nThe main idea is to find a contributing feature region (CFR) of an image by\nsimulating the human attention mechanism and then add perturbations to CFR.\nFurthermore, a soft mask matrix is designed on the basis of an activation map\nto finely represent the contributions of each pixel in CFR. With this soft\nmask, we develop a new loss function with inverse temperature to search for\noptimal perturbations in CFR. Due to the network explanations, the\nperturbations added to CFR are more effective than those added to other\nregions. Extensive experiments conducted on CIFAR-10 and ILSVRC2012 demonstrate\nthe effectiveness of the proposed method, including attack success rate,\nimperceptibility, and transferability.",
        "We aim to model unknown file processing. As the content of log files often\nevolves over time, we established a dynamic statistical model which learns and\nadapts processing and parsing rules. First, we limit the amount of unstructured\ntext by focusing only on those frequent patterns which lead to the desired\noutput table similar to Vaarandi [10]. Second, we transform the found frequent\npatterns and the output stating the parsed table into a Hidden Markov Model\n(HMM). We use this HMM as a specific, however, flexible representation of a\npattern for log file processing. With changes in the raw log file distorting\nlearned patterns, we aim the model to adapt automatically in order to maintain\nhigh quality output. After training our model on one system type, applying the\nmodel and the resulting parsing rule to a different system with slightly\ndifferent log file patterns, we achieve an accuracy over 99%.",
        "Quantizing deep neural networks is an effective method for reducing memory\nconsumption and improving inference speed, and is thus useful for\nimplementation in resource-constrained devices. However, it is still hard for\nextremely low-bit models to achieve accuracy comparable with that of\nfull-precision models. To address this issue, we propose learnable companding\nquantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and\n4-bit models. LCQ jointly optimizes model weights and learnable companding\nfunctions that can flexibly and non-uniformly control the quantization levels\nof weights and activations. We also present a new weight normalization\ntechnique that allows more stable training for quantization. Experimental\nresults show that LCQ outperforms conventional state-of-the-art methods and\nnarrows the gap between quantized and full-precision models for image\nclassification and object detection tasks. Notably, the 2-bit ResNet-50 model\non ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%,\nallowing LCQ to further exploit the potential of non-uniform quantization.",
        "Multi-task learning (MTL) has received considerable attention, and numerous\ndeep learning applications benefit from MTL with multiple objectives. However,\nconstructing multiple related tasks is difficult, and sometimes only a single\ntask is available for training in a dataset. To tackle this problem, we\nexplored the idea of using unsupervised clustering to construct a variety of\nauxiliary tasks from unlabeled data or existing labeled data. We found that\nsome of these newly constructed tasks could exhibit semantic meanings\ncorresponding to certain human-specific attributes, but some were non-ideal. In\norder to effectively reduce the impact of non-ideal auxiliary tasks on the main\ntask, we further proposed a novel meta-learning-based multi-task learning\napproach, which trained the shared hidden layers on auxiliary tasks, while the\nmeta-optimization objective was to minimize the loss on the main task, ensuring\nthat the optimizing direction led to an improvement on the main task.\nExperimental results across five image datasets demonstrated that the proposed\nmethod significantly outperformed existing single task learning,\nsemi-supervised learning, and some data augmentation methods, including an\nimprovement of more than 9% on the Omniglot dataset.",
        "Deep Neural Networks (DNNs) have shown remarkable performance in a diverse\nrange of machine learning applications. However, it is widely known that DNNs\nare vulnerable to simple adversarial perturbations, which causes the model to\nincorrectly classify inputs. In this paper, we propose a simple yet effective\nmethod to detect adversarial examples, using methods developed to explain the\nmodel's behavior. Our key observation is that adding small, humanly\nimperceptible perturbations can lead to drastic changes in the model\nexplanations, resulting in unusual or irregular forms of explanations. From\nthis insight, we propose an unsupervised detection of adversarial examples\nusing reconstructor networks trained only on model explanations of benign\nexamples. Our evaluations with MNIST handwritten dataset show that our method\nis capable of detecting adversarial examples generated by the state-of-the-art\nalgorithms with high confidence. To the best of our knowledge, this work is the\nfirst in suggesting unsupervised defense method using model explanations.",
        "Generative adversarial networks (GANs) are a family of generative models that\ndo not minimize a single training criterion. Unlike other generative models,\nthe data distribution is learned via a game between a generator (the generative\nmodel) and a discriminator (a teacher providing training signal) that each\nminimize their own cost. GANs are designed to reach a Nash equilibrium at which\neach player cannot reduce their cost without changing the other players'\nparameters. One useful approach for the theory of GANs is to show that a\ndivergence between the training distribution and the model distribution obtains\nits minimum value at equilibrium. Several recent research directions have been\nmotivated by the idea that this divergence is the primary guide for the\nlearning process and that every step of learning should decrease the\ndivergence. We show that this view is overly restrictive. During GAN training,\nthe discriminator provides learning signal in situations where the gradients of\nthe divergences between distributions would not be useful. We provide empirical\ncounterexamples to the view of GAN training as divergence minimization.\nSpecifically, we demonstrate that GANs are able to learn distributions in\nsituations where the divergence minimization point of view predicts they would\nfail. We also show that gradient penalties motivated from the divergence\nminimization perspective are equally helpful when applied in other contexts in\nwhich the divergence minimization perspective does not predict they would be\nhelpful. This contributes to a growing body of evidence that GAN training may\nbe more usefully viewed as approaching Nash equilibria via trajectories that do\nnot necessarily minimize a specific divergence at each step.",
        "Neural architecture search (NAS) has attracted increasing attentions in both\nacademia and industry. In the early age, researchers mostly applied individual\nsearch methods which sample and evaluate the candidate architectures separately\nand thus incur heavy computational overheads. To alleviate the burden,\nweight-sharing methods were proposed in which exponentially many architectures\nshare weights in the same super-network, and the costly training procedure is\nperformed only once. These methods, though being much faster, often suffer the\nissue of instability. This paper provides a literature review on NAS, in\nparticular the weight-sharing methods, and points out that the major challenge\ncomes from the optimization gap between the super-network and the\nsub-architectures. From this perspective, we summarize existing approaches into\nseveral categories according to their efforts in bridging the gap, and analyze\nboth advantages and disadvantages of these methodologies. Finally, we share our\nopinions on the future directions of NAS and AutoML. Due to the expertise of\nthe authors, this paper mainly focuses on the application of NAS to computer\nvision problems and may bias towards the work in our group.",
        "The employment of convolutional neural networks has led to significant\nperformance improvement on the task of object detection. However, when applying\nexisting detectors to continuous frames in a video, we often encounter\nmomentary miss-detection of objects, that is, objects are undetected\nexceptionally at a few frames, although they are correctly detected at all\nother frames. In this paper, we analyze the mechanism of how such\nmiss-detection occurs. For the most popular class of detectors that are based\non anchor boxes, we show the followings: i) besides apparent causes such as\nmotion blur, occlusions, background clutters, etc., the majority of remaining\nmiss-detection can be explained by an improper behavior of the detectors at\nboundaries of the anchor boxes; and ii) this can be rectified by improving the\nway of choosing positive samples from candidate anchor boxes when training the\ndetectors.",
        "Automatic skin lesion segmentation on dermoscopic images is an essential step\nin computer-aided diagnosis of melanoma. However, this task is challenging due\nto significant variations of lesion appearances across different patients. This\nchallenge is further exacerbated when dealing with a large amount of image\ndata. In this paper, we extended our previous work by developing a deeper\nnetwork architecture with smaller kernels to enhance its discriminant capacity.\nIn addition, we explicitly included color information from multiple color\nspaces to facilitate network training and thus to further improve the\nsegmentation performance. We extensively evaluated our method on the ISBI 2017\nskin lesion segmentation challenge. By training with the 2000 challenge\ntraining images, our method achieved an average Jaccard Index (JA) of 0.765 on\nthe 600 challenge testing images, which ranked itself in the first place in the\nchallenge",
        "Despite success on a wide range of problems related to vision, generative\nadversarial networks (GANs) often suffer from inferior performance due to\nunstable training, especially for text generation. To solve this issue, we\npropose a new variational GAN training framework which enjoys superior training\nstability. Our approach is inspired by a connection of GANs and reinforcement\nlearning under a variational perspective. The connection leads to (1)\nprobability ratio clipping that regularizes generator training to prevent\nexcessively large updates, and (2) a sample re-weighting mechanism that\nimproves discriminator training by downplaying bad-quality fake samples.\nMoreover, our variational GAN framework can provably overcome the training\nissue in many GANs that an optimal discriminator cannot provide any informative\ngradient to training generator. By plugging the training approach in diverse\nstate-of-the-art GAN architectures, we obtain significantly improved\nperformance over a range of tasks, including text generation, text style\ntransfer, and image generation.",
        "Learning decent representations from unlabeled time-series data with temporal\ndynamics is a very challenging task. In this paper, we propose an unsupervised\nTime-Series representation learning framework via Temporal and Contextual\nContrasting (TS-TCC), to learn time-series representation from unlabeled data.\nFirst, the raw time-series data are transformed into two different yet\ncorrelated views by using weak and strong augmentations. Second, we propose a\nnovel temporal contrasting module to learn robust temporal representations by\ndesigning a tough cross-view prediction task. Last, to further learn\ndiscriminative representations, we propose a contextual contrasting module\nbuilt upon the contexts from the temporal contrasting module. It attempts to\nmaximize the similarity among different contexts of the same sample while\nminimizing similarity among contexts of different samples. Experiments have\nbeen carried out on three real-world time-series datasets. The results manifest\nthat training a linear classifier on top of the features learned by our\nproposed TS-TCC performs comparably with the supervised training. Additionally,\nour proposed TS-TCC shows high efficiency in few-labeled data and transfer\nlearning scenarios. The code is publicly available at\nhttps://github.com/emadeldeen24/TS-TCC.",
        "Style transfer is a problem of rendering image with some content in the style\nof another image, for example a family photo in the style of a painting of some\nfamous artist. The drawback of classical style transfer algorithm is that it\nimposes style uniformly on all parts of the content image, which perturbs\ncentral objects on the content image, such as faces or text, and makes them\nunrecognizable. This work proposes a novel style transfer algorithm which\nautomatically detects central objects on the content image, generates spatial\nimportance mask and imposes style non-uniformly: central objects are stylized\nless to preserve their recognizability and other parts of the image are\nstylized as usual to preserve the style. Three methods of automatic central\nobject detection are proposed and evaluated qualitatively and via a user\nevaluation study. Both comparisons demonstrate higher quality of stylization\ncompared to the classical style transfer method.",
        "Cross-lingual Text Classification (CLC) consists of automatically\nclassifying, according to a common set C of classes, documents each written in\none of a set of languages L, and doing so more accurately than when naively\nclassifying each document via its corresponding language-specific classifier.\nIn order to obtain an increase in the classification accuracy for a given\nlanguage, the system thus needs to also leverage the training examples written\nin the other languages. We tackle multilabel CLC via funnelling, a new ensemble\nlearning method that we propose here. Funnelling consists of generating a\ntwo-tier classification system where all documents, irrespectively of language,\nare classified by the same (2nd-tier) classifier. For this classifier all\ndocuments are represented in a common, language-independent feature space\nconsisting of the posterior probabilities generated by 1st-tier,\nlanguage-dependent classifiers. This allows the classification of all test\ndocuments, of any language, to benefit from the information present in all\ntraining documents, of any language. We present substantial experiments, run on\npublicly available multilingual text collections, in which funnelling is shown\nto significantly outperform a number of state-of-the-art baselines. All code\nand datasets (in vector form) are made publicly available.",
        "We present a method for microtubule tracking in electron microscopy volumes.\nOur method first identifies a sparse set of voxels that likely belong to\nmicrotubules. Similar to prior work, we then enumerate potential edges between\nthese voxels, which we represent in a candidate graph. Tracks of microtubules\nare found by selecting nodes and edges in the candidate graph by solving a\nconstrained optimization problem incorporating biological priors on microtubule\nstructure. For this, we present a novel integer linear programming formulation,\nwhich results in speed-ups of three orders of magnitude and an increase of 53%\nin accuracy compared to prior art (evaluated on three 1.2 x 4 x 4$\\mu$m volumes\nof Drosophila neural tissue). We also propose a scheme to solve the\noptimization problem in a block-wise fashion, which allows distributed tracking\nand is necessary to process very large electron microscopy volumes. Finally, we\nrelease a benchmark dataset for microtubule tracking, here used for training,\ntesting and validation, consisting of eight 30 x 1000 x 1000 voxel blocks (1.2\nx 4 x 4$\\mu$m) of densely annotated microtubules in the CREMI data set\n(https://github.com/nilsec/micron).",
        "Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.",
        "Time Series Motif Discovery (TSMD) is defined as searching for patterns that\nare previously unknown and appear with a given frequency in time series.\nAnother problem strongly related with TSMD is Word Segmentation. This problem\nhas received much attention from the community that studies early language\nacquisition in babies and toddlers. The development of biologically plausible\nmodels for word segmentation could greatly advance this field. Therefore, in\nthis article, we propose the Variable Input Length Map (VILMAP) for Motif\nDiscovery and Word Segmentation. The model is based on the Self-Organizing Maps\nand can identify Motifs with different lengths in time series. In our\nexperiments, we show that VILMAP presents good results in finding Motifs in a\nstandard Motif discovery dataset and can avoid catastrophic forgetting when\ntrained with datasets with increasing values of input size. We also show that\nVILMAP achieves results similar or superior to other methods in the literature\ndeveloped for the task of word segmentation.",
        "Time series data that are not measured at regular intervals are commonly\ndiscretized as a preprocessing step. For example, data about customer arrival\ntimes might be simplified by summing the number of arrivals within hourly\nintervals, which produces a discrete-time time series that is easier to model.\nIn this abstract, we show that discretization introduces a bias that affects\nmodels trained for decision-making. We refer to this phenomenon as\ndiscretization bias, and show that we can avoid it by using continuous-time\nmodels instead.",
        "Knowledge distillation (KD) has been proven to be a simple and effective tool\nfor training compact models. Almost all KD variants for dense prediction tasks\nalign the student and teacher networks' feature maps in the spatial domain,\ntypically by minimizing point-wise and/or pair-wise discrepancy. Observing that\nin semantic segmentation, some layers' feature activations of each channel tend\nto encode saliency of scene categories (analogue to class activation mapping),\nwe propose to align features channel-wise between the student and teacher\nnetworks. To this end, we first transform the feature map of each channel into\na probabilty map using softmax normalization, and then minimize the\nKullback-Leibler (KL) divergence of the corresponding channels of the two\nnetworks. By doing so, our method focuses on mimicking the soft distributions\nof channels between networks. In particular, the KL divergence enables learning\nto pay more attention to the most salient regions of the channel-wise maps,\npresumably corresponding to the most useful signals for semantic segmentation.\nExperiments demonstrate that our channel-wise distillation outperforms almost\nall existing spatial distillation methods for semantic segmentation\nconsiderably, and requires less computational cost during training. We\nconsistently achieve superior performance on three benchmarks with various\nnetwork structures. Code is available at: https://git.io/Distiller",
        "Gaussian processes are used in machine learning to learn input-output\nmappings from observed data. Gaussian process regression is based on imposing a\nGaussian process prior on the unknown regressor function and statistically\nconditioning it on the observed data. In system identification, Gaussian\nprocesses are used to form time series prediction models such as non-linear\nfinite-impulse response (NFIR) models as well as non-linear autoregressive\n(NARX) models. Gaussian process state-space models (GPSS) can be used to learn\nthe dynamic and measurement models for a state-space representation of the\ninput-output data. Temporal and spatio-temporal Gaussian processes can be\ndirectly used to form regressor on the data in the time domain. The aim of this\narticle is to briefly outline the main directions in system identification\nmethods using Gaussian processes.",
        "Object detection has been a focus of research in human-computer interaction.\nSkin area detection has been a key to different recognitions like face\nrecognition, human motion detection, pornographic and nude image prediction,\netc. Most of the research done in the fields of skin detection has been trained\nand tested on human images of African, Mongolian and Anglo-Saxon ethnic\norigins. Although there are several intensity invariant approaches to skin\ndetection, the skin color of Indian sub-continentals have not been focused\nseparately. The approach of this research is to make a comparative study\nbetween three image segmentation approaches using Indian sub-continental human\nimages, to optimize the detection criteria, and to find some efficient\nparameters to detect the skin area from these images. The experiments observed\nthat HSV color model based approach to Indian sub-continental skin detection is\nmore suitable with considerable success rate of 91.1% true positives and 88.1%\ntrue negatives.",
        "Vehicle location prediction or vehicle tracking is a significant topic within\nconnected vehicles. This task, however, is difficult if only a single modal\ndata is available, probably causing bias and impeding the accuracy. With the\ndevelopment of sensor networks in connected vehicles, multimodal data are\nbecoming accessible. Therefore, we propose a framework for vehicle tracking\nwith multimodal data fusion. Specifically, we fuse the results of two\nmodalities, images and velocity, in our vehicle-tracking task. Images, being\nprocessed in the module of vehicle detection, provide direct information about\nthe features of vehicles, whereas velocity estimation can further evaluate the\npossible location of the target vehicles, which reduces the number of features\nbeing compared, and decreases the time consumption and computational cost.\nVehicle detection is designed with a color-faster R-CNN, which takes both the\nshape and color of the vehicles into consideration. Meanwhile, velocity\nestimation is through the Kalman filter, which is a classical method for\ntracking. Finally, a multimodal data fusion method is applied to integrate\nthese outcomes so that vehicle-tracking tasks can be achieved. Experimental\nresults suggest the efficiency of our methods, which can track vehicles using a\nseries of surveillance cameras in urban areas.",
        "This paper studies the potential of the return distribution for exploration\nin deterministic reinforcement learning (RL) environments. We study network\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\nmixture distributions. Combined with exploration policies that leverage this\nreturn distribution, we solve, for example, a randomized Chain task of length\n100, which has not been reported before when learning with neural networks.",
        "In this paper, a simultaneous localization and mapping (SLAM) method that\neliminates the influence of moving objects in dynamic environments is proposed.\nThis method utilizes the correlation between map points to separate points that\nare part of the static scene and points that are part of different moving\nobjects into different groups. A sparse graph is first created using Delaunay\ntriangulation from all map points. In this graph, the vertices represent map\npoints, and each edge represents the correlation between adjacent points. If\nthe relative position between two points remains consistent over time, there is\ncorrelation between them, and they are considered to be moving together\nrigidly. If not, they are considered to have no correlation and to be in\nseparate groups. After the edges between the uncorrelated points are removed\nduring point-correlation optimization, the remaining graph separates the map\npoints of the moving objects from the map points of the static scene. The\nlargest group is assumed to be the group of reliable static map points.\nFinally, motion estimation is performed using only these points. The proposed\nmethod was implemented for RGB-D sensors, evaluated with a public RGB-D\nbenchmark, and tested in several additional challenging environments. The\nexperimental results demonstrate that robust and accurate performance can be\nachieved by the proposed SLAM method in both slightly and highly dynamic\nenvironments. Compared with other state-of-the-art methods, the proposed method\ncan provide competitive accuracy with good real-time performance.",
        "This work introduces a neuro-symbolic agent that combines deep reinforcement\nlearning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e.,\nnever-seen-before, generalisation of formally specified instructions. In\nparticular, we present a neuro-symbolic framework where a symbolic module\ntransforms TL specifications into a form that helps the training of a DRL agent\ntargeting generalisation, while a neural module learns systematically to solve\nthe given tasks. We study the emergence of systematic learning in different\nsettings and find that the architecture of the convolutional layers is key when\ngeneralising to new instructions. We also provide evidence that systematic\nlearning can emerge with abstract operators such as negation when learning from\na few training examples, which previous research have struggled with.",
        "Synthesized medical images have several important applications, e.g., as an\nintermedium in cross-modality image registration and as supplementary training\nsamples to boost the generalization capability of a classifier. Especially,\nsynthesized computed tomography (CT) data can provide X-ray attenuation map for\nradiation therapy planning. In this work, we propose a generic cross-modality\nsynthesis approach with the following targets: 1) synthesizing realistic\nlooking 3D images using unpaired training data, 2) ensuring consistent\nanatomical structures, which could be changed by geometric distortion in\ncross-modality synthesis and 3) improving volume segmentation by using\nsynthetic data for modalities with limited training samples. We show that these\ngoals can be achieved with an end-to-end 3D convolutional neural network (CNN)\ncomposed of mutually-beneficial generators and segmentors for image synthesis\nand segmentation tasks. The generators are trained with an adversarial loss, a\ncycle-consistency loss, and also a shape-consistency loss, which is supervised\nby segmentors, to reduce the geometric distortion. From the segmentation view,\nthe segmentors are boosted by synthetic data from generators in an online\nmanner. Generators and segmentors prompt each other alternatively in an\nend-to-end training fashion. With extensive experiments on a dataset including\na total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular\nvolumes, we show both tasks are beneficial to each other and coupling these two\ntasks results in better performance than solving them exclusively.",
        "We present an approach to data fusion that combines the interpretability of\nstructured probabilistic graphical models with the flexibility of neural\nnetworks. The proposed method, lightweight data fusion (LDF), emphasizes\nposterior analysis over latent variables using two types of information:\nprimary data, which are well-characterized but with limited availability, and\nauxiliary data, readily available but lacking a well-characterized statistical\nrelationship to the latent quantity of interest. The lack of a forward model\nfor the auxiliary data precludes the use of standard data fusion approaches,\nwhile the inability to acquire latent variable observations severely limits\ndirect application of most supervised learning methods. LDF addresses these\nissues by utilizing neural networks as conjugate mappings of the auxiliary\ndata: nonlinear transformations into sufficient statistics with respect to the\nlatent variables. This facilitates efficient inference by preserving the\nconjugacy properties of the primary data and leads to compact representations\nof the latent variable posterior distributions. We demonstrate the LDF\nmethodology on two challenging inference problems: (1) learning electrification\nrates in Rwanda from satellite imagery, high-level grid infrastructure, and\nother sources; and (2) inferring county-level homicide rates in the USA by\nintegrating socio-economic data using a mixture model of multiple conjugate\nmappings.",
        "Architecture optimization, which is a technique for finding an efficient\nneural network that meets certain requirements, generally reduces to a set of\nmultiple-choice selection problems among alternative sub-structures or\nparameters. The discrete nature of the selection problem, however, makes this\noptimization difficult. To tackle this problem we introduce a novel concept of\na trainable gate function. The trainable gate function, which confers a\ndifferentiable property to discretevalued variables, allows us to directly\noptimize loss functions that include non-differentiable discrete values such as\n0-1 selection. The proposed trainable gate can be applied to pruning. Pruning\ncan be carried out simply by appending the proposed trainable gate functions to\neach intermediate output tensor followed by fine-tuning the overall model,\nusing any gradient-based training methods. So the proposed method can jointly\noptimize the selection of the pruned channels while fine-tuning the weights of\nthe pruned model at the same time. Our experimental results demonstrate that\nthe proposed method efficiently optimizes arbitrary neural networks in various\ntasks such as image classification, style transfer, optical flow estimation,\nand neural machine translation.",
        "Video super-resolution (VSR) aims at restoring a video in low-resolution (LR)\nand improving it to higher-resolution (HR). Due to the characteristics of video\ntasks, it is very important that motion information among frames should be well\nconcerned, summarized and utilized for guidance in a VSR algorithm. Especially,\nwhen a video contains large motion, conventional methods easily bring\nincoherent results or artifacts. In this paper, we propose a novel deep neural\nnetwork with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for\nsuper-resolution of videos with large motion. We design a new module named\nU-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit\nmotion estimation and motion compensation (MEMC) as well as coarse spatial\nfeature extraction. And we present a new Multi-Stage Communicated Upsampling\n(MSCU) module to make full use of the intermediate results of upsampling for\nguiding the VSR. Moreover, a novel dual subnet is devised to aid the training\nof our DSMC, whose dual loss helps to reduce the solution space as well as\nenhance the generalization ability. Our experimental results confirm that our\nmethod achieves superior performance on videos with large motion compared to\nstate-of-the-art methods.",
        "In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.",
        "Recently, logo detection has received more and more attention for its wide\napplications in the multimedia field, such as intellectual property protection,\nproduct brand management, and logo duration monitoring. Unlike general object\ndetection, logo detection is a challenging task, especially for small logo\nobjects and large aspect ratio logo objects in the real-world scenario. In this\npaper, we propose a novel approach, named Discriminative Semantic Feature\nPyramid Network with Guided Anchoring (DSFP-GA), which can address these\nchallenges via aggregating the semantic information and generating different\naspect ratio anchor boxes. More specifically, our approach mainly consists of\nDiscriminative Semantic Feature Pyramid (DSFP) and Guided Anchoring (GA).\nConsidering that low-level feature maps that are used to detect small logo\nobjects lack semantic information, we propose the DSFP, which can enrich more\ndiscriminative semantic features of low-level feature maps and can achieve\nbetter performance on small logo objects. Furthermore, preset anchor boxes are\nless efficient for detecting large aspect ratio logo objects. We therefore\nintegrate the GA into our method to generate large aspect ratio anchor boxes to\nmitigate this issue. Extensive experimental results on four benchmarks\ndemonstrate the effectiveness of our proposed DSFP-GA. Moreover, we further\nconduct visual analysis and ablation studies to illustrate the advantage of our\nmethod in detecting small and large aspect logo objects. The code and models\ncan be found at https://github.com/Zhangbaisong/DSFP-GA.",
        "As in other areas of medical image analysis, e.g. semantic segmentation, deep\nlearning is currently driving the development of new approaches for image\nregistration. Multi-scale encoder-decoder network architectures achieve\nstate-of-the-art accuracy on tasks such as intra-patient alignment of abdominal\nCT or brain MRI registration, especially when additional supervision, such as\nanatomical labels, is available. The success of these methods relies to a large\nextent on the outstanding ability of deep CNNs to extract descriptive visual\nfeatures from the input images. In contrast to conventional methods, the\nexplicit inclusion of geometric information plays only a minor role, if at all.\nIn this work we take a look at an exactly opposite approach by investigating a\ndeep learning framework for registration based solely on geometric features and\noptimisation. We combine graph convolutions with loopy belief message passing\nto enable highly accurate 3D point cloud registration. Our experimental\nvalidation is conducted on complex key-point graphs of inner lung structures,\nstrongly outperforming dense encoder-decoder networks and other point set\nregistration methods. Our code is publicly available at\nhttps://github.com/multimodallearning/deep-geo-reg.",
        "The detection of semantic relationships between objects represented in an\nimage is one of the fundamental challenges in image interpretation.\nNeural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the\ncombination of semantic knowledge representation and reasoning with the ability\nto efficiently learn from examples typical of neural networks. We here propose\nFaster-LTN, an object detector composed of a convolutional backbone and an LTN.\nTo the best of our knowledge, this is the first attempt to combine both\nframeworks in an end-to-end training setting. This architecture is trained by\noptimizing a grounded theory which combines labelled examples with prior\nknowledge, in the form of logical axioms. Experimental comparisons show\ncompetitive performance with respect to the traditional Faster R-CNN\narchitecture.",
        "Transfer learning has emerged as a powerful methodology for adapting\npre-trained deep neural networks on image recognition tasks to new domains.\nThis process consists of taking a neural network pre-trained on a large\nfeature-rich source dataset, freezing the early layers that encode essential\ngeneric image properties, and then fine-tuning the last few layers in order to\ncapture specific information related to the target situation. This approach is\nparticularly useful when only limited or weakly labeled data are available for\nthe new task. In this work, we demonstrate that adversarially-trained models\ntransfer better than non-adversarially-trained models, especially if only\nlimited data are available for the new domain task. Further, we observe that\nadversarial training biases the learnt representations to retaining shapes, as\nopposed to textures, which impacts the transferability of the source models.\nFinally, through the lens of influence functions, we discover that transferred\nadversarially-trained models contain more human-identifiable semantic\ninformation, which explains -- at least partly -- why adversarially-trained\nmodels transfer better.",
        "It is notoriously difficult to control the behavior of reinforcement learning\nagents. Agents often learn to exploit the environment or reward signal and need\nto be retrained multiple times. The multi-objective reinforcement learning\n(MORL) framework separates a reward function into several objectives. An ideal\nMORL agent learns to generalize to novel combinations of objectives allowing\nfor better control of an agent's behavior without requiring retraining. Many\nMORL approaches use a weight vector to parameterize the importance of each\nobjective. However, this approach suffers from lack of expressiveness and\ninterpretability. We propose using propositional logic to specify the\nimportance of multiple objectives. By using a logic where predicates correspond\ndirectly to objectives, specifications are inherently more interpretable.\nAdditionally the set of specifications that can be expressed with formal\nlanguages is a superset of what can be expressed by weight vectors. In this\npaper, we define a formal language based on propositional logic with\nquantitative semantics. We encode logical specifications using a recurrent\nneural network and show that MORL agents parameterized by these encodings are\nable to generalize to novel specifications over objectives and achieve\nperformance comparable to single objective baselines.",
        "Supervised deep learning techniques have achieved great success in various\nfields due to getting rid of the limitation of handcrafted representations.\nHowever, most previous image retargeting algorithms still employ fixed design\nprinciples such as using gradient map or handcrafted features to compute\nsaliency map, which inevitably restricts its generality. Deep learning\ntechniques may help to address this issue, but the challenging problem is that\nwe need to build a large-scale image retargeting dataset for the training of\ndeep retargeting models. However, building such a dataset requires huge human\nefforts.\n  In this paper, we propose a novel deep cyclic image retargeting approach,\ncalled Cycle-IR, to firstly implement image retargeting with a single deep\nmodel, without relying on any explicit user annotations. Our idea is built on\nthe reverse mapping from the retargeted images to the given input images. If\nthe retargeted image has serious distortion or excessive loss of important\nvisual information, the reverse mapping is unlikely to restore the input image\nwell. We constrain this forward-reverse consistency by introducing a cyclic\nperception coherence loss. In addition, we propose a simple yet effective image\nretargeting network (IRNet) to implement the image retargeting process. Our\nIRNet contains a spatial and channel attention layer, which is able to\ndiscriminate visually important regions of input images effectively, especially\nin cluttered images. Given arbitrary sizes of input images and desired aspect\nratios, our Cycle-IR can produce visually pleasing target images directly.\nExtensive experiments on the standard RetargetMe dataset show the superiority\nof our Cycle-IR. In addition, our Cycle-IR outperforms the Multiop method and\nobtains the best result in the user study. Code is available at\nhttps://github.com/mintanwei/Cycle-IR.",
        "Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\nmodel long-range dependencies by stacking layers or enlarging filters.\nTransformers and the self-attention mechanism are recently proposed to\neffectively learn long-range dependencies by modeling all pairs of word-to-word\nattention regardless of their positions. The idea has also been extended to the\ncomputer vision field by creating and treating image patches as embeddings.\nConsidering the computation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that\npotentially loses informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\nthree medical image datasets (gland segmentation, MoNuSeg, and HECKTOR\ndatasets) showed that PMTrans outperformed the latest CNN-based and\ntransformer-based models for medical image segmentation.",
        "Graph neural networks (GNNs) have achieved remarkable success as a framework\nfor deep learning on graph-structured data. However, GNNs are fundamentally\nlimited by their tree-structured inductive bias: the WL-subtree kernel\nformulation bounds the representational capacity of GNNs, and polynomial-time\nGNNs are provably incapable of recognizing triangles in a graph. In this work,\nwe propose to augment the GNN message-passing operations with information\ndefined on ego graphs (i.e., the induced subgraph surrounding each node). We\nterm these approaches Ego-GNNs and show that Ego-GNNs are provably more\npowerful than standard message-passing GNNs. In particular, we show that\nEgo-GNNs are capable of recognizing closed triangles, which is essential given\nthe prominence of transitivity in real-world graphs. We also motivate our\napproach from the perspective of graph signal processing as a form of multiplex\ngraph convolution. Experimental results on node classification using synthetic\nand real data highlight the achievable performance gains using this approach.",
        "There are currently many barriers that prevent non-experts from exploiting\nmachine learning solutions ranging from the lack of intuition on statistical\nlearning techniques to the trickiness of hyperparameter tuning. Such barriers\nhave led to an explosion of interest in automated machine learning (AutoML),\nwhereby an off-the-shelf system can take care of many of the steps for\nend-users without the need for expertise in machine learning. This paper\npresents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the\nresults of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits\nthe diversity of existing AutoML systems by leveraging the differences in their\nmodel search space and heuristics. Empirically, we show that diversity of each\nAutoML system is sufficient to justify ensembling at the AutoML system level.\nIn demonstrating this, we also establish new state-of-the-art AutoML results on\nthe OpenML tabular classification benchmark.",
        "Nowadays, graph-structured data are increasingly used to model complex\nsystems. Meanwhile, detecting anomalies from graph has become a vital research\nproblem of pressing societal concerns. Anomaly detection is an unsupervised\nlearning task of identifying rare data that differ from the majority. As one of\nthe dominant anomaly detection algorithms, One Class Support Vector Machine has\nbeen widely used to detect outliers. However, those traditional anomaly\ndetection methods lost their effectiveness in graph data. Since traditional\nanomaly detection methods are stable, robust and easy to use, it is vitally\nimportant to generalize them to graph data. In this work, we propose One Class\nGraph Neural Network (OCGNN), a one-class classification framework for graph\nanomaly detection. OCGNN is designed to combine the powerful representation\nability of Graph Neural Networks along with the classical one-class objective.\nCompared with other baselines, OCGNN achieves significant improvements in\nextensive experiments.",
        "Local surrogate approaches for explaining machine learning model predictions\nhave appealing properties, such as being model-agnostic and flexible in their\nmodelling. Several methods exist that fit this description and share this goal.\nHowever, despite their shared overall procedure, they set out different\nobjectives, extract different information from the black-box, and consequently\nproduce diverse explanations, that are -- in general -- incomparable. In this\nwork we review the similarities and differences amongst multiple methods, with\na particular focus on what information they extract from the model, as this has\nlarge impact on the output: the explanation. We discuss the implications of the\nlack of agreement, and clarity, amongst the methods' objectives on the research\nand practice of explainability.",
        "There have been many work in the literature on generation of various kinds of\nimages such as Hand-Written characters (MNIST dataset), scene images (CIFAR-10\ndataset), various objects images (ImageNet dataset), road signboard images\n(SVHN dataset) etc. Unfortunately, there have been very limited amount of work\ndone in the domain of document image processing. Automatic image generation can\nlead to the enormous increase of labeled datasets with the help of only limited\namount of labeled data. Various kinds of Deep generative models can be\nprimarily divided into two categories. First category is auto-encoder (AE) and\nthe second one is Generative Adversarial Networks (GANs). In this paper, we\nhave evaluated various kinds of AE as well as GANs and have compared their\nperformances on hand-written digits dataset (MNIST) and also on historical\nhand-written character dataset of Indonesian BALI language. Moreover, these\ngenerated characters are recognized by using character recognition tool for\ncalculating the statistical performance of these generated characters with\nrespect to original character images.",
        "Airbags are subject to strict quality control in order to ensure passengers\nsafety. The quality of fabric and sewing thread influence the final product and\ntherefore, sewing defects must be early and accurately detected, in order to\nremove the item from production. Airbag seams assembly can take various forms,\nusing linear and circle primitives, with threads of different colors and length\ndensities, creating lockstitch or double threads chainstitch. The paper\npresents a framework for the automatic detection of defects occurring during\nthe airbag sewing stage. Types of defects as skipped stitch, missed stitch or\nsuperimposed seam for lockstitch and two threads chainstitch are detected and\nmarked. Using image processing methods, the proposed framework follows the\nseams path and determines if a color pattern of the considered stitches is\nvalid.",
        "We introduce Contrastive Multivariate Singular Spectrum Analysis, a novel\nunsupervised method for dimensionality reduction and signal decomposition of\ntime series data. By utilizing an appropriate background dataset, the method\ntransforms a target time series dataset in a way that evinces the sub-signals\nthat are enhanced in the target dataset, as opposed to only those that account\nfor the greatest variance. This shifts the goal from finding signals that\nexplain the most variance to signals that matter the most to the analyst. We\ndemonstrate our method on an illustrative synthetic example, as well as show\nthe utility of our method in the downstream clustering of electrocardiogram\nsignals from the public MHEALTH dataset.",
        "The goal of this work is to recognise phrases and sentences being spoken by a\ntalking face, with or without the audio. Unlike previous works that have\nfocussed on recognising a limited number of words or phrases, we tackle lip\nreading as an open-world problem - unconstrained natural language sentences,\nand in the wild videos.\n  Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS)\nnetwork that learns to transcribe videos of mouth motion to characters; (2) a\ncurriculum learning strategy to accelerate training and to reduce overfitting;\n(3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition,\nconsisting of over 100,000 natural sentences from British television.\n  The WLAS model trained on the LRS dataset surpasses the performance of all\nprevious work on standard lip reading benchmark datasets, often by a\nsignificant margin. This lip reading performance beats a professional lip\nreader on videos from BBC television, and we also demonstrate that visual\ninformation helps to improve speech recognition performance even when the audio\nis available.",
        "A central challenge in reinforcement learning is discovering effective\npolicies for tasks where rewards are sparsely distributed. We postulate that in\nthe absence of useful reward signals, an effective exploration strategy should\nseek out {\\it decision states}. These states lie at critical junctions in the\nstate space from where the agent can transition to new, potentially unexplored\nregions. We propose to learn about decision states from prior experience. By\ntraining a goal-conditioned policy with an information bottleneck, we can\nidentify decision states by examining where the model actually leverages the\ngoal state. We find that this simple mechanism effectively identifies decision\nstates, even in partially observed settings. In effect, the model learns the\nsensory cues that correlate with potential subgoals. In new environments, this\nmodel can then identify novel subgoals for further exploration, guiding the\nagent through a sequence of potential decision states and through new regions\nof the state space.",
        "A common dilemma in 3D object detection for autonomous driving is that\nhigh-quality, dense point clouds are only available during training, but not\ntesting. We use knowledge distillation to bridge the gap between a model\ntrained on high-quality inputs at training time and another tested on\nlow-quality inputs at inference time. In particular, we design a two-stage\ntraining pipeline for point cloud object detection. First, we train an object\ndetection model on dense point clouds, which are generated from multiple frames\nusing extra information only available at training time. Then, we train the\nmodel's identical counterpart on sparse single-frame point clouds with\nconsistency regularization on features from both models. We show that this\nprocedure improves performance on low-quality data during testing, without\nadditional overhead.",
        "This paper constructs a novel intelligent medical diagnosis system, which can\nrealize automatic communication and breast cancer pathological image\nrecognition. This system contains two main parts, including a pre-training\nchatbot called M-Chatbot and an improved neural network model of\nEfficientNetV2-S named EfficientNetV2-SA, in which the activation function in\ntop layers is replaced by ACON-C. Using information retrieval mechanism,\nM-Chatbot instructs patients to send breast pathological image to\nEfficientNetV2-SA network, and then the classifier trained by transfer learning\nwill return the diagnosis results. We verify the performance of our chatbot and\nclassification on the extrinsic metrics and BreaKHis dataset, respectively. The\ntask completion rate of M-Chatbot reached 63.33\\%. For the BreaKHis dataset,\nthe highest accuracy of EfficientNetV2-SA network have achieved 84.71\\%. All\nthese experimental results illustrate that the proposed model can improve the\naccuracy performance of image recognition and our new intelligent medical\ndiagnosis system is successful and efficient in providing automatic diagnosis\nof breast cancer.",
        "Persistence diagrams concisely represent the topology of a point cloud whilst\nhaving strong theoretical guarantees, but the question of how to best integrate\nthis information into machine learning workflows remains open. In this paper we\nextend the ubiquitous Fuzzy c-Means (FCM) clustering algorithm to the space of\npersistence diagrams, enabling unsupervised learning that automatically\ncaptures the topological structure of data without the topological prior\nknowledge or additional processing of persistence diagrams that many other\ntechniques require. We give theoretical convergence guarantees that correspond\nto the Euclidean case, and empirically demonstrate the capability of our\nalgorithm to capture topological information via the fuzzy RAND index. We end\nwith experiments on two datasets that utilise both the topological and fuzzy\nnature of our algorithm: pre-trained model selection in machine learning and\nlattices structures from materials science. As pre-trained models can perform\nwell on multiple tasks, selecting the best model is a naturally fuzzy problem;\nwe show that fuzzy clustering persistence diagrams allows for model selection\nusing the topology of decision boundaries. In materials science, we classify\ntransformed lattice structure datasets for the first time, whilst the\nprobabilistic membership values let us rank candidate lattices in a scenario\nwhere further investigation requires expensive laboratory time and expertise.",
        "Multi-domain image-to-image translation has gained increasing attention\nrecently. Previous methods take an image and some target attributes as inputs\nand generate an output image with the desired attributes. However, such methods\nhave two limitations. First, these methods assume binary-valued attributes and\nthus cannot yield satisfactory results for fine-grained control. Second, these\nmethods require specifying the entire set of target attributes, even if most of\nthe attributes would not be changed. To address these limitations, we propose\nRelGAN, a new method for multi-domain image-to-image translation. The key idea\nis to use relative attributes, which describes the desired change on selected\nattributes. Our method is capable of modifying images by changing particular\nattributes of interest in a continuous manner while preserving the other\nattributes. Experimental results demonstrate both the quantitative and\nqualitative effectiveness of our method on the tasks of facial attribute\ntransfer and interpolation.",
        "Recent advances in deep learning have achieved promising performance for\nmedical image analysis, while in most cases ground-truth annotations from human\nexperts are necessary to train the deep model. In practice, such annotations\nare expensive to collect and can be scarce for medical imaging applications.\nTherefore, there is significant interest in learning representations from\nunlabelled raw data. In this paper, we propose a self-supervised learning\napproach to learn meaningful and transferable representations from medical\nimaging video without any type of human annotation. We assume that in order to\nlearn such a representation, the model should identify anatomical structures\nfrom the unlabelled data. Therefore we force the model to address anatomy-aware\ntasks with free supervision from the data itself. Specifically, the model is\ndesigned to correct the order of a reshuffled video clip and at the same time\npredict the geometric transformation applied to the video clip. Experiments on\nfetal ultrasound video show that the proposed approach can effectively learn\nmeaningful and strong representations, which transfer well to downstream tasks\nlike standard plane detection and saliency prediction.",
        "Referring expression comprehension (REC) aims to localize a target object in\nan image described by a referring expression phrased in natural language.\nDifferent from the object detection task that queried object labels have been\npre-defined, the REC problem only can observe the queries during the test. It\nthus more challenging than a conventional computer vision problem. This task\nhas attracted a lot of attention from both computer vision and natural language\nprocessing community, and several lines of work have been proposed, from\nCNN-RNN model, modular network to complex graph-based model. In this survey, we\nfirst examine the state of the art by comparing modern approaches to the\nproblem. We classify methods by their mechanism to encode the visual and\ntextual modalities. In particular, we examine the common approach of joint\nembedding images and expressions to a common feature space. We also discuss\nmodular architectures and graph-based models that interface with structured\ngraph representation. In the second part of this survey, we review the datasets\navailable for training and evaluating REC systems. We then group results\naccording to the datasets, backbone models, settings so that they can be fairly\ncompared. Finally, we discuss promising future directions for the field, in\nparticular the compositional referring expression comprehension that requires\nlonger reasoning chain to address.",
        "Pose-guided person image generation and animation aim to transform a source\nperson image to target poses. These tasks require spatial manipulation of\nsource data. However, Convolutional Neural Networks are limited by the lack of\nability to spatially transform the inputs. In this paper, we propose a\ndifferentiable global-flow local-attention framework to reassemble the inputs\nat the feature level. This framework first estimates global flow fields between\nsources and targets. Then, corresponding local source feature patches are\nsampled with content-aware local attention coefficients. We show that our\nframework can spatially transform the inputs in an efficient manner. Meanwhile,\nwe further model the temporal consistency for the person image animation task\nto generate coherent videos. The experiment results of both image generation\nand animation tasks demonstrate the superiority of our model. Besides,\nadditional results of novel view synthesis and face image animation show that\nour model is applicable to other tasks requiring spatial transformation. The\nsource code of our project is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.",
        "Attention mechanisms are developing into a viable alternative to\nconvolutional layers as elementary building block of NNs. Their main advantage\nis that they are not restricted to capture local dependencies in the input, but\ncan draw arbitrary connections. This unprecedented capability coincides with\nthe long-standing problem of modeling global atomic interactions in molecular\nforce fields and other many-body problems. In its original formulation,\nhowever, attention is not applicable to the continuous domains in which the\natoms live. For this purpose we propose a variant to describe geometric\nrelations for arbitrary atomic configurations in Euclidean space that also\nrespects all relevant physical symmetries. We furthermore demonstrate, how the\nsuccessive application of our learned attention matrices effectively translates\nthe molecular geometry into a set of individual atomic contributions\non-the-fly.",
        "Contrastive learning has delivered impressive results in many audio-visual\nrepresentation learning scenarios. However, existing approaches optimize for\nlearning either \\textit{global} representations useful for tasks such as\nclassification, or \\textit{local} representations useful for tasks such as\naudio-visual source localization and separation. While they produce\nsatisfactory results in their intended downstream scenarios, they often fail to\ngeneralize to tasks that they were not originally designed for. In this work,\nwe propose a versatile self-supervised approach to learn audio-visual\nrepresentations that generalize to both the tasks which require global semantic\ninformation (e.g., classification) and the tasks that require fine-grained\nspatio-temporal information (e.g. localization). We achieve this by optimizing\ntwo cross-modal contrastive objectives that together encourage our model to\nlearn discriminative global-local visual information given audio signals. To\nshow that our approach learns generalizable video representations, we evaluate\nit on various downstream scenarios including action/sound classification, lip\nreading, deepfake detection, and sound source localization.",
        "We propose Deep Hierarchical Machine (DHM), a model inspired from the\ndivide-and-conquer strategy while emphasizing representation learning ability\nand flexibility. A stochastic routing framework as used by recent deep neural\ndecision/regression forests is incorporated, but we remove the need to evaluate\nunnecessary computation paths by utilizing a different topology and introducing\na probabilistic pruning technique. We also show a specified version of DHM\n(DSHM) for efficiency, which inherits the sparse feature extraction process as\nin traditional decision tree with pixel-difference feature. To achieve sparse\nfeature extraction, we propose to utilize sparse convolution operation in DSHM\nand show one possibility of introducing sparse convolution kernels by using\nlocal binary convolution layer. DHM can be applied to both classification and\nregression problems, and we validate it on standard image classification and\nface alignment tasks to show its advantages over past architectures.",
        "The technological and scientific challenges involved in the development of\nautonomous vehicles (AVs) are currently of primary interest for many automobile\ncompanies and research labs. However, human-controlled vehicles are likely to\nremain on the roads for several decades to come and may share with AVs the\ntraffic environments of the future. In such mixed environments, AVs should\ndeploy human-like driving policies and negotiation skills to enable smooth\ntraffic flow. To generate automated human-like driving policies, we introduce a\nmodel-free, deep reinforcement learning approach to imitate an experienced\nhuman driver's behavior. We study a static obstacle avoidance task on a\ntwo-lane highway road in simulation (Unity). Our control algorithm receives a\nstochastic feedback signal from two sources: a model-driven part, encoding\nsimple driving rules, such as lane-keeping and speed control, and a stochastic,\ndata-driven part, incorporating human expert knowledge from driving data. To\nassess the similarity between machine and human driving, we model distributions\nof track position and speed as Gaussian processes. We demonstrate that our\napproach leads to human-like driving policies.",
        "Blind people face a lot of problems in their daily routines. They have to\nstruggle a lot just to do their day-to-day chores. In this paper, we have\nproposed a system with the objective to help the visually impaired by providing\naudio aid guiding them to avoid obstacles, which will assist them to move in\ntheir surroundings. Object Detection using YOLO will help them detect the\nnearby objects and Depth Estimation using monocular vision will tell the\napproximate distance of the detected objects from the user. Despite a higher\naccuracy, stereo vision has many hardware constraints, which makes monocular\nvision the preferred choice for this application.",
        "The present paper shows a solution to the problem of automatic distress\ndetection, more precisely the detection of holes in paved roads. To do so, the\nproposed solution uses a weightless neural network known as Wisard to decide\nwhether an image of a road has any kind of cracks. In addition, the proposed\narchitecture also shows how the use of transfer learning was able to improve\nthe overall accuracy of the decision system. As a verification step of the\nresearch, an experiment was carried out using images from the streets at the\nFederal University of Tocantins, Brazil. The architecture of the developed\nsolution presents a result of 85.71% accuracy in the dataset, proving to be\nsuperior to approaches of the state-of-the-art.",
        "Since the inception of Deep Reinforcement Learning (DRL) algorithms, there\nhas been a growing interest in both research and industrial communities in the\npromising potentials of this paradigm. The list of current and envisioned\napplications of deep RL ranges from autonomous navigation and robotics to\ncontrol applications in the critical infrastructure, air traffic control,\ndefense technologies, and cybersecurity. While the landscape of opportunities\nand the advantages of deep RL algorithms are justifiably vast, the security\nrisks and issues in such algorithms remain largely unexplored. To facilitate\nand motivate further research on these critical challenges, this paper presents\na foundational treatment of the security problem in DRL. We formulate the\nsecurity requirements of DRL, and provide a high-level threat model through the\nclassification and identification of vulnerabilities, attack vectors, and\nadversarial capabilities. Furthermore, we present a review of current\nliterature on security of deep RL from both offensive and defensive\nperspectives. Lastly, we enumerate critical research venues and open problems\nin mitigation and prevention of intentional attacks against deep RL as a\nroadmap for further research in this area.",
        "When an agent cannot represent a perfectly accurate model of its\nenvironment's dynamics, model-based reinforcement learning (MBRL) can fail\ncatastrophically. Planning involves composing the predictions of the model;\nwhen flawed predictions are composed, even minor errors can compound and render\nthe model useless for planning. Hallucinated Replay (Talvitie 2014) trains the\nmodel to \"correct\" itself when it produces errors, substantially improving MBRL\nwith flawed models. This paper theoretically analyzes this approach,\nilluminates settings in which it is likely to be effective or ineffective, and\npresents a novel error bound, showing that a model's ability to self-correct is\nmore tightly related to MBRL performance than one-step prediction error. These\nresults inspire an MBRL algorithm for deterministic MDPs with performance\nguarantees that are robust to model class limitations.",
        "Depth estimation and 3D object detection are critical for scene understanding\nbut remain challenging to perform with a single image due to the loss of 3D\ninformation during image capture. Recent models using deep neural networks have\nimproved monocular depth estimation performance, but there is still difficulty\nin predicting absolute depth and generalizing outside a standard dataset. Here\nwe introduce the paradigm of deep optics, i.e. end-to-end design of optics and\nimage processing, to the monocular depth estimation problem, using coded\ndefocus blur as an additional depth cue to be decoded by a neural network. We\nevaluate several optical coding strategies along with an end-to-end\noptimization scheme for depth estimation on three datasets, including NYU Depth\nv2 and KITTI. We find an optimized freeform lens design yields the best\nresults, but chromatic aberration from a singlet lens offers significantly\nimproved performance as well. We build a physical prototype and validate that\nchromatic aberrations improve depth estimation on real-world results. In\naddition, we train object detection networks on the KITTI dataset and show that\nthe lens optimized for depth estimation also results in improved 3D object\ndetection performance.",
        "The study of multi-type Protein-Protein Interaction (PPI) is fundamental for\nunderstanding biological processes from a systematic perspective and revealing\ndisease mechanisms. Existing methods suffer from significant performance\ndegradation when tested in unseen dataset. In this paper, we investigate the\nproblem and find that it is mainly attributed to the poor performance for\ninter-novel-protein interaction prediction. However, current evaluations\noverlook the inter-novel-protein interactions, and thus fail to give an\ninstructive assessment. As a result, we propose to address the problem from\nboth the evaluation and the methodology. Firstly, we design a new evaluation\nframework that fully respects the inter-novel-protein interactions and gives\nconsistent assessment across datasets. Secondly, we argue that correlations\nbetween proteins must provide useful information for analysis of novel\nproteins, and based on this, we propose a graph neural network based method\n(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental\nresults on real-world datasets of different scales demonstrate that GNN-PPI\nsignificantly outperforms state-of-the-art PPI prediction methods, especially\nfor the inter-novel-protein interaction prediction.",
        "Learning to generate 3D point clouds without 3D supervision is an important\nbut challenging problem. Current solutions leverage various differentiable\nrenderers to project the generated 3D point clouds onto a 2D image plane, and\ntrain deep neural networks using the per-pixel difference with 2D ground truth\nimages. However, these solutions are still struggling to fully recover fine\nstructures of 3D shapes, such as thin tubes or planes. To resolve this issue,\nwe propose an unsupervised approach for 3D point cloud generation with fine\nstructures. Specifically, we cast 3D point cloud learning as a 2D projection\nmatching problem. Rather than using entire 2D silhouette images as a regular\npixel supervision, we introduce structure adaptive sampling to randomly sample\n2D points within the silhouettes as an irregular point supervision, which\nalleviates the consistency issue of sampling from different view angles. Our\nmethod pushes the neural network to generate a 3D point cloud whose 2D\nprojections match the irregular point supervision from different view angles.\nOur 2D projection matching approach enables the neural network to learn more\naccurate structure information than using the per-pixel difference, especially\nfor fine and thin 3D structures. Our method can recover fine 3D structures from\n2D silhouette images at different resolutions, and is robust to different\nsampling methods and point number in irregular point supervision. Our method\noutperforms others under widely used benchmarks. Our code, data and models are\navailable at https://github.com/chenchao15/2D\\_projection\\_matching.",
        "The convergence of many reinforcement learning (RL) algorithms with linear\nfunction approximation has been investigated extensively but most proofs assume\nthat these methods converge to a unique solution. In this paper, we provide a\ncomplete characterization of non-uniqueness issues for a large class of\nreinforcement learning algorithms, simultaneously unifying many\ncounter-examples to convergence in a theoretical framework. We achieve this by\nproving a new condition on features that can determine whether the convergence\nassumptions are valid or non-uniqueness holds. We consider a general class of\nRL methods, which we call natural algorithms, whose solutions are characterized\nas the fixed point of a projected Bellman equation (when it exists); notably,\nbootstrapped temporal difference-based methods such as $TD(\\lambda)$ and\n$GTD(\\lambda)$ are natural algorithms. Our main result proves that natural\nalgorithms converge to the correct solution if and only if all the value\nfunctions in the approximation space satisfy a certain shape. This implies that\nnatural algorithms are, in general, inherently prone to converge to the wrong\nsolution for most feature choices even if the value function can be represented\nexactly. Given our results, we show that state aggregation based features are a\nsafe choice for natural algorithms and we also provide a condition for finding\nconvergent algorithms under other feature constructions.",
        "How do we formalize the challenge of credit assignment in reinforcement\nlearning? Common intuition would draw attention to reward sparsity as a key\ncontributor to difficult credit assignment and traditional heuristics would\nlook to temporal recency for the solution, calling upon the classic eligibility\ntrace. We posit that it is not the sparsity of the reward itself that causes\ndifficulty in credit assignment, but rather the \\emph{information sparsity}. We\npropose to use information theory to define this notion, which we then use to\ncharacterize when credit assignment is an obstacle to efficient learning. With\nthis perspective, we outline several information-theoretic mechanisms for\nmeasuring credit under a fixed behavior policy, highlighting the potential of\ninformation theory as a key tool towards provably-efficient credit assignment.",
        "Hidden Markov Models (HMMs) comprise a powerful generative approach for\nmodeling sequential data and time-series in general. However, the commonly\nemployed assumption of the dependence of the current time frame to a single or\nmultiple immediately preceding frames is unrealistic; more complicated dynamics\npotentially exist in real world scenarios. This paper revisits conventional\nsequential modeling approaches, aiming to address the problem of capturing\ntime-varying temporal dependency patterns. To this end, we propose a different\nformulation of HMMs, whereby the dependence on past frames is dynamically\ninferred from the data. Specifically, we introduce a hierarchical extension by\npostulating an additional latent variable layer; therein, the (time-varying)\ntemporal dependence patterns are treated as latent variables over which\ninference is performed. We leverage solid arguments from the Variational Bayes\nframework and derive a tractable inference algorithm based on the\nforward-backward algorithm. As we experimentally show, our approach can model\nhighly complex sequential data and can effectively handle data with missing\nvalues.",
        "Contrastive learning has been adopted as a core method for unsupervised\nvisual representation learning. Without human annotation, the common practice\nis to perform an instance discrimination task: Given a query image crop, this\ntask labels crops from the same image as positives, and crops from other\nrandomly sampled images as negatives. An important limitation of this label\nassignment strategy is that it can not reflect the heterogeneous similarity\nbetween the query crop and each crop from other images, taking them as equally\nnegative, while some of them may even belong to the same semantic class as the\nquery. To address this issue, inspired by consistency regularization in\nsemi-supervised learning on unlabeled data, we propose Consistent Contrast\n(CO2), which introduces a consistency regularization term into the current\ncontrastive learning framework. Regarding the similarity of the query crop to\neach crop from other images as \"unlabeled\", the consistency term takes the\ncorresponding similarity of a positive crop as a pseudo label, and encourages\nconsistency between these two similarities. Empirically, CO2 improves Momentum\nContrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and\n1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also\ntransfers to image classification, object detection, and semantic segmentation\non PASCAL VOC. This shows that CO2 learns better visual representations for\nthese downstream tasks.",
        "Most domain adaptation methods consider the problem of transferring knowledge\nto the target domain from a single source dataset. However, in practical\napplications, we typically have access to multiple sources. In this paper we\npropose the first approach for Multi-Source Domain Adaptation (MSDA) based on\nGenerative Adversarial Networks. Our method is inspired by the observation that\nthe appearance of a given image depends on three factors: the domain, the style\n(characterized in terms of low-level features variations) and the content. For\nthis reason we propose to project the image features onto a space where only\nthe dependence from the content is kept, and then re-project this invariant\nrepresentation onto the pixel space using the target domain and style. In this\nway, new labeled images can be generated which are used to train a final target\nclassifier. We test our approach using common MSDA benchmarks, showing that it\noutperforms state-of-the-art methods.",
        "Understanding patients' journeys in healthcare system is a fundamental\nprepositive task for a broad range of AI-based healthcare applications. This\ntask aims to learn an informative representation that can comprehensively\nencode hidden dependencies among medical events and its inner entities, and\nthen the use of encoding outputs can greatly benefit the downstream\napplication-driven tasks. A patient journey is a sequence of electronic health\nrecords (EHRs) over time that is organized at multiple levels: patient, visits\nand medical codes. The key challenge of patient journey understanding is to\ndesign an effective encoding mechanism which can properly tackle the\naforementioned multi-level structured patient journey data with temporal\nsequential visits and a set of medical codes. This paper proposes a novel\nself-attention mechanism that can simultaneously capture the contextual and\ntemporal relationships hidden in patient journeys. A multi-level self-attention\nnetwork (MusaNet) is specifically designed to learn the representations of\npatient journeys that is used to be a long sequence of activities. The MusaNet\nis trained in end-to-end manner using the training data derived from EHRs. We\nevaluated the efficacy of our method on two medical application tasks with\nreal-world benchmark datasets. The results have demonstrated the proposed\nMusaNet produces higher-quality representations than state-of-the-art baseline\nmethods. The source code is available in https://github.com/xueping/MusaNet.",
        "Depth Completion deals with the problem of converting a sparse depth map to a\ndense one, given the corresponding color image. Convolutional spatial\npropagation network (CSPN) is one of the state-of-the-art (SoTA) methods of\ndepth completion, which recovers structural details of the scene. In this\npaper, we propose CSPN++, which further improves its effectiveness and\nefficiency by learning adaptive convolutional kernel sizes and the number of\niterations for the propagation, thus the context and computational resources\nneeded at each pixel could be dynamically assigned upon requests. Specifically,\nwe formulate the learning of the two hyper-parameters as an architecture\nselection problem where various configurations of kernel sizes and numbers of\niterations are first defined, and then a set of soft weighting parameters are\ntrained to either properly assemble or select from the pre-defined\nconfigurations at each pixel. In our experiments, we find weighted assembling\ncan lead to significant accuracy improvements, which we referred to as\n\"context-aware CSPN\", while weighted selection, \"resource-aware CSPN\" can\nreduce the computational resource significantly with similar or better\naccuracy. Besides, the resource needed for CSPN++ can be adjusted w.r.t. the\ncomputational budget automatically. Finally, to avoid the side effects of noise\nor inaccurate sparse depths, we embed a gated network inside CSPN++, which\nfurther improves the performance. We demonstrate the effectiveness of CSPN++on\nthe KITTI depth completion benchmark, where it significantly improves over CSPN\nand other SoTA methods.",
        "The need for reliable systems to determine fingerprint presentation attacks\ngrows with the rising use of the fingerprint for authentication. This work\npresents a new approach to single-class classification for software-based\nfingerprint presentation attach detection. The described method utilizes a\nWasserstein GAN to apply transfer learning to a deep convolutional autoencoder.\nBy doing so, the autoencoder could be pretrained and finetuned on the\nLivDet2021 Dermalog sensor dataset with only 1122 bona fide training samples.\nWithout making use of any presentation attack samples, the model could archive\nan average classification error rate of 16.79%. The Wasserstein GAN implemented\nto pretrain the autoencoders weights can further be used to generate\nrealistic-looking artificial fingerprint patches. Extensive testing of\ndifferent autoencoder architectures and hyperparameters led to coarse\narchitectural guidelines as well as multiple implementations which can be\nutilized for future work.",
        "Learning embedding spaces of suitable geometry is critical for representation\nlearning. In order for learned representations to be effective and efficient,\nit is ideal that the geometric inductive bias aligns well with the underlying\nstructure of the data. In this paper, we propose Switch Spaces, a data-driven\napproach for learning representations in product space. Specifically, product\nspaces (or manifolds) are spaces of mixed curvature, i.e., a combination of\nmultiple euclidean and non-euclidean (hyperbolic, spherical) manifolds. To this\nend, we introduce sparse gating mechanisms that learn to choose, combine and\nswitch spaces, allowing them to be switchable depending on the input data with\nspecialization. Additionally, the proposed method is also efficient and has a\nconstant computational complexity regardless of the model size. Experiments on\nknowledge graph completion and item recommendations show that the proposed\nswitch space achieves new state-of-the-art performances, outperforming pure\nproduct spaces and recently proposed task-specific models.",
        "In most real world scenarios, a policy trained by reinforcement learning in\none environment needs to be deployed in another, potentially quite different\nenvironment. However, generalization across different environments is known to\nbe hard. A natural solution would be to keep training after deployment in the\nnew environment, but this cannot be done if the new environment offers no\nreward signal. Our work explores the use of self-supervision to allow the\npolicy to continue training after deployment without using any rewards. While\nprevious methods explicitly anticipate changes in the new environment, we\nassume no prior knowledge of those changes yet still obtain significant\nimprovements. Empirical evaluations are performed on diverse simulation\nenvironments from DeepMind Control suite and ViZDoom, as well as real robotic\nmanipulation tasks in continuously changing environments, taking observations\nfrom an uncalibrated camera. Our method improves generalization in 31 out of 36\nenvironments across various tasks and outperforms domain randomization on a\nmajority of environments.",
        "The prevalence of digital sensors, such as digital cameras and mobile phones,\nsimplifies the acquisition of photos. Digital sensors, however, suffer from\nproducing Moire when photographing objects having complex textures, which\ndeteriorates the quality of photos. Moire spreads across various frequency\nbands of images and is a dynamic texture with varying colors and shapes, which\npose two main challenges in demoireing---an important task in image\nrestoration. In this paper, towards addressing the first challenge, we design a\nmulti-scale network to process images at different spatial resolutions,\nobtaining features in different frequency bands, and thus our method can\njointly remove moire in different frequency bands. Towards solving the second\nchallenge, we propose a dynamic feature encoding module (DFE), embedded in each\nscale, for dynamic texture. Moire pattern can be eliminated more effectively\nvia DFE.Our proposed method, termed Multi-scale convolutional network with\nDynamic feature encoding for image DeMoireing (MDDM), can outperform the state\nof the arts in fidelity as well as perceptual on benchmarks.",
        "Plant root research can provide a way to attain stress-tolerant crops that\nproduce greater yield in a diverse array of conditions. Phenotyping roots in\nsoil is often challenging due to the roots being difficult to access and the\nuse of time consuming manual methods. Rhizotrons allow visual inspection of\nroot growth through transparent surfaces. Agronomists currently manually label\nphotographs of roots obtained from rhizotrons using a line-intersect method to\nobtain root length density and rooting depth measurements which are essential\nfor their experiments. We investigate the effectiveness of an automated image\nsegmentation method based on the U-Net Convolutional Neural Network (CNN)\narchitecture to enable such measurements. We design a data-set of 50 annotated\nChicory (Cichorium intybus L.) root images which we use to train, validate and\ntest the system and compare against a baseline built using the Frangi\nvesselness filter. We obtain metrics using manual annotations and\nline-intersect counts. Our results on the held out data show our proposed\nautomated segmentation system to be a viable solution for detecting and\nquantifying roots. We evaluate our system using 867 images for which we have\nobtained line-intersect counts, attaining a Spearman rank correlation of 0.9748\nand an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the\nautomated segmentation to the manual annotations, with our automated\nsegmentation system producing segmentations with higher quality than the manual\nannotations for large portions of the image.",
        "We propose a novel procedure which adds \"content-addressability\" to any given\nunconditional implicit model e.g., a generative adversarial network (GAN). The\nprocedure allows users to control the generative process by specifying a set\n(arbitrary size) of desired examples based on which similar samples are\ngenerated from the model. The proposed approach, based on kernel mean matching,\nis applicable to any generative models which transform latent vectors to\nsamples, and does not require retraining of the model. Experiments on various\nhigh-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge,\ntower) show that our approach is able to generate images which are consistent\nwith the input set, while retaining the image quality of the original model. To\nour knowledge, this is the first work that attempts to construct, at test time,\na content-addressable generative model from a trained marginal model.",
        "Weakly supervised object detection (WSOD) using only image-level annotations\nhas attracted a growing attention over the past few years. Whereas such task is\ntypically addressed with a domain-specific solution focused on natural images,\nwe show that a simple multiple instance approach applied on pre-trained deep\nfeatures yields excellent performances on non-photographic datasets, possibly\nincluding new classes. The approach does not include any fine-tuning or\ncross-domain learning and is therefore efficient and possibly applicable to\narbitrary datasets and classes. We investigate several flavors of the proposed\napproach, some including multi-layers perceptron and polyhedral classifiers.\nDespite its simplicity, our method shows competitive results on a range of\npublicly available datasets, including paintings (People-Art, IconArt),\nwatercolors, cliparts and comics and allows to quickly learn unseen visual\ncategories.",
        "In this work we show how we can build a technology platform for cognitive\nimaging sensors using recent advances in recurrent neural network architectures\nand training methods inspired from biology. We demonstrate learning and\nprocessing tasks specific to imaging sensors, including enhancement of\nsensitivity and signal-to-noise ratio (SNR) purely through neural filtering\nbeyond the fundamental limits sensor materials, and inferencing and\nspatio-temporal pattern recognition capabilities of these networks with\napplications in object detection, motion tracking and prediction. We then show\ndesigns of unit hardware cells built using complementary metal-oxide\nsemiconductor (CMOS) and emerging materials technologies for ultra-compact and\nenergy-efficient embedded neural processors for smart cameras.",
        "The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($>10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.",
        "Inspired by the fact that spreading and collecting information through the\nInternet becomes the norm, more and more people choose to post for-profit\ncontents (images and texts) in social networks. Due to the difficulty of\nnetwork censors, malicious marketing may be capable of harming society.\nTherefore, it is meaningful to detect marketing intentions online\nautomatically. However, gaps between multimodal data make it difficult to fuse\nimages and texts for content marketing detection. To this end, this paper\nproposes Two-Branch Collaborative Graph Neural Networks to collaboratively\nrepresent multimodal data by Graph Convolution Networks (GCNs) in an end-to-end\nfashion. Experimental results demonstrate that our proposed method achieves\nsuperior graph classification performance for marketing intention detection.",
        "In most of computer vision applications, motion blur is regarded as an\nundesirable artifact. However, it has been shown that motion blur in an image\nmay have practical interests in fundamental computer vision problems. In this\nwork, we propose a novel framework to estimate optical flow from a single\nmotion-blurred image in an end-to-end manner. We design our network with\ntransformer networks to learn globally and locally varying motions from encoded\nfeatures of a motion-blurred input, and decode left and right frame features\nwithout explicit frame supervision. A flow estimator network is then used to\nestimate optical flow from the decoded features in a coarse-to-fine manner. We\nqualitatively and quantitatively evaluate our model through a large set of\nexperiments on synthetic and real motion-blur datasets. We also provide\nin-depth analysis of our model in connection with related approaches to\nhighlight the effectiveness and favorability of our approach. Furthermore, we\nshowcase the applicability of the flow estimated by our method on deblurring\nand moving object segmentation tasks.",
        "Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.",
        "Vision-Language Pre-training (VLP) aims to learn multi-modal representations\nfrom image-text pairs and serves for downstream vision-language tasks in a\nfine-tuning fashion. The dominant VLP models adopt a CNN-Transformer\narchitecture, which embeds images with a CNN, and then aligns images and text\nwith a Transformer. Visual relationship between visual contents plays an\nimportant role in image understanding and is the basic for inter-modal\nalignment learning. However, CNNs have limitations in visual relation learning\ndue to local receptive field's weakness in modeling long-range dependencies.\nThus the two objectives of learning visual relation and inter-modal alignment\nare encapsulated in the same Transformer network. Such design might restrict\nthe inter-modal alignment learning in the Transformer by ignoring the\nspecialized characteristic of each objective. To tackle this, we propose a\nfully Transformer visual embedding for VLP to better learn visual relation and\nfurther promote inter-modal alignment. Specifically, we propose a metric named\nInter-Modality Flow (IMF) to measure the interaction between vision and\nlanguage modalities (i.e., inter-modality). We also design a novel masking\noptimization mechanism named Masked Feature Regression (MFR) in Transformer to\nfurther promote the inter-modality learning. To the best of our knowledge, this\nis the first study to explore the benefit of Transformer for visual feature\nlearning in VLP. We verify our method on a wide range of vision-language tasks,\nincluding Image-Text Retrieval, Visual Question Answering (VQA), Visual\nEntailment and Visual Reasoning. Our approach not only outperforms the\nstate-of-the-art VLP performance, but also shows benefits on the IMF metric.",
        "This paper describes the AVA-Kinetics localized human actions video dataset.\nThe dataset is collected by annotating videos from the Kinetics-700 dataset\nusing the AVA annotation protocol, and extending the original AVA dataset with\nthese new AVA annotated Kinetics clips. The dataset contains over 230k clips\nannotated with the 80 AVA action classes for each of the humans in key-frames.\nWe describe the annotation process and provide statistics about the new\ndataset. We also include a baseline evaluation using the Video Action\nTransformer Network on the AVA-Kinetics dataset, demonstrating improved\nperformance for action classification on the AVA test set. The dataset can be\ndownloaded from https://research.google.com/ava/",
        "Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.",
        "The rapidly growing parameter volume of deep neural networks (DNNs) hinders\nthe artificial intelligence applications on resource constrained devices, such\nas mobile and wearable devices. Neural network pruning, as one of the\nmainstream model compression techniques, is under extensive study to reduce the\nnumber of parameters and computations. In contrast to irregular pruning that\nincurs high index storage and decoding overhead, structured pruning techniques\nhave been proposed as the promising solutions. However, prior studies on\nstructured pruning tackle the problem mainly from the perspective of\nfacilitating hardware implementation, without analyzing the characteristics of\nsparse neural networks. The neglect on the study of sparse neural networks\ncauses inefficient trade-off between regularity and pruning ratio.\nConsequently, the potential of structurally pruning neural networks is not\nsufficiently mined.\n  In this work, we examine the structural characteristics of the irregularly\npruned weight matrices, such as the diverse redundancy of different rows, the\nsensitivity of different rows to pruning, and the positional characteristics of\nretained weights. By leveraging the gained insights as a guidance, we first\npropose the novel block-max weight masking (BMWM) method, which can effectively\nretain the salient weights while imposing high regularity to the weight matrix.\nAs a further optimization, we propose a density-adaptive regular-block (DARB)\npruning that outperforms prior structured pruning work with high pruning ratio\nand decoding efficiency. Our experimental results show that DARB can achieve\n13$\\times$ to 25$\\times$ pruning ratio, which are 2.8$\\times$ to 4.3$\\times$\nimprovements than the state-of-the-art counterparts on multiple neural network\nmodels and tasks. Moreover, DARB can achieve 14.3$\\times$ decoding efficiency\nthan block pruning with higher pruning ratio.",
        "Although recent model-free reinforcement learning algorithms have been shown\nto be capable of mastering complicated decision-making tasks, the sample\ncomplexity of these methods has remained a hurdle to utilizing them in many\nreal-world applications. In this regard, model-based reinforcement learning\nproposes some remedies. Yet, inherently, model-based methods are more\ncomputationally expensive and susceptible to sub-optimality. One reason is that\nmodel-generated data are always less accurate than real data, and this often\nleads to inaccurate transition and reward function models. With the aim to\nmitigate this problem, this work presents the notion of survival by discussing\ncases in which the agent's goal is to survive and its analogy to maximizing the\nexpected rewards. To that end, a substitute model for the reward function\napproximator is introduced that learns to avoid terminal states rather than to\nmaximize accumulated rewards from safe states. Focusing on terminal states, as\na small fraction of state-space, reduces the training effort drastically. Next,\na model-based reinforcement learning method is proposed (Survive) to train an\nagent to avoid dangerous states through a safety map model built upon temporal\ncredit assignment in the vicinity of terminal states. Finally, the performance\nof the presented algorithm is investigated, along with a comparison between the\nproposed and current methods.",
        "Elder people consequence a variety of problems while living Activities of\nDaily Living (ADL) for the reason of age, sense, loneliness and cognitive\nchanges. These cause the risk to ADL which leads to several falls. Getting real\nlife fall data is a difficult process and are not available whereas simulated\nfalls become ubiquitous to evaluate the proposed methodologies. From the\nliterature review, it is investigated that most of the researchers used raw and\nenergy features (time domain features) of the signal data as those are most\ndiscriminating. However, in real life situations fall signal may be noisy than\nthe current simulated data. Hence the result using raw feature may dramatically\nchanges when using in a real life scenario. This research is using frequency\ndomain Fourier coefficient features to differentiate various human activities\nof daily life. The feature vector constructed using those Fast Fourier\nTransform are robust to noise and rotation invariant. Two different supervised\nclassifiers kNN and SVM are used for evaluating the method. Two standard\npublicly available datasets are used for benchmark analysis. In this research,\nmore discriminating results are obtained applying kNN classifier than the SVM\nclassifier. Various standard measure including Standard Accuracy (SA), Macro\nAverage Accuracy (MAA), Sensitivity (SE) and Specificity (SP) has been\naccounted. In all cases, the proposed method outperforms energy features\nwhereas competitive results are shown with raw features. It is also noticed\nthat the proposed method performs better than the recently risen deep learning\napproach in which data augmentation method were not used.",
        "Single-pixel imaging is a novel imaging scheme that has gained popularity due\nto its huge computational gain and potential for a low-cost alternative to\nimaging beyond the visible spectrum. The traditional reconstruction methods\nstruggle to produce a clear recovery when one limits the number of illumination\npatterns from a spatial light modulator. As a remedy, several\ndeep-learning-based solutions have been proposed which lack good generalization\nability due to the architectural setup and loss functions. In this paper, we\npropose a generative adversarial network-based reconstruction framework for\nsingle-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images\nwith 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This\nfacilitates much faster reconstruction making our method suitable for\nsingle-pixel video. Furthermore, our ResNet-like architecture for the generator\nleads to useful representation learning that allows us to reconstruct\ncompletely unseen objects. The experimental results demonstrate that SPI-GAN\nachieves significant performance gain, e.g. near 3dB PSNR gain, over the\ncurrent state-of-the-art method.",
        "Matrix completion constantly receives tremendous attention from many research\nfields. It is commonly applied for recommender systems such as movie ratings,\ncomputer vision such as image reconstruction or completion, multi-task learning\nsuch as collaboratively modeling time-series trends of multiple sensors, and\nmany other applications. Matrix completion techniques are usually\ncomputationally exhaustive and/or fail to capture the heterogeneity in the\ndata. For example, images usually contain a heterogeneous set of objects, and\nthus it is a challenging task to reconstruct images with high levels of missing\ndata. In this paper, we propose the sparse reverse of principal component\nanalysis for matrix completion. The proposed approach maintains smoothness\nacross the matrix, produces accurate estimates of the missing data, converges\niteratively, and it is computationally tractable with a controllable upper\nbound on the number of iterations until convergence. The accuracy of the\nproposed technique is validated on natural images, movie ratings, and\nmultisensor data. It is also compared with common benchmark methods used for\nmatrix completion.",
        "State-of-the-art techniques proposed for 6D object pose recovery depend on\nocclusion-free point clouds to accurately register objects in 3D space. To\nreduce this dependency, we introduce a novel architecture called Iterative\nHough Forest with Histogram of Control Points that is capable of estimating\noccluded and cluttered objects' 6D pose given a candidate 2D bounding box. Our\nIterative Hough Forest is learnt using patches extracted only from the positive\nsamples. These patches are represented with Histogram of Control Points (HoCP),\na \"scale-variant\" implicit volumetric description, which we derive from\nrecently introduced Implicit B-Splines (IBS). The rich discriminative\ninformation provided by this scale-variance is leveraged during inference,\nwhere the initial pose estimation of the object is iteratively refined based on\nmore discriminative control points by using our Iterative Hough Forest. We\nconduct experiments on several test objects of a publicly available dataset to\ntest our architecture and to compare with the state-of-the-art.",
        "Stixels have been successfully applied to a wide range of vision tasks in\nautonomous driving, recently including instance segmentation. However, due to\ntheir sparse occurrence in the image, until now Stixels seldomly served as\ninput for Deep Learning algorithms, restricting their utility for such\napproaches. In this work we present StixelPointNet, a novel method to perform\nfast instance segmentation directly on Stixels. By regarding the Stixel\nrepresentation as unstructured data similar to point clouds, architectures like\nPointNet are able to learn features from Stixels. We use a bounding box\ndetector to propose candidate instances, for which the relevant Stixels are\nextracted from the input image. On these Stixels, a PointNet models learns\nbinary segmentations, which we then unify throughout the whole image in a final\nselection step. StixelPointNet achieves state-of-the-art performance on\nStixel-level, is considerably faster than pixel-based segmentation methods, and\nshows that with our approach the Stixel domain can be introduced to many new 3D\nDeep Learning tasks.",
        "In this paper, we propose a neuro-symbolic framework called weighted Signal\nTemporal Logic Neural Network (wSTL-NN) that combines the characteristics of\nneural networks and temporal logics. Weighted Signal Temporal Logic (wSTL)\nformulas are recursively composed of subformulas that are combined using\nlogical and temporal operators. The quantitative semantics of wSTL is defined\nsuch that the quantitative satisfaction of subformulas with higher weights has\nmore influence on the quantitative satisfaction of the overall wSTL formula. In\nthe wSTL-NN, each neuron corresponds to a wSTL subformula, and its output\ncorresponds to the quantitative satisfaction of the formula. We use wSTL-NN to\nrepresent wSTL formulas as features to classify time series data. STL features\nare more explainable than those used in classical methods. The wSTL-NN is\nend-to-end differentiable, which allows learning of wSTL formulas to be done\nusing back-propagation. To reduce the number of weights, we introduce two\ntechniques to sparsify the wSTL-NN.We apply our framework to an occupancy\ndetection time-series dataset to learn a classifier that predicts the occupancy\nstatus of an office room.",
        "Time series with long-term structure arise in a variety of contexts and\ncapturing this temporal structure is a critical challenge in time series\nanalysis for both inference and forecasting settings. Traditionally, state\nspace models have been successful in providing uncertainty estimates of\ntrajectories in the latent space. More recently, deep learning, attention-based\napproaches have achieved state of the art performance for sequence modeling,\nthough often require large amounts of data and parameters to do so. We propose\nStanza, a nonlinear, non-stationary state space model as an intermediate\napproach to fill the gap between traditional models and modern deep learning\napproaches for complex time series. Stanza strikes a balance between\ncompetitive forecasting accuracy and probabilistic, interpretable inference for\nhighly structured time series. In particular, Stanza achieves forecasting\naccuracy competitive with deep LSTMs on real-world datasets, especially for\nmulti-step ahead forecasting.",
        "To date, developing a good model for early intensive care unit (ICU)\nmortality prediction is still challenging. This paper presents a patient based\npredictive modeling framework (PPMF) to improve the performance of ICU\nmortality prediction using data collected during the first 48 hours of ICU\nadmission. PPMF consists of three main components verifying three related\nresearch hypotheses. The first component captures dynamic changes of patients\nstatus in the ICU using their time series data (e.g., vital signs and\nlaboratory tests). The second component is a local approximation algorithm that\nclassifies patients based on their similarities. The third component is a\nGradient Decent wrapper that updates feature weights according to the\nclassification feedback. Experiments using data from MIMICIII show that PPMF\nsignificantly outperforms: (1) the severity score systems, namely SASP III,\nAPACHE IV, and MPM0III, (2) the aggregation based classifiers that utilize\nsummarized time series, and (3) baseline feature selection methods.",
        "Deep learning is increasingly used as a building block of security systems.\nUnfortunately, neural networks are hard to interpret and typically opaque to\nthe practitioner. The machine learning community has started to address this\nproblem by developing methods for explaining the predictions of neural\nnetworks. While several of these approaches have been successfully applied in\nthe area of computer vision, their application in security has received little\nattention so far. It is an open question which explanation methods are\nappropriate for computer security and what requirements they need to satisfy.\nIn this paper, we introduce criteria for comparing and evaluating explanation\nmethods in the context of computer security. These cover general properties,\nsuch as the accuracy of explanations, as well as security-focused aspects, such\nas the completeness, efficiency, and robustness. Based on our criteria, we\ninvestigate six popular explanation methods and assess their utility in\nsecurity systems for malware detection and vulnerability discovery. We observe\nsignificant differences between the methods and build on these to derive\ngeneral recommendations for selecting and applying explanation methods in\ncomputer security.",
        "Boundary-based instance segmentation has drawn much attention since of its\nattractive efficiency. However, existing methods suffer from the difficulty in\nlong-distance regression. In this paper, we propose a coarse-to-fine module to\naddress the problem. Approximate boundary points are generated at the coarse\nstage and then features of these points are sampled and fed to a refined\nregressor for fine prediction. It is end-to-end trainable since differential\nsampling operation is well supported in the module. Furthermore, we design a\nholistic boundary-aware branch and introduce instance-agnostic supervision to\nassist regression. Equipped with ResNet-101, our approach achieves 31.7\\% mask\nAP on COCO dataset with single-scale training and testing, outperforming the\nbaseline 1.3\\% mask AP with less than 1\\% additional parameters and GFLOPs.\nExperiments also show that our proposed method achieves competitive performance\ncompared to existing boundary-based methods with a lightweight design and a\nsimple pipeline.",
        "Domain adaptation (DA) is transfer learning which aims to leverage labeled\ndata in a related source domain to achieve informed knowledge transfer and help\nthe classification of unlabeled data in a target domain. In this paper, we\npropose a novel DA method, namely Robust Data Geometric Structure Aligned,\nClose yet Discriminative Domain Adaptation (RSA-CDDA), which brings closer, in\na latent joint subspace, both source and target data distributions, and aligns\ninherent hidden source and target data geometric structures while performing\ndiscriminative DA in repulsing both interclass source and target data. The\nproposed method performs domain adaptation between source and target in solving\na unified model, which incorporates data distribution constraints, in\nparticular via a nonparametric distance, i.e., Maximum Mean Discrepancy (MMD),\nas well as constraints on inherent hidden data geometric structure segmentation\nand alignment between source and target, through low rank and sparse\nrepresentation. RSA-CDDA achieves the search of a joint subspace in solving the\nproposed unified model through iterative optimization, alternating Rayleigh\nquotient algorithm and inexact augmented Lagrange multiplier algorithm.\nExtensive experiments carried out on standard DA benchmarks, i.e., 16\ncross-domain image classification tasks, verify the effectiveness of the\nproposed method, which consistently outperforms the state-of-the-art methods.",
        "Automated person re-identification in a multi-camera surveillance setup is\nvery important for effective tracking and monitoring crowd movement. In the\nrecent years, few deep learning based re-identification approaches have been\ndeveloped which are quite accurate but time-intensive, and hence not very\nsuitable for practical purposes. In this paper, we propose an efficient\nhierarchical re-identification approach in which color histogram based\ncomparison is first employed to find the closest matches in the gallery set,\nand next deep feature based comparison is carried out using Siamese network.\nReduction in search space after the first level of matching helps in achieving\na fast response time as well as improving the accuracy of prediction by the\nSiamese network by eliminating vastly dissimilar elements. A silhouette\npart-based feature extraction scheme is adopted in each level of hierarchy to\npreserve the relative locations of the different body structures and make the\nappearance descriptors more discriminating in nature. The proposed approach has\nbeen evaluated on five public data sets and also a new data set captured by our\nteam in our laboratory. Results reveal that it outperforms most\nstate-of-the-art approaches in terms of overall accuracy.",
        "Federated Learning (FL) is an approach to collaboratively train a model\nacross multiple parties without sharing data between parties or an aggregator.\nIt is used both in the consumer domain to protect personal data as well as in\nenterprise settings, where dealing with data domicile regulation and the\npragmatics of data silos are the main drivers. While gradient boosted tree\nimplementations such as XGBoost have been very successful for many use cases,\nits federated learning adaptations tend to be very slow due to using\ncryptographic and privacy methods and have not experienced widespread use. We\npropose the Party-Adaptive XGBoost (PAX) for federated learning, a novel\nimplementation of gradient boosting which utilizes a party adaptive histogram\naggregation method, without the need for data encryption. It constructs a\nsurrogate representation of the data distribution for finding splits of the\ndecision tree. Our experimental results demonstrate strong model performance,\nespecially on non-IID distributions, and significantly faster training run-time\nacross different data sets than existing federated implementations. This\napproach makes the use of gradient boosted trees practical in enterprise\nfederated learning.",
        "Generative adversarial networks (GANs) can be trained to generate 3D image\ndata, which is useful for design optimisation. However, this conventionally\nrequires 3D training data, which is challenging to obtain. 2D imaging\ntechniques tend to be faster, higher resolution, better at phase identification\nand more widely available. Here, we introduce a generative adversarial network\narchitecture, SliceGAN, which is able to synthesise high fidelity 3D datasets\nusing a single representative 2D image. This is especially relevant for the\ntask of material microstructure generation, as a cross-sectional micrograph can\ncontain sufficient information to statistically reconstruct 3D samples. Our\narchitecture implements the concept of uniform information density, which both\nensures that generated volumes are equally high quality at all points in space,\nand that arbitrarily large volumes can be generated. SliceGAN has been\nsuccessfully trained on a diverse set of materials, demonstrating the\nwidespread applicability of this tool. The quality of generated micrographs is\nshown through a statistical comparison of synthetic and real datasets of a\nbattery electrode in terms of key microstructural metrics. Finally, we find\nthat the generation time for a $10^8$ voxel volume is on the order of a few\nseconds, yielding a path for future studies into high-throughput\nmicrostructural optimisation.",
        "Understanding the behavior of road users is of vital importance for the\ndevelopment of trajectory prediction systems. In this context, the latest\nadvances have focused on recurrent structures, establishing the social\ninteraction between the agents involved in the scene. More recently, simpler\nstructures have also been introduced for predicting pedestrian trajectories,\nbased on Transformer Networks, and using positional information. They allow the\nindividual modelling of each agent's trajectory separately without any complex\ninteraction terms. Our model exploits these simple structures by adding\naugmented data (position and heading), and adapting their use to the problem of\nvehicle trajectory prediction in urban scenarios in prediction horizons up to 5\nseconds. In addition, a cross-performance analysis is performed between\ndifferent types of scenarios, including highways, intersections and\nroundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our\nmodel achieves state-of-the-art results and proves to be flexible and adaptable\nto different types of urban contexts.",
        "Multi-step prediction is considered of major significance for time series\nanalysis in many real life problems. Existing methods mainly focus on\none-step-ahead forecasting, since multiple step forecasting generally fails due\nto accumulation of prediction errors. This paper presents a novel approach for\npredicting plant growth in agriculture, focusing on prediction of plant Stem\nDiameter Variations (SDV). The proposed approach consists of three main steps.\nAt first, wavelet decomposition is applied to the original data, as to\nfacilitate model fitting and reduce noise in them. Then an encoder-decoder\nframework is developed using Long Short Term Memory (LSTM) and used for\nappropriate feature extraction from the data. Finally, a recurrent neural\nnetwork including LSTM and an attention mechanism is proposed for modelling\nlong-term dependencies in the time series data. Experimental results are\npresented which illustrate the good performance of the proposed approach and\nthat it significantly outperforms the existing models, in terms of error\ncriteria such as RMSE, MAE and MAPE.",
        "Machine learning has shown growing success in recent years. However, current\nmachine learning systems are highly specialized, trained for particular\nproblems or domains, and typically on a single narrow dataset. Human learning,\non the other hand, is highly general and adaptable. Never-ending learning is a\nmachine learning paradigm that aims to bridge this gap, with the goal of\nencouraging researchers to design machine learning systems that can learn to\nperform a wider variety of inter-related tasks in more complex environments. To\ndate, there is no environment or testbed to facilitate the development and\nevaluation of never-ending learning systems. To this end, we propose the Jelly\nBean World testbed. The Jelly Bean World allows experimentation over\ntwo-dimensional grid worlds which are filled with items and in which agents can\nnavigate. This testbed provides environments that are sufficiently complex and\nwhere more generally intelligent algorithms ought to perform better than\ncurrent state-of-the-art reinforcement learning approaches. It does so by\nproducing non-stationary environments and facilitating experimentation with\nmulti-task, multi-agent, multi-modal, and curriculum learning settings. We hope\nthat this new freely-available software will prompt new research and interest\nin the development and evaluation of never-ending learning systems and more\nbroadly, general intelligence systems.",
        "The availability of labeled image datasets has been shown critical for\nhigh-level image understanding, which continuously drives the progress of\nfeature designing and models developing. However, constructing labeled image\ndatasets is laborious and monotonous. To eliminate manual annotation, in this\nwork, we propose a novel image dataset construction framework by employing\nmultiple textual queries. We aim at collecting diverse and accurate images for\ngiven queries from the Web. Specifically, we formulate noisy textual queries\nremoving and noisy images filtering as a multi-view and multi-instance learning\nproblem separately. Our proposed approach not only improves the accuracy but\nalso enhances the diversity of the selected images. To verify the effectiveness\nof our proposed approach, we construct an image dataset with 100 categories.\nThe experiments show significant performance gains by using the generated data\nof our approach on several tasks, such as image classification, cross-dataset\ngeneralization, and object detection. The proposed method also consistently\noutperforms existing weakly supervised and web-supervised approaches.",
        "In this paper, we present a new method for detecting road users in an urban\nenvironment which leads to an improvement in multiple object tracking. Our\nmethod takes as an input a foreground image and improves the object detection\nand segmentation. This new image can be used as an input to trackers that use\nforeground blobs from background subtraction. The first step is to create\nforeground images for all the frames in an urban video. Then, starting from the\noriginal blobs of the foreground image, we merge the blobs that are close to\none another and that have similar optical flow. The next step is extracting the\nedges of the different objects to detect multiple objects that might be very\nclose (and be merged in the same blob) and to adjust the size of the original\nblobs. At the same time, we use the optical flow to detect occlusion of objects\nthat are moving in opposite directions. Finally, we make a decision on which\ninformation we keep in order to construct a new foreground image with blobs\nthat can be used for tracking. The system is validated on four videos of an\nurban traffic dataset. Our method improves the recall and precision metrics for\nthe object detection task compared to the vanilla background subtraction method\nand improves the CLEAR MOT metrics in the tracking tasks for most videos.",
        "In this paper, we present a new feature representation for first-person\nvideos. In first-person video understanding (e.g., activity recognition), it is\nvery important to capture both entire scene dynamics (i.e., egomotion) and\nsalient local motion observed in videos. We describe a representation framework\nbased on time series pooling, which is designed to abstract\nshort-term/long-term changes in feature descriptor elements. The idea is to\nkeep track of how descriptor values are changing over time and summarize them\nto represent motion in the activity video. The framework is general, handling\nany types of per-frame feature descriptors including conventional motion\ndescriptors like histogram of optical flows (HOF) as well as appearance\ndescriptors from more recent convolutional neural networks (CNN). We\nexperimentally confirm that our approach clearly outperforms previous feature\nrepresentations including bag-of-visual-words and improved Fisher vector (IFV)\nwhen using identical underlying feature descriptors. We also confirm that our\nfeature representation has superior performance to existing state-of-the-art\nfeatures like local spatio-temporal features and Improved Trajectory Features\n(originally developed for 3rd-person videos) when handling first-person videos.\nMultiple first-person activity datasets were tested under various settings to\nconfirm these findings.",
        "The problem of segmenting a given image into coherent regions is important in\nComputer Vision and many industrial applications require segmenting a known\nobject into its components. Examples include identifying individual parts of a\ncomponent for process control work in a manufacturing plant and identifying\nparts of a car from a photo for automatic damage detection. Unfortunately most\nof an object's parts of interest in such applications share the same pixel\ncharacteristics, having similar colour and texture. This makes segmenting the\nobject into its components a non-trivial task for conventional image\nsegmentation algorithms. In this paper, we propose a \"Model Assisted\nSegmentation\" method to tackle this problem. A 3D model of the object is\nregistered over the given image by optimising a novel gradient based loss\nfunction. This registration obtains the full 3D pose from an image of the\nobject. The image can have an arbitrary view of the object and is not limited\nto a particular set of views. The segmentation is subsequently performed using\na level-set based method, using the projected contours of the registered 3D\nmodel as initialisation curves. The method is fully automatic and requires no\nuser interaction. Also, the system does not require any prior training. We\npresent our results on photographs of a real car.",
        "Training data is the key ingredient for deep learning approaches, but\ndifficult to obtain for the specialized domains often encountered in robotics.\nWe describe a synthesis pipeline capable of producing training data for\ncluttered scene perception tasks such as semantic segmentation, object\ndetection, and correspondence or pose estimation. Our approach arranges object\nmeshes in physically realistic, dense scenes using physics simulation. The\narranged scenes are rendered using high-quality rasterization with randomized\nappearance and material parameters. Noise and other transformations introduced\nby the camera sensors are simulated. Our pipeline can be run online during\ntraining of a deep neural network, yielding applications in life-long learning\nand in iterative render-and-compare approaches. We demonstrate the usability by\nlearning semantic segmentation on the challenging YCB-Video dataset without\nactually using any training frames, where our method achieves performance\ncomparable to a conventionally trained model. Additionally, we show successful\napplication in a real-world regrasping system.",
        "Referring expression comprehension (REF) aims at identifying a particular\nobject in a scene by a natural language expression. It requires joint reasoning\nover the textual and visual domains to solve the problem. Some popular\nreferring expression datasets, however, fail to provide an ideal test bed for\nevaluating the reasoning ability of the models, mainly because 1) their\nexpressions typically describe only some simple distinctive properties of the\nobject and 2) their images contain limited distracting information. To bridge\nthe gap, we propose a new dataset for visual reasoning in context of referring\nexpression comprehension with two main features. First, we design a novel\nexpression engine rendering various reasoning logics that can be flexibly\ncombined with rich visual properties to generate expressions with varying\ncompositionality. Second, to better exploit the full reasoning chain embodied\nin an expression, we propose a new test setting by adding additional\ndistracting images containing objects sharing similar properties with the\nreferent, thus minimising the success rate of reasoning-free cross-domain\nalignment. We evaluate several state-of-the-art REF models, but find none of\nthem can achieve promising performance. A proposed modular hard mining strategy\nperforms the best but still leaves substantial room for improvement. We hope\nthis new dataset and task can serve as a benchmark for deeper visual reasoning\nanalysis and foster the research on referring expression comprehension.",
        "Telecommunication (Telco) outdoor position recovery aims to localize outdoor\nmobile devices by leveraging measurement report (MR) data. Unfortunately, Telco\nposition recovery requires sufficient amount of MR samples across different\nareas and suffers from high data collection cost. For an area with scarce MR\nsamples, it is hard to achieve good accuracy. In this paper, by leveraging the\nrecently developed transfer learning techniques, we design a novel Telco\nposition recovery framework, called TLoc, to transfer good models in the\ncarefully selected source domains (those fine-grained small subareas) to a\ntarget one which originally suffers from poor localization accuracy.\nSpecifically, TLoc introduces three dedicated components: 1) a new coordinate\nspace to divide an area of interest into smaller domains, 2) a similarity\nmeasurement to select best source domains, and 3) an adaptation of an existing\ntransfer learning approach. To the best of our knowledge, TLoc is the first\nframework that demonstrates the efficacy of applying transfer learning in the\nTelco outdoor position recovery. To exemplify, on the 2G GSM and 4G LTE MR\ndatasets in Shanghai, TLoc outperforms a nontransfer approach by 27.58% and\n26.12% less median errors, and further leads to 47.77% and 49.22% less median\nerrors than a recent fingerprinting approach NBL.",
        "The beer game is a widely used in-class game that is played in supply chain\nmanagement classes to demonstrate the bullwhip effect. The game is a\ndecentralized, multi-agent, cooperative problem that can be modeled as a serial\nsupply chain network in which agents cooperatively attempt to minimize the\ntotal cost of the network even though each agent can only observe its own local\ninformation. Each agent chooses order quantities to replenish its stock. Under\nsome conditions, a base-stock replenishment policy is known to be optimal.\nHowever, in a decentralized supply chain in which some agents (stages) may act\nirrationally (as they do in the beer game), there is no known optimal policy\nfor an agent wishing to act optimally.\n  We propose a machine learning algorithm, based on deep Q-networks, to\noptimize the replenishment decisions at a given stage. When playing alongside\nagents who follow a base-stock policy, our algorithm obtains near-optimal order\nquantities. It performs much better than a base-stock policy when the other\nagents use a more realistic model of human ordering behavior. Unlike most other\nalgorithms in the literature, our algorithm does not have any limits on the\nbeer game parameter values. Like any deep learning algorithm, training the\nalgorithm can be computationally intensive, but this can be performed ahead of\ntime; the algorithm executes in real time when the game is played. Moreover, we\npropose a transfer learning approach so that the training performed for one\nagent and one set of cost coefficients can be adapted quickly for other agents\nand costs. Our algorithm can be extended to other decentralized multi-agent\ncooperative games with partially observed information, which is a common type\nof situation in real-world supply chain problems.",
        "Lane detection in driving scenes is an important module for autonomous\nvehicles and advanced driver assistance systems. In recent years, many\nsophisticated lane detection methods have been proposed. However, most methods\nfocus on detecting the lane from one single image, and often lead to\nunsatisfactory performance in handling some extremely-bad situations such as\nheavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In\nfact, lanes are continuous line structures on the road. Consequently, the lane\nthat cannot be accurately detected in one current frame may potentially be\ninferred out by incorporating information of previous frames. To this end, we\ninvestigate lane detection by using multiple frames of a continuous driving\nscene, and propose a hybrid deep architecture by combining the convolutional\nneural network (CNN) and the recurrent neural network (RNN). Specifically,\ninformation of each frame is abstracted by a CNN block, and the CNN features of\nmultiple continuous frames, holding the property of time-series, are then fed\ninto the RNN block for feature learning and lane prediction. Extensive\nexperiments on two large-scale datasets demonstrate that, the proposed method\noutperforms the competing methods in lane detection, especially in handling\ndifficult situations.",
        "In this work, we explore the problem of aligning two time-ordered point\nclouds which are spatially transformed and re-parameterized versions of each\nother. This has a diverse array of applications such as cross modal time series\nsynchronization (e.g. MOCAP to video) and alignment of discretized curves in\nimages. Most other works that address this problem attempt to jointly uncover a\nspatial alignment and correspondences between the two point clouds, or to\nderive local invariants to spatial transformations such as curvature before\ncomputing correspondences. By contrast, we sidestep spatial alignment\ncompletely by using self-similarity matrices (SSMs) as a proxy to the\ntime-ordered point clouds, since self-similarity matrices are blind to\nisometries and respect global geometry. Our algorithm, dubbed \"Isometry Blind\nDynamic Time Warping\" (IBDTW), is simple and general, and we show that its\nassociated dissimilarity measure lower bounds the L1 Gromov-Hausdorff distance\nbetween the two point sets when restricted to warping paths. We also present a\nlocal, partial alignment extension of IBDTW based on the Smith Waterman\nalgorithm. This eliminates the need for tedious manual cropping of time series,\nwhich is ordinarily necessary for global alignment algorithms to function\nproperly.",
        "The information bottleneck (IB) principle has been adopted to explain deep\nlearning in terms of information compression and prediction, which are balanced\nby a trade-off hyperparameter. How to optimize the IB principle for better\nrobustness and figure out the effects of compression through the trade-off\nhyperparameter are two challenging problems. Previous methods attempted to\noptimize the IB principle by introducing random noise into learning the\nrepresentation and achieved state-of-the-art performance in the nuisance\ninformation compression and semantic information extraction. However, their\nperformance on resisting adversarial perturbations is far less impressive. To\nthis end, we propose an adversarial information bottleneck (AIB) method without\nany explicit assumptions about the underlying distribution of the\nrepresentations, which can be optimized effectively by solving a Min-Max\noptimization problem. Numerical experiments on synthetic and real-world\ndatasets demonstrate its effectiveness on learning more invariant\nrepresentations and mitigating adversarial perturbations compared to several\ncompeting IB methods. In addition, we analyse the adversarial robustness of\ndiverse IB methods contrasting with their IB curves, and reveal that IB models\nwith the hyperparameter $\\beta$ corresponding to the knee point in the IB curve\nachieve the best trade-off between compression and prediction, and has best\nrobustness against various attacks.",
        "Modern CNN-based object detectors assign anchors for ground-truth objects\nunder the restriction of object-anchor Intersection-over-Unit (IoU). In this\nstudy, we propose a learning-to-match approach to break IoU restriction,\nallowing objects to match anchors in a flexible manner. Our approach, referred\nto as FreeAnchor, updates hand-crafted anchor assignment to \"free\" anchor\nmatching by formulating detector training as a maximum likelihood estimation\n(MLE) procedure. FreeAnchor targets at learning features which best explain a\nclass of objects in terms of both classification and localization. FreeAnchor\nis implemented by optimizing detection customized likelihood and can be fused\nwith CNN-based detectors in a plug-and-play manner. Experiments on COCO\ndemonstrate that FreeAnchor consistently outperforms their counterparts with\nsignificant margins.",
        "In this paper, a new task is proposed, namely, weather translation, which\nrefers to transferring weather conditions of the image from one category to\nanother. It is important for photographic style transfer. Although lots of\napproaches have been proposed in traditional image translation tasks, few of\nthem can handle the multi-category weather translation task, since weather\nconditions have rich categories and highly complex semantic structures. To\naddress this problem, we develop a multi-domain weather translation approach\nbased on generative adversarial networks (GAN), denoted as Weather GAN, which\ncan achieve the transferring of weather conditions among sunny, cloudy, foggy,\nrainy and snowy. Specifically, the weather conditions in the image are\ndetermined by various weather-cues, such as cloud, blue sky, wet ground, etc.\nTherefore, it is essential for weather translation to focus the main attention\non weather-cues. To this end, the generator of Weather GAN is composed of an\ninitial translation module, an attention module and a weather-cue segmentation\nmodule. The initial translation module performs global translation during\ngeneration procedure. The weather-cue segmentation module identifies the\nstructure and exact distribution of weather-cues. The attention module learns\nto focus on the interesting areas of the image while keeping other areas\nunaltered. The final generated result is synthesized by these three parts. This\napproach suppresses the distortion and deformation caused by weather\ntranslation. our approach outperforms the state-of-the-arts has been shown by a\nlarge number of experiments and evaluations.",
        "For ego-motion estimation, the feature representation of the scenes is\ncrucial. Previous methods indicate that both the low-level and semantic\nfeature-based methods can achieve promising results. Therefore, the\nincorporation of hierarchical feature representation may benefit from both\nmethods. From this perspective, we propose a novel direct feature odometry\nframework, named DFO, for depth estimation and hierarchical feature\nrepresentation learning from monocular videos. By exploiting the metric\ndistance, our framework is able to learn the hierarchical feature\nrepresentation without supervision. The pose is obtained with a coarse-to-fine\napproach from high-level to low-level features in enlarged feature maps. The\npixel-level attention mask can be self-learned to provide the prior\ninformation. In contrast to the previous methods, our proposed method\ncalculates the camera motion with a direct method rather than regressing the\nego-motion from the pose network. With this approach, the consistency of the\nscale factor of translation can be constrained. Additionally, the proposed\nmethod is thus compatible with the traditional SLAM pipeline. Experiments on\nthe KITTI dataset demonstrate the effectiveness of our method.",
        "We study a decentralized cooperative multi-agent multi-armed bandit problem\nwith $K$ arms and $N$ agents connected over a network. In our model, each arm's\nreward distribution is same for all agents, and rewards are drawn independently\nacross agents and over time steps. In each round, agents choose an arm to play\nand subsequently send a message to their neighbors. The goal is to minimize\ncumulative regret averaged over the entire network. We propose a decentralized\nBayesian multi-armed bandit framework that extends single-agent Bayesian bandit\nalgorithms to the decentralized setting. Specifically, we study an information\nassimilation algorithm that can be combined with existing Bayesian algorithms,\nand using this, we propose a decentralized Thompson Sampling algorithm and\ndecentralized Bayes-UCB algorithm. We analyze the decentralized Thompson\nSampling algorithm under Bernoulli rewards and establish a problem-dependent\nupper bound on the cumulative regret. We show that regret incurred scales\nlogarithmically over the time horizon with constants that match those of an\noptimal centralized agent with access to all observations across the network.\nOur analysis also characterizes the cumulative regret in terms of the network\nstructure. Through extensive numerical studies, we show that our extensions of\nThompson Sampling and Bayes-UCB incur lesser cumulative regret than the\nstate-of-art algorithms inspired by the Upper Confidence Bound algorithm. We\nimplement our proposed decentralized Thompson Sampling under gossip protocol,\nand over time-varying networks, where each communication link has a fixed\nprobability of failure.",
        "We propose a framework for top-down salient object detection that\nincorporates a tightly coupled image classification module. The classifier is\ntrained on novel category-aware sparse codes computed on object dictionaries\nused for saliency modeling. A misclassification indicates that the\ncorresponding saliency model is inaccurate. Hence, the classifier selects\nimages for which the saliency models need to be updated. The category-aware\nsparse coding produces better image classification accuracy as compared to\nconventional sparse coding with a reduced computational complexity. A\nsaliency-weighted max-pooling is proposed to improve image classification,\nwhich is further used to refine the saliency maps. Experimental results on\nGraz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salient\nobject detection. Although the role of the classifier is to support salient\nobject detection, we evaluate its performance in image classification and also\nillustrate the utility of thresholded saliency maps for image segmentation.",
        "Localizing objects in 3D space and understanding their associated 3D\nproperties is challenging given only monocular RGB images. The situation is\ncompounded by the loss of depth information during perspective projection. We\npresent Center3D, a one-stage anchor-free approach, to efficiently estimate 3D\nlocation and depth using only monocular RGB images. By exploiting the\ndifference between 2D and 3D centers, we are able to estimate depth\nconsistently. Center3D uses a combination of classification and regression to\nunderstand the hidden depth information more robustly than each method alone.\nOur method employs two joint approaches: (1) LID: a classification-dominated\napproach with sequential Linear Increasing Discretization. (2) DepJoint: a\nregression-dominated approach with multiple Eigen's transformations for depth\nestimation. Evaluating on KITTI dataset for moderate objects, Center3D improved\nthe AP in BEV from $29.7\\%$ to $42.8\\%$, and the AP in 3D from $18.6\\%$ to\n$39.1\\%$. Compared with state-of-the-art detectors, Center3D has achieved the\nbest speed-accuracy trade-off in realtime monocular object detection.",
        "SDYNA is a general framework designed to address large stochastic\nreinforcement learning problems. Unlike previous model based methods in FMDPs,\nit incrementally learns the structure and the parameters of a RL problem using\nsupervised learning techniques. Then, it integrates decision-theoric planning\nalgorithms based on FMDPs to compute its policy. SPITI is an instanciation of\nSDYNA that exploits ITI, an incremental decision tree algorithm, to learn the\nreward function and the Dynamic Bayesian Networks with local structures\nrepresenting the transition function of the problem. These representations are\nused by an incremental version of the Structured Value Iteration algorithm. In\norder to learn the structure, SPITI uses Chi-Square tests to detect the\nindependence between two probability distributions. Thus, we study the relation\nbetween the threshold used in the Chi-Square test, the size of the model built\nand the relative error of the value function of the induced policy with respect\nto the optimal value. We show that, on stochastic problems, one can tune the\nthreshold so as to generate both a compact model and an efficient policy. Then,\nwe show that SPITI, while keeping its model compact, uses the generalization\nproperty of its learning method to perform better than a stochastic classical\ntabular algorithm in large RL problem with an unknown structure. We also\nintroduce a new measure based on Chi-Square to qualify the accuracy of the\nmodel learned by SPITI. We qualitatively show that the generalization property\nin SPITI within the FMDP framework may prevent an exponential growth of the\ntime required to learn the structure of large stochastic RL problems.",
        "Human motion prediction is an essential part for human-robot collaboration.\nUnlike most of the existing methods mainly focusing on improving the\neffectiveness of spatiotemporal modeling for accurate prediction, we take\neffectiveness and efficiency into consideration, aiming at the prediction\nquality, computational efficiency and the lightweight of the model. A\nmulti-grained trajectory graph convolutional networks based and lightweight\nframework is proposed for habit-unrelated human motion prediction.\nSpecifically, we represent human motion as multi-grained trajectories,\nincluding joint trajectory and sub-joint trajectory. Based on the advanced\nrepresentation, multi-grained trajectory graph convolutional networks are\nproposed to explore the spatiotemporal dependencies at the multiple\ngranularities. Moreover, considering the right-handedness habit of the vast\nmajority of people, a new motion generation method is proposed to generate the\nmotion with left-handedness, to better model the motion with less bias to the\nhuman habit. Experimental results on challenging datasets, including Human3.6M\nand CMU Mocap, show that the proposed model outperforms state-of-the-art with\nless than 0.12 times parameters, which demonstrates the effectiveness and\nefficiency of our proposed method.",
        "We propose randomized least-squares value iteration (RLSVI) -- a new\nreinforcement learning algorithm designed to explore and generalize efficiently\nvia linearly parameterized value functions. We explain why versions of\nleast-squares value iteration that use Boltzmann or epsilon-greedy exploration\ncan be highly inefficient, and we present computational results that\ndemonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish\nan upper bound on the expected regret of RLSVI that demonstrates\nnear-optimality in a tabula rasa learning context. More broadly, our results\nsuggest that randomized value functions offer a promising approach to tackling\na critical challenge in reinforcement learning: synthesizing efficient\nexploration and effective generalization.",
        "Tracking of objects in 3D is a fundamental task in computer vision that finds\nuse in a wide range of applications such as autonomous driving, robotics or\naugmented reality. Most recent approaches for 3D multi object tracking (MOT)\nfrom LIDAR use object dynamics together with a set of handcrafted features to\nmatch detections of objects. However, manually designing such features and\nheuristics is cumbersome and often leads to suboptimal performance. In this\nwork, we instead strive towards a unified and learning based approach to the 3D\nMOT problem. We design a graph structure to jointly process detection and track\nstates in an online manner. To this end, we employ a Neural Message Passing\nnetwork for data association that is fully trainable. Our approach provides a\nnatural way for track initialization and handling of false positive detections,\nwhile significantly improving track stability. We show the merit of the\nproposed approach on the publicly available nuScenes dataset by achieving\nstate-of-the-art performance of 65.6% AMOTA and 58% fewer ID-switches.",
        "Convolutional neural network (CNN) based methods have recently achieved great\nsuccess for image super-resolution (SR). However, most deep CNN based SR models\nattempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while\nresulting in poor quantified perceptual quality (e.g. human opinion score,\nno-reference quality measures such as NIQE). Few works have attempted to\nimprove the perceptual quality at the cost of performance reduction in\ndistortion measures. A very recent study has revealed that distortion and\nperceptual quality are at odds with each other and there is always a trade-off\nbetween the two. Often the restoration algorithms that are superior in terms of\nperceptual quality, are inferior in terms of distortion measures. Our work\nattempts to analyze the trade-off between distortion and perceptual quality for\nthe problem of single image SR. To this end, we use the well-known SR\narchitecture-enhanced deep super-resolution (EDSR) network and show that it can\nbe adapted to achieve better perceptual quality for a specific range of the\ndistortion measure. While the original network of EDSR was trained to minimize\nthe error defined based on per-pixel accuracy alone, we train our network using\na generative adversarial network framework with EDSR as the generator module.\nOur proposed network, called enhanced perceptual super-resolution network\n(EPSR), is trained with a combination of mean squared error loss, perceptual\nloss, and adversarial loss. Our experiments reveal that EPSR achieves the\nstate-of-the-art trade-off between distortion and perceptual quality while the\nexisting methods perform well in either of these measures alone.",
        "Recommender systems have become an essential instrument in a wide range of\nindustries to personalize the user experience. A significant issue that has\ncaptured both researchers' and industry experts' attention is the cold start\nproblem for new items. In this work, we present a graph neural network\nrecommender system using item hierarchy graphs and a bespoke architecture to\nhandle the cold start case for items. The experimental study on multiple\ndatasets and millions of users and interactions indicates that our method\nachieves better forecasting quality than the state-of-the-art with a comparable\ncomputational time.",
        "Diagnosis and therapeutic effect assessment of Parkinson disease based on\nvoice data are very important,but its few-shot learning problem is\nchallenging.Although deep learning is good at automatic feature extraction, it\nsuffers from few-shot learning problem. Therefore, the general effective method\nis first conduct feature extraction based on prior knowledge, and then carry\nout feature reduction for subsequent classification. However, there are two\nmajor problems: 1) Structural information among speech features has not been\nmined and new features of higher quality have not been reconstructed. 2)\nStructural information between data samples has not been mined and new samples\nwith higher quality have not been reconstructed. To solve these two problems,\nbased on the existing Parkinson speech feature data set, a deep double-side\nlearning ensemble model is designed in this paper that can reconstruct speech\nfeatures and samples deeply and simultaneously. As to feature reconstruction,\nan embedded deep stacked group sparse auto-encoder is designed in this paper to\nconduct nonlinear feature transformation, so as to acquire new high-level deep\nfeatures, and then the deep features are fused with original speech features by\nL1 regularization feature selection method. As to speech sample reconstruction,\na deep sample learning algorithm is designed in this paper based on iterative\nmean clustering to conduct samples transformation, so as to obtain new\nhigh-level deep samples. Finally, the bagging ensemble learning mode is adopted\nto fuse the deep feature learning algorithm and the deep samples learning\nalgorithm together, thereby constructing a deep double-side learning ensemble\nmodel. At the end of this paper, two representative speech datasets of\nParkinson's disease were used for verification. The experimental results show\nthat the proposed algorithm are effective.",
        "The task of localizing and categorizing objects in medical images often\nremains formulated as a semantic segmentation problem. This approach, however,\nonly indirectly solves the coarse localization task by predicting pixel-level\nscores, requiring ad-hoc heuristics when mapping back to object-level scores.\nState-of-the-art object detectors on the other hand, allow for individual\nobject scoring in an end-to-end fashion, while ironically trading in the\nability to exploit the full pixel-wise supervision signal. This can be\nparticularly disadvantageous in the setting of medical image analysis, where\ndata sets are notoriously small. In this paper, we propose Retina U-Net, a\nsimple architecture, which naturally fuses the Retina Net one-stage detector\nwith the U-Net architecture widely used for semantic segmentation in medical\nimages. The proposed architecture recaptures discarded supervision signals by\ncomplementing object detection with an auxiliary task in the form of semantic\nsegmentation without introducing the additional complexity of previously\nproposed two-stage detectors. We evaluate the importance of full segmentation\nsupervision on two medical data sets, provide an in-depth analysis on a series\nof toy experiments and show how the corresponding performance gain grows in the\nlimit of small data sets. Retina U-Net yields strong detection performance only\nreached by its more complex two-staged counterparts. Our framework including\nall methods implemented for operation on 2D and 3D images is available at\ngithub.com/pfjaeger/medicaldetectiontoolkit.",
        "Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.",
        "This paper addresses the task of separating ground points from airborne LiDAR\npoint cloud data in urban areas. A novel ground filtering method using scan\nline segmentation is proposed here, which we call SLSGF. It utilizes the scan\nline information in LiDAR data to segment the LiDAR data. The similarity\nmeasurements are designed to make it possible to segment complex roof\nstructures into a single segment as much as possible so the topological\nrelationships between the roof and the ground are simpler, which will benefit\nthe labeling process. In the labeling process, the initial ground segments are\ndetected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011\nare used to test the accuracy of SLSGF; and our analytical and experimental\nresults show that this method is computationally-efficient and\nnoise-insensitive, thereby making a denoising process unnecessary before\nfiltering.",
        "Image normalization is a critical step in medical imaging. This step is often\ndone on a per-dataset basis, preventing current segmentation algorithms from\nthe full potential of exploiting jointly normalized information across multiple\ndatasets. To solve this problem, we propose an adversarial normalization\napproach for image segmentation which learns common normalizing functions\nacross multiple datasets while retaining image realism. The adversarial\ntraining provides an optimal normalizer that improves both the segmentation\naccuracy and the discrimination of unrealistic normalizing functions. Our\ncontribution therefore leverages common imaging information from multiple\ndomains. The optimality of our common normalizer is evaluated by combining\nbrain images from both infants and adults. Results on the challenging iSEG and\nMRBrainS datasets reveal the potential of our adversarial normalization\napproach for segmentation, with Dice improvements of up to 59.6% over the\nbaseline.",
        "In this paper, we present a conditional generative adversarial network-based\nmodel for real-time underwater image enhancement. To supervise the adversarial\ntraining, we formulate an objective function that evaluates the perceptual\nimage quality based on its global content, color, local texture, and style\ninformation. We also present EUVP, a large-scale dataset of a paired and\nunpaired collection of underwater images (of `poor' and `good' quality) that\nare captured using seven different cameras over various visibility conditions\nduring oceanic explorations and human-robot collaborative experiments. In\naddition, we perform several qualitative and quantitative evaluations which\nsuggest that the proposed model can learn to enhance underwater image quality\nfrom both paired and unpaired training. More importantly, the enhanced images\nprovide improved performances of standard models for underwater object\ndetection, human pose estimation, and saliency prediction. These results\nvalidate that it is suitable for real-time preprocessing in the autonomy\npipeline by visually-guided underwater robots. The model and associated\ntraining pipelines are available at https://github.com/xahidbuffon/funie-gan.",
        "We present an efficient multi-view stereo (MVS) network for 3D reconstruction\nfrom multiview images. While previous learning based reconstruction approaches\nperformed quite well, most of them estimate depth maps at a fixed resolution\nusing plane sweep volumes with a fixed depth hypothesis at each plane, which\nrequires densely sampled planes for desired accuracy and therefore is difficult\nto achieve high resolution depth maps. In this paper we introduce a\ncoarseto-fine depth inference strategy to achieve high resolution depth. This\nstrategy estimates the depth map at coarsest level, while the depth maps at\nfiner levels are considered as the upsampled depth map from previous level with\npixel-wise depth residual. Thus, we narrow the depth searching range with\npriori information from previous level and construct new cost volumes from the\npixel-wise depth residual to perform depth map refinement. Then the final depth\nmap could be achieved iteratively since all the parameters are shared between\ndifferent levels. At each level, the self-attention layer is introduced to the\nfeature extraction block for capturing the long range dependencies for depth\ninference task, and the cost volume is generated using similarity measurement\ninstead of the variance based methods used in previous work. Experiments were\nconducted on both the DTU benchmark dataset and recently released BlendedMVS\ndataset. The results demonstrated that our model could outperform most\nstate-of-the-arts (SOTA) methods. The codebase of this project is at\nhttps://github.com/ArthasMil/AACVP-MVSNet.",
        "Sign language translation (SLT) aims to interpret sign video sequences into\ntext-based natural language sentences. Sign videos consist of continuous\nsequences of sign gestures with no clear boundaries in between. Existing SLT\nmodels usually represent sign visual features in a frame-wise manner so as to\navoid needing to explicitly segmenting the videos into isolated signs. However,\nthese methods neglect the temporal information of signs and lead to substantial\nambiguity in translation. In this paper, we explore the temporal semantic\nstructures of signvideos to learn more discriminative features. To this end, we\nfirst present a novel sign video segment representation which takes into\naccount multiple temporal granularities, thus alleviating the need for accurate\nvideo segmentation. Taking advantage of the proposed segment representation, we\ndevelop a novel hierarchical sign video feature learning method via a temporal\nsemantic pyramid network, called TSPNet. Specifically, TSPNet introduces an\ninter-scale attention to evaluate and enhance local semantic consistency of\nsign segments and an intra-scale attention to resolve semantic ambiguity by\nusing non-local video context. Experiments show that our TSPNet outperforms the\nstate-of-the-art with significant improvements on the BLEU score (from 9.58 to\n13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT\ndataset. Our implementation is available at\nhttps://github.com/verashira/TSPNet.",
        "Although group convolution operators are increasingly used in deep\nconvolutional neural networks to improve the computational efficiency and to\nreduce the number of parameters, most existing methods construct their group\nconvolution architectures by a predefined partitioning of the filters of each\nconvolutional layer into multiple regular filter groups with an equal spatial\ngroup size and data-independence, which prevents a full exploitation of their\npotential. To tackle this issue, we propose a novel method of designing\nself-grouping convolutional neural networks, called SG-CNN, in which the\nfilters of each convolutional layer group themselves based on the similarity of\ntheir importance vectors. Concretely, for each filter, we first evaluate the\nimportance value of their input channels to identify the importance vectors,\nand then group these vectors by clustering. Using the resulting\n\\emph{data-dependent} centroids, we prune the less important connections, which\nimplicitly minimizes the accuracy loss of the pruning, thus yielding a set of\n\\emph{diverse} group convolution filters. Subsequently, we develop two\nfine-tuning schemes, i.e. (1) both local and global fine-tuning and (2) global\nonly fine-tuning, which experimentally deliver comparable results, to recover\nthe recognition capacity of the pruned network. Comprehensive experiments\ncarried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our\nself-grouping convolution method adapts to various state-of-the-art CNN\narchitectures, such as ResNet and DenseNet, and delivers superior performance\nin terms of compression ratio, speedup and recognition accuracy. We demonstrate\nthe ability of SG-CNN to generalise by transfer learning, including domain\nadaption and object detection, showing competitive results. Our source code is\navailable at https://github.com/QingbeiGuo/SG-CNN.git.",
        "With the wide adoption of mobile devices, today's location tracking systems\nsuch as satellites, cellular base stations and wireless access points are\ncontinuously producing tremendous amounts of location data of moving objects.\nThe ability to discover moving objects that travel together, i.e., traveling\ncompanions, from their trajectories is desired by many applications such as\nintelligent transportation systems and location-based services. Existing\nalgorithms are either based on pattern mining methods that define a particular\npattern of traveling companions or based on representation learning methods\nthat learn similar representations for similar trajectories. The former methods\nsuffer from the pairwise point-matching problem and the latter often ignore the\ntemporal proximity between trajectories. In this work, we propose a generic\ndeep representation learning model using autoencoders, namely, ATTN-MEAN, for\nthe discovery of traveling companions. ATTN-MEAN collectively injects spatial\nand temporal information into its input embeddings using skip-gram, positional\nencoding techniques, respectively. Besides, our model further encourages\ntrajectories to learn from their neighbours by leveraging the\nSort-Tile-Recursive algorithm, mean operation and global attention mechanism.\nAfter obtaining the representations from the encoders, we run DBSCAN to cluster\nthe representations to find travelling companion. The corresponding\ntrajectories in the same cluster are considered as traveling companions.\nExperimental results suggest that ATTN-MEAN performs better than the\nstate-of-the-art algorithms on finding traveling companions.",
        "We present techniques for automatically inferring formal properties of\nfeed-forward neural networks. We observe that a significant part (if not all)\nof the logic of feed forward networks is captured in the activation status\n('on' or 'off') of its neurons. We propose to extract patterns based on neuron\ndecisions as preconditions that imply certain desirable output property e.g.,\nthe prediction being a certain class. We present techniques to extract input\nproperties, encoding convex predicates on the input space that imply given\noutput properties and layer properties, representing network properties\ncaptured in the hidden layers that imply the desired output behavior. We apply\nour techniques on networks for the MNIST and ACASXU applications. Our\nexperiments highlight the use of the inferred properties in a variety of tasks,\nsuch as explaining predictions, providing robustness guarantees, simplifying\nproofs, and network distillation.",
        "Visual cues of enforcing bilaterally symmetric anatomies as normal findings\nare widely used in clinical practice to disambiguate subtle abnormalities from\nmedical images. So far, inadequate research attention has been received on\neffectively emulating this practice in CAD methods. In this work, we exploit\nsemantic anatomical symmetry or asymmetry analysis in a complex CAD scenario,\ni.e., anterior pelvic fracture detection in trauma PXRs, where semantically\npathological (refer to as fracture) and non-pathological (e.g., pose)\nasymmetries both occur. Visually subtle yet pathologically critical fracture\nsites can be missed even by experienced clinicians, when limited diagnosis time\nis permitted in emergency care. We propose a novel fracture detection framework\nthat builds upon a Siamese network enhanced with a spatial transformer layer to\nholistically analyze symmetric image features. Image features are spatially\nformatted to encode bilaterally symmetric anatomies. A new contrastive feature\nlearning component in our Siamese network is designed to optimize the deep\nimage features being more salient corresponding to the underlying semantic\nasymmetries (caused by pelvic fracture occurrences). Our proposed method have\nbeen extensively evaluated on 2,359 PXRs from unique patients (the largest\nstudy to-date), and report an area under ROC curve score of 0.9771. This is the\nhighest among state-of-the-art fracture detection methods, with improved\nclinical indications.",
        "We introduce a generic framework that reduces the computational cost of\nobject detection while retaining accuracy for scenarios where objects with\nvaried sizes appear in high resolution images. Detection progresses in a\ncoarse-to-fine manner, first on a down-sampled version of the image and then on\na sequence of higher resolution regions identified as likely to improve the\ndetection accuracy. Built upon reinforcement learning, our approach consists of\na model (R-net) that uses coarse detection results to predict the potential\naccuracy gain for analyzing a region at a higher resolution and another model\n(Q-net) that sequentially selects regions to zoom in. Experiments on the\nCaltech Pedestrians dataset show that our approach reduces the number of\nprocessed pixels by over 50% without a drop in detection accuracy. The merits\nof our approach become more significant on a high resolution test set collected\nfrom YFCC100M dataset, where our approach maintains high detection performance\nwhile reducing the number of processed pixels by about 70% and the detection\ntime by over 50%.",
        "Mixup is a powerful data augmentation method that interpolates between two or\nmore examples in the input or feature space and between the corresponding\ntarget labels. Many recent mixup methods focus on cutting and pasting two or\nmore objects into one image, which is more about efficient processing than\ninterpolation. However, how to best interpolate images is not well defined. In\nthis sense, mixup has been connected to autoencoders, because often\nautoencoders \"interpolate well\", for instance generating an image that\ncontinuously deforms into another.\n  In this work, we revisit mixup from the interpolation perspective and\nintroduce AlignMix, where we geometrically align two images in the feature\nspace. The correspondences allow us to interpolate between two sets of\nfeatures, while keeping the locations of one set. Interestingly, this gives\nrise to a situation where mixup retains mostly the geometry or pose of one\nimage and the texture of the other, connecting it to style transfer. More than\nthat, we show that an autoencoder can still improve representation learning\nunder mixup, without the classifier ever seeing decoded images. AlignMix\noutperforms state-of-the-art mixup methods on five different benchmarks.",
        "The successful deployment of artificial intelligence (AI) in many domains\nfrom healthcare to hiring requires their responsible use, particularly in model\nexplanations and privacy. Explainable artificial intelligence (XAI) provides\nmore information to help users to understand model decisions, yet this\nadditional knowledge exposes additional risks for privacy attacks. Hence,\nproviding explanation harms privacy. We study this risk for image-based model\ninversion attacks and identified several attack architectures with increasing\nperformance to reconstruct private image data from model explanations. We have\ndeveloped several multi-modal transposed CNN architectures that achieve\nsignificantly higher inversion performance than using the target model\nprediction only. These XAI-aware inversion models were designed to exploit the\nspatial knowledge in image explanations. To understand which explanations have\nhigher privacy risk, we analyzed how various explanation types and factors\ninfluence inversion performance. In spite of some models not providing\nexplanations, we further demonstrate increased inversion performance even for\nnon-explainable target models by exploiting explanations of surrogate models\nthrough attention transfer. This method first inverts an explanation from the\ntarget prediction, then reconstructs the target image. These threats highlight\nthe urgent and significant privacy risks of explanations and calls attention\nfor new privacy preservation techniques that balance the dual-requirement for\nAI explainability and privacy.",
        "Current work in explainable reinforcement learning generally produces\npolicies in the form of a decision tree over the state space. Such policies can\nbe used for formal safety verification, agent behavior prediction, and manual\ninspection of important features. However, existing approaches fit a decision\ntree after training or use a custom learning procedure which is not compatible\nwith new learning techniques, such as those which use neural networks. To\naddress this limitation, we propose a novel Markov Decision Process (MDP) type\nfor learning decision tree policies: Iterative Bounding MDPs (IBMDPs). An IBMDP\nis constructed around a base MDP so each IBMDP policy is guaranteed to\ncorrespond to a decision tree policy for the base MDP when using a\nmethod-agnostic masking procedure. Because of this decision tree equivalence,\nany function approximator can be used during training, including a neural\nnetwork, while yielding a decision tree policy for the base MDP. We present the\nrequired masking procedure as well as a modified value update step which allows\nIBMDPs to be solved using existing algorithms. We apply this procedure to\nproduce IBMDP variants of recent reinforcement learning methods. We empirically\nshow the benefits of our approach by solving IBMDPs to produce decision tree\npolicies for the base MDPs.",
        "The ability to learn disentangled representations that split underlying\nsources of variation in high dimensional, unstructured data is important for\ndata efficient and robust use of neural networks. While various approaches\naiming towards this goal have been proposed in recent times, a commonly\naccepted definition and validation procedure is missing. We provide a causal\nperspective on representation learning which covers disentanglement and domain\nshift robustness as special cases. Our causal framework allows us to introduce\na new metric for the quantitative evaluation of deep latent variable models. We\nshow how this metric can be estimated from labeled observational data and\nfurther provide an efficient estimation algorithm that scales linearly in the\ndataset size.",
        "We presented an optical system to perform imaging interested objects in\ncomplex scenes, like the creature easy see the interested prey in the hunt for\ncomplex environments. It utilized Deep-learning network to learn the interested\nobjects's vision features and designed the corresponding \"imaging matrices\",\nfurthermore the learned matrixes act as the measurement matrix to complete\ncompressive imaging with a single-pixel camera, finally we can using the\ncompressed image data to only image the interested objects without the rest\nobjects and backgrounds of the scenes with the previous Deep-learning network.\nOur results demonstrate that no matter interested object is single feature or\nrich details, the interference can be successfully filtered out and this idea\ncan be applied in some common applications that effectively improve the\nperformance. This bio-inspired optical system can act as the creature eye to\nachieve success on interested-based object imaging, object detection, object\nrecognition and object tracking, etc.",
        "We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.",
        "Hierarchical support vector regression (HSVR) models a function from data as\na linear combination of SVR models at a range of scales, starting at a coarse\nscale and moving to finer scales as the hierarchy continues. In the original\nformulation of HSVR, there were no rules for choosing the depth of the model.\nIn this paper, we observe in a number of models a phase transition in the\ntraining error -- the error remains relatively constant as layers are added,\nuntil a critical scale is passed, at which point the training error drops close\nto zero and remains nearly constant for added layers. We introduce a method to\npredict this critical scale a priori with the prediction based on the support\nof either a Fourier transform of the data or the Dynamic Mode Decomposition\n(DMD) spectrum. This allows us to determine the required number of layers prior\nto training any models.",
        "In e-commerce, a growing number of user-generated videos are used for product\npromotion. How to generate video descriptions that narrate the user-preferred\nproduct characteristics depicted in the video is vital for successful\npromoting. Traditional video captioning methods, which focus on routinely\ndescribing what exists and happens in a video, are not amenable for\nproduct-oriented video captioning. To address this problem, we propose a\nproduct-oriented video captioner framework, abbreviated as Poet. Poet firstly\nrepresents the videos as product-oriented spatial-temporal graphs. Then, based\non the aspects of the video-associated product, we perform knowledge-enhanced\nspatial-temporal inference on those graphs for capturing the dynamic change of\nfine-grained product-part characteristics. The knowledge leveraging module in\nPoet differs from the traditional design by performing knowledge filtering and\ndynamic memory modeling. We show that Poet achieves consistent performance\nimprovement over previous methods concerning generation quality, product\naspects capturing, and lexical diversity. Experiments are performed on two\nproduct-oriented video captioning datasets, buyer-generated fashion video\ndataset (BFVD) and fan-generated fashion video dataset (FFVD), collected from\nMobile Taobao. We will release the desensitized datasets to promote further\ninvestigations on both video captioning and general video analysis problems.",
        "Applying network science approaches to investigate the functions and anatomy\nof the human brain is prevalent in modern medical imaging analysis. Due to the\ncomplex network topology, for an individual brain, mining a discriminative\nnetwork representation from the multimodal brain networks is non-trivial. The\nrecent success of deep learning techniques on graph-structured data suggests a\nnew way to model the non-linear cross-modality relationship. However, current\ndeep brain network methods either ignore the intrinsic graph topology or\nrequire a network basis shared within a group. To address these challenges, we\npropose a novel end-to-end deep graph representation learning (Deep Multimodal\nBrain Networks - DMBN) to fuse multimodal brain networks. Specifically, we\ndecipher the cross-modality relationship through a graph encoding and decoding\nprocess. The higher-order network mappings from brain structural networks to\nfunctional networks are learned in the node domain. The learned network\nrepresentation is a set of node features that are informative to induce brain\nsaliency maps in a supervised manner. We test our framework in both synthetic\nand real image data. The experimental results show the superiority of the\nproposed method over some other state-of-the-art deep brain network models.",
        "We present a new way of study of Mercer kernels, by corresponding to a\nspecial kernel $K$ a pseudo-differential operator $p({\\mathbf x}, D)$ such that\n$\\mathcal{F} p({\\mathbf x}, D)^\\dag p({\\mathbf x}, D) \\mathcal{F}^{-1}$ acts on\nsmooth functions in the same way as an integral operator associated with $K$\n(where $\\mathcal{F}$ is the Fourier transform). We show that kernels defined by\npseudo-differential operators are able to approximate uniformly any continuous\nMercer kernel on a compact set.\n  The symbol $p({\\mathbf x}, {\\mathbf y})$ encapsulates a lot of useful\ninformation about the structure of the Maximum Mean Discrepancy distance\ndefined by the kernel $K$. We approximate $p({\\mathbf x}, {\\mathbf y})$ with\nthe sum of the first $r$ terms of the Singular Value Decomposition of $p$,\ndenoted by $p_r({\\mathbf x}, {\\mathbf y})$. If ordered singular values of the\nintegral operator associated with $p({\\mathbf x}, {\\mathbf y})$ die down\nrapidly, the MMD distance defined by the new symbol $p_r$ differs from the\ninitial one only slightly. Moreover, the new MMD distance can be interpreted as\nan aggregated result of comparing $r$ local moments of two probability\ndistributions.\n  The latter results holds under the condition that right singular vectors of\nthe integral operator associated with $p$ are uniformly bounded. But even if\nthis is not satisfied we can still hold that the Hilbert-Schmidt distance\nbetween $p$ and $p_r$ vanishes. Thus, we report an interesting phenomenon: the\nMMD distance measures the difference of two probability distributions with\nrespect to a certain number of local moments, $r^\\ast$, and this number\n$r^\\ast$ depends on the speed with which singular values of $p$ die down.",
        "The accuracy and robustness of image classification with supervised deep\nlearning are dependent on the availability of large-scale, annotated training\ndata. However, there is a paucity of annotated data available due to the\ncomplexity of manual annotation. To overcome this problem, a popular approach\nis to use transferable knowledge across different domains by: 1) using a\ngeneric feature extractor that has been pre-trained on large-scale general\nimages (i.e., transfer-learned) but which not suited to capture characteristics\nfrom medical images; or 2) fine-tuning generic knowledge with a relatively\nsmaller number of annotated images. Our aim is to reduce the reliance on\nannotated training data by using a new hierarchical unsupervised feature\nextractor with a convolutional auto-encoder placed atop of a pre-trained\nconvolutional neural network. Our approach constrains the rich and generic\nimage features from the pre-trained domain to a sophisticated representation of\nthe local image characteristics from the unannotated medical image domain. Our\napproach has a higher classification accuracy than transfer-learned approaches\nand is competitive with state-of-the-art supervised fine-tuned methods.",
        "Hidden Markov Models (HMM) have been used for several years in many time\nseries analysis or pattern recognitions tasks. HMM are often trained by means\nof the Baum-Welch algorithm which can be seen as a special variant of an\nexpectation maximization (EM) algorithm. Second-order training techniques such\nas Variational Bayesian Inference (VI) for probabilistic models regard the\nparameters of the probabilistic models as random variables and define\ndistributions over these distribution parameters, hence the name of this\ntechnique. VI can also bee regarded as a special case of an EM algorithm. In\nthis article, we bring both together and train HMM with multivariate Gaussian\noutput distributions with VI. The article defines the new training technique\nfor HMM. An evaluation based on some case studies and a comparison to related\napproaches is part of our ongoing work.",
        "Many problems in machine learning can be cast as learning functions from sets\nto graphs, or more generally to hypergraphs; in short, Set2Graph functions.\nExamples include clustering, learning vertex and edge features on graphs, and\nlearning features on triplets in a collection. A natural approach for building\nSet2Graph models is to characterize all linear equivariant set-to-hypergraph\nlayers and stack them with non-linear activations. This poses two challenges:\n(i) the expressive power of these networks is not well understood; and (ii)\nthese models would suffer from high, often intractable computational and memory\ncomplexity, as their dimension grows exponentially. This paper advocates a\nfamily of neural network models for learning Set2Graph functions that is both\npractical and of maximal expressive power (universal), that is, can approximate\narbitrary continuous Set2Graph functions over compact sets. Testing these\nmodels on different machine learning tasks, mainly an application to particle\nphysics, we find them favorable to existing baselines.",
        "We study how to effectively leverage expert feedback to learn sequential\ndecision-making policies. We focus on problems with sparse rewards and long\ntime horizons, which typically pose significant challenges in reinforcement\nlearning. We propose an algorithmic framework, called hierarchical guidance,\nthat leverages the hierarchical structure of the underlying problem to\nintegrate different modes of expert interaction. Our framework can incorporate\ndifferent combinations of imitation learning (IL) and reinforcement learning\n(RL) at different levels, leading to dramatic reductions in both expert effort\nand cost of exploration. Using long-horizon benchmarks, including Montezuma's\nRevenge, we demonstrate that our approach can learn significantly faster than\nhierarchical RL, and be significantly more label-efficient than standard IL. We\nalso theoretically analyze labeling cost for certain instantiations of our\nframework.",
        "In this paper, we propose to tackle the problem of reducing discrepancies\nbetween multiple domains referred to as multi-source domain adaptation and\nconsider it under the target shift assumption: in all domains we aim to solve a\nclassification problem with the same output classes, but with labels'\nproportions differing across them. This problem, generally ignored in the vast\nmajority papers on domain adaptation papers, is nevertheless critical in\nreal-world applications, and we theoretically show its impact on the adaptation\nsuccess. To address this issue, we design a method based on optimal transport,\na theory that has been successfully used to tackle adaptation problems in\nmachine learning. Our method performs multi-source adaptation and target shift\ncorrection simultaneously by learning the class probabilities of the unlabeled\ntarget sample and the coupling allowing to align two (or more) probability\ndistributions. Experiments on both synthetic and real-world data related to\nsatellite image segmentation task show the superiority of the proposed method\nover the state-of-the-art.",
        "Automated semantic segmentation and object detection are of great importance\nin geospatial data analysis. However, supervised machine learning systems such\nas convolutional neural networks require large corpora of annotated training\ndata. Especially in the geospatial domain, such datasets are quite scarce.\nWithin this paper, we aim to alleviate this issue by introducing a new\nannotated 3D dataset that is unique in three ways: i) The dataset consists of\nboth an Unmanned Aerial Vehicle (UAV) laser scanning point cloud and a 3D\ntextured mesh. ii) The point cloud features a mean point density of about 800\npts/sqm and the oblique imagery used for 3D mesh texturing realizes a ground\nsampling distance of about 2-3 cm. This enables the identification of\nfine-grained structures and represents the state of the art in UAV-based\nmapping. iii) Both data modalities will be published for a total of three\nepochs allowing applications such as change detection. The dataset depicts the\nvillage of Hessigheim (Germany), henceforth referred to as H3D. It is designed\nto promote research in the field of 3D data analysis on one hand and to\nevaluate and rank existing and emerging approaches for semantic segmentation of\nboth data modalities on the other hand. Ultimately, we hope that H3D will\nbecome a widely used benchmark dataset in company with the well-established\nISPRS Vaihingen 3D Semantic Labeling Challenge benchmark (V3D). The dataset can\nbe downloaded from\nhttps://ifpwww.ifp.uni-stuttgart.de/benchmark/hessigheim/default.aspx.",
        "Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a\n3D solid volume, exhibits advantages in numerous application domains. However,\nexisting methods generally synthesize solid texture with specific features,\nwhich may result in the failure of capturing diversified textural information.\nIn this paper, we propose a novel generative adversarial nets-based approach\n(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our\nmulti-scale discriminators evaluate the similarity between patch from exemplar\nand slice from the generated volume, promoting the generator to synthesize\nrealistic solid textures. Experimental results demonstrate that the proposed\nmethod can generate high-quality solid textures with similar visual\ncharacteristics to the exemplar.",
        "In recent years, inductive graph embedding models, \\emph{viz.}, graph neural\nnetworks (GNNs) have become increasingly accurate at link prediction (LP) in\nonline social networks. The performance of such networks depends strongly on\nthe input node features, which vary across networks and applications. Selecting\nappropriate node features remains application-dependent and generally an open\nquestion. Moreover, owing to privacy and ethical issues, use of personalized\nnode features is often restricted. In fact, many publicly available data from\nonline social network do not contain any node features (e.g., demography). In\nthis work, we provide a comprehensive experimental analysis which shows that\nharnessing a transductive technique (e.g., Node2Vec) for obtaining initial node\nrepresentations, after which an inductive node embedding technique takes over,\nleads to substantial improvements in link prediction accuracy. We demonstrate\nthat, for a wide variety of GNN variants, node representation vectors obtained\nfrom Node2Vec serve as high quality input features to GNNs, thereby improving\nLP performance.",
        "In a world where security issues have been gaining growing importance, face\nrecognition systems have attracted increasing attention in multiple application\nareas, ranging from forensics and surveillance to commerce and entertainment.\nTo help understanding the landscape and abstraction levels relevant for face\nrecognition systems, face recognition taxonomies allow a deeper dissection and\ncomparison of the existing solutions. This paper proposes a new, more\nencompassing and richer multi-level face recognition taxonomy, facilitating the\norganization and categorization of available and emerging face recognition\nsolutions; this taxonomy may also guide researchers in the development of more\nefficient face recognition solutions. The proposed multi-level taxonomy\nconsiders levels related to the face structure, feature support and feature\nextraction approach. Following the proposed taxonomy, a comprehensive survey of\nrepresentative face recognition solutions is presented. The paper concludes\nwith a discussion on current algorithmic and application related challenges\nwhich may define future research directions for face recognition.",
        "State-of-the-art methods in image-to-image translation are capable of\nlearning a mapping from a source domain to a target domain with unpaired image\ndata. Though the existing methods have achieved promising results, they still\nproduce visual artifacts, being able to translate low-level information but not\nhigh-level semantics of input images. One possible reason is that generators do\nnot have the ability to perceive the most discriminative parts between the\nsource and target domains, thus making the generated images low quality. In\nthis paper, we propose a new Attention-Guided Generative Adversarial Networks\n(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN\ncan identify the most discriminative foreground objects and minimize the change\nof the background. The attention-guided generators in AttentionGAN are able to\nproduce attention masks, and then fuse the generation output with the attention\nmasks to obtain high-quality target images. Accordingly, we also design a novel\nattention-guided discriminator which only considers attended regions. Extensive\nexperiments are conducted on several generative tasks with eight public\ndatasets, demonstrating that the proposed method is effective to generate\nsharper and more realistic images compared with existing competitive models.\nThe code is available at https://github.com/Ha0Tang/AttentionGAN.",
        "We present a method that learns to answer visual questions by selecting image\nregions relevant to the text-based query. Our method exhibits significant\nimprovements in answering questions such as \"what color,\" where it is necessary\nto evaluate a specific location, and \"what room,\" where it selectively\nidentifies informative image regions. Our model is tested on the VQA dataset\nwhich is the largest human-annotated visual question answering dataset to our\nknowledge.",
        "Learning useful representations without supervision remains a key challenge\nin machine learning. In this paper, we propose a simple yet powerful generative\nmodel that learns such discrete representations. Our model, the Vector\nQuantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:\nthe encoder network outputs discrete, rather than continuous, codes; and the\nprior is learnt rather than static. In order to learn a discrete latent\nrepresentation, we incorporate ideas from vector quantisation (VQ). Using the\nVQ method allows the model to circumvent issues of \"posterior collapse\" --\nwhere the latents are ignored when they are paired with a powerful\nautoregressive decoder -- typically observed in the VAE framework. Pairing\nthese representations with an autoregressive prior, the model can generate high\nquality images, videos, and speech as well as doing high quality speaker\nconversion and unsupervised learning of phonemes, providing further evidence of\nthe utility of the learnt representations.",
        "This paper investigates the idea of encoding object-centered representations\nin the design of the reward function and policy architectures of a\nlanguage-guided reinforcement learning agent. This is done using a combination\nof object-wise permutation invariant networks inspired from Deep Sets and\ngated-attention mechanisms. In a 2D procedurally-generated world where agents\ntargeting goals in natural language navigate and interact with objects, we show\nthat these architectures demonstrate strong generalization capacities to\nout-of-distribution goals. We study the generalization to varying numbers of\nobjects at test time and further extend the object-centered architectures to\ngoals involving relational reasoning.",
        "Batch Normalization (BN) is a common technique used to speed-up and stabilize\ntraining. On the other hand, the learnable parameters of BN are commonly used\nin conditional Generative Adversarial Networks (cGANs) for representing\nclass-specific information using conditional Batch Normalization (cBN). In this\npaper we propose to generalize both BN and cBN using a Whitening and Coloring\nbased batch normalization. We show that our conditional Coloring can represent\ncategorical conditioning information which largely helps the cGAN qualitative\nresults. Moreover, we show that full-feature whitening is important in a\ngeneral GAN scenario in which the training process is known to be highly\nunstable. We test our approach on different datasets and using different GAN\nnetworks and training protocols, showing a consistent improvement in all the\ntested frameworks. Our CIFAR-10 conditioned results are higher than all\nprevious works on this dataset.",
        "The sound of crashing waves, the roar of fast-moving cars -- sound conveys\nimportant information about the objects in our surroundings. In this work, we\nshow that ambient sounds can be used as a supervisory signal for learning\nvisual models. To demonstrate this, we train a convolutional neural network to\npredict a statistical summary of the sound associated with a video frame. We\nshow that, through this process, the network learns a representation that\nconveys information about objects and scenes. We evaluate this representation\non several recognition tasks, finding that its performance is comparable to\nthat of other state-of-the-art unsupervised learning methods. Finally, we show\nthrough visualizations that the network learns units that are selective to\nobjects that are often associated with characteristic sounds. This paper\nextends an earlier conference paper, Owens et al. 2016, with additional\nexperiments and discussion.",
        "Modern autonomous driving systems rely heavily on deep learning models to\nprocess point cloud sensory data; meanwhile, deep models have been shown to be\nsusceptible to adversarial attacks with visually imperceptible perturbations.\nDespite the fact that this poses a security concern for the self-driving\nindustry, there has been very little exploration in terms of 3D perception, as\nmost adversarial attacks have only been applied to 2D flat images. In this\npaper, we address this issue and present a method to generate universal 3D\nadversarial objects to fool LiDAR detectors. In particular, we demonstrate that\nplacing an adversarial object on the rooftop of any target vehicle to hide the\nvehicle entirely from LiDAR detectors with a success rate of 80%. We report\nattack results on a suite of detectors using various input representation of\npoint clouds. We also conduct a pilot study on adversarial defense using data\naugmentation. This is one step closer towards safer self-driving under unseen\nconditions from limited training data.",
        "When developing and analyzing new hyperparameter optimization (HPO) methods,\nit is vital to empirically evaluate and compare them on well-curated benchmark\nsuites. In this work, we list desirable properties and requirements for such\nbenchmarks and propose a new set of challenging and relevant multifidelity HPO\nbenchmark problems motivated by these requirements. For this, we revisit the\nconcept of surrogate-based benchmarks and empirically compare them to more\nwidely-used tabular benchmarks, showing that the latter ones may induce bias in\nperformance estimation and ranking of HPO methods. We present a new\nsurrogate-based benchmark suite for multifidelity HPO methods consisting of 9\nbenchmark collections that constitute over 700 multifidelity HPO problems in\ntotal. All our benchmarks also allow for querying of multiple optimization\ntargets, enabling the benchmarking of multi-objective HPO. We examine and\ncompare our benchmark suite with respect to the defined requirements and show\nthat our benchmarks provide viable additions to existing suites.",
        "Continuous-depth neural models, where the derivative of the model's hidden\nstate is defined by a neural network, have enabled strong sequential data\nprocessing capabilities. However, these models rely on advanced numerical\ndifferential equation (DE) solvers resulting in a significant overhead both in\nterms of computational cost and model complexity. In this paper, we present a\nnew family of models, termed Closed-form Continuous-depth (CfC) networks, that\nare simple to describe and at least one order of magnitude faster while\nexhibiting equally strong modeling abilities compared to their ODE-based\ncounterparts. The models are hereby derived from the analytical closed-form\nsolution of an expressive subset of time-continuous models, thus alleviating\nthe need for complex DE solvers all together. In our experimental evaluations,\nwe demonstrate that CfC networks outperform advanced, recurrent models over a\ndiverse set of time-series prediction tasks, including those with long-term\ndependencies and irregularly sampled data. We believe our findings open new\nopportunities to train and deploy rich, continuous neural models in\nresource-constrained settings, which demand both performance and efficiency.",
        "To be truly understandable and accepted by Deaf communities, an automatic\nSign Language Production (SLP) system must generate a photo-realistic signer.\nPrior approaches based on graphical avatars have proven unpopular, whereas\nrecent neural SLP works that produce skeleton pose sequences have been shown to\nbe not understandable to Deaf viewers.\n  In this paper, we propose SignGAN, the first SLP model to produce\nphoto-realistic continuous sign language videos directly from spoken language.\nWe employ a transformer architecture with a Mixture Density Network (MDN)\nformulation to handle the translation from spoken language to skeletal pose. A\npose-conditioned human synthesis model is then introduced to generate a\nphoto-realistic sign language video from the skeletal pose sequence. This\nallows the photo-realistic production of sign videos directly translated from\nwritten text.\n  We further propose a novel keypoint-based loss function, which significantly\nimproves the quality of synthesized hand images, operating in the keypoint\nspace to avoid issues caused by motion blur. In addition, we introduce a method\nfor controllable video generation, enabling training on large, diverse sign\nlanguage datasets and providing the ability to control the signer appearance at\ninference.\n  Using a dataset of eight different sign language interpreters extracted from\nbroadcast footage, we show that SignGAN significantly outperforms all baseline\nmethods for quantitative metrics and human perceptual studies.",
        "One of the major characteristics of financial time series is that they\ncontain a large amount of non-stationary noise, which is challenging for deep\nneural networks. People normally use various features to address this problem.\nHowever, the performance of these features depends on the choice of\nhyper-parameters. In this paper, we propose to use neural networks to represent\nthese indicators and train a large network constructed of smaller networks as\nfeature layers to fine-tune the prior knowledge represented by the indicators.\nDuring back propagation, prior knowledge is transferred from human logic to\nmachine logic via gradient descent. Prior knowledge is the deep belief of\nneural network and teaches the network to not be affected by non-stationary\nnoise. Moreover, co-distillation is applied to distill the structure into a\nmuch smaller size to reduce redundant features and the risk of overfitting. In\naddition, the decisions of the smaller networks in terms of gradient descent\nare more robust and cautious than those of large networks. In numerical\nexperiments, we find that our algorithm is faster and more accurate than\ntraditional methods on real financial datasets. We also conduct experiments to\nverify and comprehend the method.",
        "There is a longstanding debate whether the Kolmogorov-Arnold representation\ntheorem can explain the use of more than one hidden layer in neural networks.\nThe Kolmogorov-Arnold representation decomposes a multivariate function into an\ninterior and an outer function and therefore has indeed a similar structure as\na neural network with two hidden layers. But there are distinctive differences.\nOne of the main obstacles is that the outer function depends on the represented\nfunction and can be wildly varying even if the represented function is smooth.\nWe derive modifications of the Kolmogorov-Arnold representation that transfer\nsmoothness properties of the represented function to the outer function and can\nbe well approximated by ReLU networks. It appears that instead of two hidden\nlayers, a more natural interpretation of the Kolmogorov-Arnold representation\nis that of a deep neural network where most of the layers are required to\napproximate the interior function.",
        "Optical coherence tomography (OCT) has become a favorable device in the\nDermatology discipline due to its moderate resolution and penetration depth.\nOCT images however contain a grainy pattern, called speckle, due to the use of\na broadband source in the configuration of OCT. So far, a variety of filtering\n(de-speckling) techniques is introduced to reduce speckle in OCT images. Most\nof these methods are generic and can be applied to OCT images of different\ntissues. The ambition of this work is to provide a de-speckling framework\nspecialized for filtering skin tissues for the community to utilize, adapt or\nbuild upon. In this paper, we present an adaptive cluster-based filtering\nframework, optimized for speckle reduction of OCT skin images. In this\nframework, by considering the layered structure of skin, first the OCT skin\nimages are segmented into differentiable layers utilizing clustering\nalgorithms, and then each cluster is de-speckled individually using adaptive\nfiltering techniques. In this study, hierarchical clustering algorithm and\nadaptive Wiener filtering technique are utilized to develop the framework. The\nproposed method is tested on optical solid phantoms with predetermined optical\nproperties. The method is also tested on healthy human skin images. The results\nshow that the proposed cluster-based filtering method can effectively reduce\nthe speckle and increase the signal-to-noise ratio and contrast while\npreserving the edges in the image. The proposed cluster-based filtering\nframework enables researchers to develop unsupervised learning solutions for\nde-speckling OCT skin images using adaptive filtering methods, or extend the\nframework to new applications.",
        "3D object detection is an important module in autonomous driving and\nrobotics. However, many existing methods focus on using single frames to\nperform 3D detection, and do not fully utilize information from multiple\nframes. In this paper, we present 3D-MAN: a 3D multi-frame attention network\nthat effectively aggregates features from multiple perspectives and achieves\nstate-of-the-art performance on Waymo Open Dataset. 3D-MAN first uses a novel\nfast single-frame detector to produce box proposals. The box proposals and\ntheir corresponding feature maps are then stored in a memory bank. We design a\nmulti-view alignment and aggregation module, using attention networks, to\nextract and aggregate the temporal features stored in the memory bank. This\neffectively combines the features coming from different perspectives of the\nscene. We demonstrate the effectiveness of our approach on the large-scale\ncomplex Waymo Open Dataset, achieving state-of-the-art results compared to\npublished single-frame and multi-frame methods.",
        "Extracting text objects from the PDF images is a challenging problem. The\ntext data present in the PDF images contain certain useful information for\nautomatic annotation, indexing etc. However variations of the text due to\ndifferences in text style, font, size, orientation, alignment as well as\ncomplex structure make the problem of automatic text extraction extremely\ndifficult and challenging job. This paper presents two techniques under\nblock-based classification. After a brief introduction of the classification\nmethods, two methods were enhanced and results were evaluated. The performance\nmetrics for segmentation and time consumption are tested for both the models.",
        "This work investigates the well-known problem of morphing attacks, which has\ndrawn considerable attention in the biometrics community. Morphed images have\nexposed face recognition systems' susceptibility to false acceptance, resulting\nin dire consequences, especially for national security applications. To detect\nmorphing attacks, we propose a method which is based on a discriminative 2D\nDiscrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can\nhighlight inconsistencies between a real and a morphed image. We observe that\nthere is a salient discrepancy between the entropy of a given sub-band in a\nbona fide image, and the same sub-band's entropy in a morphed sample.\nConsidering this dissimilarity between these two entropy values, we find the\nKullback-Leibler divergence between the two distributions, namely the entropy\nof the bona fide and the corresponding morphed images. The most discriminative\nwavelet sub-bands are those with the highest corresponding KL-divergence\nvalues. Accordingly, 22 sub-bands are selected as the most discriminative ones\nin terms of morph detection. We show that a Deep Neural Network (DNN) trained\non the 22 discriminative sub-bands can detect morphed samples precisely. Most\nimportantly, the effectiveness of our algorithm is validated through\nexperiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an\nablation study on the sub-band selection.",
        "Data augmentation technique from computer vision has been widely considered\nas a regularization method to improve data efficiency and generalization\nperformance in vision-based reinforcement learning. We variate the timing of\nusing augmentation, which is, in turn, critical depending on tasks to be solved\nin training and testing. According to our experiments on Open AI Procgen\nBenchmark, if the regularization imposed by augmentation is helpful only in\ntesting, it is better to procrastinate the augmentation after training than to\nuse it during training in terms of sample and computation complexity. We note\nthat some of such augmentations can disturb the training process. Conversely,\nan augmentation providing regularization useful in training needs to be used\nduring the whole training period to fully utilize its benefit in terms of not\nonly generalization but also data efficiency. These phenomena suggest a useful\ntiming control of data augmentation in reinforcement learning.",
        "We present DeepPerimeter, a deep learning based pipeline for inferring a full\nindoor perimeter (i.e. exterior boundary map) from a sequence of posed RGB\nimages. Our method relies on robust deep methods for depth estimation and wall\nsegmentation to generate an exterior boundary point cloud, and then uses deep\nunsupervised clustering to fit wall planes to obtain a final boundary map of\nthe room. We demonstrate that DeepPerimeter results in excellent visual and\nquantitative performance on the popular ScanNet and FloorNet datasets and works\nfor room shapes of various complexities as well as in multiroom scenarios. We\nalso establish important baselines for future work on indoor perimeter\nestimation, topics which will become increasingly prevalent as application\nareas like augmented reality and robotics become more significant.",
        "Generative adversarial networks (GANs) are widely used in image generation\ntasks, yet the generated images are usually lack of texture details. In this\npaper, we propose a general framework, called Progressively Unfreezing\nPerceptual GAN (PUPGAN), which can generate images with fine texture details.\nParticularly, we propose an adaptive perceptual discriminator with a\npre-trained perceptual feature extractor, which can efficiently measure the\ndiscrepancy between multi-level features of the generated and real images. In\naddition, we propose a progressively unfreezing scheme for the adaptive\nperceptual discriminator, which ensures a smooth transfer process from a large\nscale classification task to a specified image generation task. The qualitative\nand quantitative experiments with comparison to the classical baselines on\nthree image generation tasks, i.e. single image super-resolution, paired\nimage-to-image translation and unpaired image-to-image translation demonstrate\nthe superiority of PUPGAN over the compared approaches.",
        "In multi-agent games, the complexity of the environment can grow\nexponentially as the number of agents increases, so it is particularly\nchallenging to learn good policies when the agent population is large. In this\npaper, we introduce Evolutionary Population Curriculum (EPC), a curriculum\nlearning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by\nprogressively increasing the population of training agents in a stage-wise\nmanner. Furthermore, EPC uses an evolutionary approach to fix an objective\nmisalignment issue throughout the curriculum: agents successfully trained in an\nearly stage with a small population are not necessarily the best candidates for\nadapting to later stages with scaled populations. Concretely, EPC maintains\nmultiple sets of agents in each stage, performs mix-and-match and fine-tuning\nover these sets and promotes the sets of agents with the best adaptability to\nthe next stage. We implement EPC on a popular MARL algorithm, MADDPG, and\nempirically show that our approach consistently outperforms baselines by a\nlarge margin as the number of agents grows exponentially.",
        "The Hierarchical Vote Collective of Transformation-based Ensembles\n(HIVE-COTE) is a heterogeneous meta ensemble for time series classification.\nHIVE-COTE forms its ensemble from classifiers of multiple domains, including\nphase-independent shapelets, bag-of-words based dictionaries and\nphase-dependent intervals. Since it was first proposed in 2016, the algorithm\nhas remained state of the art for accuracy on the UCR time series\nclassification archive. Over time it has been incrementally updated,\nculminating in its current state, HIVE-COTE 1.0. During this time a number of\nalgorithms have been proposed which match the accuracy of HIVE-COTE. We propose\ncomprehensive changes to the HIVE-COTE algorithm which significantly improve\nits accuracy and usability, presenting this upgrade as HIVE-COTE 2.0. We\nintroduce two novel classifiers, the Temporal Dictionary Ensemble (TDE) and\nDiverse Representation Canonical Interval Forest (DrCIF), which replace\nexisting ensemble members. Additionally, we introduce the Arsenal, an ensemble\nof ROCKET classifiers as a new HIVE-COTE 2.0 constituent. We demonstrate that\nHIVE-COTE 2.0 is significantly more accurate than the current state of the art\non 112 univariate UCR archive datasets and 26 multivariate UEA archive\ndatasets.",
        "Pose-guided person image generation is to transform a source person image to\na target pose. This task requires spatial manipulations of source data.\nHowever, Convolutional Neural Networks are limited by the lack of ability to\nspatially transform the inputs. In this paper, we propose a differentiable\nglobal-flow local-attention framework to reassemble the inputs at the feature\nlevel. Specifically, our model first calculates the global correlations between\nsources and targets to predict flow fields. Then, the flowed local patch pairs\nare extracted from the feature maps to calculate the local attention\ncoefficients. Finally, we warp the source features using a content-aware\nsampling method with the obtained local attention coefficients. The results of\nboth subjective and objective experiments demonstrate the superiority of our\nmodel. Besides, additional results in video animation and view synthesis show\nthat our model is applicable to other tasks requiring spatial transformation.\nOur source code is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.",
        "The main success stories of deep learning, starting with ImageNet, depend on\ndeep convolutional networks, which on certain tasks perform significantly\nbetter than traditional shallow classifiers, such as support vector machines,\nand also better than deep fully connected networks; but what is so special\nabout deep convolutional networks? Recent results in approximation theory\nproved an exponential advantage of deep convolutional networks with or without\nshared weights in approximating functions with hierarchical locality in their\ncompositional structure. More recently, the hierarchical structure was proved\nto be hard to learn from data, suggesting that it is a powerful prior embedded\nin the architecture of the network. These mathematical results, however, do not\nsay which real-life tasks correspond to input-output functions with\nhierarchical locality. To evaluate this, we consider a set of visual tasks\nwhere we disrupt the local organization of images via \"deterministic\nscrambling\" to later perform a visual task on these images structurally-altered\nin the same way for training and testing. For object recognition we find, as\nexpected, that scrambling does not affect the performance of shallow or deep\nfully connected networks contrary to the out-performance of convolutional\nnetworks. Not all tasks involving images are however affected. Texture\nperception and global color estimation are much less sensitive to deterministic\nscrambling showing that the underlying functions corresponding to these tasks\nare not hierarchically local; and also counter-intuitively showing that these\ntasks are better approximated by networks that are not deep (texture) nor\nconvolutional (color). Altogether, these results shed light into the importance\nof matching a network architecture with its embedded prior of the task to be\nlearned.",
        "Lifelong learning, the problem of continual learning where tasks arrive in\nsequence, has been lately attracting more attention in the computer vision\ncommunity. The aim of lifelong learning is to develop a system that can learn\nnew tasks while maintaining the performance on the previously learned tasks.\nHowever, there are two obstacles for lifelong learning of deep neural networks:\ncatastrophic forgetting and capacity limitation. To solve the above issues,\ninspired by the recent breakthroughs in automatically learning good neural\nnetwork architectures, we develop a Multi-task based lifelong learning via\nnonexpansive AutoML framework termed Regularize, Expand and Compress (REC). REC\nis composed of three stages: 1) continually learns the sequential tasks without\nthe learned tasks' data via a newly proposed multi-task weight consolidation\n(MWC) algorithm; 2) expands the network to help the lifelong learning with\npotentially improved model capability and performance by network-transformation\nbased AutoML; 3) compresses the expanded model after learning every new task to\nmaintain model efficiency and performance. The proposed MWC and REC algorithms\nachieve superior performance over other lifelong learning algorithms on four\ndifferent datasets.",
        "Human activity recognition and analysis has always been one of the most\nactive areas of pattern recognition and machine intelligence, with applications\nin various fields, including but not limited to exertion games, surveillance,\nsports analytics and healthcare. Especially in Human-Robot Interaction, human\nactivity understanding plays a crucial role as household robotic assistants are\na trend of the near future. However, state-of-the-art infrastructures that can\nsupport complex machine intelligence tasks are not always available, and may\nnot be for the average consumer, as robotic hardware is expensive. In this\npaper we propose a novel action sequence encoding scheme which efficiently\ntransforms spatio-temporal action sequences into compact representations, using\nMahalanobis distance-based shape features and the Radon transform. This\nrepresentation can be used as input for a lightweight convolutional neural\nnetwork. Experiments show that the proposed pipeline, when based on\nstate-of-the-art human pose estimation techniques, can provide a robust\nend-to-end online action recognition scheme, deployable on hardware lacking\nextreme computing capabilities.",
        "Mass spectrometry (MS) is an important technique for chemical profiling which\ncalculates for a sample a high dimensional histogram-like spectrum. A crucial\nstep of MS data processing is the peak picking which selects peaks containing\ninformation about molecules with high concentrations which are of interest in\nan MS investigation. We present a new procedure of the peak picking based on a\nsparse coding algorithm. Given a set of spectra of different classes, i.e. with\ndifferent positions and heights of the peaks, this procedure can extract peaks\nby means of unsupervised learning. Instead of an $l_1$-regularization penalty\nterm used in the original sparse coding algorithm we propose using an\nelastic-net penalty term for better regularization. The evaluation is done by\nmeans of simulation. We show that for a large region of parameters the proposed\npeak picking method based on the sparse coding features outperforms a mean\nspectrum-based method. Moreover, we demonstrate the procedure applying it to\ntwo real-life datasets.",
        "Projecting high-dimensional environment observations into lower-dimensional\nstructured representations can considerably improve data-efficiency for\nreinforcement learning in domains with limited data such as robotics. Can a\nsingle generally useful representation be found? In order to answer this\nquestion, it is important to understand how the representation will be used by\nthe agent and what properties such a 'good' representation should have. In this\npaper we systematically evaluate a number of common learnt and hand-engineered\nrepresentations in the context of three robotics tasks: lifting, stacking and\npushing of 3D blocks. The representations are evaluated in two use-cases: as\ninput to the agent, or as a source of auxiliary tasks. Furthermore, the value\nof each representation is evaluated in terms of three properties:\ndimensionality, observability and disentanglement. We can significantly improve\nperformance in both use-cases and demonstrate that some representations can\nperform commensurate to simulator states as agent inputs. Finally, our results\nchallenge common intuitions by demonstrating that: 1) dimensionality strongly\nmatters for task generation, but is negligible for inputs, 2) observability of\ntask-relevant aspects mostly affects the input representation use-case, and 3)\ndisentanglement leads to better auxiliary tasks, but has only limited benefits\nfor input representations. This work serves as a step towards a more systematic\nunderstanding of what makes a 'good' representation for control in robotics,\nenabling practitioners to make more informed choices for developing new learned\nor hand-engineered representations.",
        "Most recent work on interpretability of complex machine learning models has\nfocused on estimating $\\textit{a posteriori}$ explanations for previously\ntrained models around specific predictions. $\\textit{Self-explaining}$ models\nwhere interpretability plays a key role already during learning have received\nmuch less attention. We propose three desiderata for explanations in general --\nexplicitness, faithfulness, and stability -- and show that existing methods do\nnot satisfy them. In response, we design self-explaining models in stages,\nprogressively generalizing linear classifiers to complex yet architecturally\nexplicit models. Faithfulness and stability are enforced via regularization\nspecifically tailored to such models. Experimental results across various\nbenchmark datasets show that our framework offers a promising direction for\nreconciling model complexity and interpretability.",
        "Non-negative matrix factorization (NMF) is a powerful tool for dimensionality\nreduction and clustering. Unfortunately, the interpretation of the clustering\nresults from NMF is difficult, especially for the high-dimensional biological\ndata without effective feature selection. In this paper, we first introduce a\nrow-sparse NMF with $\\ell_{2,0}$-norm constraint (NMF_$\\ell_{20}$), where the\nbasis matrix $W$ is constrained by the $\\ell_{2,0}$-norm, such that $W$ has a\nrow-sparsity pattern with feature selection. It is a challenge to solve the\nmodel, because the $\\ell_{2,0}$-norm is non-convex and non-smooth. Fortunately,\nwe prove that the $\\ell_{2,0}$-norm satisfies the Kurdyka-\\L{ojasiewicz}\nproperty. Based on the finding, we present a proximal alternating linearized\nminimization algorithm and its monotone accelerated version to solve the\nNMF_$\\ell_{20}$ model. In addition, we also present a orthogonal NMF with\n$\\ell_{2,0}$-norm constraint (ONMF_$\\ell_{20}$) to enhance the clustering\nperformance by using a non-negative orthogonal constraint. We propose an\nefficient algorithm to solve ONMF_$\\ell_{20}$ by transforming it into a series\nof constrained and penalized matrix factorization problems. The results on\nnumerical and scRNA-seq datasets demonstrate the efficiency of our methods in\ncomparison with existing methods.",
        "We consider the problem of recommending resilient and predictive actions for\nan IoT network in the presence of faulty components, considering the presence\nof human operators manipulating the information of the environment the agent\nsees for containment purposes. The IoT network is formulated as a directed\ngraph with a known topology whose objective is to maintain a constant and\nresilient flow between a source and a destination node. The optimal route\nthrough this network is evaluated via a predictive and resilient Q-learning\nalgorithm which takes into account historical data about irregular operation,\ndue to faults, as well as the feedback from the human operators that are\nconsidered to have extra information about the status of the network concerning\nlocations likely to be targeted by attacks. To showcase our method, we utilize\nanonymized data from Arlington County, Virginia, to compute predictive and\nresilient scheduling policies for a smart water supply system, while avoiding\n(i) all the locations indicated to be attacked according to human operators\n(ii) as many as possible neighborhoods detected to have leaks or other faults.\nThis method incorporates both the adaptability of the human and the computation\ncapability of the machine to achieve optimal implementation containment and\nrecovery actions in water distribution.",
        "Causal discovery, beyond the inference of a network as a collection of\nconnected dots, offers a crucial functionality in scientific discovery using\nartificial intelligence. The questions that arise in multiple domains, such as\nphysics, physiology, the strategic decision in uncertain environments with\nmultiple agents, climatology, among many others, have roots in causality and\nreasoning. It became apparent that many real-world temporal observations are\nnonlinearly related to each other. While the number of observations can be as\nhigh as millions of points, the number of temporal samples can be minimal due\nto ethical or practical reasons, leading to the curse-of-dimensionality in\nlarge-scale systems. This paper proposes a novel method using kernel principal\ncomponent analysis and pre-images to obtain nonlinear dependencies of\nmultivariate time-series data. We show that our method outperforms\nstate-of-the-art causal discovery methods when the observations are restricted\nby time and are nonlinearly related. Extensive simulations on both real-world\nand synthetic datasets with various topologies are provided to evaluate our\nproposed methods.",
        "A molecular and cellular understanding of how SARS-CoV-2 variably infects and\ncauses severe COVID-19 remains a bottleneck in developing interventions to end\nthe pandemic. We sought to use deep learning to study the biology of SARS-CoV-2\ninfection and COVID-19 severity by identifying transcriptomic patterns and cell\ntypes associated with SARS-CoV-2 infection and COVID-19 severity. To do this,\nwe developed a new approach to generating self-supervised edge features. We\npropose a model that builds on Graph Attention Networks (GAT), creates edge\nfeatures using self-supervised learning, and ingests these edge features via a\nSet Transformer. This model achieves significant improvements in predicting the\ndisease state of individual cells, given their transcriptome. We apply our\nmodel to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung\norganoids and bronchoalveolar lavage fluid samples of patients with COVID-19,\nachieving state-of-the-art performance on both datasets with our model. We then\nborrow from the field of explainable AI (XAI) to identify the features (genes)\nand cell types that discriminate bystander vs. infected cells across time and\nmoderate vs. severe COVID-19 disease. To the best of our knowledge, this\nrepresents the first application of deep learning to identifying the molecular\nand cellular determinants of SARS-CoV-2 infection and COVID-19 severity using\nsingle-cell omics data.",
        "Reinforcement learning has attracted great attention recently, especially\npolicy gradient algorithms, which have been demonstrated on challenging\ndecision making and control tasks. In this paper, we propose an active\nmulti-step TD algorithm with adaptive stepsizes to learn actor and critic.\nSpecifically, our model consists of two components: active stepsize learning\nand adaptive multi-step TD algorithm. Firstly, we divide the time horizon into\nchunks and actively select state and action inside each chunk. Then given the\nselected samples, we propose the adaptive multi-step TD, which generalizes\nTD($\\lambda$), but adaptively switch on/off the backups from future returns of\ndifferent steps. Particularly, the adaptive multi-step TD introduces a\ncontext-aware mechanism, here a binary classifier, which decides whether or not\nto turn on its future backups based on the context changes. Thus, our model is\nkind of combination of active learning and multi-step TD algorithm, which has\nthe capacity for learning off-policy without the need of importance sampling.\nWe evaluate our approach on both discrete and continuous space tasks in an\noff-policy setting respectively, and demonstrate competitive results compared\nto other reinforcement learning baselines.",
        "The estimation of advantage is crucial for a number of reinforcement learning\nalgorithms, as it directly influences the choices of future paths. In this\nwork, we propose a family of estimates based on the order statistics over the\npath ensemble, which allows one to flexibly drive the learning process, towards\nor against risks. On top of this formulation, we systematically study the\nimpacts of different methods for estimating advantages. Our findings reveal\nthat biased estimates, when chosen appropriately, can result in significant\nbenefits. In particular, for the environments with sparse rewards, optimistic\nestimates would lead to more efficient exploration of the policy space; while\nfor those where individual actions can have critical impacts, conservative\nestimates are preferable. On various benchmarks, including MuJoCo continuous\ncontrol, Terrain locomotion, Atari games, and sparse-reward environments, the\nproposed biased estimation schemes consistently demonstrate improvement over\nmainstream methods, not only accelerating the learning process but also\nobtaining substantial performance gains.",
        "Several regularization methods have recently been introduced which force the\nlatent activations of an autoencoder or deep neural network to conform to\neither a Gaussian or hyperspherical distribution, or to minimize the implicit\nrank of the distribution in latent space. In the present work, we introduce a\nnovel regularizing loss function which simulates a pairwise repulsive force\nbetween items and an attractive force of each item toward the origin. We show\nthat minimizing this loss function in isolation achieves a hyperspherical\ndistribution. Moreover, when used as a regularizing term, the scaling factor\ncan be adjusted to allow greater flexibility and tolerance of eccentricity,\nthus allowing the latent variables to be stratified according to their relative\nimportance, while still promoting diversity. We apply this method of Eccentric\nRegularization to an autoencoder, and demonstrate its effectiveness in image\ngeneration, representation learning and downstream classification tasks.",
        "Detecting out-of-distribution (OOD) samples plays a key role in open-world\nand safety-critical applications such as autonomous systems and healthcare.\nSelf-supervised representation learning techniques (e.g., contrastive learning\nand pretext learning) are well suited for learning representation that can\nidentify OOD samples. In this paper, we propose a simple framework that\nleverages multi-task transformation learning for training effective\nrepresentation for OOD detection which outperforms state-of-the-art OOD\ndetection performance and robustness on several image datasets. We empirically\nobserve that the OOD performance depends on the choice of data transformations\nwhich itself depends on the in-domain training set. To address this problem, we\npropose a simple mechanism for selecting the transformations automatically and\nmodulate their effect on representation learning without requiring any OOD\ntraining samples. We characterize the criteria for a desirable OOD detector for\nreal-world applications and demonstrate the efficacy of our proposed technique\nagainst a diverse range of the state-of-the-art OOD detection techniques.",
        "An important logistics application of robotics involves manipulators that\npick-and-place objects placed in warehouse shelves. A critical aspect of this\ntask corre- sponds to detecting the pose of a known object in the shelf using\nvisual data. Solving this problem can be assisted by the use of an RGB-D\nsensor, which also provides depth information beyond visual data. Nevertheless,\nit remains a challenging problem since multiple issues need to be addressed,\nsuch as low illumination inside shelves, clutter, texture-less and reflective\nobjects as well as the limitations of depth sensors. This paper provides a new\nrich data set for advancing the state-of-the-art in RGBD- based 3D object pose\nestimation, which is focused on the challenges that arise when solving\nwarehouse pick- and-place tasks. The publicly available data set includes\nthousands of images and corresponding ground truth data for the objects used\nduring the first Amazon Picking Challenge at different poses and clutter\nconditions. Each image is accompanied with ground truth information to assist\nin the evaluation of algorithms for object detection. To show the utility of\nthe data set, a recent algorithm for RGBD-based pose estimation is evaluated in\nthis paper. Based on the measured performance of the algorithm on the data set,\nvarious modifications and improvements are applied to increase the accuracy of\ndetection. These steps can be easily applied to a variety of different\nmethodologies for object pose detection and improve performance in the domain\nof warehouse pick-and-place.",
        "Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.",
        "Recently, Vision Transformers (ViTs) have shown competitive performance on\nimage recognition while requiring less vision-specific inductive biases. In\nthis paper, we investigate if such observation can be extended to image\ngeneration. To this end, we integrate the ViT architecture into generative\nadversarial networks (GANs). We observe that existing regularization methods\nfor GANs interact poorly with self-attention, causing serious instability\nduring training. To resolve this issue, we introduce novel regularization\ntechniques for training GANs with ViTs. Empirically, our approach, named\nViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2\non CIFAR-10, CelebA, and LSUN bedroom datasets.",
        "The normal distributions transform (NDT) is an effective paradigm for the\npoint set registration. This method is originally designed for pair-wise\nregistration and it will suffer from great challenges when applied to\nmulti-view registration. Under the NDT framework, this paper proposes a novel\nmulti-view registration method, named 3D multi-view registration based on the\nnormal distributions transform (3DMNDT), which integrates the K-means\nclustering and Lie algebra solver to achieve multi-view registration. More\nspecifically, the multi-view registration is cast into the problem of maximum\nlikelihood estimation. Then, the K-means algorithm is utilized to divide all\ndata points into different clusters, where a normal distribution is computed to\nlocally models the probability of measuring a data point in each cluster.\nSubsequently, the registration problem is formulated by the NDT-based\nlikelihood function. To maximize this likelihood function, the Lie algebra\nsolver is developed to sequentially optimize each rigid transformation. The\nproposed method alternately implements data point clustering, NDT computing,\nand likelihood maximization until desired registration results are obtained.\nExperimental results tested on benchmark data sets illustrate that the proposed\nmethod can achieve state-of-the-art performance for multi-view registration.",
        "Graph convolutional neural networks (GCNs) generalize tradition convolutional\nneural networks (CNNs) from low-dimensional regular graphs (e.g., image) to\nhigh dimensional irregular graphs (e.g., text documents on word embeddings).\nDue to inevitable faulty data collection instruments, deceptive data\nmanipulation, or other system errors, the data might be error-contaminated.\nEven a small amount of error such as noise can compromise the ability of GCNs\nand render them inadmissible to a large extent. The key challenge is how to\neffectively and efficiently employ GCNs in the presence of erroneous data. In\nthis paper, we propose a novel Robust Graph Convolutional Neural Networks for\npossible erroneous single-view or multi-view data where data may come from\nmultiple sources. By incorporating an extra layers via Autoencoders into\ntraditional graph convolutional networks, we characterize and handle typical\nerror models explicitly. Experimental results on various real-world datasets\ndemonstrate the superiority of the proposed model over the baseline methods and\nits robustness against different types of error.",
        "The success of deep neural networks in real-world problems has prompted many\nattempts to explain their training dynamics and generalization performance, but\nmore guiding principles for the training of neural networks are still needed.\nMotivated by the edge of chaos principle behind the optimal performance of\nneural networks, we study the role of various hyperparameters in modern neural\nnetwork training algorithms in terms of the order-chaos phase diagram. In\nparticular, we study a fully analytical feedforward neural network trained on\nthe widely adopted Fashion-MNIST dataset, and study the dynamics associated\nwith the hyperparameters in back-propagation during the training process. We\nfind that for the basic algorithm of stochastic gradient descent with momentum,\nin the range around the commonly used hyperparameter values, clear scaling\nrelations are present with respect to the training time during the ordered\nphase in the phase diagram, and the model's optimal generalization power at the\nedge of chaos is similar across different training parameter combinations. In\nthe chaotic phase, the same scaling no longer exists. The scaling allows us to\nchoose the training parameters to achieve faster training without sacrificing\nperformance. In addition, we find that the commonly used model regularization\nmethod - weight decay - effectively pushes the model towards the ordered phase\nto achieve better performance. Leveraging on this fact and the scaling\nrelations in the other hyperparameters, we derived a principled guideline for\nhyperparameter determination, such that the model can achieve optimal\nperformance by saturating it at the edge of chaos. Demonstrated on this simple\nneural network model and training algorithm, our work improves the\nunderstanding of neural network training dynamics, and can potentially be\nextended to guiding principles of more complex model architectures and\nalgorithms.",
        "Knowledge distillation extracts general knowledge from a pre-trained teacher\nnetwork and provides guidance to a target student network. Most studies\nmanually tie intermediate features of the teacher and student, and transfer\nknowledge through pre-defined links. However, manual selection often constructs\nineffective links that limit the improvement from the distillation. There has\nbeen an attempt to address the problem, but it is still challenging to identify\neffective links under practical scenarios. In this paper, we introduce an\neffective and efficient feature distillation method utilizing all the feature\nlevels of the teacher without manually selecting the links. Specifically, our\nmethod utilizes an attention-based meta-network that learns relative\nsimilarities between features, and applies identified similarities to control\ndistillation intensities of all possible pairs. As a result, our method\ndetermines competent links more efficiently than the previous approach and\nprovides better performance on model compression and transfer learning tasks.\nFurther qualitative analyses and ablative studies describe how our method\ncontributes to better distillation. The implementation code is available at\ngithub.com/clovaai/attention-feature-distillation.",
        "Wind farms are a crucial driver toward the generation of ecological and\nrenewable energy. Due to their rapid increase in capacity, contemporary wind\nfarms need to adhere to strict constraints on power output to ensure stability\nof the electricity grid. Specifically, a wind farm controller is required to\nmatch the farm's power production with a power demand imposed by the grid\noperator. This is a non-trivial optimization problem, as complex dependencies\nexist between the wind turbines. State-of-the-art wind farm control typically\nrelies on physics-based heuristics that fail to capture the full load spectrum\nthat defines a turbine's health status. When this is not taken into account,\nthe long-term viability of the farm's turbines is put at risk. Given the\ncomplex dependencies that determine a turbine's lifetime, learning a flexible\nand optimal control strategy requires a data-driven approach. However, as wind\nfarms are large-scale multi-agent systems, optimizing control strategies over\nthe full joint action space is intractable. We propose a new learning method\nfor wind farm control that leverages the sparse wind farm structure to\nfactorize the optimization problem. Using a Bayesian approach, based on\nmulti-agent Thompson sampling, we explore the factored joint action space for\nconfigurations that match the demand, while considering the lifetime of\nturbines. We apply our method to a grid-like wind farm layout, and evaluate\nconfigurations using a state-of-the-art wind flow simulator. Our results are\ncompetitive with a physics-based heuristic approach in terms of demand error,\nwhile, contrary to the heuristic, our method prolongs the lifetime of high-risk\nturbines.",
        "State-of-the-art offline handwriting text recognition systems tend to use\nneural networks and therefore require a large amount of annotated data to be\ntrained. In order to partially satisfy this requirement, we propose a system\nbased on Generative Adversarial Networks (GAN) to produce synthetic images of\nhandwritten words. We use bidirectional LSTM recurrent layers to get an\nembedding of the word to be rendered, and we feed it to the generator network.\nWe also modify the standard GAN by adding an auxiliary network for text\nrecognition. The system is then trained with a balanced combination of an\nadversarial loss and a CTC loss. Together, these extensions to GAN enable to\ncontrol the textual content of the generated word images. We obtain realistic\nimages on both French and Arabic datasets, and we show that integrating these\nsynthetic images into the existing training data of a text recognition system\ncan slightly enhance its performance.",
        "We present a new paradigm for Neural ODE algorithms, called ODEtoODE, where\ntime-dependent parameters of the main flow evolve according to a matrix flow on\nthe orthogonal group O(d). This nested system of two flows, where the\nparameter-flow is constrained to lie on the compact manifold, provides\nstability and effectiveness of training and provably solves the gradient\nvanishing-explosion problem which is intrinsically related to training deep\nneural network architectures such as Neural ODEs. Consequently, it leads to\nbetter downstream models, as we show on the example of training reinforcement\nlearning policies with evolution strategies, and in the supervised learning\nsetting, by comparing with previous SOTA baselines. We provide strong\nconvergence results for our proposed mechanism that are independent of the\ndepth of the network, supporting our empirical studies. Our results show an\nintriguing connection between the theory of deep neural networks and the field\nof matrix flows on compact manifolds.",
        "Curvature in form of the Hessian or its generalized Gauss-Newton (GGN)\napproximation is valuable for algorithms that rely on a local model for the\nloss to train, compress, or explain deep networks. Existing methods based on\nimplicit multiplication via automatic differentiation or Kronecker-factored\nblock diagonal approximations do not consider noise in the mini-batch. We\npresent ViViT, a curvature model that leverages the GGN's low-rank structure\nwithout further approximations. It allows for efficient computation of\neigenvalues, eigenvectors, as well as per-sample first- and second-order\ndirectional derivatives. The representation is computed in parallel with\ngradients in one backward pass and offers a fine-grained cost-accuracy\ntrade-off, which allows it to scale. As examples for ViViT's usefulness, we\ninvestigate the directional gradients and curvatures during training, and how\nnoise information can be used to improve the stability of second-order methods.",
        "Computational color constancy is a preprocessing step used in many camera\nsystems. The main aim is to discount the effect of the illumination on the\ncolors in the scene and restore the original colors of the objects. Recently,\nseveral deep learning-based approaches have been proposed to solve this problem\nand they often led to state-of-the-art performance in terms of average errors.\nHowever, for extreme samples, these methods fail and lead to high errors. In\nthis paper, we address this limitation by proposing to aggregate different deep\nlearning methods according to their output uncertainty. We estimate the\nrelative uncertainty of each approach using Monte Carlo dropout and the final\nillumination estimate is obtained as the sum of the different model estimates\nweighted by the log-inverse of their corresponding uncertainties. The proposed\nframework leads to state-of-the-art performance on INTEL-TAU dataset.",
        "Current reinforcement learning (RL) algorithms can be brittle and difficult\nto use, especially when learning goal-reaching behaviors from sparse rewards.\nAlthough supervised imitation learning provides a simple and stable\nalternative, it requires access to demonstrations from a human supervisor. In\nthis paper, we study RL algorithms that use imitation learning to acquire goal\nreaching policies from scratch, without the need for expert demonstrations or a\nvalue function. In lieu of demonstrations, we leverage the property that any\ntrajectory is a successful demonstration for reaching the final state in that\nsame trajectory. We propose a simple algorithm in which an agent continually\nrelabels and imitates the trajectories it generates to progressively learn\ngoal-reaching behaviors from scratch. Each iteration, the agent collects new\ntrajectories using the latest policy, and maximizes the likelihood of the\nactions along these trajectories under the goal that was actually reached, so\nas to improve the policy. We formally show that this iterated supervised\nlearning procedure optimizes a bound on the RL objective, derive performance\nbounds of the learned policy, and empirically demonstrate improved\ngoal-reaching performance and robustness over current RL algorithms in several\nbenchmark tasks.",
        "In this work, we propose a special cascade network for image segmentation,\nwhich is based on the U-Net networks as building blocks and the idea of the\niterative refinement. The model was mainly applied to achieve higher\nrecognition quality for the task of finding borders of the optic disc and cup,\nwhich are relevant to the presence of glaucoma. Compared to a single U-Net and\nthe state-of-the-art methods for the investigated tasks, very high segmentation\nquality has been achieved without a need for increasing the volume of datasets.\nOur experiments include comparison with the best-known methods on publicly\navailable databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a\nprivate data set collected in collaboration with University of California San\nFrancisco Medical School. The analysis of the architecture details is\npresented, and it is argued that the model can be employed for a broad scope of\nimage segmentation problems of similar nature.",
        "Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n  Code and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.",
        "Recognising individual trees within remotely sensed imagery has important\napplications in forest ecology and management. Several algorithms for tree\ndelineation have been suggested, mostly based on locating local maxima or\ninverted basins in raster canopy height models (CHMs) derived from Light\nDetection And Ranging (LiDAR) data or photographs. However, these algorithms\noften lead to inaccurate estimates of forest stand characteristics due to the\nlimited information content of raster CHMs. Here we develop a 3D tree\ndelineation method which uses graph cut to delineate trees from the full 3D\nLiDAR point cloud, and also makes use of any optical imagery available\n(hyperspectral imagery in our case). First, conventional methods are used to\nlocate local maxima in the CHM and generate an initial map of trees. Second, a\ngraph is built from the LiDAR point cloud, fused with the hyperspectral data.\nFor computational efficiency, the feature space of hyperspectral imagery is\nreduced using robust PCA. Third, a multi-class normalised cut is applied to the\ngraph, using the initial map of trees to constrain the number of clusters and\ntheir locations. Finally, recursive normalised cut is used to subdivide, if\nnecessary, each of the clusters identified by the initial analysis. We call\nthis approach Multiclass Cut followed by Recursive Cut (MCRC). The\neffectiveness of MCRC was tested using three datasets: i) NewFor, ii) a\nconiferous forest in the Italian Alps, and iii) a deciduous woodland in the UK.\nThe performance of MCRC was usually superior to that of other delineation\nmethods, and was further improved by including high-resolution optical imagery.\nSince MCRC delineates the entire LiDAR point cloud in 3D, it allows individual\ncrown characteristics to be measured. By making full use of the data available,\ngraph cut has the potential to considerably improve the accuracy of tree\ndelineation.",
        "After the tremendous success of convolutional neural networks in image\nclassification, object detection, speech recognition, etc., there is now rising\ndemand for deployment of these compute-intensive ML models on tightly power\nconstrained embedded and mobile systems at low cost as well as for pushing the\nthroughput in data centers. This has triggered a wave of research towards\nspecialized hardware accelerators. Their performance is often constrained by\nI/O bandwidth and the energy consumption is dominated by I/O transfers to\noff-chip memory. We introduce and evaluate a novel, hardware-friendly\ncompression scheme for the feature maps present within convolutional neural\nnetworks. We show that an average compression ratio of 4.4x relative to\nuncompressed data and a gain of 60% over existing method can be achieved for\nResNet-34 with a compression block requiring <300 bit of sequential cells and\nminimal combinational logic.",
        "The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.",
        "To rapidly learn a new task, it is often essential for agents to explore\nefficiently -- especially when performance matters from the first timestep. One\nway to learn such behaviour is via meta-learning. Many existing methods however\nrely on dense rewards for meta-training, and can fail catastrophically if the\nrewards are sparse. Without a suitable reward signal, the need for exploration\nduring meta-training is exacerbated. To address this, we propose HyperX, which\nuses novel reward bonuses for meta-training to explore in approximate\nhyper-state space (where hyper-states represent the environment state and the\nagent's task belief). We show empirically that HyperX meta-learns better\ntask-exploration and adapts more successfully to new tasks than existing\nmethods.",
        "The quantization error in a fixed-size Self-Organizing Map (SOM) with\nunsupervised winner-take-all learning has previously been used successfully to\ndetect, in minimal computation time, highly meaningful changes across images in\nmedical time series and in time series of satellite images. Here, the\nfunctional properties of the quantization error in SOM are explored further to\nshow that the metric is capable of reliably discriminating between the finest\ndifferences in local contrast intensities and contrast signs. While this\ncapability of the QE is akin to functional characteristics of a specific class\nof retinal ganglion cells (the so-called Y-cells) in the visual systems of the\nprimate and the cat, the sensitivity of the QE surpasses the capacity limits of\nhuman visual detection. Here, the quantization error in the SOM is found to\nreliably signal changes in contrast or colour when contrast information is\nremoved from or added to the image, but not when the amount and relative weight\nof contrast information is constant and only the local spatial position of\ncontrast elements in the pattern changes. While the RGB Mean reflects coarser\nchanges in colour or contrast well enough, the SOM-QE is shown to outperform\nthe RGB Mean in the detection of single-pixel changes in images with up to five\nmillion pixels. This could have important implications in the context of\nunsupervised image learning and computational building block approaches to\nlarge sets of image data (big data), including deep learning blocks, and\nautomatic detection of contrast change at the nanoscale in Transmission or\nScanning Electron Micrographs (TEM, SEM), or at the subpixel level in\nmultispectral and hyper-spectral imaging data.",
        "Graph Convolutional Networks (GCNs) have been extensively used to classify\nvertices in graphs and have been shown to outperform other vertex\nclassification methods. GCNs have been extended to graph classification tasks\n(GCT). In GCT, graphs with different numbers of edges and vertices belong to\ndifferent classes, and one attempts to predict the graph class. GCN based GCT\nhave mostly used pooling and attention-based models. The accuracy of existing\nGCT methods is still limited. We here propose a novel solution combining GCN,\nmethods from knowledge graphs, and a new self-regularized activation function\nto significantly improve the accuracy of the GCN based GCT. We present\nquadratic GCN (QGCN) - A GCN formalism with a quadratic layer. Such a layer\nproduces an output with fixed dimensions, independent of the graph vertex\nnumber. We applied this method to a wide range of graph classification\nproblems, and show that when using a self regularized activation function, QGCN\noutperforms the state of the art methods for all graph classification tasks\ntested with or without external input on each graph. The code for QGCN is\navailable at: https://github.com/Unknown-Data/QGCN .",
        "In this work, we consider the regret minimization problem for reinforcement\nlearning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is\nrandomly drawn from a set of $M$ possible MDPs at the beginning of the\ninteraction, but the identity of the chosen MDP is not revealed to the agent.\nWe first show that a general instance of LMDPs requires at least\n$\\Omega((SA)^M)$ episodes to even approximate the optimal policy. Then, we\nconsider sufficient assumptions under which learning good policies requires\npolynomial number of episodes. We show that the key link is a notion of\nseparation between the MDP system dynamics. With sufficient separation, we\nprovide an efficient algorithm with local guarantee, {\\it i.e.,} providing a\nsublinear regret guarantee when we are given a good initialization. Finally, if\nwe are given standard statistical sufficiency assumptions common in the\nPredictive State Representation (PSR) literature (e.g., Boots et al.) and a\nreachability assumption, we show that the need for initialization can be\nremoved.",
        "Weakly Supervised Object Detection (WSOD), aiming to train detectors with\nonly image-level annotations, has arisen increasing attention. Current\nstate-of-the-art approaches mainly follow a two-stage training strategy\nwhichintegrates a fully supervised detector (FSD) with a pure WSOD model. There\nare two main problems hindering the performance of the two-phase WSOD\napproaches, i.e., insufficient learning problem and strict reliance between the\nFSD and the pseudo ground truth (PGT) generated by theWSOD model. This paper\nproposes pseudo ground truth refinement network (PGTRNet), a simple yet\neffective method without introducing any extra learnable parameters, to cope\nwith these problems. PGTRNet utilizes multiple bounding boxes to establish the\nPGT, mitigating the insufficient learning problem. Besides, we propose a novel\nonline PGT refinement approach to steadily improve the quality of PGTby fully\ntaking advantage of the power of FSD during the second-phase training,\ndecoupling the first and second-phase models. Elaborate experiments are\nconducted on the PASCAL VOC 2007 benchmark to verify the effectiveness of our\nmethods. Experimental results demonstrate that PGTRNet boosts the backbone\nmodel by 2.074% mAP and achieves the state-of-the-art performance, showing the\nsignificant potentials of the second-phase training.",
        "We address the problem of estimating depth with multi modal audio visual\ndata. Inspired by the ability of animals, such as bats and dolphins, to infer\ndistance of objects with echolocation, some recent methods have utilized echoes\nfor depth estimation. We propose an end-to-end deep learning based pipeline\nutilizing RGB images, binaural echoes and estimated material properties of\nvarious objects within a scene. We argue that the relation between image,\nechoes and depth, for different scene elements, is greatly influenced by the\nproperties of those elements, and a method designed to leverage this\ninformation can lead to significantly improved depth estimation from audio\nvisual inputs. We propose a novel multi modal fusion technique, which\nincorporates the material properties explicitly while combining audio (echoes)\nand visual modalities to predict the scene depth. We show empirically, with\nexperiments on Replica dataset, that the proposed method obtains 28%\nimprovement in RMSE compared to the state-of-the-art audio-visual depth\nprediction method. To demonstrate the effectiveness of our method on larger\ndataset, we report competitive performance on Matterport3D, proposing to use it\nas a multimodal depth prediction benchmark with echoes for the first time. We\nalso analyse the proposed method with exhaustive ablation experiments and\nqualitative results. The code and models are available at\nhttps://krantiparida.github.io/projects/bimgdepth.html",
        "Interpreting how does deep neural networks (DNNs) make predictions is a vital\nfield in artificial intelligence, which hinders wide applications of DNNs.\nVisualization of learned representations helps we humans understand the vision\nof DNNs. In this work, visualized images that can activate the neural network\nto the target classes are generated by back-propagation method. Here, rotation\nand scaling operations are applied to introduce the transformation invariance\nin the image generating process, which we find a significant improvement on\nvisualization effect. Finally, we show some cases that such method can help us\nto gain insight into neural networks.",
        "We propose a method for constructing generative models of 3D objects from a\nsingle 3D mesh. Our method produces a 3D morphable model that represents shape\nand albedo in terms of Gaussian processes. We define the shape deformations in\nphysical (3D) space and the albedo deformations as a combination of\nphysical-space and color-space deformations. Whereas previous approaches have\ntypically built 3D morphable models from multiple high-quality 3D scans through\nprincipal component analysis, we build 3D morphable models from a single scan\nor template. We demonstrate the utility of these models in the domain of face\nmodeling through inverse rendering and registration tasks. Specifically, we\nshow that our approach can be used to perform face recognition using only a\nsingle 3D scan (one scan total, not one per person), and further demonstrate\nhow multiple scans can be incorporated to improve performance without requiring\ndense correspondence. Our approach enables the synthesis of 3D morphable models\nfor 3D object categories where dense correspondence between multiple scans is\nunavailable. We demonstrate this by constructing additional 3D morphable models\nfor fish and birds and use them to perform simple inverse rendering tasks.",
        "Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.",
        "Deep reinforcement learning (RL) has proved successful at solving challenging\nenvironments but often requires scaling to large sampling and computing\nresources. Furthermore, advancing RL requires tools that are flexible enough to\neasily prototype new methods, yet avoiding impractically slow experimental\nturnaround times. To this end, we present PyTorchRL, a PyTorch-based library\nfor RL with a modular design that allows composing agents from a set of\nreusable and easily extendable modules. Additionally, PyTorchRL permits the\ndefinition of distributed training architectures with flexibility and\nindependence of the Agent components. In combination, these two features can\naccelerate the pace at which ideas are implemented and tested, simplifying\nresearch and enabling to tackle more challenging RL problems. We present\nseveral interesting use-cases of PyTorchRL and showcase the library by\nobtaining the highest to-date test performance on the Obstacle Tower Unity3D\nchallenge environment.",
        "We propose a Reinforcement Learning based approach to approximately solve the\nTree Decomposition (TD) problem. TD is a combinatorial problem, which is\ncentral to the analysis of graph minor structure and computational complexity,\nas well as in the algorithms of probabilistic inference, register allocation,\nand other practical tasks. Recently, it has been shown that combinatorial\nproblems can be successively solved by learned heuristics. However, the\nmajority of existing works do not address the question of the generalization of\nlearning-based solutions. Our model is based on the graph convolution neural\nnetwork (GCN) for learning graph representations. We show that the agent\nbuilton GCN and trained on a single graph using an Actor-Critic method can\nefficiently generalize to real-world TD problem instances. We establish that\nour method successfully generalizes from small graphs, where TD can be found by\nexact algorithms, to large instances of practical interest, while still having\nvery low time-to-solution. On the other hand, the agent-based approach\nsurpasses all greedy heuristics by the quality of the solution.",
        "Learning to classify time series with limited data is a practical yet\nchallenging problem. Current methods are primarily based on hand-designed\nfeature extraction rules or domain-specific data augmentation. Motivated by the\nadvances in deep speech processing models and the fact that voice data are\nunivariate temporal signals, in this paper, we propose Voice2Series (V2S), a\nnovel end-to-end approach that reprograms acoustic models for time series\nclassification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained\nspeech processing model, on 30 different time series tasks we show that V2S\neither outperforms or is tied with state-of-the-art methods on 20 tasks, and\nimproves their average accuracy by 1.84%. We further provide a theoretical\njustification of V2S by proving its population risk is upper bounded by the\nsource risk and a Wasserstein distance accounting for feature alignment via\nreprogramming. Our results offer new and effective means to time series\nclassification.",
        "With recent innovations in dense image captioning, it is now possible to\ndescribe every object of the scene with a caption while objects are determined\nby bounding boxes. However, interpretation of such an output is not trivial due\nto the existence of many overlapping bounding boxes. Furthermore, in current\ncaptioning frameworks, the user is not able to involve personal preferences to\nexclude out of interest areas. In this paper, we propose a novel hybrid deep\nlearning architecture for interactive region segmentation and captioning where\nthe user is able to specify an arbitrary region of the image that should be\nprocessed. To this end, a dedicated Fully Convolutional Network (FCN) named\nLyncean FCN (LFCN) is trained using our special training data to isolate the\nUser Intention Region (UIR) as the output of an efficient segmentation. In\nparallel, a dense image captioning model is utilized to provide a wide variety\nof captions for that region. Then, the UIR will be explained with the caption\nof the best match bounding box. To the best of our knowledge, this is the first\nwork that provides such a comprehensive output. Our experiments show the\nsuperiority of the proposed approach over state-of-the-art interactive\nsegmentation methods on several well-known datasets. In addition, replacement\nof the bounding boxes with the result of the interactive segmentation leads to\na better understanding of the dense image captioning output as well as accuracy\nenhancement for the object detection in terms of Intersection over Union (IoU).",
        "In just a few years, the photo-realism of images synthesized by Generative\nAdversarial Networks (GANs) has gone from somewhat reasonable to almost perfect\nlargely by increasing the complexity of the networks, e.g., adding layers,\nintermediate latent spaces, style-transfer parameters, etc. This trajectory has\nled many of the state-of-the-art GANs to be inaccessibly large, disengaging\nmany without large computational resources. Recognizing this, we explore a\nmethod for squeezing additional performance from existing, low-complexity GANs.\nFormally, we present an unsupervised method to find a direction in the latent\nspace that aligns with improved photo-realism. Our approach leaves the network\nunchanged while enhancing the fidelity of the generated image. We use a simple\ngenerator inversion to find the direction in the latent space that results in\nthe smallest change in the image space. Leveraging the learned structure of the\nlatent space, we find moving in this direction corrects many image artifacts\nand brings the image into greater realism. We verify our findings qualitatively\nand quantitatively, showing an improvement in Frechet Inception Distance (FID)\nexists along our trajectory which surpasses the original GAN and other\napproaches including a supervised method. We expand further and provide an\noptimization method to automatically select latent vectors along the path that\nbalance the variation and realism of samples. We apply our method to several\ndiverse datasets and three architectures of varying complexity to illustrate\nthe generalizability of our approach. By expanding the utility of\nlow-complexity and existing networks, we hope to encourage the democratization\nof GANs.",
        "Increasing the visibility of nighttime hazy images is challenging because of\nuneven illumination from active artificial light sources and haze\nabsorbing/scattering. The absence of large-scale benchmark datasets hampers\nprogress in this area. To address this issue, we propose a novel synthetic\nmethod called 3R to simulate nighttime hazy images from daytime clear images,\nwhich first reconstructs the scene geometry, then simulates the light rays and\nobject reflectance, and finally renders the haze effects. Based on it, we\ngenerate realistic nighttime hazy images by sampling real-world light colors\nfrom a prior empirical distribution. Experiments on the synthetic benchmark\nshow that the degrading factors jointly reduce the image quality. To address\nthis issue, we propose an optimal-scale maximum reflectance prior to\ndisentangle the color correction from haze removal and address them\nsequentially. Besides, we also devise a simple but effective learning-based\nbaseline which has an encoder-decoder structure based on the MobileNet-v2\nbackbone. Experiment results demonstrate their superiority over\nstate-of-the-art methods in terms of both image quality and runtime. Both the\ndataset and source code will be available at https://github.com/chaimi2013/3R.",
        "In deep representational learning, it is often desired to isolate a\nparticular factor (termed {\\em content}) from other factors (referred to as\n{\\em style}). What constitutes the content is typically specified by users\nthrough explicit labels in the data, while all unlabeled/unknown factors are\nregarded as style. Recently, it has been shown that such content-labeled data\ncan be effectively exploited by modifying the deep latent factor models (e.g.,\nVAE) such that the style and content are well separated in the latent\nrepresentations. However, the approach assumes that the content factor is\ncategorical-valued (e.g., subject ID in face image data, or digit class in the\nMNIST dataset). In certain situations, the content is ordinal-valued, that is,\nthe values the content factor takes are {\\em ordered} rather than categorical,\nmaking content-labeled VAEs, including the latent space they infer, suboptimal.\nIn this paper, we propose a novel extension of VAE that imposes a partially\nordered set (poset) structure in the content latent space, while simultaneously\nmaking it aligned with the ordinal content values. To this end, instead of the\niid Gaussian latent prior adopted in prior approaches, we introduce a\nconditional Gaussian spacing prior model. This model admits a tractable joint\nGaussian prior, but also effectively places negligible density values on the\ncontent latent configurations that violate the poset constraint. To evaluate\nthis model, we consider two specific ordinal structured problems: estimating a\nsubject's age in a face image and elucidating the calorie amount in a food meal\nimage. We demonstrate significant improvements in content-style separation over\nprevious non-ordinal approaches.",
        "Clustering is one of the major roles in data mining that is widely\napplication in pattern recognition and image segmentation. Fuzzy C-means (FCM)\nis the most used clustering algorithm that proven efficient, fast and easy to\nimplement, however, FCM uses the Euclidean distance that often leads to\nclustering errors, especially when handling multidimensional and noisy data. In\nthe last few years, many distances metric have been proposed by researchers to\nimprove the performance of the FCM algorithms, and the majority of researchers\npropose weighted distance. In this paper, we proposed Canberra Weighted\nDistance to improved performance of the FCM algorithm. The experimental result\nusing the UCI data set show the proposed method is superior to the original\nmethod and other clustering methods.",
        "There is a strong need for automated systems to improve diagnostic quality\nand reduce the analysis time in histopathology image processing. Automated\ndetection and classification of pathological tissue characteristics with\ncomputer-aided diagnostic systems are a critical step in the early diagnosis\nand treatment of diseases. Once a pathology image is scanned by a microscope\nand loaded onto a computer, it can be used for automated detection and\nclassification of diseases. In this study, the DenseNet-161 and ResNet-50\npre-trained CNN models have been used to classify digital histopathology\npatches into the corresponding whole slide images via transfer learning\ntechnique. The proposed pre-trained models were tested on grayscale and color\nhistopathology images. The DenseNet-161 pre-trained model achieved a\nclassification accuracy of 97.89% using grayscale images and the ResNet-50\nmodel obtained the accuracy of 98.87% for color images. The proposed\npre-trained models outperform state-of-the-art methods in all performance\nmetrics to classify digital pathology patches into 24 categories.",
        "It is believed that a model-based approach for reinforcement learning (RL) is\nthe key to reduce sample complexity. However, the understanding of the sample\noptimality of model-based RL is still largely missing, even for the linear\ncase. This work considers sample complexity of finding an $\\epsilon$-optimal\npolicy in a Markov decision process (MDP) that admits a linear additive feature\nrepresentation, given only access to a generative model. We solve this problem\nvia a plug-in solver approach, which builds an empirical model and plans in\nthis empirical model via an arbitrary plug-in solver. We prove that under the\nanchor-state assumption, which implies implicit non-negativity in the feature\nspace, the minimax sample complexity of finding an $\\epsilon$-optimal policy in\na $\\gamma$-discounted MDP is $O(K/(1-\\gamma)^3\\epsilon^2)$, which only depends\non the dimensionality $K$ of the feature space and has no dependence on the\nstate or action space. We further extend our results to a relaxed setting where\nanchor-states may not exist and show that a plug-in approach can be sample\nefficient as well, providing a flexible approach to design model-based\nalgorithms for RL.",
        "Soccer broadcast video understanding has been drawing a lot of attention in\nrecent years within data scientists and industrial companies. This is mainly\ndue to the lucrative potential unlocked by effective deep learning techniques\ndeveloped in the field of computer vision. In this work, we focus on the topic\nof camera calibration and on its current limitations for the scientific\ncommunity. More precisely, we tackle the absence of a large-scale calibration\ndataset and of a public calibration network trained on such a dataset.\nSpecifically, we distill a powerful commercial calibration tool in a recent\nneural network architecture on the large-scale SoccerNet dataset, composed of\nuntrimmed broadcast videos of 500 soccer games. We further release our\ndistilled network, and leverage it to provide 3 ways of representing the\ncalibration results along with player localization. Finally, we exploit those\nrepresentations within the current best architecture for the action spotting\ntask of SoccerNet-v2, and achieve new state-of-the-art performances.",
        "Fine-grained 3D shape classification is important for shape understanding and\nanalysis, which poses a challenging research problem. However, the studies on\nthe fine-grained 3D shape classification have rarely been explored, due to the\nlack of fine-grained 3D shape benchmarks. To address this issue, we first\nintroduce a new 3D shape dataset (named FG3D dataset) with fine-grained class\nlabels, which consists of three categories including airplane, car and chair.\nEach category consists of several subcategories at a fine-grained level.\nAccording to our experiments under this fine-grained dataset, we find that\nstate-of-the-art methods are significantly limited by the small variance among\nsubcategories in the same category. To resolve this problem, we further propose\na novel fine-grained 3D shape classification method named FG3D-Net to capture\nthe fine-grained local details of 3D shapes from multiple rendered views.\nSpecifically, we first train a Region Proposal Network (RPN) to detect the\ngenerally semantic parts inside multiple views under the benchmark of generally\nsemantic part detection. Then, we design a hierarchical part-view attention\naggregation module to learn a global shape representation by aggregating\ngenerally semantic part features, which preserves the local details of 3D\nshapes. The part-view attention module hierarchically leverages part-level and\nview-level attention to increase the discriminability of our features. The\npart-level attention highlights the important parts in each view while the\nview-level attention highlights the discriminative views among all the views of\nthe same object. In addition, we integrate a Recurrent Neural Network (RNN) to\ncapture the spatial relationships among sequential views from different\nviewpoints. Our results under the fine-grained 3D shape dataset show that our\nmethod outperforms other state-of-the-art methods.",
        "The ability to act in multiple environments and transfer previous knowledge\nto new situations can be considered a critical aspect of any intelligent agent.\nTowards this goal, we define a novel method of multitask and transfer learning\nthat enables an autonomous agent to learn how to behave in multiple tasks\nsimultaneously, and then generalize its knowledge to new domains. This method,\ntermed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model\ncompression techniques to train a single policy network that learns how to act\nin a set of distinct tasks by using the guidance of several expert teachers. We\nthen show that the representations learnt by the deep policy network are\ncapable of generalizing to new tasks with no prior expert guidance, speeding up\nlearning in novel environments. Although our method can in general be applied\nto a wide range of problems, we use Atari games as a testing environment to\ndemonstrate these methods.",
        "Classical computation of optical flow involves generic priors (regularizers)\nthat capture rudimentary statistics of images, but not long-range correlations\nor semantics. On the other hand, fully supervised methods learn the regularity\nin the annotated data, without explicit regularization and with the risk of\noverfitting. We seek to learn richer priors on the set of possible flows that\nare statistically compatible with an image. Once the prior is learned in a\nsupervised fashion, one can easily learn the full map to infer optical flow\ndirectly from two or more images, without any need for (additional)\nsupervision. We introduce a novel architecture, called Conditional Prior\nNetwork (CPN), and show how to train it to yield a conditional prior. When used\nin conjunction with a simple optical flow architecture, the CPN beats all\nvariational methods and all unsupervised learning-based ones using the same\ndata term. It performs comparably to fully supervised ones, that however are\nfine-tuned to a particular dataset. Our method, on the other hand, performs\nwell even when transferred between datasets.",
        "Object-oriented maps are important for scene understanding since they jointly\ncapture geometry and semantics, allow individual instantiation and meaningful\nreasoning about objects. We introduce FroDO, a method for accurate 3D\nreconstruction of object instances from RGB video that infers object location,\npose and shape in a coarse-to-fine manner. Key to FroDO is to embed object\nshapes in a novel learnt space that allows seamless switching between sparse\npoint cloud and dense DeepSDF decoding. Given an input sequence of localized\nRGB frames, FroDO first aggregates 2D detections to instantiate a\ncategory-aware 3D bounding box per object. A shape code is regressed using an\nencoder network before optimizing shape and pose further under the learnt shape\npriors using sparse and dense shape representations. The optimization uses\nmulti-view geometric, photometric and silhouette losses. We evaluate on\nreal-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view,\nmulti-view, and multi-object reconstruction.",
        "Nowadays, digital content is widespread and simply redistributable, either\nlawfully or unlawfully. For example, after images are posted on the internet,\nother web users can modify them and then repost their versions, thereby\ngenerating near-duplicate images. The presence of near-duplicates affects the\nperformance of the search engines critically. Computer vision is concerned with\nthe automatic extraction, analysis and understanding of useful information from\ndigital images. The main application of computer vision is image understanding.\nThere are several tasks in image understanding such as feature extraction,\nobject detection, object recognition, image cleaning, image transformation,\netc. There is no proper survey in literature related to near duplicate\ndetection of images. In this paper, we review the state-of-the-art computer\nvision-based approaches and feature extraction methods for the detection of\nnear duplicate images. We also discuss the main challenges in this field and\nhow other researchers addressed those challenges. This review provides research\ndirections to the fellow researchers who are interested to work in this field.",
        "There exists a high variability in mobility data volumes across different\nregions, which deteriorates the performance of spatial recommender systems that\nrely on region-specific data. In this paper, we propose a novel transfer\nlearning framework called REFORMD, for continuous-time location prediction for\nregions with sparse checkin data. Specifically, we model user-specific\ncheckin-sequences in a region using a marked temporal point process (MTPP) with\nnormalizing flows to learn the inter-checkin time and geo-distributions. Later,\nwe transfer the model parameters of spatial and temporal flows trained on a\ndata-rich origin region for the next check-in and time prediction in a target\nregion with scarce checkin data. We capture the evolving region-specific\ncheckin dynamics for MTPP and spatial-temporal flows by maximizing the joint\nlikelihood of next checkin with three channels (1) checkin-category prediction,\n(2) checkin-time prediction, and (3) travel distance prediction. Extensive\nexperiments on different user mobility datasets across the U.S. and Japan show\nthat our model significantly outperforms state-of-the-art methods for modeling\ncontinuous-time sequences. Moreover, we also show that REFORMD can be easily\nadapted for product recommendations i.e., sequences without any spatial\ncomponent.",
        "Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are\nbecoming prevalent in various sectors of modern society such as transportation,\nindustrial, and power grids. Recent studies in deep reinforcement learning\n(DRL) have demonstrated its benefits in a large variety of data-driven\ndecisions and control applications. As reliance on ML-enabled systems grows, it\nis imperative to study the performance of these systems under malicious state\nand actuator attacks. Traditional control systems employ\nresilient/fault-tolerant controllers that counter these attacks by correcting\nthe system via error observations. However, in some applications, a resilient\ncontroller may not be sufficient to avoid a catastrophic failure. Ideally, a\nrobust approach is more useful in these scenarios where a system is inherently\nrobust (by design) to adversarial attacks. While robust control has a long\nhistory of development, robust ML is an emerging research area that has already\ndemonstrated its relevance and urgency. However, the majority of robust ML\nresearch has focused on perception tasks and not on decision and control tasks,\nalthough the ML (specifically RL) models used for control applications are\nequally vulnerable to adversarial attacks. In this paper, we show that a\nwell-performing DRL agent that is initially susceptible to action space\nperturbations (e.g. actuator attacks) can be robustified against similar\nperturbations through adversarial training.",
        "The 3D modelling of indoor environments and the generation of process\nsimulations play an important role in factory and assembly planning. In\nbrownfield planning cases existing data are often outdated and incomplete\nespecially for older plants, which were mostly planned in 2D. Thus, current\nenvironment models cannot be generated directly on the basis of existing data\nand a holistic approach on how to build such a factory model in a highly\nautomated fashion is mostly non-existent. Major steps in generating an\nenvironment model in a production plant include data collection and\npre-processing, object identification as well as pose estimation. In this work,\nwe elaborate a methodical workflow, which starts with the digitalization of\nlarge-scale indoor environments and ends with the generation of a static\nenvironment or simulation model. The object identification step is realized\nusing a Bayesian neural network capable of point cloud segmentation. We\nelaborate how the information on network uncertainty generated by a Bayesian\nsegmentation framework can be used in order to build up a more accurate\nenvironment model. The steps of data collection and point cloud segmentation as\nwell as the resulting model accuracy are evaluated on a real-world data set\ncollected at the assembly line of a large-scale automotive production plant.\nThe segmentation network is further evaluated on the publicly available\nStanford Large-Scale 3D Indoor Spaces data set. The Bayesian segmentation\nnetwork clearly surpasses the performance of the frequentist baseline and\nallows us to increase the accuracy of the model placement in a simulation scene\nconsiderably.",
        "We propose a robust implementation of the Nerlove--Arrow model using a\nBayesian structural time series model to explain the relationship between\nadvertising expenditures of a country-wide fast-food franchise network with its\nweekly sales. Thanks to the flexibility and modularity of the model, it is well\nsuited to generalization to other markets or situations. Its Bayesian nature\nfacilitates incorporating \\emph{a priori} information (the manager's views),\nwhich can be updated with relevant data. This aspect of the model will be used\nto present a strategy of budget scheduling across time and channels.",
        "In this paper, we introduce a new dataset consisting of 360,001 focused\nnatural language descriptions for 10,738 images. This dataset, the Visual\nMadlibs dataset, is collected using automatically produced fill-in-the-blank\ntemplates designed to gather targeted descriptions about: people and objects,\ntheir appearances, activities, and interactions, as well as inferences about\nthe general scene or its broader context. We provide several analyses of the\nVisual Madlibs dataset and demonstrate its applicability to two new description\ngeneration tasks: focused description generation, and multiple-choice\nquestion-answering for images. Experiments using joint-embedding and deep\nlearning methods show promising results on these tasks.",
        "Identifying objects in an image and their mutual relationships as a scene\ngraph leads to a deep understanding of image content. Despite the recent\nadvancement in deep learning, the detection and labeling of visual object\nrelationships remain a challenging task. This work proposes a novel\nlocal-context aware architecture named relation transformer, which exploits\ncomplex global objects to object and object to edge (relation) interactions.\nOur hierarchical multi-head attention-based approach efficiently captures\ncontextual dependencies between objects and predicts their relationships. In\ncomparison to state-of-the-art approaches, we have achieved an overall mean\n\\textbf{4.85\\%} improvement and a new benchmark across all the scene graph\ngeneration tasks on the Visual Genome dataset.",
        "We summarized both common and novel predictive models used for stock price\nprediction and combined them with technical indices, fundamental\ncharacteristics and text-based sentiment data to predict S&P stock prices. A\n66.18% accuracy in S&P 500 index directional prediction and 62.09% accuracy in\nindividual stock directional prediction was achieved by combining different\nmachine learning models such as Random Forest and LSTM together into\nstate-of-the-art ensemble models. The data we use contains weekly historical\nprices, finance reports, and text information from news items associated with\n518 different common stocks issued by current and former S&P 500 large-cap\ncompanies, from January 1, 2000 to December 31, 2019. Our study's innovation\nincludes utilizing deep language models to categorize and infer financial news\nitem sentiment; fusing different models containing different combinations of\nvariables and stocks to jointly make predictions; and overcoming the\ninsufficient data problem for machine learning models in time series by using\ndata across different stocks.",
        "The development of recommender systems that optimize multi-turn interaction\nwith users, and model the interactions of different agents (e.g., users,\ncontent providers, vendors) in the recommender ecosystem have drawn increasing\nattention in recent years. Developing and training models and algorithms for\nsuch recommenders can be especially difficult using static datasets, which\noften fail to offer the types of counterfactual predictions needed to evaluate\npolicies over extended horizons. To address this, we develop RecSim NG, a\nprobabilistic platform for the simulation of multi-agent recommender systems.\nRecSim NG is a scalable, modular, differentiable simulator implemented in\nEdward2 and TensorFlow. It offers: a powerful, general probabilistic\nprogramming language for agent-behavior specification; tools for probabilistic\ninference and latent-variable model learning, backed by automatic\ndifferentiation and tracing; and a TensorFlow-based runtime for running\nsimulations on accelerated hardware. We describe RecSim NG and illustrate how\nit can be used to create transparent, configurable, end-to-end models of a\nrecommender ecosystem, complemented by a small set of simple use cases that\ndemonstrate how RecSim NG can help both researchers and practitioners easily\ndevelop and train novel algorithms for recommender systems.",
        "The problem of grounding VQA tasks has seen an increased attention in the\nresearch community recently, with most attempts usually focusing on solving\nthis task by using pretrained object detectors. However, pre-trained object\ndetectors require bounding box annotations for detecting relevant objects in\nthe vocabulary, which may not always be feasible for real-life large-scale\napplications. In this paper, we focus on a more relaxed setting: the grounding\nof relevant visual entities in a weakly supervised manner by training on the\nVQA task alone. To address this problem, we propose a visual capsule module\nwith a query-based selection mechanism of capsule features, that allows the\nmodel to focus on relevant regions based on the textual cues about visual\ninformation in the question. We show that integrating the proposed capsule\nmodule in existing VQA systems significantly improves their performance on the\nweakly supervised grounding task. Overall, we demonstrate the effectiveness of\nour approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the\nCLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with\nground truth bounding boxes for objects that are relevant for the correct\nanswer, as well as on GQA, a real world VQA dataset with compositional\nquestions. We show that the systems with the proposed capsule module\nconsistently outperform the respective baseline systems in terms of answer\ngrounding, while achieving comparable performance on VQA task.",
        "Weakly supervised object detection(WSOD) task uses only image-level\nannotations to train object detection task. WSOD does not require\ntime-consuming instance-level annotations, so the study of this task has\nattracted more and more attention. Previous weakly supervised object detection\nmethods iteratively update detectors and pseudo-labels, or use feature-based\nmask-out methods. Most of these methods do not generate complete and accurate\nproposals, often only the most discriminative parts of the object, or too many\nbackground areas. To solve this problem, we added the box regression module to\nthe weakly supervised object detection network and proposed a proposal scoring\nnetwork (PSNet) to supervise it. The box regression module modifies proposal to\nimprove the IoU of proposal and ground truth. PSNet scores the proposal output\nfrom the box regression network and utilize the score to improve the box\nregression module. In addition, we take advantage of the PRS algorithm for\ngenerating a more accurate pseudo label to train the box regression module.\nUsing these methods, we train the detector on the PASCAL VOC 2007 and 2012 and\nobtain significantly improved results.",
        "This paper presents the novel approach towards table structure recognition by\nleveraging the guided anchors. The concept differs from current\nstate-of-the-art approaches for table structure recognition that naively apply\nobject detection methods. In contrast to prior techniques, first, we estimate\nthe viable anchors for table structure recognition. Subsequently, these anchors\nare exploited to locate the rows and columns in tabular images. Furthermore,\nthe paper introduces a simple and effective method that improves the results by\nusing tabular layouts in realistic scenarios. The proposed method is\nexhaustively evaluated on the two publicly available datasets of table\nstructure recognition i.e ICDAR-2013 and TabStructDB. We accomplished\nstate-of-the-art results on the ICDAR-2013 dataset with an average F-Measure of\n95.05$\\%$ (94.6$\\%$ for rows and 96.32$\\%$ for columns) and surpassed the\nbaseline results on the TabStructDB dataset with an average F-Measure of\n94.17$\\%$ (94.08$\\%$ for rows and 95.06$\\%$ for columns).",
        "The Luther condition states that if the spectral sensitivity responses of a\ncamera are a linear transform from the color matching functions of the human\nvisual system, the camera is colorimetric. Previous work proposed to solve for\na filter which, when placed in front of a camera, results in sensitivities that\nbest satisfy the Luther condition. By construction, the prior art solves for a\nfilter for a given set of human visual sensitivities, e.g. the XYZ color\nmatching functions or the cone response functions. However, depending on the\ntarget spectral sensitivity set, a different optimal filter is found.\n  This paper begins with the observation that the cone fundamentals, XYZ color\nmatching functions or any linear combination thereof span the same\n3-dimensional subspace. Thus, we set out to solve for a filter that makes the\nvector space spanned by the filtered camera sensitivities as similar as\npossible to the space spanned by human vision sensors. We argue that the\nVora-Value is a suitable way to measure subspace similarity and we develop an\noptimization method for finding a filter that maximizes the Vora-Value measure.\n  Experiments demonstrate that our new optimization leads to filtered camera\nsensitivities which have a significantly higher Vora-Value compared with\nantecedent methods.",
        "In recent years, studying and predicting alternative mobility (e.g., sharing\nservices) patterns in urban environments has become increasingly important as\naccurate and timely information on current and future vehicle flows can\nsuccessfully increase the quality and availability of transportation services.\nThis need is aggravated during the current pandemic crisis, which pushes\npolicymakers and private citizens to seek social-distancing compliant urban\nmobility services, such as electric bikes and scooter sharing offerings.\nHowever, predicting the number of incoming and outgoing vehicles for different\ncity areas is challenging due to the nonlinear spatial and temporal\ndependencies typical of urban mobility patterns. In this work, we propose\nSTREED-Net, a novel deep learning network with a multi-attention (spatial and\ntemporal) mechanism that effectively captures and exploits complex spatial and\ntemporal patterns in mobility data. The results of a thorough experimental\nanalysis using real-life data are reported, indicating that the proposed model\nimproves the state-of-the-art for this task.",
        "Network representation learning (NRL) is a powerful technique for learning\nlow-dimensional vector representation of high-dimensional and sparse graphs.\nMost studies explore the structure and metadata associated with the graph using\nrandom walks and employ an unsupervised or semi-supervised learning schemes.\nLearning in these methods is context-free, because only a single representation\nper node is learned. Recently studies have argued on the sufficiency of a\nsingle representation and proposed a context-sensitive approach that proved to\nbe highly effective in applications such as link prediction and ranking.\n  However, most of these methods rely on additional textual features that\nrequire RNNs or CNNs to capture high-level features or rely on a community\ndetection algorithm to identify multiple contexts of a node.\n  In this study, without requiring additional features nor a community\ndetection algorithm, we propose a novel context-sensitive algorithm called GAP\nthat learns to attend on different parts of a node's neighborhood using\nattentive pooling networks. We show the efficacy of GAP using three real-world\ndatasets on link prediction and node clustering tasks and compare it against 10\npopular and state-of-the-art (SOTA) baselines. GAP consistently outperforms\nthem and achieves up to ~9% and ~20% gain over the best performing methods on\nlink prediction and clustering tasks, respectively.",
        "The (variational) graph auto-encoder and its variants have been popularly\nused for representation learning on graph-structured data. While the encoder is\noften a powerful graph convolutional network, the decoder reconstructs the\ngraph structure by only considering two nodes at a time, thus ignoring possible\ninteractions among edges. On the other hand, structured prediction, which\nconsiders the whole graph simultaneously, is computationally expensive. In this\npaper, we utilize the well-known triadic closure property which is exhibited in\nmany real-world networks. We propose the triad decoder, which considers and\npredicts the three edges involved in a local triad together. The triad decoder\ncan be readily used in any graph-based auto-encoder. In particular, we\nincorporate this to the (variational) graph auto-encoder. Experiments on link\nprediction, node clustering and graph generation show that the use of triads\nleads to more accurate prediction, clustering and better preservation of the\ngraph characteristics.",
        "Semidefinite Programming (SDP) and Sums-of-Squares (SOS) relaxations have led\nto certifiably optimal non-minimal solvers for several robotics and computer\nvision problems. However, most non-minimal solvers rely on least-squares\nformulations, and, as a result, are brittle against outliers. While a standard\napproach to regain robustness against outliers is to use robust cost functions,\nthe latter typically introduce other non-convexities, preventing the use of\nexisting non-minimal solvers. In this paper, we enable the simultaneous use of\nnon-minimal solvers and robust estimation by providing a general-purpose\napproach for robust global estimation, which can be applied to any problem\nwhere a non-minimal solver is available for the outlier-free case. To this end,\nwe leverage the Black-Rangarajan duality between robust estimation and outlier\nprocesses (which has been traditionally applied to early vision problems), and\nshow that graduated non-convexity (GNC) can be used in conjunction with\nnon-minimal solvers to compute robust solutions, without requiring an initial\nguess. Although GNC's global optimality cannot be guaranteed, we demonstrate\nthe empirical robustness of the resulting robust non-minimal solvers in\napplications, including point cloud and mesh registration, pose graph\noptimization, and image-based object pose estimation (also called shape\nalignment). Our solvers are robust to 70-80% of outliers, outperform RANSAC,\nare more accurate than specialized local solvers, and faster than specialized\nglobal solvers. We also propose the first certifiably optimal non-minimal\nsolver for shape alignment using SOS relaxation.",
        "While a wide range of interpretable generative procedures for graphs exist,\nmatching observed graph topologies with such procedures and choices for its\nparameters remains an open problem. Devising generative models that closely\nreproduce real-world graphs requires domain knowledge and time-consuming\nsimulation. While existing deep learning approaches rely on less manual\nmodelling, they offer little interpretability. This work approaches graph\ngeneration (decoding) as the inverse of graph compression (encoding). We show\nthat in a disentanglement-focused deep autoencoding framework, specifically\nBeta-Variational Autoencoders (Beta-VAE), choices of generative procedures and\ntheir parameters arise naturally in the latent space. Our model is capable of\nlearning disentangled, interpretable latent variables that represent the\ngenerative parameters of procedurally generated random graphs and real-world\ngraphs. The degree of disentanglement is quantitatively measured using the\nMutual Information Gap (MIG). When training our Beta-VAE model on ER random\ngraphs, its latent variables have a near one-to-one mapping to the ER random\ngraph parameters n and p. We deploy the model to analyse the correlation\nbetween graph topology and node attributes measuring their mutual dependence\nwithout handpicking topological properties.",
        "Firefighting is a dynamic activity, in which numerous operations occur\nsimultaneously. Maintaining situational awareness (i.e., knowledge of current\nconditions and activities at the scene) is critical to the accurate\ndecision-making necessary for the safe and successful navigation of a fire\nenvironment by firefighters. Conversely, the disorientation caused by hazards\nsuch as smoke and extreme heat can lead to injury or even fatality. This\nresearch implements recent advancements in technology such as deep learning,\npoint cloud and thermal imaging, and augmented reality platforms to improve a\nfirefighter's situational awareness and scene navigation through improved\ninterpretation of that scene. We have designed and built a prototype embedded\nsystem that can leverage data streamed from cameras built into a firefighter's\npersonal protective equipment (PPE) to capture thermal, RGB color, and depth\nimagery and then deploy already developed deep learning models to analyze the\ninput data in real time. The embedded system analyzes and returns the processed\nimages via wireless streaming, where they can be viewed remotely and relayed\nback to the firefighter using an augmented reality platform that visualizes the\nresults of the analyzed inputs and draws the firefighter's attention to objects\nof interest, such as doors and windows otherwise invisible through smoke and\nflames.",
        "Deep learning models have demonstrated high-quality performance in areas such\nas image classification and speech processing. However, creating a deep\nlearning model using electronic health record (EHR) data, requires addressing\nparticular privacy challenges that are unique to researchers in this domain.\nThis matter focuses attention on generating realistic synthetic data while\nensuring privacy. In this paper, we propose a novel framework called\ncorrelation-capturing Generative Adversarial Network (CorGAN), to generate\nsynthetic healthcare records. In CorGAN we utilize Convolutional Neural\nNetworks to capture the correlations between adjacent medical features in the\ndata representation space by combining Convolutional Generative Adversarial\nNetworks and Convolutional Autoencoders. To demonstrate the model fidelity, we\nshow that CorGAN generates synthetic data with performance similar to that of\nreal data in various Machine Learning settings such as classification and\nprediction. We also give a privacy assessment and report on statistical\nanalysis regarding realistic characteristics of the synthetic data. The\nsoftware of this work is open-source and is available at:\nhttps://github.com/astorfi/cor-gan.",
        "Cooperation between different data owners may lead to an improvement in\nforecast quality - for instance by benefiting from spatial-temporal\ndependencies in geographically distributed time series. Due to business\ncompetitive factors and personal data protection questions, said data owners\nmight be unwilling to share their data, which increases the interest in\ncollaborative privacy-preserving forecasting. This paper analyses the\nstate-of-the-art and unveils several shortcomings of existing methods in\nguaranteeing data privacy when employing Vector Autoregressive (VAR) models.\nThe paper also provides mathematical proofs and numerical analysis to evaluate\nexisting privacy-preserving methods, dividing them into three groups: data\ntransformation, secure multi-party computations, and decomposition methods. The\nanalysis shows that state-of-the-art techniques have limitations in preserving\ndata privacy, such as a trade-off between privacy and forecasting accuracy,\nwhile the original data in iterative model fitting processes, in which\nintermediate results are shared, can be inferred after some iterations.",
        "We introduce Segment-Phrase Table (SPT), a large collection of bijective\nassociations between textual phrases and their corresponding segmentations.\nLeveraging recent progress in object recognition and natural language\nsemantics, we show how we can successfully build a high-quality segment-phrase\ntable using minimal human supervision. More importantly, we demonstrate the\nunique value unleashed by this rich bimodal resource, for both vision as well\nas natural language understanding. First, we show that fine-grained textual\nlabels facilitate contextual reasoning that helps in satisfying semantic\nconstraints across image segments. This feature enables us to achieve\nstate-of-the-art segmentation results on benchmark datasets. Next, we show that\nthe association of high-quality segmentations to textual phrases aids in richer\nsemantic understanding and reasoning of these textual phrases. Leveraging this\nfeature, we motivate the problem of visual entailment and visual paraphrasing,\nand demonstrate its utility on a large dataset.",
        "We introduce DeepMorph, an information embedding technique for vector\ndrawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG)\nfile, our method embeds bitstrings in the image by perturbing the drawing\nprimitives (lines, circles, etc.). This results in a morphed image that can be\ndecoded to recover the original bitstring. The use-case is similar to that of\nthe well-known QR code, but our solution provides creatives with artistic\nfreedom to transfer digital information via drawings of their own design. The\nmethod comprises two neural networks, which are trained jointly: an encoder\nnetwork that transforms a bitstring into a perturbation of the drawing\nprimitives, and a decoder network that recovers the bitstring from an image of\nthe morphed drawing. To enable end-to-end training via back propagation, we\nintroduce a soft rasterizer, which is differentiable with respect to\nperturbations of the drawing primitives. In order to add robustness towards\nreal-world image capture conditions, image corruptions are injected between the\nsoft rasterizer and the decoder. Further, the addition of an object detection\nand camera pose estimation system enables decoding of drawings in complex\nscenes as well as use of the drawings as markers for use in augmented reality\napplications. We demonstrate that our method reliably recovers bitstrings from\nreal-world photos of printed drawings, thereby providing a novel solution for\ncreatives to transfer digital information via artistic imagery.",
        "This paper proposes an image-to-painting translation method that generates\nvivid and realistic painting artworks with controllable styles. Different from\nprevious image-to-image translation methods that formulate the translation as\npixel-wise prediction, we deal with such an artistic creation process in a\nvectorized environment and produce a sequence of physically meaningful stroke\nparameters that can be further used for rendering. Since a typical vector\nrender is not differentiable, we design a novel neural renderer which imitates\nthe behavior of the vector renderer and then frame the stroke prediction as a\nparameter searching process that maximizes the similarity between the input and\nthe rendering output. We explored the zero-gradient problem on parameter\nsearching and propose to solve this problem from an optimal transportation\nperspective. We also show that previous neural renderers have a parameter\ncoupling problem and we re-design the rendering network with a rasterization\nnetwork and a shading network that better handles the disentanglement of shape\nand color. Experiments show that the paintings generated by our method have a\nhigh degree of fidelity in both global appearance and local textures. Our\nmethod can be also jointly optimized with neural style transfer that further\ntransfers visual style from other images. Our code and animated results are\navailable at \\url{https://jiupinjia.github.io/neuralpainter/}.",
        "Recently, the task of image generation has attracted much attention. In\nparticular, the recent empirical successes of the Markov Chain Monte Carlo\n(MCMC) technique of Langevin Dynamics have prompted a number of theoretical\nadvances; despite this, several outstanding problems remain. First, the\nLangevin Dynamics is run in very high dimension on a nonconvex landscape; in\nthe worst case, due to the NP-hardness of nonconvex optimization, it is thought\nthat Langevin Dynamics mixes only in time exponential in the dimension. In this\nwork, we demonstrate how the manifold hypothesis allows for the considerable\nreduction of mixing time, from exponential in the ambient dimension to\ndepending only on the (much smaller) intrinsic dimension of the data. Second,\nthe high dimension of the sampling space significantly hurts the performance of\nLangevin Dynamics; we leverage a multi-scale approach to help ameliorate this\nissue and observe that this multi-resolution algorithm allows for a trade-off\nbetween image quality and computational expense in generation.",
        "Deep neural networks have achieved state-of-art performance in many domains\nincluding computer vision, natural language processing and self-driving cars.\nHowever, they are very computationally expensive and memory intensive which\nraises significant challenges when it comes to deploy or train them on strict\nlatency applications or resource-limited environments. As a result, many\nattempts have been introduced to accelerate and compress deep learning models,\nhowever the majority were not able to maintain the same accuracy of the\nbaseline models. In this paper, we describe EnSyth, a deep learning ensemble\napproach to enhance the predictability of compact neural network's models.\nFirst, we generate a set of diverse compressed deep learning models using\ndifferent hyperparameters for a pruning method, after that we utilise ensemble\nlearning to synthesise the outputs of the compressed models to compose a new\npool of classifiers. Finally, we apply backward elimination on the generated\npool to explore the best performing combinations of models. On CIFAR-10,\nCIFAR-5 data-sets with LeNet-5, EnSyth outperforms the predictability of the\nbaseline model.",
        "Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I",
        "Recently, Generative Adversarial Network (GAN) has been found wide\napplications in style transfer, image-to-image translation and image\nsuper-resolution. In this paper, a color-depth conditional GAN is proposed to\nconcurrently resolve the problems of depth super-resolution and color\nsuper-resolution in 3D videos. Firstly, given the low-resolution depth image\nand low-resolution color image, a generative network is proposed to leverage\nmutual information of color image and depth image to enhance each other in\nconsideration of the geometry structural dependency of color-depth image in the\nsame scene. Secondly, three loss functions, including data loss, total\nvariation loss, and 8-connected gradient difference loss are introduced to\ntrain this generative network in order to keep generated images close to the\nreal ones, in addition to the adversarial loss. Experimental results\ndemonstrate that the proposed approach produces high-quality color image and\ndepth image from low-quality image pair, and it is superior to several other\nleading methods. Besides, we use the same neural network framework to resolve\nthe problem of image smoothing and edge detection at the same time.",
        "Representation learning for graphs enables the application of standard\nmachine learning algorithms and data analysis tools to graph data. Replacing\ndiscrete unordered objects such as graph nodes by real-valued vectors is at the\nheart of many approaches to learning from graph data. Such vector\nrepresentations, or embeddings, capture the discrete relationships in the\noriginal data by representing nodes as vectors in a high-dimensional space.\n  In most applications graphs model the relationship between real-life objects\nand often nodes contain valuable meta-information about the original objects.\nWhile being a powerful machine learning tool, embeddings are not able to\npreserve such node attributes. We address this shortcoming and consider the\nproblem of learning discrete node embeddings such that the coordinates of the\nnode vector representations are graph nodes. This opens the door to designing\ninterpretable machine learning algorithms for graphs as all attributes\noriginally present in the nodes are preserved.\n  We present a framework for coordinated local graph neighborhood sampling\n(COLOGNE) such that each node is represented by a fixed number of graph nodes,\ntogether with their attributes. Individual samples are coordinated and they\npreserve the similarity between node neighborhoods. We consider different\nnotions of similarity for which we design scalable algorithms. We show\ntheoretical results for all proposed algorithms. Experiments on benchmark\ngraphs evaluate the quality of the designed embeddings and demonstrate how the\nproposed embeddings can be used in training interpretable machine learning\nalgorithms for graph data.",
        "Graph neural networks have recently achieved remarkable success in\nrepresenting graph-structured data, with rapid progress in both the node\nembedding and graph pooling methods. Yet, they mostly focus on capturing\ninformation from the nodes considering their connectivity, and not much work\nhas been done in representing the edges, which are essential components of a\ngraph. However, for tasks such as graph reconstruction and generation, as well\nas graph classification tasks for which the edges are important for\ndiscrimination, accurately representing edges of a given graph is crucial to\nthe success of the graph representation learning. To this end, we propose a\nnovel edge representation learning framework based on Dual Hypergraph\nTransformation (DHT), which transforms the edges of a graph into the nodes of a\nhypergraph. This dual hypergraph construction allows us to apply message\npassing techniques for node representations to edges. After obtaining edge\nrepresentations from the hypergraphs, we then cluster or drop edges to obtain\nholistic graph-level edge representations. We validate our edge representation\nlearning method with hypergraphs on diverse graph datasets for graph\nrepresentation and generation performance, on which our method largely\noutperforms existing graph representation learning methods. Moreover, our edge\nrepresentation learning and pooling method also largely outperforms\nstate-of-the-art graph pooling methods on graph classification, not only\nbecause of its accurate edge representation learning, but also due to its\nlossless compression of the nodes and removal of irrelevant edges for effective\nmessage passing.",
        "Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation",
        "Over the long history of machine learning, which dates back several decades,\nrecurrent neural networks (RNNs) have been used mainly for sequential data and\ntime series and generally with 1D information. Even in some rare studies on 2D\nimages, these networks are used merely to learn and generate data sequentially\nrather than for image recognition tasks. In this study, we propose integrating\nan RNN as an additional layer when designing image recognition models. We also\ndevelop end-to-end multimodel ensembles that produce expert predictions using\nseveral models. In addition, we extend the training strategy so that our model\nperforms comparably to leading models and can even match the state-of-the-art\nmodels on several challenging datasets (e.g., SVHN (0.99), Cifar-100 (0.9027)\nand Cifar-10 (0.9852)). Moreover, our model sets a new record on the Surrey\ndataset (0.949). The source code of the methods provided in this article is\navailable at https://github.com/leonlha/e2e-3m and http://nguyenhuuphong.me.",
        "A fundamental question in reinforcement learning is whether model-free\nalgorithms are sample efficient. Recently, Jin et al. \\cite{jin2018q} proposed\na Q-learning algorithm with UCB exploration policy, and proved it has nearly\noptimal regret bound for finite-horizon episodic MDP. In this paper, we adapt\nQ-learning with UCB-exploration bonus to infinite-horizon MDP with discounted\nrewards \\emph{without} accessing a generative model. We show that the\n\\textit{sample complexity of exploration} of our algorithm is bounded by\n$\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously\nbest known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this\nsetting achieved by delayed Q-learning \\cite{strehl2006pac}, and matches the\nlower bound in terms of $\\epsilon$ as well as $S$ and $A$ except for\nlogarithmic factors.",
        "The segmentation of synthetic aperture radar (SAR) images is a longstanding\nyet challenging task, not only because of the presence of speckle, but also due\nto the variations of surface backscattering properties in the images.\nTremendous investigations have been made to eliminate the speckle effects for\nthe segmentation of SAR images, while few work devotes to dealing with the\nvariations of backscattering coefficients in the images. In order to overcome\nboth the two difficulties, this paper presents a novel SAR image segmentation\nmethod by exploiting a multi-scale active contour model based on the non-local\nprocessing principle. More precisely, we first formulize the SAR segmentation\nproblem with an active contour model by integrating the non-local interactions\nbetween pairs of patches inside and outside the segmented regions. Secondly, a\nmulti-scale strategy is proposed to speed up the non-local active contour\nsegmentation procedure and to avoid falling into local minimum for achieving\nmore accurate segmentation results. Experimental results on simulated and real\nSAR images demonstrate the efficiency and feasibility of the proposed method:\nit can not only achieve precise segmentations for images with heavy speckles\nand non-local intensity variations, but also can be used for SAR images from\ndifferent types of sensors.",
        "In this work we propose an adversarial learning approach to generate high\nresolution MRI scans from low resolution images. The architecture, based on the\nSRGAN model, adopts 3D convolutions to exploit volumetric information. For the\ndiscriminator, the adversarial loss uses least squares in order to stabilize\nthe training. For the generator, the loss function is a combination of a least\nsquares adversarial loss and a content term based on mean square error and\nimage gradients in order to improve the quality of the generated images. We\nexplore different solutions for the upsampling phase. We present promising\nresults that improve classical interpolation, showing the potential of the\napproach for 3D medical imaging super-resolution. Source code available at\nhttps://github.com/imatge-upc/3D-GAN-superresolution",
        "Image splicing detection is of fundamental importance in digital forensics\nand therefore has attracted increasing attention recently. In this paper, a\ncolor image splicing detection approach is proposed based on Markov transition\nprobability of quaternion component separation in quaternion discrete cosine\ntransform (QDCT) domain and quaternion wavelet transform (QWT) domain. Firstly,\nMarkov features of the intra-block and inter-block between block QDCT\ncoefficients are obtained from the real part and three imaginary parts of QDCT\ncoefficients respectively. Then, additional Markov features are extracted from\nluminance (Y) channel in quaternion wavelet transform domain to characterize\nthe dependency of position among quaternion wavelet subband coefficients.\nFinally, ensemble classifier (EC) is exploited to classify the spliced and\nauthentic color images. The experiment results demonstrate that the proposed\napproach can outperforms some state-of-the-art methods.",
        "Along with the desire to address more complex problems, feature selection\nmethods have gained in importance. Feature selection methods can be classified\ninto wrapper method, filter method, and embedded method. Being a powerful\nembedded feature selection method, Lasso has attracted the attention of many\nresearchers. However, as a linear approach, the applicability of Lasso has been\nlimited. In this work, we propose LassoLayer that is one-to-one connected and\ntrained by L1 optimization, which work to drop out unnecessary units for\nprediction. For nonlinear feature selections, we build LassoMLP: the network\nequipped with LassoLayer as its first layer. Because we can insert LassoLayer\nin any network structure, it can harness the strength of neural network\nsuitable for tasks where feature selection is needed. We evaluate LassoMLP in\nfeature selection with regression and classification tasks. LassoMLP receives\nfeatures including considerable numbers of noisy factors that is harmful for\noverfitting. In the experiments using MNIST dataset, we confirm that LassoMLP\noutperforms the state-of-the-art method.",
        "We introduce the $2$-simplicial Transformer, an extension of the Transformer\nwhich includes a form of higher-dimensional attention generalising the\ndot-product attention, and uses this attention to update entity representations\nwith tensor products of value vectors. We show that this architecture is a\nuseful inductive bias for logical reasoning in the context of deep\nreinforcement learning.",
        "Cooperative game is a critical research area in the multi-agent reinforcement\nlearning (MARL). Global reward game is a subclass of cooperative games, where\nall agents aim to maximize the global reward. Credit assignment is an important\nproblem studied in the global reward game. Most of previous works stood by the\nview of non-cooperative-game theoretical framework with the shared reward\napproach, i.e., each agent being assigned a shared global reward directly.\nThis, however, may give each agent an inaccurate reward on its contribution to\nthe group, which could cause inefficient learning. To deal with this problem,\nwe i) introduce a cooperative-game theoretical framework called extended convex\ngame (ECG) that is a superset of global reward game, and ii) propose a local\nreward approach called Shapley Q-value. Shapley Q-value is able to distribute\nthe global reward, reflecting each agent's own contribution in contrast to the\nshared reward approach. Moreover, we derive an MARL algorithm called Shapley\nQ-value deep deterministic policy gradient (SQDDPG), using Shapley Q-value as\nthe critic for each agent. We evaluate SQDDPG on Cooperative Navigation,\nPrey-and-Predator and Traffic Junction, compared with the state-of-the-art\nalgorithms, e.g., MADDPG, COMA, Independent DDPG and Independent A2C. In the\nexperiments, SQDDPG shows a significant improvement on the convergence rate.\nFinally, we plot Shapley Q-value and validate the property of fair credit\nassignment.",
        "We present a deep learning pipeline that leverages network self-prior to\nrecover a full 3D model consisting of both a triangular mesh and a texture map\nfrom the colored 3D point cloud. Different from previous methods either\nexploiting 2D self-prior for image editing or 3D self-prior for pure surface\nreconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep\nneural networks to significantly improve the geometry quality and produce a\nhigh-resolution texture map, which is typically missing from the output of\ncommodity-level 3D scanners. In particular, we first generate an initial mesh\nusing a 3D convolutional neural network with 3D self-prior, and then encode\nboth 3D information and color information in the 2D UV atlas, which is further\nrefined by 2D convolutional neural networks with the self-prior. In this way,\nboth 2D and 3D self-priors are utilized for the mesh and texture recovery.\nExperiments show that, without the need of any additional training data, our\nmethod recovers the 3D textured mesh model of high quality from sparse input,\nand outperforms the state-of-the-art methods in terms of both the geometry and\ntexture quality.",
        "Scene understanding is a critical problem in computer vision. In this paper,\nwe propose a 3D point-based scene graph generation ($\\mathbf{SGG_{point}}$)\nframework to effectively bridge perception and reasoning to achieve scene\nunderstanding via three sequential stages, namely scene graph construction,\nreasoning, and inference. Within the reasoning stage, an EDGE-oriented Graph\nConvolutional Network ($\\texttt{EdgeGCN}$) is created to exploit\nmulti-dimensional edge features for explicit relationship modeling, together\nwith the exploration of two associated twinning interaction mechanisms between\nnodes and edges for the independent evolution of scene graph representations.\nOverall, our integrated $\\mathbf{SGG_{point}}$ framework is established to seek\nand infer scene structures of interest from both real-world and synthetic 3D\npoint-based scenes. Our experimental results show promising edge-oriented\nreasoning effects on scene graph generation studies. We also demonstrate our\nmethod advantage on several traditional graph representation learning benchmark\ndatasets, including the node-wise classification on citation networks and\nwhole-graph recognition problems for molecular analysis.",
        "Database activity monitoring (DAM) systems are commonly used by organizations\nto protect the organizational data, knowledge and intellectual properties. In\norder to protect organizations database DAM systems have two main roles,\nmonitoring (documenting activity) and alerting to anomalous activity. Due to\nhigh-velocity streams and operating costs, such systems are restricted to\nexamining only a sample of the activity. Current solutions use policies,\nmanually crafted by experts, to decide which transactions to monitor and log.\nThis limits the diversity of the data collected. Bandit algorithms, which use\nreward functions as the basis for optimization while adding diversity to the\nrecommended set, have gained increased attention in recommendation systems for\nimproving diversity.\n  In this work, we redefine the data sampling problem as a special case of the\nmulti-armed bandit (MAB) problem and present a novel algorithm, which combines\nexpert knowledge with random exploration. We analyze the effect of diversity on\ncoverage and downstream event detection tasks using a simulated dataset. In\ndoing so, we find that adding diversity to the sampling using the bandit-based\napproach works well for this task and maximizing population coverage without\ndecreasing the quality in terms of issuing alerts about events.",
        "We utilize the PageRank vector to generalize the $k$-means clustering\nalgorithm to directed and undirected graphs. We demonstrate that PageRank and\nother centrality measures can be used in our setting to robustly compute\ncentrality of nodes in a given graph. Furthermore, we show how our method can\nbe generalized to metric spaces and apply it to other domains such as point\nclouds and triangulated meshes",
        "The construction of efficient and effective decision trees remains a key\ntopic in machine learning because of their simplicity and flexibility. A lot of\nheuristic algorithms have been proposed to construct near-optimal decision\ntrees. ID3, C4.5 and CART are classical decision tree algorithms and the split\ncriteria they used are Shannon entropy, Gain Ratio and Gini index respectively.\nAll the split criteria seem to be independent, actually, they can be unified in\na Tsallis entropy framework. Tsallis entropy is a generalization of Shannon\nentropy and provides a new approach to enhance decision trees' performance with\nan adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)\nalgorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,\nwhich generalizes the split criteria of decision trees. More importantly, we\nreveal the relations between Tsallis entropy with different $q$ and other split\ncriteria. Experimental results on UCI data sets indicate that the TEC algorithm\nachieves statistically significant improvement over the classical algorithms.",
        "The ability to amplify or reduce subtle image changes over time is useful in\ncontexts such as video editing, medical video analysis, product quality control\nand sports. In these contexts there is often large motion present which\nseverely distorts current video amplification methods that magnify change\nlinearly. In this work we propose a method to cope with large motions while\nstill magnifying small changes. We make the following two observations: i)\nlarge motions are linear on the temporal scale of the small changes; ii) small\nchanges deviate from this linearity. We ignore linear motion and propose to\nmagnify acceleration. Our method is pure Eulerian and does not require any\noptical flow, temporal alignment or region annotations. We link temporal\nsecond-order derivative filtering to spatial acceleration magnification. We\napply our method to moving objects where we show motion magnification and color\nmagnification. We provide quantitative as well as qualitative evidence for our\nmethod while comparing to the state-of-the-art.",
        "We present an empirical study of scaling properties of encoder-decoder\nTransformer models used in neural machine translation (NMT). We show that\ncross-entropy loss as a function of model size follows a certain scaling law.\nSpecifically (i) We propose a formula which describes the scaling behavior of\ncross-entropy loss as a bivariate function of encoder and decoder size, and\nshow that it gives accurate predictions under a variety of scaling approaches\nand languages; we show that the total number of parameters alone is not\nsufficient for such purposes. (ii) We observe different power law exponents\nwhen scaling the decoder vs scaling the encoder, and provide recommendations\nfor optimal allocation of encoder/decoder capacity based on this observation.\n(iii) We also report that the scaling behavior of the model is acutely\ninfluenced by composition bias of the train/test sets, which we define as any\ndeviation from naturally generated text (either via machine generated or human\ntranslated text). We observe that natural text on the target side enjoys\nscaling, which manifests as successful reduction of the cross-entropy loss.\n(iv) Finally, we investigate the relationship between the cross-entropy loss\nand the quality of the generated translations. We find two different behaviors,\ndepending on the nature of the test data. For test sets which were originally\ntranslated from target language to source language, both loss and BLEU score\nimprove as model size increases. In contrast, for test sets originally\ntranslated from source language to target language, the loss improves, but the\nBLEU score stops improving after a certain threshold. We release generated text\nfrom all models used in this study.",
        "Training (source) domain bias affects state-of-the-art object detectors, such\nas Faster R-CNN, when applied to new (target) domains. To alleviate this\nproblem, researchers proposed various domain adaptation methods to improve\nobject detection results in the cross-domain setting, e.g. by translating\nimages with ground-truth labels from the source domain to the target domain\nusing Cycle-GAN. On top of combining Cycle-GAN transformations and self-paced\nlearning in a smart and efficient way, in this paper, we propose a novel\nself-paced algorithm that learns from easy to hard. Our method is simple and\neffective, without any overhead during inference. It uses only pseudo-labels\nfor samples taken from the target domain, i.e. the domain adaptation is\nunsupervised. We conduct experiments on four cross-domain benchmarks, showing\nbetter results than the state of the art. We also perform an ablation study\ndemonstrating the utility of each component in our framework. Additionally, we\nstudy the applicability of our framework to other object detectors.\nFurthermore, we compare our difficulty measure with other measures from the\nrelated literature, proving that it yields superior results and that it\ncorrelates well with the performance metric.",
        "We introduce a parameterization method called Neural Bayes which allows\ncomputing statistical quantities that are in general difficult to compute and\nopens avenues for formulating new objectives for unsupervised representation\nlearning. Specifically, given an observed random variable $\\mathbf{x}$ and a\nlatent discrete variable $z$, we can express $p(\\mathbf{x}|z)$,\n$p(z|\\mathbf{x})$ and $p(z)$ in closed form in terms of a sufficiently\nexpressive function (Eg. neural network) using our parameterization without\nrestricting the class of these distributions. To demonstrate its usefulness, we\ndevelop two independent use cases for this parameterization:\n  1. Mutual Information Maximization (MIM): MIM has become a popular means for\nself-supervised representation learning. Neural Bayes allows us to compute\nmutual information between observed random variables $\\mathbf{x}$ and latent\ndiscrete random variables $z$ in closed form. We use this for learning image\nrepresentations and show its usefulness on downstream classification tasks.\n  2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an\nobjective which can optimally label samples from disjoint manifolds present in\nthe support of a continuous distribution. This can be seen as a specific form\nof clustering where each disjoint manifold in the support is a separate\ncluster. We design clustering tasks that obey this formulation and empirically\nshow that the model optimally labels the disjoint manifolds. Our code is\navailable at \\url{https://github.com/salesforce/NeuralBayes}",
        "This paper presents a novel unsupervised domain adaptation framework, called\nSynergistic Image and Feature Adaptation (SIFA), to effectively tackle the\nproblem of domain shift. Domain adaptation has become an important and hot\ntopic in recent studies on deep learning, aiming to recover performance\ndegradation when applying the neural networks to new testing domains. Our\nproposed SIFA is an elegant learning diagram which presents synergistic fusion\nof adaptations from both image and feature perspectives. In particular, we\nsimultaneously transform the appearance of images across domains and enhance\ndomain-invariance of the extracted features towards the segmentation task. The\nfeature encoder layers are shared by both perspectives to grasp their mutual\nbenefits during the end-to-end learning procedure. Without using any annotation\nfrom the target domain, the learning of our unified model is guided by\nadversarial losses, with multiple discriminators employed from various aspects.\nWe have extensively validated our method with a challenging application of\ncross-modality medical image segmentation of cardiac structures. Experimental\nresults demonstrate that our SIFA model recovers the degraded performance from\n17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant\nmargin.",
        "Hyperspectral imaging is a cutting-edge type of remote sensing used for\nmapping vegetation properties, rock minerals and other materials. A major\ndrawback of hyperspectral imaging devices is their intrinsic low spatial\nresolution. In this paper, we propose a method for increasing the spatial\nresolution of a hyperspectral image by fusing it with an image of higher\nspatial resolution that was obtained with a different imaging modality. This is\naccomplished by solving a variational problem in which the regularization\nfunctional is the directional total variation. To accommodate for possible\nmis-registrations between the two images, we consider a non-convex blind\nsuper-resolution problem where both a fused image and the corresponding\nconvolution kernel are estimated. Using this approach, our model can realign\nthe given images if needed. Our experimental results indicate that the\nnon-convexity is negligible in practice and that reliable solutions can be\ncomputed using a variety of different optimization algorithms. Numerical\nresults on real remote sensing data from plant sciences and urban monitoring\nshow the potential of the proposed method and suggests that it is robust with\nrespect to the regularization parameters, mis-registration and the shape of the\nkernel.",
        "Recently, video streams have occupied a large proportion of Internet traffic,\nmost of which contain human faces. Hence, it is necessary to predict saliency\non multiple-face videos, which can provide attention cues for many content\nbased applications. However, most of multiple-face saliency prediction works\nonly consider visual information and ignore audio, which is not consistent with\nthe naturalistic scenarios. Several behavioral studies have established that\nsound influences human attention, especially during the speech turn-taking in\nmultiple-face videos. In this paper, we thoroughly investigate such influences\nby establishing a large-scale eye-tracking database of Multiple-face Video in\nVisual-Audio condition (MVVA). Inspired by the findings of our investigation,\nwe propose a novel multi-modal video saliency model consisting of three\nbranches: visual, audio and face. The visual branch takes the RGB frames as the\ninput and encodes them into visual feature maps. The audio and face branches\nencode the audio signal and multiple cropped faces, respectively. A fusion\nmodule is introduced to integrate the information from three modalities, and to\ngenerate the final saliency map. Experimental results show that the proposed\nmethod outperforms 11 state-of-the-art saliency prediction works. It performs\ncloser to human multi-modal attention.",
        "To achieve parsimonious inference in per-pixel labeling tasks with a limited\ncomputational budget, we propose a \\emph{Pixel-wise Attentional Gating} unit\n(\\emph{PAG}) that learns to selectively process a subset of spatial locations\nat each layer of a deep convolutional network. PAG is a generic,\narchitecture-independent, problem-agnostic mechanism that can be readily\n\"plugged in\" to an existing model with fine-tuning. We utilize PAG in two ways:\n1) learning spatially varying pooling fields that improve model performance\nwithout the extra computation cost associated with multi-scale pooling, and 2)\nlearning a dynamic computation policy for each pixel to decrease total\ncomputation while maintaining accuracy.\n  We extensively evaluate PAG on a variety of per-pixel labeling tasks,\nincluding semantic segmentation, boundary detection, monocular depth and\nsurface normal estimation. We demonstrate that PAG allows competitive or\nstate-of-the-art performance on these tasks. Our experiments show that PAG\nlearns dynamic spatial allocation of computation over the input image which\nprovides better performance trade-offs compared to related approaches (e.g.,\ntruncating deep models or dynamically skipping whole layers). Generally, we\nobserve PAG can reduce computation by $10\\%$ without noticeable loss in\naccuracy and performance degrades gracefully when imposing stronger\ncomputational constraints.",
        "Face attribute estimation has many potential applications in video\nsurveillance, face retrieval, and social media. While a number of methods have\nbeen proposed for face attribute estimation, most of them did not explicitly\nconsider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal\nand holistic vs. local) during feature representation learning. In this paper,\nwe present a Deep Multi-Task Learning (DMTL) approach to jointly estimate\nmultiple heterogeneous attributes from a single face image. In DMTL, we tackle\nattribute correlation and heterogeneity with convolutional neural networks\n(CNNs) consisting of shared feature learning for all the attributes, and\ncategory-specific feature learning for heterogeneous attributes. We also\nintroduce an unconstrained face database (LFW+), an extension of public-domain\nLFW, with heterogeneous demographic attributes (age, gender, and race) obtained\nvia crowdsourcing. Experimental results on benchmarks with multiple face\nattributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed\napproach has superior performance compared to state of the art. Finally,\nevaluations on a public-domain face database (LAP) with a single attribute show\nthat the proposed approach has excellent generalization ability.",
        "We present in this paper an image segmentation approach that combines a fuzzy\nsemantic region classification and a context based region-growing. Input image\nis first over-segmented. Then, prior domain knowledge is used to perform a\nfuzzy classification of these regions to provide a fuzzy semantic labeling.\nThis allows the proposed approach to operate at high level instead of using\nlow-level features and consequently to remedy to the problem of the semantic\ngap. Each over-segmented region is represented by a vector giving its\ncorresponding membership degrees to the different thematic labels and the whole\nimage is therefore represented by a Regions Partition Matrix. The segmentation\nis achieved on this matrix instead of the image pixels through two main phases:\nfocusing and propagation. The focusing aims at selecting seeds regions from\nwhich information propagation will be performed. Thepropagation phase allows to\nspread toward others regions and using fuzzy contextual information the needed\nknowledge ensuring the semantic segmentation. An application of the proposed\napproach on mammograms shows promising results",
        "Variational Autoencoders (VAE) and their variants have been widely used in a\nvariety of applications, such as dialog generation, image generation and\ndisentangled representation learning. However, the existing VAE models have\nsome limitations in different applications. For example, a VAE easily suffers\nfrom KL vanishing in language modeling and low reconstruction quality for\ndisentangling. To address these issues, we propose a novel controllable\nvariational autoencoder framework, ControlVAE, that combines a controller,\ninspired by automatic control theory, with the basic VAE to improve the\nperformance of resulting generative models. Specifically, we design a new\nnon-linear PI controller, a variant of the proportional-integral-derivative\n(PID) control, to automatically tune the hyperparameter (weight) added in the\nVAE objective using the output KL-divergence as feedback during model training.\nThe framework is evaluated using three applications; namely, language modeling,\ndisentangled representation learning, and image generation. The results show\nthat ControlVAE can achieve better disentangling and reconstruction quality\nthan the existing methods. For language modelling, it not only averts the\nKL-vanishing, but also improves the diversity of generated text. Finally, we\nalso demonstrate that ControlVAE improves the reconstruction quality of\ngenerated images compared to the original VAE.",
        "Metric-based meta-learning has attracted a lot of attention due to its\neffectiveness and efficiency in few-shot learning. Recent studies show that\nmetric scaling plays a crucial role in the performance of metric-based\nmeta-learning algorithms. However, there still lacks a principled method for\nlearning the metric scaling parameter automatically. In this paper, we recast\nmetric-based meta-learning from a Bayesian perspective and develop a\nvariational metric scaling framework for learning a proper metric scaling\nparameter. Firstly, we propose a stochastic variational method to learn a\nsingle global scaling parameter. To better fit the embedding space to a given\ndata distribution, we extend our method to learn a dimensional scaling vector\nto transform the embedding space. Furthermore, to learn task-specific\nembeddings, we generate task-dependent dimensional scaling vectors with\namortized variational inference. Our method is end-to-end without any\npre-training and can be used as a simple plug-and-play module for existing\nmetric-based meta-algorithms. Experiments on mini-ImageNet show that our\nmethods can be used to consistently improve the performance of existing\nmetric-based meta-algorithms including prototypical networks and TADAM. The\nsource code can be downloaded from\nhttps://github.com/jiaxinchen666/variational-scaling.",
        "We present the first differentiable Network Architecture Search (NAS) for\nGraph Neural Networks (GNNs). GNNs show promising performance on a wide range\nof tasks, but require a large amount of architecture engineering. First, graphs\nare inherently a non-Euclidean and sophisticated data structure, leading to\npoor adaptivity of GNN architectures across different datasets. Second, a\ntypical graph block contains numerous different components, such as aggregation\nand attention, generating a large combinatorial search space. To counter these\nproblems, we propose a Probabilistic Dual Network Architecture Search (PDNAS)\nframework for GNNs. PDNAS not only optimises the operations within a single\ngraph block (micro-architecture), but also considers how these blocks should be\nconnected to each other (macro-architecture). The dual architecture (micro- and\nmarco-architectures) optimisation allows PDNAS to find deeper GNNs on diverse\ndatasets with better performance compared to other graph NAS methods. Moreover,\nwe use a fully gradient-based search approach to update architectural\nparameters, making it the first differentiable graph NAS method. PDNAS\noutperforms existing hand-designed GNNs and NAS results, for example, on the\nPPI dataset, PDNAS beats its best competitors by 1.67 and 0.17 in F1 scores.",
        "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.",
        "Many supervised learning problems involve high-dimensional data such as\nimages, text, or graphs. In order to make efficient use of data, it is often\nuseful to leverage certain geometric priors in the problem at hand, such as\ninvariance to translations, permutation subgroups, or stability to small\ndeformations. We study the sample complexity of learning problems where the\ntarget function presents such invariance and stability properties, by\nconsidering spherical harmonic decompositions of such functions on the sphere.\nWe provide non-parametric rates of convergence for kernel methods, and show\nimprovements in sample complexity by a factor equal to the size of the group\nwhen using an invariant kernel over the group, compared to the corresponding\nnon-invariant kernel. These improvements are valid when the sample size is\nlarge enough, with an asymptotic behavior that depends on spectral properties\nof the group. Finally, these gains are extended beyond invariance groups to\nalso cover geometric stability to small deformations, modeled here as subsets\n(not necessarily subgroups) of permutations.",
        "Image aesthetic quality assessment has been a relatively hot topic during the\nlast decade. Most recently, comments type assessment (aesthetic captions) has\nbeen proposed to describe the general aesthetic impression of an image using\ntext. In this paper, we propose Aesthetic Attributes Assessment of Images,\nwhich means the aesthetic attributes captioning. This is a new formula of image\naesthetic assessment, which predicts aesthetic attributes captions together\nwith the aesthetic score of each attribute. We introduce a new dataset named\n\\emph{DPC-Captions} which contains comments of up to 5 aesthetic attributes of\none image through knowledge transfer from a full-annotated small-scale dataset.\nThen, we propose Aesthetic Multi-Attribute Network (AMAN), which is trained on\na mixture of fully-annotated small-scale PCCD dataset and weakly-annotated\nlarge-scale DPC-Captions dataset. Our AMAN makes full use of transfer learning\nand attention model in a single framework. The experimental results on our\nDPC-Captions and PCCD dataset reveal that our method can predict captions of 5\naesthetic attributes together with numerical score assessment of each\nattribute. We use the evaluation criteria used in image captions to prove that\nour specially designed AMAN model outperforms traditional CNN-LSTM model and\nmodern SCA-CNN model of image captions.",
        "We consider differentially private algorithms for reinforcement learning in\ncontinuous spaces, such that neighboring reward functions are\nindistinguishable. This protects the reward information from being exploited by\nmethods such as inverse reinforcement learning. Existing studies that guarantee\ndifferential privacy are not extendable to infinite state spaces, as the noise\nlevel to ensure privacy will scale accordingly to infinity. Our aim is to\nprotect the value function approximator, without regard to the number of states\nqueried to the function. It is achieved by adding functional noise to the value\nfunction iteratively in the training. We show rigorous privacy guarantees by a\nseries of analyses on the kernel of the noise space, the probabilistic bound of\nsuch noise samples, and the composition over the iterations. We gain insight\ninto the utility analysis by proving the algorithm's approximate optimality\nwhen the state space is discrete. Experiments corroborate our theoretical\nfindings and show improvement over existing approaches.",
        "We study a fundamental problem in Bayesian learning, where the goal is to\nselect a set of data sources with minimum cost while achieving a certain\nlearning performance based on the data streams provided by the selected data\nsources. First, we show that the data source selection problem for Bayesian\nlearning is NP-hard. We then show that the data source selection problem can be\ntransformed into an instance of the submodular set covering problem studied in\nthe literature, and provide a standard greedy algorithm to solve the data\nsource selection problem with provable performance guarantees. Next, we propose\na fast greedy algorithm that improves the running times of the standard greedy\nalgorithm, while achieving performance guarantees that are comparable to those\nof the standard greedy algorithm. The fast greedy algorithm can also be applied\nto solve the general submodular set covering problem with performance\nguarantees. Finally, we validate the theoretical results using numerical\nexamples, and show that the greedy algorithms work well in practice.",
        "Analysis and interpretation of egocentric video data is becoming more and\nmore important with the increasing availability and use of wearable cameras.\nExploring and fully understanding affinities and differences between ego and\nallo (or third-person) vision is paramount for the design of effective methods\nto process, analyse and interpret egocentric data. In addition, a deeper\nunderstanding of ego-vision and its peculiarities may enable new research\nperspectives in which first person viewpoints can act either as a mean for\neasily acquiring large amounts of data to be employed in general-purpose\nrecognition systems, and as a challenging test-bed to assess the usability of\ntechniques specifically tailored to deal with allocentric vision on more\nchallenging settings. Our work, with an eye to cognitive science findings,\nleverages transfer learning in Convolutional Neural Networks to demonstrate\ncapabilities and limitations of an implicitly learnt view-invariant\nrepresentation in the specific case of action recognition.",
        "Object detection in point clouds is an important aspect of many robotics\napplications such as autonomous driving. In this paper we consider the problem\nof encoding a point cloud into a format appropriate for a downstream detection\npipeline. Recent literature suggests two types of encoders; fixed encoders tend\nto be fast but sacrifice accuracy, while encoders that are learned from data\nare more accurate, but slower. In this work we propose PointPillars, a novel\nencoder which utilizes PointNets to learn a representation of point clouds\norganized in vertical columns (pillars). While the encoded features can be used\nwith any standard 2D convolutional detection architecture, we further propose a\nlean downstream network. Extensive experimentation shows that PointPillars\noutperforms previous encoders with respect to both speed and accuracy by a\nlarge margin. Despite only using lidar, our full detection pipeline\nsignificantly outperforms the state of the art, even among fusion methods, with\nrespect to both the 3D and bird's eye view KITTI benchmarks. This detection\nperformance is achieved while running at 62 Hz: a 2 - 4 fold runtime\nimprovement. A faster version of our method matches the state of the art at 105\nHz. These benchmarks suggest that PointPillars is an appropriate encoding for\nobject detection in point clouds.",
        "As a widely deployed security scheme, text-based CAPTCHAs have become more\nand more difficult to resist machine learning-based attacks. So far, many\nresearchers have conducted attacking research on text-based CAPTCHAs deployed\nby different companies (such as Microsoft, Amazon, and Apple) and achieved\ncertain results.However, most of these attacks have some shortcomings, such as\npoor portability of attack methods, requiring a series of data preprocessing\nsteps, and relying on large amounts of labeled CAPTCHAs. In this paper, we\npropose an efficient and simple end-to-end attack method based on\ncycle-consistent generative adversarial networks. Compared with previous\nstudies, our method greatly reduces the cost of data labeling. In addition,\nthis method has high portability. It can attack common text-based CAPTCHA\nschemes only by modifying a few configuration parameters, which makes the\nattack easier. Firstly, we train CAPTCHA synthesizers based on the cycle-GAN to\ngenerate some fake samples. Basic recognizers based on the convolutional\nrecurrent neural network are trained with the fake data. Subsequently, an\nactive transfer learning method is employed to optimize the basic recognizer\nutilizing tiny amounts of labeled real-world CAPTCHA samples. Our approach\nefficiently cracked the CAPTCHA schemes deployed by 10 popular websites,\nindicating that our attack is likely very general. Additionally, we analyzed\nthe current most popular anti-recognition mechanisms. The results show that the\ncombination of more anti-recognition mechanisms can improve the security of\nCAPTCHA, but the improvement is limited. Conversely, generating more complex\nCAPTCHAs may cost more resources and reduce the availability of CAPTCHAs.",
        "Terrestrial laser scanning technology provides an efficient and accuracy\nsolution for acquiring three-dimensional information of plants. The leaf-wood\nclassification of plant point cloud data is a fundamental step for some\nforestry and biological research. An automatic sampling and training method for\nclassification was proposed based on tree point cloud data. The plane fitting\nmethod was used for selecting leaf sample points and wood sample points\nautomatically, then two local features were calculated for training and\nclassification by using support vector machine (SVM) algorithm. The point cloud\ndata of ten trees were tested by using the proposed method and a manual\nselection method. The average correct classification rate and kappa coefficient\nare 0.9305 and 0.7904, respectively. The results show that the proposed method\nhad better efficiency and accuracy comparing to the manual selection method.",
        "Knowledge distillation has demonstrated encouraging performances in deep\nmodel compression. Most existing approaches, however, require massive labeled\ndata to accomplish the knowledge transfer, making the model compression a\ncumbersome and costly process. In this paper, we investigate the practical\nfew-shot knowledge distillation scenario, where we assume only a few samples\nwithout human annotations are available for each category. To this end, we\nintroduce a principled dual-stage distillation scheme tailored for few-shot\ndata. In the first step, we graft the student blocks one by one onto the\nteacher, and learn the parameters of the grafted block intertwined with those\nof the other teacher blocks. In the second step, the trained student blocks are\nprogressively connected and then together grafted onto the teacher network,\nallowing the learned student blocks to adapt themselves to each other and\neventually replace the teacher network. Experiments demonstrate that our\napproach, with only a few unlabeled samples, achieves gratifying results on\nCIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances\nare even on par with those of knowledge distillation schemes that utilize the\nfull datasets. The source code is available at\nhttps://github.com/zju-vipa/NetGraft.",
        "We present GraphMix, a regularization method for Graph Neural Network based\nsemi-supervised object classification, whereby we propose to train a\nfully-connected network jointly with the graph neural network via parameter\nsharing and interpolation-based regularization. Further, we provide a\ntheoretical analysis of how GraphMix improves the generalization bounds of the\nunderlying graph neural network, without making any assumptions about the\n\"aggregation\" layer or the depth of the graph neural networks. We\nexperimentally validate this analysis by applying GraphMix to various\narchitectures such as Graph Convolutional Networks, Graph Attention Networks\nand Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can\nconsistently improve or closely match state-of-the-art performance using even\nsimpler architectures such as Graph Convolutional Networks, across three\nestablished graph benchmarks: Cora, Citeseer and Pubmed citation network\ndatasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and\nCo-author-Physics.",
        "Registration of 3D point clouds is a fundamental task in several applications\nof robotics and computer vision. While registration methods such as iterative\nclosest point and variants are very popular, they are only locally optimal.\nThere has been some recent work on globally optimal registration, but they\nperform poorly in the presence of noise in the measurements. In this work we\ndevelop a mixed integer programming-based approach for globally optimal\nregistration that explicitly considers uncertainty in its optimization, and\nhence produces more accurate estimates. Furthermore, from a practical\nimplementation perspective we develop a multi-step optimization that combines\nfast local methods with our accurate global formulation. Through extensive\nsimulation and real world experiments we demonstrate improved performance over\nstate-of-the-art methods for various level of noise and outliers in the data as\nwell as for partial geometric overlap."
    ]
}