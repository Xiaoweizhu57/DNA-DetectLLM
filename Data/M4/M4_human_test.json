{
    "human_text": [
        "This paper discusses a method for computing vector representations for documents by using a skip-gram style learning mechanism with an added regularizer in the form of a global context vector with various bits of drop out. While none of the individual components proposed in this paper are new, I believe that the combination in this fashion is. Further, I appreciated the detailed analysis of model behaviour in section 3.\n\nThe main downside to this submission is in its relative weakness on the empirical front. Arguably there are more interesting tasks than sentiment analysis and k-way classification! Likewise, why waste 2/3 of a page on t-sne projections rather than use that space for further analysis?\n\nWhile I am a bit disappointed by this reduced evaluation and agree with the other reviewers concerning soft baselines, I think this paper should be accepted: it's an interesting algorithm, nicely composed and very efficient, so it's reasonable to assume that other readers might have use for some of the ideas presented here.",
        "Midway City is a city in Orange County, California.\n\nMidway City may also refer to:\n\n Midway City, a fictional city in the Big Bang Comics universe, known for being the home of Knight Watchman\n Midway City, a fictional city in the DC Comics universe, known for being the home of Hawkman (Katar Hol)\n\nSee also \n Midway (disambiguation)\n Midway Township (disambiguation)",
        " The federal government does not issue copies of birth certificates. You will need to request the certificate from the state of birth (not your current state of residence). Requirements for ordering and issuing a new birth certificate vary by state, so be sure to check before requesting.;\n, Some states will require you to indicate a specific reason for your request, and you may not be able to get a copy of your birth certificate (or a family member's) without providing a valid reason.\n\n\nValid reasons may include:Passport applications\nDriver's licenses\nA child's school registration\nSocial security benefits requests\nEmployment benefits requests\nOther personal identification needs, especially those of an official or legal nature\n\n\n\n, The federal Freedom of Information Act and its corresponding state laws govern open records requests, but only apply to public records. Birth certificates are not considered \"public\" under these laws. As a result, you are only entitled to receive another person's birth certificate if you have a certain type of connection to the person whose birth certificate you are requesting. These may include:\n\n\nYourself, provided you are over 18\nSpouse\nParent\nStep-parent\nSibling or half-sibling\nSon or step-son\nDaughter or step-daughter\nGrandparent\nGreat-grandparent\nPower of Attorney\nLegal representative\nNote that this list varies by state. For example, in New York, you must have a court order to request a birth certificate as a spouse, child, or grandparent, but you do not need one as the individual named on the certificate or as a birth parent named on the certificate., The cost for a new birth certificate varies by state. Basic fees for a single certified copy range from about $5 to $40. Additional fees may apply if you request more than one copy. You may have to pay the full fee twice or you may receive a discount on the second copy, depending on state regulations. A processing fee between around $2 to $10 may apply for orders submitted online. Additional fees may also apply if you request an expedited service, special type of shipping and handling, or other special service. , You may need to present one form of primary photo identification and two forms of secondary identification that show your name and address. Accepted forms of identification can vary by state. Primary identification may include :\n\nDriver's license\nState-issued non-driver photo ID card\nUS Military issued photo ID\nPassport\n\n\nSecondary identification may include :\n\nUtility bills\nTelephone bills\nRecent letter from a government agency\nGovernment-issued employee ID badge\nBankbook or checkbook\nCredit card or credit card statement\nHealth insurance card\nTraffic ticket\nRecent lease\n\n\n\n, A certified copy will have a raised state seal and the state registrar's signature. It may also be printed on security paper.Only a certified copy can serve as identification for legal purposes, such as obtaining a passport or driver's license. Uncertified copies do not suffice for these purposes, and their use is generally limited to personal records, such as genealogy projects.The restrictions for requesting an uncertified copy are usually more lax, and they may cost less to obtain than certified copies. In some states, this record is available to anyone who applies for it, regardless of that person's connection to the individuals listed on the certificate. ,\n\n\nIf you do not have access to a phone book or consistent access to the internet, you can contact the city government and ask for the necessary contact information.\nOffices for a state's Division of Vital Records are usually scattered throughout the state, but you may have to visit the nearest big city in your state to find one. Worst case scenario, you will need to pay a visit to the state capital.\n\n, Check with the state's requirements concerning acceptable identification. Make sure that you have all the required identification on hand when you visit the office. Otherwise, your request may be denied.\n\n, The office should have vital records request forms on hand, including an application for birth certificate copies. Fill out the form in the office in plain sight of an office worker.\n\n\nFill out the form completely and truthfully.\nIf you do not know all of the information requested on the form, the office may still be willing to perform the search. Note, however, that searches with incomplete information may take longer and may not be successful.\n\n, Pay the fee by check or money order.\n\n\nMany states will also accept major credit cards.\nSome states will not accept cash.\n\n, The processing time can vary by state, but you can usually expect to receive the certificate in the mail within 10 to 12 weeks. Expedited requests may take as little as two weeks. , You can access the mailing address through a phone book or online. The fax number, when available, can usually be found online.\n\n\nIf you cannot find the contact information on your own, ask the local government office for the address or fax address. Most city governments will have this information in their records.\nYou will typically send your request to the main office, which is usually located in the state capital. Sometimes, however, you should direct your request to the nearest branch of the Vital Records office. Check with the state to determine the correct office to use.\nMost states will permit you to make requests by mail, but not all states will allow you to do so via fax.\n\n, Access the form from the website for the state's Division of Vital records. Print out a hard copy and fill it out neatly using black ink.\n\n\nFill out the form completely and accurately.\nNote that many states will permit you to leave some areas blank, but you must find out which areas are allowed to be blank and which are mandatory.\nIf you do not have access to a printer, call the office for the state's Division of Vital Records and ask for a form to be sent to your mailing address.\n\n, Mail and fax requests must still be accompanied by all the required forms of identification. Make copies and attach them to your application.\n\n\nMake sure that the copies are clear and complete.\n\n, Some states will require you to sign a sworn statement stating that the information and identification you submit is accurate. This statement must be signed in front of a notary public and bear the notary public's seal.You can usually find a notary public at a local bank branch, post office, law office, or city government office.\nA notary public may charge a small fee for his/her service.\n\n, Send a check or money order along with your request form, copied identification documents, and sworn statement.\n\n\nDo not send cash.\nMake a copy of your request form in case you need to resubmit it.\n\n, Processing time can vary depending on state, but your requested birth certificate should arrive in the mail within 10 to 12 weeks.Expedited requests may take as little as two weeks. Delays may occur if the information you provided is incomplete or inaccurate.\n\n, The CDC's website contains a list of these offices for every state and territory. This information can also be found by performing a simple internet search, or through the state government's main website. If you have difficulty finding the right website, you can call the office's phone number and ask for the web address.\n48 states (excluding Vermont and Wyoming), plus Washington D.C., American Samoa, and Puerto Rico have outsourced the birth certificate ordering process to VitalChek.com. You can order your birth certificate online from VitalChek by filling out the online form and paying a service fee.\n\n, The state office may have a downloadable form that you will need to fill out, save, and send to an email address. If not, it will have a \"live\" form that you can fill out and submit via a secure server on the website itself.\n\n\nIf the form requires an actual signature (not a digital copy), you should download the form, print it, fill it out completely (including the signature), then scan it back into your computer and email it.\nFill out the form completely and accurately.\nRequired fields are usually indicated on the form. Make sure that all required fields are filled out, and fill out as many optional fields as possible.\n\n, Scan copies of your required identification to attach to your request form.\n\n\nIf sending the form via email, attach the digital identification documents as separate attachments.\nIf sending the form via a secure server, upload the identification documents to the website using the on-screen instructions provided.\n\n, When making your request online, you will need to have a valid credit card in order to pay.\n\n\nYou will not be permitted to mail your payment in separately.\nSome state websites may require you to use a credit card issued by a major credit card company.\n\n, The exact wait time may vary by state, but requests made online usually take significantly less time to process and be returned. Expect to see your new birth certificate within one or two months.\n\n\nThe birth certificate will arrive by mail.\nExpect further delays if the information you provided is incomplete or inaccurate.\n\n, If you (or a family member) were born in another country but qualify as a US citizen, you can obtain a copy of the Consular Report of Birth Abroad from the Department of State.You can order a birth certificate by following the instructions located here.\n\n\nOnly the individual, parent or guardian, an authorized government agency, or a person with written authorization can make the request.\nObtain the FS-240 form from the State Department's website. You will need to fill out information such as the full birth name, date and place of birth, parents' information, and mailing address.\nThe request form must be notarized. The State Department will not process forms that are not notarized.\nMail the request form, a check or money order for the fee (currently $50), and a copy of your identification to the Department of State. You will receive the copy of the Consular Report of Birth Abroad by mail, or you may pay extra (currently $14.85) for overnight delivery.\n\n,\n\n\nYou can usually request a birth certificate in person at a Vital Statistics Office, online using a secure electronic ordering system, or by mail.\nAdditional identification documents will be required, and restrictions do apply. You can usually order a new birth certificate if you are over 19 years of age and are the person listed on the certificate. You may also make the request as a legal guardian or parent of someone under the age of 19, or as a government officer.\nProcessing fees do apply and vary by province and territory.\n\n,\n\n\nYou can also apply by post or in person at the local register office.\nCertificates usually cost around £9.25, but priority service certificates cost around £23.40.\nYou can call the General Registrar Office for additional information by calling 0300-123-1837. Note that this phone number is formatted for calls within the UK.\nYou will need to provide details about the birth on the appropriate request form. You will also need to provide your own contact information.\n\n, You can apply for a birth certificate in person from a participating Australia Post outlet.\n\n\nYou will need to provide at least three forms of identification with your application.\nYou can request a birth certificate as the person named on the certificate or as that person's parent. Otherwise, you must provide proof of authority from the person named on the certificate. A solicitor or welfare group acting on that individual's behalf or that person's Power of Attorney may also apply.\nStandard cost is around $48, while urgent requests cost around $71.\n\n",
        "The authors introduce a semi-supervised method for neural networks, inspired from label propagation.\n\nThe method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).\n\nAs possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.\n\nExperiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.\n\nIn summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.",
        "The paper presents a simple method for constructing a visual hierarchy of ImageNet classes based on a CNN trained on discriminate between the classes. It investigates two metrics for measuring inter-class similarity: (1) softmax probability outputs, i.e., the class confusion matrix, and (2) L2 distance between fc7 features, along with three methods for constructing the hierarchy given the distance matrix: (1) approximation central point, (2) minimal spanning tree, and (3) multidimensional scaling of Borg&Groenen 2005.\n\nThere are two claimed contributions: (1) Constructs a biology evolutionary tree, and (2) Gives insight into the representations produced by deep networks. \n\nRegarding (1), while the motivation of the work is grounded in biology, in practice the method is based only on visual similarity. The constructed trees thus can’t be expected to reflect the evolutionary hierarchy, and in fact there are no quantitative experiments that demonstrate that they do. \n\nRegarding (2), the technical depth of the exploration is not sufficient for ICLR. I’m not sure what we can conclude from the paper beyond the fact that CNNs are able to group categories together based on visual similarities, and deeper networks are able to do this better than more shallow networks (Fig 2).\n\nIn summary, this paper is unfortunately not ready for publication at this time. ",
        " By now you've probably been looking at colleges and have a few potential ones in mind. Find out what each school's required SAT score for entry is. This will give you a goal score.There are several ways to get this information.\n\n\nSchools often publish this information on official websites, usually under the \"Admissions\" section. If you can't find what you need on the school websites, try doing an internet search of the school's name and \"SAT score requirement.\" This way you can avoid searching around the school websites.\nCall or email the school's admissions office and ask for their SAT score requirements. There are advisers there whose job it is to answer any questions prospective students may have, so you shouldn't be shy about asking them for information.\nSome schools don't have a minimum score for entry. If this is the case, use these methods to find the average SAT scores for their incoming freshman class. This will give you an idea of what you should aim for.\nIf there is a lot of variation in the scores each school requires, make the highest one your goal. That way, you'll cover all your bases.;\n, There are a number of quality SAT review books available at most book stores and local libraries.Some top test books include:College Board Official SAT Study Guide. Also known as the Blue Book, this prep book is published by the company that makes the SAT. It contains 10 full-length tests. It is known to be particularly useful for math sections, because it contains detailed descriptions of every problem. It also comes with a DVD that has further advice and tips.\nKaplan SAT Premier. This has eight full tests with explanations to answers. It also comes with a companion DVD, offering videos with further explanation.\nGruber's Complete SAT Guide. This book focuses on test taking techniques that will improve your scores and help you find the right answers quickly. In addition, it has five full tests.\nBaron's SAT. Baron's publishes quality books for almost all standardized tests. The SAT book explains all answers, but is known for having a high-quality reading section. The 2015 edition had five full tests, plus another two on the companion CD.\nIt is best to use a combination of these books and others. That way, you'll maximize your chances of seeing the most questions possible.\nMake sure these books were published in or after 2005. The SAT was reformatted in 2005 and older books, while still helpful, won't prepare you for this new version as well as a current book.\n\n, While it may seem strange to do this before any prep, doing this will give you a good idea of what your strengths and weaknesses are. It will also give you the baseline score you need to improve.Make sure you're in a quiet area where you won't be disturbed at all. and you have several hours to devote to this. Time yourself and stick to the time requirements.\nThe full SAT is 3 hours and 45 minutes.\nThere is a 70 minute math section. This breaks down into two 25 minute parts and one 20 minute part.\nThere is a 70 minute critical reading section. This breaks down into two 25 minute parts and one 20 minute part.\nThere is a 60 minute writing section. This breaks down to two 25 minute parts and one 10 minute part.\nThere is a 25 minute experimental section. This is ungraded, but you should still complete it so you can build up your endurance.\nYou are allowed three 5 minute breaks, one after each section. During this time you're allowed to stretch, go to the bathroom, and have a drink and snack. Only eat or drink during breaks to accurately recreate the testing experience.\nStick carefully to these time limits. One of your problems could be that you have trouble finishing in the allotted time, so you'll need to know if this is something you have to improve on.\n\n, This is more than a matter of finding out what your score is. You need to go through the test and find out exactly where you made mistakes. As you move on and work to improve your grade, you'll need to know where your trouble spots are and where you need to focus your efforts., Many students decide to self-prep, which is essentially just utilizing study guides to prepare for the SAT. However, many people fall short of their goals, and in some cases a quality prep course would have significantly improved a students chances of scoring higher on their SAT. A student must assess whether or not they need that extra push to study effectively. Prep courses do a great job at holding students accountable and keeping them engaged.\n\n\nIf you are the student that has a history of not following through with studying or has trouble staying focused, then a prep course might be your best bet. If you do decide to use a prep course, it's extremely important to consider many factors when assessing what prep course might be the best for you.\nOne of the most important factors to pay attention to is whether or not you're getting quality content with your prep course purchase. There are many companies trying to get in the prep course game, but they all don't have the same level of content quality. Make sure to do your research on the company before you make a prep course purchase, and seriously consider trying to get access to a free trial before making your prep course purchase.\n\n, There are many words that repeat themselves from year to year. If you read enough practice tests, chances are good that when you take the test you've seen a majority of the words before.You don't need to purchase these practice books- you can just flip through these sections at the bookstore or library.\nCollege Board, the company that designs the SAT, offers free sample test questions and sections on its website., Keep a notebook and write down words you don't know as you come across them. Take a dictionary and define them afterwards. Also include whether the word is a noun, verb, etc.Set up your book in columns. Draw a line down the middle of the page to divide the columns. Put the words on the left, and the definitions on the right. This way, you can either put a piece of paper or your hand over the definitions while you study.\nMake sure your notebook is small enough to conveniently carry around with you. That way, any down time you have can be used for study.\nYou could also try a few websites dedicated to improving vocabulary. That way you can access them on your smartphone anywhere., Set time aside to work on your vocabulary. Go through your notebook, cover the definitions, and define as many words as you can without looking.\n\n\nFor words you don't know, say the definitions aloud a few times to help you remember them for next time.\nUse the words in a sentence. This will not only help you understand their meanings, but prepare you to use them in your essay and in the reading comprehension sections.\nUse the words in everyday life. When you get comfortable with a word's definition, put it to use. When these words become normal parts of your vocabulary, they'll be much easier to figure out on the SAT.\n\n, Many companies that publish review books also make vocabulary flashcards. They can usually fit easily in your pocket or bag, so they may be more convenient to carry around than your notebook.\n\n, With apps, you'll always have a study resource at your fingertips. For example, SAT and ACT Vocabulary Builder by AccelaStudy uses a dynamic approach to flashcards by specifically focusing on words you've had trouble with., Sentence completion questions have one or two blank spaces. You have to pick the word or words that best complete the sentence. Improving your vocabulary will help here, but it's unlikely you'll know all the words. That's why it's important to know some strategies to help you break down sentences and pick the right answer.\n\n, You don't need to know the definitions of the words most of the time, you just need the context the sentence provides., A clause is a part of a sentence that contains a noun and a verb, and conveys a complete idea. The sentence completion questions almost always present you with compound sentences, meaning they contain two or more clauses. The clauses are usually linked by a conjunction, which are words like \"but,\" \"and,\" \"so,\" and \"because.\"For example: \"I went to the store, but it was closed.\" This sentence contains two clauses, linked by the conjunction \"but.\"\n\n, The type of conjunction tells you how the clauses are related.One-way switch. These conjunctions tell you that the sentence flows in one direction. That means the clauses both support the main idea of the sentence. Some one-way switches are: and, because, since, so, and therefore.\nHere's a one-way switch: \"I got in my car and drove to work.\" The \"and\" tells us that the two clauses support each other.\nTwo-way switch. This type of conjunction tells us that there will be a break in the sentence's flow. That means the second clause will negate something in the first. Some of these are: but, although, despite, however, and while.\nHere's a two-way switch: \"I got in my car but it wouldn't start.\" The \"but\" tells us that the second clause will be inconsistent with the first. Presumably, this person got in their car to go somewhere, but they now can't go anywhere because the car won't start.\nSentences without a switch are all one-way, unless they're about change over time. Sentences that describe change over time are two-way.\n\n, Now that you know how to break down a sentence, you can start working out how to solve a sentence completion question. Using the rules about one-and two-way switches, figure out the flow of the sentence.\n\n\n\"Johnny didn't even know how to ride a bike, but he _____ the race anyway.\" The \"but\" tells us that we have a two-way switch here. Already, without even seeing any choices, you know that the right choice will contrast with Johnny not knowing how to ride a bike.\n\n, That way, you can pick the choice most like the word you had in mind.\n\n, When you've broken the sentence down, figured out its flow, and picked a word to complete it, then look at the choices. All the previous steps will make it much easier to spot the correct choice.\n\n\nIf you're completely stuck on vocabulary, start eliminating choices. Even if you have to guess, every choice you eliminate makes your chances better.\n\n, These are usually the typical length and difficulty of SAT reading sections. Read an article and understand its point.Ask yourself what the main idea of the article is.\nAfter deciding on the main idea, make the article's scope more specific. If you read about volcanoes in Hawaii, saying that the article was about Earth science is too broad. In a short sentence, say the article's topic and scope.\nIf you're unsure that you understood an article properly, ask a teacher or tutor to read it. Then tell them the short summary sentence you came up with. They can tell you if you've properly understood the article or if you made a mistake somewhere.\nMake notes next to the article to help yourself comprehend it better.\n\n, You don't get to pick the topics for the reading comprehension sections, so chances are good you'll find at least one of the sections very boring. Unfortunately, we tend to skim and not pay attention to topics we have no interest in, which could lead you to get questions wrong. Get yourself used to paying attention to boring topics by purposely reading things you don't find interesting., There are a few types of questions the SAT usually asks. Understanding these can help you figure out the answer.Main Idea. These ask about the point of the article as a whole.\nInference. This will ask you about something that isn't directly stated in the article, but rather implied.\nDetail. These will ask you about a specific section of the article.\nVocabulary. These will ask you to define a word based on how it is used in the passage.\nFunction. These ask you about the purpose of a specific part of the passage.\n\n, We all think differently, and there really isn't one golden rule on how to tackle reading comprehension sections. All strategies have positives and negatives. Try these on your practice tests and see which one works best for you.Read through passages fully, then move onto the questions.\nRead the questions first, then read the passages.\nSkim the passage, then read the questions and refer back to the passage when you have to.\nIf one strategy isn't working for you, try another.\n\n, Because the SAT has strict time constraints, you shouldn't read every word of every passage. Use the following techniques to get all the necessary information in the shortest amount of time.Read the introduction paragraph. This should layout the main idea and topic of the passage.\nOnly read the first and last sentences of paragraphs after this. Effectively constructed paragraphs will give an introduction that summarizes the paragraph, and a conclusion that wraps it up.\nCircle or underline important words or terms. Key terms will vary from passage to passage and will be based on the passage's main idea. For example, if you're reading a passage about climate change, terms like \"rising sea levels,\" \"greenhouse gas,\" and \"pollution\" will likely be important.\n\n, Longer passages usually have several paragraphs. Each paragraph has a particular topic in relation to the main idea of the reading. Take only a few seconds to come up with a very fast, one-sentence summary of each paragraph. That way, if questions ask about a particular section of the passage, you know exactly where to go., Since you've read the passage, you'll probably have an idea of what the right answer is. Find the answer that best matches your prediction. This will prevent you from wasting time by referring back to the passage looking for answers., Many students have trouble sticking to the time constraints in reading comprehension sections. If it takes you more than a minute to answer a question, skip it, or you risk running out of time. If you can eliminate even one choice, then it is best to take a guess., Just like with vocabulary words, there are a limited number of math questions the SAT can ask, so over the years there will be similar or identical questions.\n\n\nSAT Math breaks down into four areas: Numbers and Operations, Algebra, Geometry, and Data Analysis, Statistics, and Probability.There are 44 multiple choice questions and 10 fill-ins.\nMultiple choice questions give you a few opportunities to solve them. If you know how, you can solve them mathematically and pick the right answer. If you're stuck and don't know how to solve a problem, you can try substitution. This means you plug answers into the equation from the question and see what you get. Remember that SAT answers always go from lowest to highest, so this can give you an idea of which answer to start with. With multiple choice questions, you can also always guess.\nFill-ins. These have no answer choices, so this eliminates the possibility of substitution or guessing easily. You'll have to actually solve them. The best thing you can do here is study and practice your mathematical concepts., You may find that you have issues with one particular type of problem. If all the questions you get wrong are geometry, you know what you need to focus on.\n\n, It's true that the SAT booklet includes all relevant formulas.However, you still need to know when and how to use them. Just like with your vocabulary words, make note of formulas you need to know and write down what situations you would use them for.\n\n, Although you aren't graded for showing your work, this will prevent you from making careless mistakes. If you work everything out in your head, you could lose track of your work and make a preventable mistake., These can be tricky, and many students get them wrong because they are confused by the wording.\n\n\nBefore doing any work, ask yourself: \"What is this question asking me?\"That way, you can make sure you understand what exactly you're supposed to solve in this question.Write out all important parts of the question. Word problems often confuse us with unimportant sections. For instance, if a question starts with, \"John's mom put three apples in his lunchbox for school today,\" write \"3 apples.\" That way, by the end you should have a simple math problem without all the confusing wording.\n\n, This is helpful in many ways. For example, if you're in a rush and a problem asks you to calculate a certain angle, you may not have time to do a lengthy equation. However, three of the choices are acute angles and the fourth is obtuse. You see that the angle is clearly greater than a right angle. Since the figure is drawn to scale, you can correctly choose the obtuse angle., Don't rely on your imagination to visualize shapes- draw them. Like showing your work, this will help you catch mistakes before you make a careless error.,\n\n\niTooch SAT Math is an app currently for iPhones and iPads, and is under construction for Windows and Androids. It has a large database of questions and includes math games to help you practice in a more entertaining way.SAT Math Testbank. This app is compatible with Apple products. It isn't flashy and includes a large question database., The writing section asks you to take a side on a broad issue, then defend it with examples. So while grammar and spelling are important as well, what graders are looking for is how well you can support an argument., The top bun is the introduction, the middle is the body and supporting examples, the bottom bun is the conclusion.Introduction. Here you should state what your position on the debate is. It helps to rephrase the original question in some way. Then, tell the reader what your position is and why.\nBody. Here is where your examples will go. Use one paragraph per example. This is enough room to elaborate on the example and why it proves your point, but not so long that you'll take too long and won't finish.\nConclusion. Remind the reader what your position was. Explain why the examples you used demonstrate this point.\n\n, It is much better to have two well-thought out and coherent points than five jumbled and poorly considered points. It is helpful to pre-plan a few examples that are applicable in many different situations.Historical examples. In your history classes, you've probably encountered a number of people and events that would be great supporting evidence in an essay. For example, if the essay prompt is \"Can we learn from failure?\" you could say that Abraham Lincoln was a great leader because he was always willing to learn from his past mistakes.\nLiterary examples. Authors almost always include moral or social lessons in their work. You've probably read plenty of these in English class. Novels like 1984 and To Kill a Mockingbird are standards in most high schools, and contain great examples you can use in a number of different essays.\nCurrent events examples. The news is full of examples you can use in almost any argument. Good topics include censorship, military actions, political developments, and social justice.\n\n, Essays don't have to be long to be good. It will be obvious to the grader if you're just adding words and sections to make your essay longer. It's much better to have a shorter essay with strong examples than a long essay full of fluff., Although the point is to defend your position, it shows that you are a powerful writer if you can at least acknowledge the other point of view. This doesn't have to be long- even a sentence will show that you've considered other side of the issue.For example, if the question is asking whether it is right for the government to monitor its citizens, you could say, \"While many would argue that monitoring citizens is essential for national security, it is a violation of our constitutional rights.\"\n\n, Although the main point is to defend your argument, it shows a level of maturity in your writing if you can go beyond words like \"good\" and \"bad.\"Make absolutely sure, however, that you're using complicated words correctly. It could be a mark against you if you keep misusing words.\n\n, It will hurt your chances of a good score if your essay is full of grammar and spelling mistakes. Also be sure to write legibly- the grader can't give you a good score if he can't read your essay.\n\n, If writing is your weak spot, the best way to improve your score is to do write out the essay questions from SAT books and websites. Do these as if you were actually taking the test. Do not look at the question before you start. Stick to the 25 minute time limit and see if you can write an entire essay with an introduction, conclusion, and examples. Then on test day, you'll be in good shape to write an essay with little trouble., There are typically five choices for SAT answers. That means you have a 1 in 5 chance of guessing correctly. However, for every answer you eliminate, it's more likely you'll guess correctly. If you don't know an answer but are absolutely sure 4 choices are wrong, you've just found the correct answer., It's very important to understand exactly what questions are asking you. If you only breeze through or skip the instructions, you'll almost certainly make mistakes.\n\n, If you get stuck on a question and can't eliminate any choices, skip it. You'll waste time trying to figure it out when you could be answering questions you know. When you finish that section, you can come back to questions you skipped if there's time left. However, remember with every choice you eliminate your chances of guessing right improve. If you can eliminate even one answer, it is better to guess.If you skip questions, be certain you fill in the correct bubbles on your scantron as you move on. If you don't pay attention, you could end up filling in the wrong bubbles for all the questions after the one you skipped, meaning the whole section will be wrong.\n\n, The SAT is an exhausting test, so reading in general will help you. This is effective not just for reading sections, but for the whole test. It will help you keep your concentration for long periods of time.,\n\n\nBring a watch. While proctors are supposed to keep track for you, you should be sure you know exactly how much time you have left.Figure out how much time you have per question. In a 20 minute section with 25 questions, you have 48 seconds per question. Keep this in mind to see if you're falling behind and have to pick up the pace.\nIf you start running out of time, just answer easy questions. That way you'll complete the section as much as possible. You'll also probably have time to go back and work on harder ones with this method.\nBe prepared. After practicing all this time, you know the deal. You know the test's layout and what's required in each section. You've done timed practice tests. This will all help make it easier for you to finish in enough time., How long you'll spend prepping really depends on your goals. It also depends on how much free time you have for study. As a general rule, less than two weeks is not enough time for adequate prep. But you don't want to take too long either- if you start a year in advance, you'll probably forget a lot.Overall, 1 to 3 months is a good goal. This allows you enough time to let all the information sink in without risking forgetting anything., With flashcards and apps, you can study pretty much anywhere. If you're on the bus on the way to school, try doing a few math problems or flipping through some vocabulary flashcards. Little things like this can make huge differences.\n\n, If you have any questions or are having trouble in a particular area, ask your teacher in that subject for help. Teachers will be happy to give you extra help.\n\n, Working through test books by yourself may not be enough, and you might require some explanation to help you.\n\n\nMany schools offer after school classes for SAT prep. Find out if your school has one of these, or if you could get one started.\nCheck the internet and see if there are prep classes in your area. Remember that these are sometimes expensive.\n\n",
        "In terms of \"last used\", it depends on what type of use you mean. According to multiple sources (an [online example](_URL_3_) ) an imperial decree in 404 ended gladiatorial combat, meaning that year was the official end of human combat and was more than likely a key moment in the history of he Colosseum (in terms of use). However, from what I've read animal hunts and entertainment continued on for a few decades, until money ran out to fund the importing of animals from Africa (around [600 AD](_URL_2_) is considered the end of all spectator sports).\n\nI can't really find anything on the actual appearance of the structure, though it's worth noting that it was completely abandoned at some points and was ravaged by multiple earthquakes and stone-robbers. In the post-Gladiatorial combat and animal hunting era, the site was used for various religious orders and obviously continues to be used today - which is why your question of the \"last use\" of the Colosseum is a hard question to answer.\n\nHope this helps you!",
        "Neither one is inherently/absolutely more genetically similar or dissimilar to you.\n\n\nAs someone else said, the average shared DNA is the same, 50%.\n\n\nHowever, siblings can vary somewhat substantially from this percentage. **Your sibling might share more than 50%, or less than 50% with you.** Others have mentioned that it's theoretically possible that your sibling could share 0% or 100% with you. This is technically true but the probability of these extreme percentages is so remote that we can safely ignore it. [Here's a previous post I've written to another question that you might find interesting](_URL_0_) Generally the lowest shared percentage that any researcher would ever encounter would be in the high 30s and the highest would be in the mid 60s. In general, the percent of DNA shared between siblings is **strongly** clustered around 50%.\n\n\nHere's a sample of sibling pairs I've looked at in the past:\n\n* Sister vs. sister: n=10, average 51.567%, population std dev 2.65942\n\n* Sister vs. brother: n=21, average 49.50%, population std dev 3.70366\n\n* Brother vs. brother: n=6, average 48.67%, population std dev 3.48541\n\nHighest observed: Sister vs. sister – 57.64%\n\nLowest observed: Brother vs. brother – 40.99%\n\n\nSorry, went off on a bit of a tangent there, but anyway, in contrast to the array of percentages seen among sibling pairs, **your parent will always share 50% with you.** *Disclaimer, I am referring to nuclear DNA. Also, different sources may evaluate the sex chromosomes differently. The X chromosome is much larger in size than the Y chromosome, and so you could say that a boy (who gets his X from his mother and his Y from his father) shares more DNA with his mother than his father (as in, more base pairs because the X is a larger chromosome).* But, to keep it simple, a kid gets 23 chromosomes from the mother and 23 chromosomes from the father, period. There's no variation/fluctuation seen with this relationship like there is with sibling pairs.\n\n\nEdit, okay, additional disclaimer to stave off the flood of PMs...*Trisomies, monosomies, uneven crossing over, inbreeding, translocations, deletions, insertions, chimerism, mosaicism, many various meiotic errors etc etc etc could affect the shared percentage.* I was trying to keep my answer simple-ish. OP was asking about the genetic similarities among *typical* parent/child/sibling pairs and that is what I addressed.",
        "The Daycroft School was a co-educational private boarding school founded in 1928. Initially located at a private home in Darien, Connecticut, it relocated to Stamford in 1935, and in 1963, to the neighboring town of Greenwich, Connecticut. Relocating again in Greenwich, it eventually occupied the Rosemary Hall campus from 1971 until Daycroft's closing in 1991. Smart founded the school for the children of local Christian Scientists.\n\nSchool history \nDaycroft was founded by Sarah Pyle Smart as a private school for the children of area Christian Scientists so they could be educated in an environment akin to their home life, where the teachings of Christian Science were \"an integral part\" of life. To that end, the faculty were all members of The Mother Church. Admission to the school was restricted to the children of parents who were students of Christian Science. The school had no official tie with the Christian Science Church. In later years, admission was extended to those had a close relative who was a member of The Mother Church or one of its branch churches.\n\nThe school was a member of the Secondary Education Board and offered pre-school through high school, in general subjects and college preparatory education. The school offered partial scholarships and was incorporated in 1939 as a non-profit institution.\n\nThe school's initial home was in Darien, Connecticut, where the Smarts lived. In 1935, a portion of an estate was purchased on Noroton Hill, near the  Stamford Cove and became the new home for the school. The 46-acre Stamford campus was sold to Clairol in 1963, when the school moved to the Rock Ridge area of Greenwich, Connecticut. In 1971, the school moved to the former Rosemary Hall campus, also in Rock Ridge. In 1998, the campus was placed on the National Register of Historic Places because of its architectural significance. Its chapel was designed in the Middle English Gothic style by Theodore E. Blake of Carrère & Hastings.\n\nPolio outbreak \nIn 1972, the Daycroft school was one of the last sites of a polio outbreak in the United States. Due to their Christian Science belief, very few students at the school were immunized for polio. The outbreak prompted an emergency quarantine and mass immunization, which successfully prevented polio from spreading to the rest of the state. Ultimately, at least 11 school children (9 boys and 2 girls) were stricken by the paralytic form of the disease.\n\nNotable alumni and staff \nRichard Bergenheim\nJacqueline Moss\nZack Snyder\n\nReferences\n\nExternal links\nDaycroft School Foundation\nCS e-News Spotlight\n\nSchools in Fairfield County, Connecticut\nGreenwich, Connecticut\nChristian Science in the United States\nChristian Science in Connecticut\nFormer Christian Science churches, societies and buildings in the United States\nEducational institutions established in 1928\n1928 establishments in Connecticut",
        "  Recent galaxy redshift surveys have brought in a large amount of accurate\ncosmological data out to redshift 0.3, and future surveys are expected to\nachieve a high degree of completeness out to a redshift exceeding 1.\nConsequently, a numerical programme for determining the metric of the universe\nfrom observational data will soon become practical; and thereby realise the\nultimate application of Einstein's equations. Apart from detailing the cosmic\ngeometry, this would allow us to verify and quantify homogeneity, rather than\nassuming it, as has been necessary up to now, and to do that on a metric level,\nand not merely at the mass distribution level. This paper is the beginning of a\nproject aimed at such a numerical implementation. The primary observational\ndata from our past light cone consists of galaxy redshifts, apparent\nluminosities, angular diameters and number densities, together with source\nevolution functions, absolute luminosities, true diameters and masses of\nsources. Here we start with the simplest case, that of spherical symmetry and a\ndust equation of state, and execute an algorithm that determines the unknown\nmetric functions from this data. We discuss the challenges of turning the\ntheoretical algorithm into a workable numerical procedure, particularly\naddressing the origin and the maximum in the area distance. Our numerical\nmethod is tested with several artificial data sets for homogeneous and\ninhomogeneous models, successfully reproducing the original models. This\ndemonstrates the basic viability of such a scheme. Although current surveys\ndon't have sufficient completeness or accuracy, we expect this situation to\nchange in the near future, and in the meantime there are many refinements and\ngeneralisations to be added.\n",
        "OCBC Centre is a , 52-storey skyscraper in Singapore. serving as the current headquarters of OCBC Bank, the building was completed in 1976 and was the tallest building in the country, and South East Asia, at that time. There are two extensions, OCBC Centre South and OCBC Centre East. There is an Executive Club on one of the higher floors of the building. OCBC Centre East has food and beverage outlets.\n\nHistory \nOCBC Centre was the result of the second Sale of Sites of the Urban Renewal Department of the Housing and Development Board in 1968.  The building was designed by I. M. Pei & Partners (now Pei Cobb Freed & Partners) together with now defunct BEP Akitek (Pte) Singapore and started construction in 1975. Its construction period was only two years due to a three-tier system. The building was completed on 1 October 1976 and was Southeast Asia's tallest building at the time. A bronze sculpture designed by Tan Teng Kee sat at the building until 1983 when it was moved to the now defunct Bras Basah Park. Large Reclining Figure, a large bronze sculpture by Henry Moore, replaced it in 1984, and a new plaza and reflecting pool were built outside the front entrance of the building. The building has undergone several modernisations and OCBC Centre East and South was constructed at a later date.\n\nArchitecture\nThe building is an example of Brutalist architecture, a popular architectural style in the 1970s.\n\nIt is designed to be a symbol of strength and permanence, and its structure consists of two semi-circular reinforced concrete cores as well as three lateral girders which helped make construction faster. The building is divided into three sections due to the steel trusses being  constructed off-site and were put into position. Each section consists of floors that are cantilevered 6 metres from each column, with load transfer girders spanning at each end taking up boxed sections of the pre-stressed concrete. Lattice steel models strengthened by steel and  concrete compression was installed on the 20th and 35th floors of the building. The building has been nicknamed \"The Calculator\" due to its flat shape and windows which look like button pads.\n\nSee also \n List of tallest buildings in Singapore\n\nReferences\n\nFurther reading\n\nExternal links\n \n\n1976 establishments in Singapore\nBrutalist architecture in Singapore\nDowntown Core (Singapore)\nI. M. Pei buildings\nModernist architecture\nOffice buildings completed in 1976\nRaffles Place\nSkyscraper office buildings in Singapore",
        "  We consider the problem of estimating the unconditional distribution of a\npost-model-selection estimator. The notion of a post-model-selection estimator\nhere refers to the combined procedure resulting from first selecting a model\n(e.g., by a model selection criterion like AIC or by a hypothesis testing\nprocedure) and then estimating the parameters in the selected model (e.g., by\nleast-squares or maximum likelihood), all based on the same data set. We show\nthat it is impossible to estimate the unconditional distribution with\nreasonable accuracy even asymptotically. In particular, we show that no\nestimator for this distribution can be uniformly consistent (not even locally).\nThis follows as a corollary to (local) minimax lower bounds on the performance\nof estimators for the distribution; performance is here measured by the\nprobability that the estimation error exceeds a given threshold. These lower\nbounds are shown to approach 1/2 or even 1 in large samples, depending on the\nsituation considered. Similar impossibility results are also obtained for the\ndistribution of linear functions (e.g., predictors) of the post-model-selection\nestimator.\n",
        "The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.",
        " The pictured design includes a \"landing area\"; cut grooves in that also. For an airtight seal, don't cut all the way to the sides of the boards. The purpose of these cuts is to provide the bats something to cling to. Some designs suggest using a screen, but \"roughing up the wood\" is cheaper and more durable.\n\n,, Repeat for the second marked 22 inch (55.9 cm) board. This makes the sides of your bat house.\n,,,,,\nView image at full scale to appreciate the kerf cuts in the back board.\n, (Side pieces overlap the front and back pieces at their edges.),\nIt may be easier to fit the partitions a set distance apart by using a small \"spacer stick\" as depicted.\nSince the stick is 3/4\" thick, this gives the proper thickness to each chamber.\n,\nPosition each partition so that its angled edge will lay flush with the angled roof.\nTwo partitions are 3/4\" apart, the larger back chamber is 1 1/2\" wide (two sticks).\nThe shortest partitions should be at the front of the box, while the 13 inches (33.0 cm) should be positioned at the back. Cut a passageway through the partitions to allow movement from one section to the other.\n\n\n\n\n\n\n,,, It is good to add a little bead of caulk along the seams as you are assembling the parts. The box needs to be air-tight in the top part.\n\nSide view of bat box interior arrangement.\n\n\n\n\n\n\n,, The holes should be drilled in the front of the box, upward at a 40 degree angle. This angle is used to prevent rain from running in.\nA chain can be added by attaching to two screw in the top of the back part.\n, Bats prefer a temperature range of 80–100 °F (27–38 °C). Locate your bat house accordingly. (orientation to sun, prevailing winds, etc). They like morning sun on the box. This box was hung up in a tree which catches the morning sun.",
        "Katzenjammer () is a German word literally meaning \"cat's wail\" (caterwaul) and hence \"discordant sound\", sometimes used to indicate a general state of depression or bewilderment. In the English speaking world it is often used as a term for a hangover, with the sufferer's groans of discomfort being likened to a wailing cat. In fact, the German language uses the term \"Kater\" (tomcat) for this situation.\n\nKatzenjammer may refer to:\n\n Katzenjammer (band), a Norwegian pop/folk band\n Katzenjammer Kabarett, a French rock band\n The Katzenjammer Kids, an American comic strip\n Fran Katzenjammer, a fictional character in the British sitcom Black Books\n \"Katzenjammer\", a song by Big Talk\n \"Katzenjammer\", a song by Kyuss on the album Wretch\n Katzenjammer Cave, in the Makapansgat paleontological site\n Worbey & Farrell, previously known as Katzenjammer, a British piano musical comedy duo\n The Katzenjammer Kids a short-lived band founded by Randy Rhoads\n Fred Katz and his Jammers, a 1959 jazz cello album\n “The Katzenjammers” a Trinidad Steelband led by Percy Thomas, winners of the Trinidad Steelband Festival competition (year unknown), recorded by Cook Laboratories and published circa 1957.\n \"Katzenjammer,\" a 2009 album by Car Seat Headrest's Will Toledo as Nervous Young Men",
        " This involves preparing your presentation beforehand and listening to your audience during the presentation to find out what they need and don't need. Usually you don't have to involve the audience as you are speaking, but if the talk is more of a;\n, Your audience will remember what you were talking about if they can visually construct an image in their heads. Make sure there is a clear beginning and end if you are making a scientific presentation about a complex area of research. However, not all presentations need to be structured this way; for example, if you are making a proposal about a new business plan or idea that you want to develop, it may be interesting to flash images at the audience as you talk about them, giving them a more real and personal understanding of your idea.\n\n, Take the specific and place it in the context of what you have explained to be important or true in the general scientific community.\n\n, If they are not relevant, the audience will lose the greater purpose of what you are trying to talk about.\n\n, Remember, you are the presentation, not the slides or whatever medium you choose to go with. Your presentation is only as good as the way you present it, so let your passion for the subject show in the words you say and the movements you make. If you can make the audience excited about the topic, chances are they won't get lost and stop paying attention. Visual aids should contain all the information your audience needs, but it shouldn't be what helps them understand it. Your purpose is to translate that information and uncover its meaning and relevance.\n\n, If you know you tend to sway back and forth, or clap your hands between each point you make, adjust your style beforehand to minimize anything distracting. The more you practice and give talks, the more you will know yourself and understand your strengths and weaknesses.\n\n, You don't have to! The content of your presentation will stay mostly the same, but how you choose to state it will more or less change.\n\n, Even detail of your work will be seen by a different set of eyes each time you make your presentation, which means that any mistake will stand out to at least one person in the room. Even more important than your grammar is how you handle the structure and organization of your ideas. Remember that if you can't explain how you got from one step to the next, the audience will be just as confused as you are.\n\n, Know your topic well that you can offhandedly create a well executed argument in clear and simple terms. This will not only please those who are well-versed in the subject, but keep the younger and less experienced individuals on their toes and attentive.\n\n, You are less likely to make mistakes in your speaking and more likely to speak clearly if you are completely focused on the subject matter.\n\n, The main reason you should do this is to cut down or lengthen the presentation to the ideal time. When you actually says words verbally, you get a sense for how long it takes to convey a particular idea and when it is time to move on to the next subject. The other reason you should practice is for your own sake, so that you know when your next slide is coming up and what it says, so you can start talking and introducing the audience to your topic before it gets to them. The process of practicing will help you gain more confidence as you speak, as well as familiarize yourself with your topic.\n\n, Conduct a survey or brief questionnaire to see if people really understood the basic concepts of your talk. You might want to ask some questions about your presentation style, but also about the material itself, so you can see which aspects you can stress more and which ones aren't as worthwhile.\n\n",
        " The text in question is the starting point for any literary analysis. You will want to read the text at least once and take careful notes to prepare for writing your essay. As you read:Think about what most interests you: imagery, characters, plot, pacing, tone, etc. Note examples.\nConsider the context. Is this text influenced by other texts, such as the Bible or Shakespeare or even contemporary pop music? Does it use a style or form popular in a particular era, like the epistolary novel of the 18th century?\nReflect on what you know about the author. How might his or her biography influence the text?\nFocus on why certain elements are in the story and how they work. How do characters contribute to the story or theme? Why does the author choose particular a particular setting, image, or tone?Decide what argument you think the text is making or what theme it is exploring. What do you think the main idea is that the author wanted you to understand once you finished reading?;\n, A topic is the subject you will focus on. The earlier you choose a topic, the earlier you can begin collecting evidence to support it. Ideally, you'll have an idea of what you want to write about before you finish reading the text. There are two types of essay, each with their own type of topic:Expository essays give information to the reader.\n\nYour topic may be a single literary element of the work, such as character, plot, structure, theme, symbols, style, imagery, tone, etc.Another common topic is how a work illustrates or breaks the forms of a particular genre or school of thought.You may also draw parallels between the work and real-life subject matter such as historical events or the author's life.Argumentative essays take a position on a debatable topic in order to change the reader's mind. Your topic will typically be your thesis.\n\nYour topic can't be factual: i.e. The nobles in Shakespeare's Hamlet talk in iambic pentameter.\nAnd the argument can't be too easy to win: i.e. Nobles speak in formal iambic pentameter in Hamlet to emphasize their class.\nIt needs to be something people might reasonably disagree about: i.e. In Hamlet, Shakespeare writes noble speech in iambic pentameter not to emphasize their elite status, but rather to underscore how constrained noble characters actually are vis-à-vis the freer commoners.\n\n\n\n, A good topic needs to be narrow enough that you can completely address it within the page limit. The key is to start broad and then narrow your focus.Eventually, you'll want to develop an argument regarding the topic. That argument will be your thesis. For example:\n\n\nBroad topic – The use of humor in Hamlet.\nAdd words that make it more specific – Hamlet's use of humor in Hamlet.\nTurn it into an even more specific sentence – Hamlet's use of humor belies his madness.\n\n, The next step is to transform your topic into an argument – i.e. Hamlet's sense of humor is key to convincing the reader he is in fact sane. While your thesis will likely be revised as you write, it is still important to produce a preliminary thesis regarding the text, what it is trying to achieve, and the techniques the author uses to do so. A thesis will help you organize your ideas.\n\n\nBe specific. For example: \"The technique of allowing the reader the freedom to flesh out sparsely described images reinforces the theme of freedom versus destiny explored in the The Night Circus.\" This works better than something vague like: \"The author uses rich visual imagery to great effect in The Night Circus.\"\n\n, Now that you have chosen a topic and preliminary thesis, you can focus your research. Reread the work or selected sections, looking for quotes that you can use to develop your argument. You will probably want to search for evidence in conjunction with the next step: outlining your essay.\n\n, It is always a good idea to write an outline. At a minimum, you'll want to include your thesis statement and a description of what each succeeding paragraph is about. For example:\n\n\nThesis: The technique of allowing the reader the freedom to flesh out sparsely described images reinforces the theme of freedom versus destiny explored in the The Night Circus.\nParagraph 1: Summary – The Night Circus is a novel about a young man and woman competing in a magical contest they do not fully understand, with a fantastical circus as the setting for their wonders.\nParagraph 2: While the novel is visually compelling, the descriptions are in fact surprisingly sparse, as in this description of the fantastic clock that sits at the entrance to the circus.\nParagraph 3: Her description of the magical snow garden is also surprisingly simple.\nParagraph 4: The simplicity gives the reader the freedom to fill out the descriptions using the guidelines offered by the text, just as the main characters flesh out the circus within the boundaries of the game's rules.\nParagraph 5: The simple yet beautiful descriptions thus reinforce the underlying theme of freedom versus destiny in the book.\n\n, If you write an outline early in the research process, you can use it to help focus on the specific areas you need to explore in more detail. This will save you time, by sparing you research into areas that won't figure in your paper.\n\n, An outline can also provide \"container\" in which to put information. Once you have an outline, you can start placing evidence and analysis into it as you come up with them to produce a more detailed outline that incorporates quotes and evidence for each paragraph, as in this sample outline.\n\n, Include the title and author of the main works you deal with, as well as your thesis. The goal is to clearly define the issues your essay will deal with.\n\n\nDon't write something vague like \"This story deals with the problems of human civilization.\"\nBe specific: \"By the end of the story, Rainsford becomes another Zargoff, another civilized murderer. He has adopted his adversary's brutal attitudes so easily that we are led to question civilization's claim to control human aggression.\", If the text is one that the entire class has read, you will not need to summarize it. However, if it is one that your reader might not be familiar with, you will need to give a brief (one paragraph) summary.Be careful that you do not spend too much time on summary, however. If you have more summary than analysis, your paper won't show your ideas about the text.\n\n, If you are discussing a particular aspect of the work – characters, plot, style, etc. – you will want to begin with a representative example of the literary device you will analyze., Each paragraph will provide evidence to support the claim made in your thesis, as well as analysis of that evidence. You will also want to anticipate counter-arguments. A good essay will include:Evidence – Examples from the text under discussion that support your thesis.\nWarrant – An explanation of how the evidence supports your thesis.\nBacking – Additional reasoning that may be necessary to support the warrant.\nCounterclaims – Anticipate arguments that disagree with your thesis.\nRebuttal – Evidence and argumentation put forward by you that negates the counterclaim you introduced.\n\n, Start by considering what you want your readers to take away from your paper. However, a good conclusion will not simply restate the thesis. It will go on to discuss why it is important, to speculate on its broader implications (i.e. is it true for a genre or period as a whole?) or to provide suggestions on how it might be pursued further., Don't worry about style, spelling, or length. Your first draft should focus on the argument you are making and on marshalling evidence to support that argument. It is very difficult to perfect your prose, grammar, essay structure, and argument all at the same time. You can actually save time by writing multiple drafts that focus on these elements one at a time., Once you have your main ideas and supporting evidence down on paper, it's time to move them around to create an essay that flows logically from one point to the next.\n\n\nTry using a reverse outline to understand the structure of your essay. In the left hand margin, write out the topic of each paragraph in as few words as possible. In the right margin, write how each paragraph advances the overall argument. This will help you see how each paragraph fits into your paper, and which ones might be shifted for better effect.Once your paragraph is in the right order, focus on smoothing out transitions between paragraphs. If your paragraph flows well, you shouldn't need transition words (see https://writing.wisc.edu/Handbook/Transitions.html for a list of such words). The end of one paragraph should logically lead to the start of the next.\n\n, Once you have a paper that you feel makes a clear argument, develops that argument with ample supporting evidence, and which is well organized, it's time to get someone else's opinion. Many universities have writing centers that will provide feedback on drafts. At the very least, let a fellow student take a look. They will be able to point out areas that are confusing or statements that are poorly supported. Use their feedback to write a third draft.\n\n, Now that you have a clear, well-organized paper, it's time to edit it line by line to make sure your prose is concise and it contains no errors of spelling or grammar. It is easier to find errors on the printed page, so be sure to work form a hard copy. It can also help to change the font, as this tricks your brain into viewing the paper as a new document.You can find a list of common errors at http://www.indiana.edu/~wts/pamphlets/proofing_grammar.shtml.\n\n, Watch out for wordy verbs (i.e. \"has knowledge of\" instead of \"knows\"), adverbs, unnecessary prepositional phrases, and overly inflated language (i.e. \"utilize\" instead of \"use\"). For a list of common unnecessary words, see http://writing.wisc.edu/Handbook/Clear,_Concise,_and_Direct_Sentences.pdf.\n\n, When drafting, writers commonly say the same thing two sentences in a row in slightly different ways as they work through ideas. Delete or combine the sentences to avoid this, as in the example below:\n\n\nOriginal text: \"Rachel Watson, the protagonist of Paula Hawkin's Girl on a Train, is a classic unreliable narrator. In particular, her inability to remember events in her own life due to alcohol-induced blackouts leaves the reader unable to rely on her version of events.\"\nRewrite as: \"Rachel Watson, the protagonist of Paula Hawkin's Girl on a Train, suffers from alcohol-induced blackouts that leave her unsure what has happened in her own life, making her a classic unreliable narrator.\"\n\n, How you came to your conclusions does not matter. Present your ideas stripped of \"I\". It's already clear that the work reflects your beliefs because you put your name on it.Don't write: \"I initially thought Hamlet was only feigning madness, but then after a second reading, came to believe that he actually is mad.\"\nInstead, you might write: \"Many critics take for granted that Hamlet is only feigning madness, but their analysis relies on a more modern understanding of insanity. If we take the worldview of Shakespeare's audience into account, it seems more likely that Hamlet was in fact mad.\"\nOr, more simply: \"A close reading that takes into account the worldview of the time reveals that Hamlet is indeed mad.\"\n\n, This technique is especially effective for catching run on sentences and other grammatical errors, like the excessive use of commas., Reading one sentence at a time, starting at the end, interrupts the flow of your essay and helps you to see what the sentences actually say, rather than what you meant to say.\n\n, Always run spell check as a last step. Be careful to pay close attention to names and jargon, as spell check may suggest you substitute different words for already correct spellings.\n\n, Writing does not progress at a linear pace. Sometimes you can write a dozen pages in an hour. Other times, you might struggle to grind out a page. In both cases you are doing productive work. However, if you start your paper too late, a period of slow writing can induce panic that leads to writer's block. To avoid this, start early and schedule plenty of time to write.\n\n\nWork in 2 to 6 hour sessions spread over multiple days. It is hard to stay productive past 6 hours.\nStart at least a week before your paper is due.\n\n, A page goal – particularly a large one like 15 or 20 pages – can be intimidating. Don't worry about pages. It is time that matters. Focus on putting in time, not the number of pages you write in a given session.\n\n, Sometimes when your stuck, it helps to change the format in which you are writing. Writing on a computer, in particular, can slow you down due to the easy ability to repeatedly edit each sentence. Pen and paper can help you move through a draft more quickly.\n\n, If you're having a hard time getting started with a part of your paper – thesis, outline, or draft – start a new document and write whatever comes to mind. It might be an analysis of a particular passage, a summary of the story, or just a list of ideas. It doesn't matter. The important thing is to get writing again.\n\n, The challenge of writing well-constructed sentences while also formulating a clear argument and organizing evidence can sometimes be too much. If you're stuck drafting your essay, try writing in detailed outline form: plug in evidence and you analysis of it, and don't worry about the language or even writing complete sentences. This can be a great way to speed through a first draft. You can then focus on fleshing out your sentences in draft 2.\n\n, If none of these strategies work, it often helps just to step away. Take a walk and give your mind time to sort things out. Sit down and focus on your breathing. Take a nap and let your subconscious have a crack at it.",
        " Try to take routes that have long stretches with few stops with speed limits of at least 30–35 mph (48–56 km/h).\n\n\nAt speeds less than 45 mph (72 km/h), when you reach speed, remove your foot from the accelerator pedal, this turns off the gasoline engine. Slowly depress the accelerator pedal to maintain your speed while keeping the indicator below the ECO line.;\n, The Prius is streamlined for head-on gusts and tail-winds.\n\n,, A Prius will get its poorest gas mileage until it has warmed up several minutes after starting it.,, The air is less dense and will produce less air resistance.\n\n, It's torturous with any car, and with the Prius, all the stops and starts to waste gas.\n\n,, Try to maintain this pressure, plus 2 PSI at all times. Never use the pressure stated on the tire sidewall, as this is a maximum inflation pressure for the tire, without consideration to what type of car it is mounted on!\n\n, For the 2014 Prius you should use 87 octane rating or higher.\n\n, Take your foot off the accelerator as soon as you know you'll need to to slow down or stop. Put your car into \"Glide\" mode under these circumstances.\n\n\nCreate space between your car and the car in front of you, to give yourself time to stop and coast.\n\n, This alone will show you the direction of what part of your car's system is powering the other. Monitor it.\n\n\nDrivers get the best mileage when they are able to read changes in the display. By pressing on the accelerator and brakes to move energy from the gasoline engine to the wheels and/or batteries-or to get all the arrows to disappear. This form is called glide mode.\n\n, In other conditions, accelerate quickly until at speed then maintain your desired speed.\n\n\nLift your foot slightly and reapply your foot after a few seconds until the Energy display shows energy going to the wheels and battery. This is an especially critical tool to use when you find you need energy (such as at night with the battery at a low charge).\n\n\n\n\n\n\n\n, The battery power will assist the engine, thereby reducing your gas use.\n\n,\n\n\nFor every mph over 55 mph (89 km/h), you loose approximately one mpg.\n\n,,, Use the vent system along with the fan's temperature to properly control the vehicle's temperature. Try not to use the \"Max\" setting, at all costs. Turn off the heating, cooling, lights and all other electric accessories, as much as possible.\n\n\nOn hot days, set the air conditioning two degrees lower than the outside temperature, or to 85 (whichever is lower).\nOn cold days, once the cabin is at a comfortable temperature, turn off the climate control. At highway speeds it will maintain that temperature purely by the air rushing into the car's interior.\n\n, It works well on flat roads, and is excellent on non-congested higher-speed highways.\n\n\nCruise control is not good in hilly terrain, or where uphills produce aggressive driving and too much regenerative braking on downhills. Give the car a little push on the accelerator when you disengage regenerative braking.\n\n, The Prius does a good job stopping quickly, coast more than stop whenever possible, thereby decreasing the amount of gas you use. Follow road signs, but hold off on braking until the last few moments.\n\n\nJust like you learned when you first learned to drive an automatic car: Never, under any circumstances, keep your foot on the brake and accelerator pedal at the same time.\n\n,,,,,, Don't put the \"pedal-to-the-metal\" the second the light changes from red to green, or when entering the freeway and/or passing that slow car in your lane.\n\n,, Keep your car tuned, engine and transmission fluids at the proper level, and all other areas cleaned (inside and out).\n\n",
        "In introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4× increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn’t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?",
        "Infant Eyes is an album by organist Charlie Earland which was recorded in 1978 and released on the Muse label the following year.\n\nTrack listing\nAll compositions by Charles Earland except where noted\n \"We Are Not Alone\" – 5:03\n \"Blues for Rudy\" – 12:00\n \"The Thang\" – 7:10\n \"Infant Eyes\" (Wayne Shorter) – 6:14\n \"Is It Necessary?\" – 4:58\n\nPersonnel\nCharles Earland – organ\nBill Hardman – trumpet\nFrank Wess – tenor saxophone, flute\nMack Goldsbury – tenor saxophone (track 2)\nJimmy Ponder (tracks 1 & 2), Melvin Sparks (tracks 3–5) – guitar\nGrady Tate – drums \nLawrence Killian – percussion\n\nReferences\n\nMuse Records albums\nCharles Earland albums\n1979 albums\nAlbums produced by Ozzie Cadena\nAlbums recorded at Van Gelder Studio",
        "There were thousands of Medieval Castles, many of which never were besieged, so trying to determine which castles were the hardest to besiege would be very difficult.\n\nOne feature which made a castle hard to take by assault or siege, was to build it on an island.  It was hard to get troops up to the walls to assault a castle on an island, and it was also hard to starve a castle into submission if boats could re-supply the castle.  Even if the besiegers had local control of the sea, a storm could always drive their blockading ships away and let re-supply boats sneak in.\n\nAn example of a castle on an island is Mont St. Michel in Normandy.\n\nAccording to legend, the Archangel Michael appeared to St. Aubert, the bishop of Avrances, and instructed him to build a church on this island.\n\n(I did not mention the key castle defense of having Archangelic protection for your castle, but it may have been the best defense of all.)\n\nDuring the 100 years war, the castle was held by the French.  It was assaulted and besieged many times by the English, but managed to resist all assaults and sieges.\n\nNo castle (unless afforded divine protection) was truly invulnerable, but being built on an island made a castle more defensible than most.\n\nSources:\n\n_URL_0_\n\n_URL_1_",
        "All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.",
        "Dear Reviewers,\n\nWe have reduced the length of the paper further and it is now 9 pages (excluding references and Appendix). Here is a list of major changes:\n\n- We removed equation (2) of the previous revision since it is redundant.\n- We removed Section 2 of the previous revision, and added Section 3.1 instead in this revision.\n- We removed Remarks 1-3 in Section 4.1 of the previous revision, and moved all of them to Appendix A.1 instead in this revision.\n- We removed the second paragraph of Section 4.5 in the previous revision.\n- We reduced the length of Remark 4 in Section 5.3 of the previous revision.\n\nWe are open to reduce it further if needed. Please let us know your opinion. Thank you.\n\nBest,\nYoojin",
        "They did, several times.\n\n1347, a small ship with a load of lumber and a crew of 18 landed in Iceland after being blown off course by a storm. They were Greenlandic Norse who had gone to Markland (America) for lumber for Greenland, as noted in the Icelandic Annals.\n\nIt is commonly agreed that the Greenlanders continued to travel to America for lumber and fur, and probably to some extent trade with the natives. The Maine Penny, a Norwegian silver coin from the reign of King Olaf Kyrre (1067-1093) found in Maine might be evidence of trade between the Norse and the natives of America - however, the authencity of the find is heavily debated, with some stating that it is a clear forgery, while others simply say that the evidence in inconclusive.\n\nThe end of the Medieval Warm Period and the plague disrupted the ability of the Norse Greenland colony to survive, while inuits spreading southwards started to compete with the Norse over space and fishing and hunting grounds.\n\nThe plague killed 50-75% of the Norwegian population and ended all lack of land, as well as the Norwegian ability to have a ruling class that could compete for the throne, which became the prize of fighting Swedish and Danish Kings, with the Danes coming out on top.\n\nThus, there were no royal power that could fit expeditions and form colonies like the Iberian powers and England did. The abundant supply of land after the plague in Norway meant that there were little to no incentive to emigrate and the failure of the Greenland colony meant that the stepping stone to America was lost.\n\nWhile the Norse on Greenland did travel to America several times, as their settlement died out and the plague hit Norway, the incentive to go to America was lost, and the Norwegians (and their Icelandic subjects) discountinued their journies.",
        "  (Abridged) Using cosmological hydrodynamic simulations in combination with\nanalytic modeling, we show that the galaxy stellar mass-metallicity relation\n(MZR) provides strong constraints on galactic outflows across cosmic time. We\ncompare three outflow models: No outflows, a \"constant wind\" (cw) model that\nemulates the popular Dekel & Silk (1986) scenario, and a ``momentum-driven\nwind\" (vzw) model. We find that only the vzw scaling reproduces the observed\nz~2 MZR's slope, amplitude, and scatter. Comparing our fully three-dimensional\nsimulations with a simple one-zone chemical evolution model, we find that the\nMZR can be understood in terms of three parameters: (1) The equilibrium\nmetallicity Z_eq=y*SFR/ACC (where y=true yield), reflecting the enrichment\nbalance between star formation rate SFR and gas accretion rate ACC; (2) the\ndilution time t_d=M_g/ACC, representing the timescale for a galaxy to return to\nZ_eq after a metallicity-perturbing interaction; and (3) the blowout mass\nM_blowout, which is the galaxy stellar mass above which winds can escape its\nhalo. When outflows with mass loading factor MLF are present, galaxies below\nM_blowout obey Z_eq = y/(1+MLF), while above M_blowout, Z_eq->y. Our cw model\nhas M_blowout ~ 10^10 M_sun, which yields a sharp upturn in the MZR above this\nscale and a flat MZR with large scatter below it, in strong disagreement with\nobservations. Our vzw model reproduces the observed Z_g \\propto M_*^0.3 because\nZ_eq \\propto MLF^-1 \\propto M_*^1/3 when MLF >> 1 (i.e. at low masses). The\nflattening of the MZR at M_* > 10^10.5 M_sun observed by Tremonti et al. (2004)\nreflects the mass scale where MLF~1, rather than a characteristic wind speed.\nThe tight observed MZR scatter is ensured when t_d<1 dynamical time, which is\nonly satisified at all masses and epochs in our momentum-driven wind model.\n",
        "  We analyze the spectral properties of the volume operator of Ashtekar and\nLewandowski in Loop Quantum Gravity, which is the quantum analogue of the\nclassical volume expression for regions in three dimensional Riemannian space.\nOur analysis considers for the first time generic graph vertices of valence\ngreater than four. Here we find that the geometry of the underlying vertex\ncharacterizes the spectral properties of the volume operator, in particular the\npresence of a `volume gap' (a smallest non-zero eigenvalue in the spectrum) is\nfound to depend on the vertex embedding. We compute the set of all\nnon-spatially diffeomorphic non-coplanar vertex embeddings for vertices of\nvalence 5--7, and argue that these sets can be used to label spatial\ndiffeomorphism invariant states. We observe how gauge invariance connects\nvertex geometry and representation properties of the underlying gauge group in\na natural way. Analytical results on the spectrum on 4-valent vertices are\nincluded, for which the presence of a volume gap is proved. This paper presents\nour main results; details are provided by a companion paper arXiv:0706.0382v1.\n",
        "Sir Richard Vesey Hamilton  (28 May 1829 – 17 September 1912) was a Royal Navy officer. As a junior officer he twice volunteered to take part in missions to search for Sir John Franklin's ill-fated expedition to find the Northwest Passage. He also took part in the Battle of Fatshan Creek in June 1857 during the Second Opium War.\n\nLater in his career he became commander-in-chief at China Station and took his fleet into Vladivostok harbour in 1886, which surprised the Russians. He became First Naval Lord in July 1889 and in that role he was primarily concerned with implementing the recommendations contained in a report on the disposition of the ships of the Royal Navy many of which were unarmoured and together incapable of meeting the combined threat from any two of the other naval powers (\"the Two-power Standard\"): these recommendations had been enshrined in the Naval Defence Act 1889. He finished his career as President of the Royal Naval College at Greenwich.\n\nEarly career\nBorn the son of the Revd John Vesey Hamilton and his wife Frances Agnes Hamilton (née Malone), Hamilton was educated at the Royal Naval School in Camberwell and joined the Royal Navy in July 1843. He was posted to the sloop  in the Mediterranean Fleet.\n\nHe volunteered to become a mate on the barque  which was despatched in 1850, under the command of Captain Erasmus Ommanney, on a mission to search for Sir John Franklin and his ill-fated expedition to find the Northwest Passage. Promoted to lieutenant on 11 October 1851, he volunteered for a second mission this time in the barque  which was despatched in 1852, under the command of Captain Henry Kellett, in search of Franklin. Resolute became stuck in the ice in the spring of 1854 and Kellett and his crew were ordered to abandon ship.\n\nHamilton was given command of the gunboat  in February 1856 and took part in the Battle of Fatshan Creek in June 1857 during the Second Opium War. Promoted to[commander on 10 August 1857, he was given command of the sloop  on the West Indies Station in June 1858.\n\nPromoted to captain on 27 January 1862, he took command of the sloop  on the West Indies Station in July 1862, the sloop  on the West Indies Station in 1865 and the broadside ironclad  on coast guard service at Portland Harbour in April 1870. He became commander of the steam reserve at Devonport in 1873 and captain-superintendent of Pembroke Dock in March 1875 and was appointed a Companion of the Order of the Bath on 29 May 1875.\n\nSenior command\n\nPromoted to rear admiral on 27 September 1877, Hamilton was appointed Director of Naval Ordnance at the Admiralty in 1878. He was given command of the Coast of Ireland Station in 1880 and, having been promoted to vice admiral on 17 February 1884, he became commander-in-chief of China Station in September 1885; he took his fleet into Vladivostok harbour the following year and gave the Russians a surprise. He was advanced to Knight Commander of the Order of the Bath on 21 June 1887 and promoted to full admiral on 18 October 1887.\n\nHamilton went on to be Second Naval Lord in December 1888, and First Naval Lord in July 1889. In that role he was primarily concerned with implementing the recommendations contained in a report on the disposition of the ships of the Royal Navy many of which were unarmoured and together incapable of meeting the combined threat from any two of the other naval powers (\"the Two-power Standard\"): these recommendations had been enshrined in the Naval Defence Act 1889. He became President of the Royal Naval Collega at Greenwich, in September 1891 and retired from the Navy in May 1894. He was advanced to Knight Grand Cross of the Order of the Bath on 25 May 1895.\n\nIn retirement he wrote Naval Administration; The Constitution, Character, and Functions of the Board of Admiralty, and of the Civil Departments It Directs. He died at his home in Chalfont St Peter in Buckinghamshire on 17 September 1912, and is buried at Eltham in South London.\n\nFamily\nIn 1862 Hamilton married he Julia Frances Delmé Murray; they had two sons and two daughters. William John Warburton Hamilton was his eldest brother.\n\nPublications\n\nReferences\n\nSources\n\nExternal links\n William Loney RN Career History\n \n\n1829 births\n1912 deaths\nAdmiral presidents of the Royal Naval College, Greenwich\nFirst Sea Lords and Chiefs of the Naval Staff\nKnights Grand Cross of the Order of the Bath\nLords of the Admiralty\nRoyal Navy admirals\nMilitary personnel from Kent\nRoyal Navy personnel of the Second Opium War",
        "- Strengths:\n New Dataset, \n NLP on Resource poor language\n\n- Weaknesses:\n Incomplete related work references, \n No comparison with recent methods and approaches, \n Lack of technical contribution, \n Weak experiments,\n\n- General Discussion:\n\nIn this paper the authors present a simple formula for readability assessment\nof Vietnamese Text. Using a combination of features such as word count,\nsentence length etc they train a simple regression model to estimate the\nreadability of the documents. \n\nOne of the major weaknesses of the paper its lack of technical contribution -\nwhile early work in readability assessment employed simple methods like the one\noutlined in this paper, recent work on predicting readability uses more robust\nmethods that rely on language models for instance (Eg :\nhttp://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,\nhttp://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10\n-camera.pdf). A comparison with such methods could be a useful contribution and\nmake the paper stronger especially if simple methods such as those outlined in\nthis paper can compete with more complicated models. \n\nBaseline experiments with SMOG, Gunning Fog index etc should also be presented\nas well as the other Vietnamese metrics and datasets that the authors cite. \n\nAnother problem is that while previous readability indices were more selective\nand classified content into granular levels corresponding to grade levels (for\ninstance), the authors use a coarse classification scheme to label documents as\neasy, medium and hard which makes the metric uninteresting. (Also, why not use\na classifier?)\n\nThe work is probably a bit too pre-mature and suffers from significant\nweaknesses to be accepted at this stage. I would encourage the authors to\nincorporate suggested feedback to make it better. \n\nThe paper also has quite a few grammatical errors which should be addressed in\nany future submission.",
        "\nSummary:\n--------\nThe authors propose a histogram based state representation with differentiable motion models and observation updates for state tracking from observations. Linear model with Gaussian noise is used as the motion model, while a neural network is used to learn the measurement model. They track robot states in: (1) 1-D hallway, and (2) a 2D arena.\n\n\nPositives:\n----------\n1. Show how to encode prior knowledge about state-transitions in the architecture.\n2. No assumptions about the observation model, which is learned purely from data.\n3. Better accuracy than baselines with limited training data.\n\nNegatives:\n----------\n1. The motion model is too simplistic. The authors in their response to earlier questions say that a generic feed-forward neural network could be used to model more complicated motions. However, then the novelty of their framework is not clear -- as then the proposed model would just be a couple of neural networks to learn the motion and observation models.\n\n2. The observation model again is too simplistic (e.g., one dimensional observations), and is proposed to be a generic feed-forward network. Here again, the technical novelty is not clear.\n\n3. The histogram based representation is not scalable as also highlighted by the authors. Hence, the proposed approach as it is, cannot be applied to more complicated settings.\n\n4. In Figure 5(a,b), where they compare the state-estimation accuracy with other baselines (i.e., LSTMs), it is clear that the accuracy of the LSTM has not saturated, while that of their model has. They should do larger scale experiments with more training data (e.g., 10k,100k,500k samples). \nNote that while sample efficiency is a desirable property (also discussed in Section 6.2), we do expect models with prior knowledge to work better for small number of samples than models which do not assume any structure. Experiments with larger number of samples would be insightful.",
        " You will have to operate this to put a cartridge in the chamber, clear stoppages and clear the weapon (make it safe).;\n, Load it with cartridges, and insert it into the magazine well. Make sure it latches, or locks into place.\n\n, A round (cartridge) should feed up from the magazine. Release the handle, and the bolt should slam home, feed a round into the chamber, and lock into place. If you are not immediately about to fire, engage the safety catch. DO NOT touch the trigger until you are about to fire.\n\n, Next, retract the operation rod and inspect the chamber visually (look to see it is empty) and physically (put a finger into the chamber to check it is empty). It is usually a good idea to lock the bolt back if possible, to show at a distance that it is empty. Not all rifles are able to do this. AK/Kalashnikov rifles for instance cannot, though AR/AR15/M16-style ones can.\n\n, If you are not sure of what is behind the target, do not fire. Ideally, you will be doing this at a shooting range, or a quarry with a high bank to shoot toward to stop stray bullets.\n\n, This will probably be a lever or button on the side of the action, but this varies considerably according to the rifle in question.\n\n, DO NOT touch the trigger until you are ready to fire.\n\n, Note that the closer your off hand is to the front of the barrel, the more steady your support will be. (support hand)(if you're right handed use your left hand; if you're left handed use your right hand).\n\n, If this feels awkward, move your support hand down the barrel or towards you until it feels right.\n\n, Place this eye where it can see past the rear sight but not to the sides of it (don't put your eye right next to the sight, but make sure you are not looking over the sight). If you are pulling the trigger with your right hand the left side of the butt/stock should be firmly pressed against your right cheek below the cheek bone (this is called a gun mold).\n\n, You will want to put the front sight in the 6 o'clock position, in the center of your sight picture, aligned with your target.\n\n,, when you have just breathed out. U.S. Marine training tutorials advise that single shots should be taken while holding ones breath for no more than 3 seconds.\n\n, If you determine that your round went right, correct to the left, if it goes left, correct right. This adjustment is called \"windage\". If you are shooting high, aim lower, or if you hit low, aim higher. This is known as elevation.\n\n, Plan your next target or method.\n\n, Most legal versions of assault rifles in the United States are semi-automatic, so they only fire once each time the trigger is squeezed, but by holding the rifle and rapidly squeezing and releasing the trigger, you may be able to achieve a faster rate of fire.\n\n, Whenever possible, always clean the barrel by inserting the cleaning rod into the breach (back) of the barrel, not the muzzle (front). Depending on how the gas system is designed to operate, you may or may not need to clean the gas tube. Check for cleaning instructions specific to you rifle before you start, and beware small pins and springs. A magnet can help to keep track of these small parts, which are more common on modern AR designs.\n\n",
        "We have clarified some of the questions on paragraph 2 on page 4, on paragraph 2 on page 6 and on paragraph 2 on page 15, respectively. Thanks!",
        "Heauton Timorumenos (Ἑαυτὸν τιμωρούμενος, Greek for The Self-Tormentor) is a play written in Latin by Terence (Latin: Publius Terentius Afer), a dramatist of the Roman Republic, in 163 BC. The play has presented academics with some problems. Firstly it is not entirely clear whether Heauton Timorumenos is Terence's second or third play. More importantly, due to the scant survival of Menander's play of the same name, there is no simple way to judge how much of Terence's version is translation and how much is invention. It is set in a village in the countryside of Attica.\n\nCharacters \n Menedemus – an Athenian nobleman, newly moved to the countryside, father of Clinia\n Chremes – Menedemus' neighbour, father of Clitipho\n Clinia – Menedemus' estranged son, in love with Antiphila\n Clitipho – Chremes' son and a friend of Clinia, in love with Bacchis\n Syrus – Clitipho's slave\n Dromo – Clinia's slave\n Antiphila – a girl raised by a weaveress, beloved of Clinia\n Bacchis – a wealthy courtesan, beloved of Clitipho\n Sostrata – Chremes' wife\n Phrygia – Bacchis' slave\n Canthara – a nurse, servant of Sostrata\n\nPlot\n\nPrologue \nThe prologue serves to defend Terence's method of playwriting. He asks the audience to judge the play by its merits, rather than by the opinions of critics.\n\nAct one \nMenedemus, a wealthy farmer, explains to his neighbour Chremes why he is punishing himself by working hard in his fields. Menedemus explains that he had reproached his son Clinia for his having a relationship with a penniless girl, and had held up his own youth as a soldier as a virtuous contrast. Clinia, shamed, has taken Menedemus more literally than he intended and has gone to live as a soldier in the East. By coincidence, immediately after Menedemus exits, Chremes encounters his own son, Clitipho with Clinia, who has returned from the East. Clitipho tells Chremes not to tell Menedemus, as Clinia is still afraid of his father's wrath. Chremes agrees for the moment but adds that a father's duty is to be severe. Once alone, Clitipho swears he will never be a tyrant in the mould of his father.\n\nAct two \nClinia has sent for his lover, Antiphila, who has been in mourning for the old weaving-woman who brought her up. Antiphila arrives accompanied by Bacchis, the wealthy courtesan with whom Clitipho is in love. Clitipho is angered that his slave, Syrus, has presumed to invite his mistress to his father's house, as his father will disapprove of her. Syrus conceives a ruse for the meantime where Bacchis will pose as Clinia's mistress and Antiphila as her servant. The women arrive; Bacchis praises Antiphila for her virtue and beauty but warns that beauty and men's attention fade, and that she ought to find a man to love who will be constant for life. They meet Clinia and the young lovers are overcome with joy at the reunion.\n\nAct three \nChremes informs Menedemus that his son is returned, but believing that Bacchis is Clinia's mistress, he warns Menedemus against welcoming him home, explaining that Clinia is now in love with a spendthrift mistress. He advises Menedemus to wait while Syrus works out a plan. When Menedemus exits, Chremes is surprised to find Clitipho embracing Bacchis, and tells him off. Syrus agrees to help Chremes, but only because it dovetails with his own scheme directed against Chremes: Syrus needs money because he had promised Bacchis money for her part in the deception. Syrus tells Chremes that Antiphila had been pawned to Bacchis by the old weaveress, and Bacchis now wants to sell her, and he advises Chremes to tell Menedemus to buy Antiphila as she is a good bargain: a captive from Caria whose friends will pay handsomely for her release. Chremes thinks it  unlikely that Menedemus will go for this.\n\nAct four \nSostrata, Chremes' wife, has discovered, by way of a ring that Antiphila has given to her for safekeeping while she bathes, that Antiphila is her long-lost daughter whom she had given away to be exposed on Chremes' direction. Syrus realizes that his deception may thus be found out and he may lose the chance to pay off Bacchis, and may be punished. He withdraws to consider a better plan. Clinia, on the other hand, is overjoyed because Antiphila is now revealed to be a suitable wife for him, so he will be able to abandon the deception. But Syrus says that while Clinia may tell his father the truth, he must keep up the pretense to Chremes for a while longer because Clitipho will be in trouble if Chremes discovers that Bacchis is Clitipho's mistress. When Clinia objects that Chremes will not allow him to marry his daughter while he believes Bacchis is Clinia's lover, Syrus persuades him to maintain the ruse for a day to give Syrus the time to get Bacchis' money. Syrus then tells Bacchis, who is threatening to expose him, to go to Menedemus' house where she will get paid. Syrus then tells Chremes the truth as if it is a trick: he tells him that Clinia has told his father that Bacchis is Clitipho's mistress and that he wishes to marry Antiphila. Syrus advises Chremes that he should go along with this 'trick' and offer to give Clinia dowry money, as well as giving Clitipho money to give to Bacchis to pay off her pledge. Meanwhile, Menedemus has been reunited with Clinia, but he then encounters Chremes who tells him that his son is deceiving him with a false declaration that he wishes to marry Antiphila so that he can extract more money for his mistress Bacchis. Menedemus is dismayed and agrees to pretend to believe his son's plan while Chremes engineers a plot to entrap him.\n\nAct five \nFollowing his agreement with Chremes, Menedemus tells his son that his match with Antiphila will go ahead. Chremes is puzzled that Clinia does not respond by trying to extract expenses for the nuptials but then realises that it is he and not Menedemus who is the subject of Syrus' plot. He is in despair as he only has sufficient money to keep up his family for ten days. Menedemus repeats the advice that Chremes gave to him at the start of the play: he should make his son abide by his wishes. Although Chremes approves of the match now between Clinia and Antiphila, because of his financial woe the dowry he can offer is too small. He asks Menedemus to help save his son by pretending that he, Chremes, is giving away all his estate to make a sufficient dowry. Clitipho is distraught when he hears this news, but his father tells him he would rather have his estate be thus disposed of than go to Bacchis by way of his heir. Menedemus upbraids Chremes for treating his son too harshly and Chremes relents, but on the condition that Clitipho give up Bacchis and take a different wife. Clitipho, preferring a full stomach to passion, agrees to marry a respectable girl. In the last lines of the play, Clitipho begs Chremes to reward Syrus for everything he has done for them.\n\nSee also \nQuod licet Iovi, non licet bovi\nLucius Ambivius Turpio\n\nReferences\n\nExternal links \n \n English translation by Henry Thomas Riley at Perseus: Heautontimorumenos\n \n \n\nWorks by Terence",
        "This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of \"rate\" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.\n\nThe authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific \"form\" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.\n",
        "After reading the rebuttal, I decided to increase my score. I think ALI somehow stabilizes the GAN training as demonstrated in Fig. 8 and learns a reasonable inference network.\n\n---------------\nInitial Review:\n\nThis paper proposes a new method for learning an inference network in the GAN framework. ALI's objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. ALI is trained on multiple datasets, and it seems to have a good reconstruction even though it does not have an explicit reconstruction term in the cost function. This shows it is learning a decent inference network for GAN.\n\nThere are currently many ways to learn an inference network for GANs: One can learn an inference network after training the GAN by sampling from the GAN and learning a separate network to map X to Z. There is also the infoGAN approach (not cited) which trains the inference network at the same time with the generative path. I think this paper should have an extensive comparison with these other methods and have a discussion for why ALI's inference network is superior to previous works.\n\nSince ALI's inference network is stochastic, it would be great if different reconstructions of a same image is included. I believe the inference network of the BiGAN paper is deterministic which is the main difference with this work. So maybe it is worth highlighting this difference.\n\nThe quality of samples is very good, but there is no quantitative experiment to compare ALI's samples with other GAN variants. So I am not sure if learning an inference network has contributed to better generative samples. Maybe including an inception score for comparison can help.\n\nThere are two sets of semi-supervised results: \nThe first one concatenate the hidden layers of the inference network and uses an L2-SVM afterwards. Ideally, concatenating feature maps is not the best way for semi-supervised learning and one would want to train the semi-supervised path at the same time with the generative path. It would have been much more interesting if part of the hidden code was a categorical distribution and another part of it was a continuous distribution like Gaussian, and the inference network on the categorical latent variable was used directly for classification (like semi-supervised VAE). In this case, the inference network would be trained at the same time with the generative path. Also if the authors can show that ALI can disentangle factors of variations with a discrete latent variable like infoGAN, it will significantly improve the quality of the paper.\n\nThe second semi-supervised learning results show that ALI can match the state-of-the-art. But my impression is that the significant gain is mainly coming from the adaptation of Salimans et al. (2016) in which the discriminator is used for classification. It is unclear to me why learning an inference network help the discriminator do a better job in classification. How do we know the proposed method is improving the stability of the GAN? My understanding is that one of the main points of learning an inference network is to learn a mapping from the image to the high-level features such as class labels. So it would have been more interesting if the inference path was directly used for semi-supervised learning as I explained above.",
        "  In this paper, it is shown that the cosmological model that was introduced in\na sequence of three earlier papers under the title, A Dust Universe Solution to\nthe Dark Energy Problem, can be used to resolve the problem of the great\nmismatch of numerical values between dark energy from cosmology and zero point\nenergy from quantum theory. It is shown that, if the zero point energies for\nthe cosmic microwave background and for all the rest of the universe that is\nnot cosmic microwave background are introduced into this model as two entities,\ntheir separate values appear within this theory in the form of a numerical\ndifference. It is this difference that gives the numerical value for the zero\npoint value of Einstein's dark energy density. Consequently, although the two\nzero point energies may be large, their difference can give the known small\ndark energy value from cosmology for dark energy density. Issues relating to\ninterpretation, calculation and measurement associated with this result and an\ninterpretation of dark energy as a measure of polarisation of the vacuum are\ndiscussed. In the first appendix to this paper, problems associated with the\nstandard model of cosmology are solved by redefining temperature in the dust\nuniverse model. In the second appendix of this paper, an examination of the\ndark matter problem in relation to a general relativistic generalisation of\nNewton's inverse square law is undertaken. In the third appendix to this paper,\nthe formalism is used to derive a formula that gives a possible value for the\nmass of the universe in terms of Newton's gravitation constant, Einstein's\nLambda and the velocity of light. All three appendices have their own detailed\nabstracts.\n",
        "Absolutely, but its difficult. As others in this thread have said the yield of a specific isotope in a supernova will be a function of things like the starting composition of the star, the distribution of that composition and the degree of mixing between layers in the star, the time dependent temperature and density profile of the star/supernova and the energy (and thus temperature) dependent cross sections of every possible nuclear reaction that can occur.\n\nStellar nucleosynthesis is typically modeled with network calculations that track the abundances of all possible isotopes. For example [here is the network of isotopes and reactions relevant to big bang nucleosynthesis](_URL_0_).^* Neutron capture, proton capture, alpha and beta decay transform the initial isotopic abundances to a new steady state that depends on the temperature and density. In a supernova where temperature and density change rapidly this gets even more complicated. Additionally photonuclear reactions like (gamma,n) and (gamma,p) may come into play.\n\nAccurately modeling stellar nucleosynthesis is limited by uncertainties in the reaction cross sections, lack of knowledge about the mixing mechanisms that bring material from the core to the surface of a star (and vice-versa), and the ability to model the hydrodynamics of a star as it goes through the supernova process. These are all active areas of research.\n\nIf you're interested in learning more there is a [nice white paper](_URL_2_) on the current state of nuclear astrophysics that has some information on the different network calculation codes available. \n\n^* Edit: For the sake of clarity I linked the simpler big bang reaction network. A supernova deals with significantly more neutron rich nuclei due to rapid neutron capture (the r-process). The reaction network would then have to model the blue regions [in the chart of nuclides](_URL_1_).",
        "I don't believe that's fair to say. The U.S. devoted considerable resources both during and after WWII to maintaining the UK. This included lend-lease aid, eventually direct military support, and the Marshall Plan following the war. \n\nAs a parallel point, the U.S. also devoted considerable aid to France following the war as well. In July 1950, in exchange for France allowing West Germany into NATO, the U.S. provided \"substantial military aid\" to French forces fighting in SE Asia. The point I'm trying to make here is: why would the U.S. deliberately try to fell British influence while enhancing French? \n\nIn any case, as I'm sure you're aware, it wasn't enough. In 1954, the U.S. gave up support because the French couldn't win the war. Then, in a fit of geopolitical genius (/s), America shortly afterwards got itself into exactly the same problem the French had and received the same result.     \n\nNow can you dig around and find some document from some member of the US government from 1926 speculating on the fall of Britain and how America could take advantage? Probably. But that's not why it happened. \n\nWhat i'm getting at with all this is that the fall of the European empires following World War II was not because of the United States. They were simply too expensive to maintain at a time that Europe was struggling to provide for itself. The people that were ruled by Europe were not oblivious to this fact and took advantage to throw off the yoke.\n\nSource:\nPostwar: A History of Europe Since 1945 by Tony Judt",
        "\nThe authors propose a recurrent neural network approach for constructing a\nstochastic volatility model for financial time series. They introduce an\ninference network based on a recurrent neural network that computes the\napproximation to the posterior distribution for the latent variables given the\npast data. This variational approximation is used to maximize the marginal\nlikelihood in order to learn the parameters of the model. The proposed method\nis validated in experiments with synthetic and real-world time series, showing\nto outperform parametric GARCH models and a Gaussian process volatility model.\n\nQuality:\n\nThe method proposed seems technically correct, with the exception that in\nequation (19) the inference model is doing filtering and not smoothing, in the\nsense that the posterior for z_t' only depends on those other z_t and x_t\nvalues with t",
        "I was holding off on this review hoping to get the missing details from the code at ",
        " You should have the following items when setting up your Fitbit Flex:\n\n\nFitbit tracker (May be inserted in a wristband)\nUSB charger\nUSB Bluetooth dongle\nTwo wristbands;\n, Before you setup your new Fitbit, make sure it has a charge:\n\n\nRemove the tracker from the wristband if necessary.\nInsert the tracker into the USB charger, round end first.\nPush the tracker down and in until you hear a click.\nPlug the charger into a USB port or wall adapter.\nCharge until at least three lights are on. This indicates a 60% charge.\n\n, You can download the Fitbit Connect program for Windows or Mac by visiting fitbit.com/setup. This program will track your Fitbit information., If you scroll down the page a little, you should see a button for downloading the setup program. The website will attempt to detect your current operating system and provide the correct link. If the wrong button is showing, select your operating system underneath the Download button.\n\nNote: If you're using Windows 10, the Download button will take you to the Windows Store. Windows 10 uses the same app as the Windows Phone, so follow the method below after installing it. If you'd prefer to use the traditional Windows program, select \"PC\" as your operating system.\n\n, Once the download is finished, run the installer and follow the prompts to install Fitbit Connect on your computer., This will allow you to create a new Fitbit account and setup your device.\n\nNote: If you have a previous Fitbit account, select \"Existing User\" to sign in with your existing account and setup your new Flex.\n\n, You can enter in an email address and create a password, or you can sign in using your Facebook or Google account., Fitbit will use this to help track your performance. Enter your name, gender, birthday, height, and select your time zone., This will allow you to start setting up your Flex., Insert it into the wristband with the arrow facing out and pointing towards the black band., Attach the wristband to your wrist using the clasp. The wristband should be snug but not constricting.\n\n, You won't be able to continue with the Setup process until it's been inserted.\n\nThis isn't required for computer's that already have Bluetooth capability.\n\n, It may take a moment for your computer to find your tracker., Once the tracker has connected, you'll see two indicator lights on the black band. Tap your wristband twice and you'll feel the tracker vibrate., Your Flex is now setup, and the initial goal of 10,000 steps will start. You can check your progress by double-tapping your Flex. Each light indicates 20% of your goal., Once your device is synced, you can view your data from your Fitbit dashboard. You can use this to log activities, food, and track your progress towards your goals. You can open your dashboard any time at fitbit.com/login using your Fitbit account., This app is available for free for iOS, Android, and Windows Phone. You can get it from your device's Play Store., Insert the tracker so that the arrow is facing out and pointing towards the black band on the wristband.\n\n, This will start the account creation and device setup process.\n\nIf you already created an account while setting up your Fitbit on your computer, log in with your Fitbit account instead.\n\n, This may take a moment., This will start the setup process for the Flex., This will begin the account creation process., You can enter an email address and password, or you can use your Facebook or Google account to create one., You'll be prompted to enter your name, birthday, height, weight, and gender. Your age, height, weight, and gender are used to calculate your BMR (basal metabolic rate)., Follow the prompts to pair your Fitbit tracker with your mobile device.\n\nNote: If you're using the Windows 10 app on a computer that doesn't have Bluetooth, you'll need to insert the USB Bluetooth dongle.\nIf your phone is already paired with another device, such as a headset or your computer, you may not be able to pair the Fitbit.\n\n, This may take a while to complete. Make sure to leave the app open while it sets up., Once setup is complete, you'll be taken to the Fitbit dashboard. This tracks your progress and allows you to adjust your goals. You can access the dashboard at any time by launching the Fitbit app., Plug your tracker into the charger and make sure at least three of the lights are on. For best results, charge it until all five lights are on., If your tracker stops functioning properly, a reset will usually fix it. This won't delete any of the data on the tracker.Plug the USB charging cable into a USB port.\nInsert the tracker into the charging unit.\nUse a paper clip to press and hold the pin hole on the back of the charger for about four seconds.\n\n, If the setup process fails, you may want to try again from the beginning. Uninstall the Fitbit Connect program or app and then download and install it again. Follow the prompts to try setting up your device., If you can't get your Fitbit to connect to your computer, try setting it up with a mobile device, or vice-versa.",
        "This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.\n\nOne of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. \n\nMinors: one line before Eq. (3.1), U \\in R ? \\times k\n\n",
        "Unfortunately not open access, but [this article](_URL_0_) is about those exact questions.\n\nTwo answers to your questions, from that article:\n\n > If polar bears were transferred to Antarctica could they survive? And would penguins survive in the Arctic?\n\n > Polar bears would probably survive in the Antarctic, and the Southern Ocean around it, but they could devastate the native wildlife. In the Arctic polar bears feed mainly on seals, especially young pups born on ice floes or beaches. Many of the differences in breeding habits between Arctic and Antarctic seals can be interpreted as adaptations to evading predation by bears.\n\n > Polar bears would find plenty of fish-eating mammals and birds around Antarctica. Penguins would tie particularly vulnerable because they are flightless and breed on open ground, with larger species taking months to raise a single chick. Bears can only run in short bursts, but they could catch a fat, sassy penguin chick or grab an egg from an incubating parent.\n\n > In the Arctic polar bears hunt mainly on the edge of the sea ice, where it is thick enough to support their weight but thin enough for seals to make breathing holes. The numerous islands off the north coast of Canada, Alaska and north-west Europe provide plenty of suitable habitats. The Antarctic continent is colder, with only a few offshore islands, so bears would probably thrive at lower latitudes in the Southern Ocean than in the Arctic.\n\n > We can only hope that nobody ever tries what the questioner suggests. Artificially introduced predators often devastate indigenous wildlife, as it is not accustomed to dealing with them. This occurred with stoats in New Zealand, foxes and cats in Australia, and rats on many isolated islands.\nLarge, heavy animals would also trample the slow-growing, mechanically weak plants and lichens of the Antarctic. For instance, Norwegian reindeer have decimated many native plants in South Georgia, an island in the South Atlantic Ocean, since they were introduced 80 years ago.\n\n > C. M. Pond\nDepartment of Biological Sciences,\nThe Open University,\nMilton Keynes, Buckinghamshire, UK\n\nAnd\n\n > While, as far as I know, no one has ever been stupid enough to introduce polar bears into the Antarctic, there have been at least two practical attempts to transplant penguins to the Arctic.\nThe original \"penguin\" was in fact the late great auk (Pinguinus impennis), once found in vast numbers around northern shores of the Atlantic. Although no relation to southern hemisphere penguins, it was very similar in appearance, and filled much the same ecological niche as penguins, particularly the king penguins of the subantarctic region.\n\n > With any attempt to introduce an alien species, there must actually exist an appropriate ecological niche for it to fill, and it must be vacant. For the most part, the ecological niches occupied by penguins in the south are filled by the auk family to the north. But the demise of the great auk in the mid-19th century at the hands of hungry whalers created not only a vacancy that one of the larger penguins might neatly slot into, but also a potential economic demand for the penguin's fatty meat and protein-rich eggs.\n\n > It was perhaps the possible economic opportunities that prompted two separate bids to introduce penguins into Norwegian waters in the late 1930s. The first, by Carl Schoyen of the Norwegian Nature Protection Society, released groups of nine king penguins at Røst, Lofoten, Gjesvaer and Finnmark in October 1936. Two years later, the National Federation for the Protection of Nature, in an equally bizarre operation, released several macaroni and jackass penguins in the same areas, even though these smaller birds would clearly find themselves competing directly with auks or other native seabirds.\n\n > The outcome was unhappy for the experimenters and, most particularly, for the penguins. Among those whose fate is known, one king was quickly despatched by a local woman who thought it was some kind of demon, while a macaroni died on a fishing line in 1944, although from its condition it had apparently thrived during its six years in alien waters.\n\n > And it soon became obvious that the real reason why any attempt to fill the ecological gap left by the great auk was destined to fail was the very reason that the niche was vacant in the first place — such large seabirds could not happily coexist with a large and predatory human population. Of course, it is the steadily increasing human presence in the far south that is now threatening penguins in their native habitat.\n\n > Hadrian Jeffs\nNorwich, Norfolk, UK",
        "The simple answer is they did not have to react to the discovery of micro-organisms as their scriptures maintain the existence of beings called '[nigoda](_URL_2_)' that are for all intents and purposes, microbes- these things are submicroscopic, live in clusters, have 'only the sense of touch' and live everywhere, from the air, land to tissues of plants and animals [warning-pdf link](_URL_0_). This was mentioned in a text called the Acharamga or Ayaramga Sutra, which basically gave guidelines on how a monk should behave and dates back to atleast the 5th century BC. \n\nSince people cant help but kill millions of such microbes every day, Jains try to minimise the damage by having [severe food restrictions](_URL_3_)- foods like potatoes and garlic are genrally not eaten (they believe tubers and roots have an inordinate amount of microbes), some also avoid drinking unfiltered water. Monks tend to cover their faces and noses with a sheet of cheesecloth ([like this](_URL_1_)) so they can avoid breathing in as many microbes as possible.",
        " Aching bones can be more than just an annoyance. Pain in your bones can be a sign of a serious underlying condition, so talk to your doctor right away if you are having bone pain. Potential causes of aching bones include:\n\n\nPaget’s Disease of the Bone\nSolid Tissue Cancer\nSickle Cell Disease\nMultiple Myeloma\nOsteomalacia (softening of the bones, often caused by a vitamin D deficiency during a time of bone growth)Other cancers such as breast or prostate cancer. Bone pain from cancer may start with a dull or deep ache that may come and go at first, but gradually becomes persistent.LeukemiaFractures (including hairline fractures)\nOveruse or overexertion\nArthritis\nObesity\nOsteoporosis (bone pain is not common with this condition, but it is possible);\n, Your doctor can provide a more specific diagnosis and treatment plan if you let him or her know about the symptoms you have been experiencing. Pay attention to exactly when and how your bones hurt, and be prepared to answer your doctor’s questions, like:Where does it hurt?\nWhen does it hurt? At night? During the day?Does the pain move, or stay in one place?\nHow long have you been experiencing aching bones?\nHave the aches gotten worse?\nHave you been experiencing any other symptoms?\n\n, Some examinations and tests can help determine what is causing your aching bones, and how to treat them. These tests may include:Blood draws (such as for a CBC, or “Complete Blood Count”)\nX-rays of your bone(s)\nA CT or MRI scan\nAssessment of your hormone levels\nUrine studies\nDexa scan\nCalcium, phosphorus, and vitamin D levels\n\n, Your doctor can suggest one or more for you to take. Many over-the-counter options are available, but always use them as directed by the label or your doctor. Common anti-inflammatory medications include:\n\n\nAcetaminophen\nNaproxen\nIbuprofen\nAspirin\n\n, You can ensure you are getting enough vitamin D in a number of ways.\n\n\nThe amount of vitamin D a person needs varies by age, but ranges from 400-800 International Units (IU) per day.Those with osteoporosis or women past menopause will need 800 international units of vitamin D daily\nVitamin D supplements are available at pharmacies, supermarkets, health stores, and many other locations.You can also eat foods high in vitamin D, including: oily fish (salmon, mackerel, sardines), egg yolks, fortified cereals and breads, and many milks and yogurts.Getting adequate exposure to sunlight is important, because your body needs it to help process vitamin D. However, overexposure may be harmful, so talk to your doctor about how much sunlight exposure you should have.\n\n, If you have aching bones, the pain may be linked to a calcium deficiency. In any case, you should ensure that you are getting an adequate amount of calcium in your diet.\n\n\nThe amount of calcium a person needs per day varies widely by age.For example, infants need between 200-260 mg, children 700-1000 mg, and teenagers 1300 mg. Many adults need around 1000 mg a day; however, women over 50 need 1200 mg a day. Postmenopausal women and those with osteoporosis need to take 1200 mg of calcium every day divided in two or three doses.\nCalcium supplements are available at pharmacies, supermarkets, health stores, and many other locations.\nYou can also eat foods high in calcium, including: dairy products (milk, yogurt, cheese), kale, broccoli, fish with soft, edible bones (such as salmon and sardines), and certain fortified foods (including some grains, breads, cereals, pastas, juices, etc.).\n\n, In other cases, an infection may be linked to an underlying cause for which aching bones are also a symptom. If your doctor determines that your bone pain is related to an infection, he or she may prescribe antibiotics to combat it.\n\n\nMake sure to take the antibiotics exactly as prescribed by your doctor, and for as long as you are told to. You should continue to take the antibiotics as long as you are told to, even if the pain or other symptoms disappear. This helps to ensure that the infection is completely eradicated.\n\n, In some cases, aching bones can be caused by or linked to a deficiency of one or more hormones. Your doctor will run tests to see if this is the case for you. If it is, he or she may prescribe a treatment plan (often hormone injections) to make up for this deficiency, and ultimately ease your aches.\n\n\nAlways carefully follow your doctor’s instructions regarding hormone treatments.\n\n, Heat will increase blood flow to the sore area, soothing it as a result. Cold will provide relief by numbing the sore area, and reducing any swelling. You can choose whichever method feels best to you, or alternate between the two. You can apply heat or cold to the aching area using several methods.\n\n\nApply a cold pack (available at pharmacies and many other stores) to the aching area. Make sure to put a towel over your skin to protect it, rather than just setting the cold pack directly on the site of pain.\nApply a homemade ice pack (put some ice cubes in a sealed plastic bag) to the aching area. Make sure to put a towel over your skin to protect it, rather than just setting the ice pack directly on the site of pain.\nWrap warm towels around the aching area.\nPut a sock filled with uncooked rice in the microwave for one minute to form a quick and easy heat pack. Make sure to put a towel over your skin to protect it, rather than just setting the rice heat pack directly on the site of pain.\nTake a warm bath or shower.\n\n, In some cases, your aching bones may be caused by overuse, overexertion, or poor posture.Physical therapy treatments may help ease your aches, and encourage you to move, sit, and stand properly. Talk to your doctor or physical therapist about developing a program to help you.\n\n,, Its effectiveness for treating bone aches is unclear, although some research indicates it may help some causes, such as osteoarthritis.\n\n, The effectiveness of herbal treatments—generally, or to treat aching bones in particular—has not been well-studied in many cases. However, you may investigate one or more herbal treatments instead of or in addition to other methods to relief bone pain (look for them at supermarkets, pharmacies or herbal and health stores).\n\n\nWhite willow bark has effects that are similar to aspirin’s.\nSome research indicates that turmeric has an anti-inflammatory effect.\nGreen tea has long been recognized as an antioxidant and now as an anti-inflammatory as well.\nResearch indicates that a substance in chili peppers known as capsaicin has an anti-inflammatory effect.\nAlways talk to your doctor about your plans to take any herbal supplement or treatment, since they can have side effects or interfere with other medications you are taking.\n\n",
        " Plastic sheeting comes in different thicknesses and sizes.It also comes in black or clear. The most commonly used thickness is 6 mil, although hardware stores also sell plastic in 4, 3, and 2 mil thicknesses. Some people have complained about traditional vinyl mattress covers tearing and ripping and since the hardware store plastic is very tough it should last for a long time. Plastic sheeting is measured in thousandths of an inch or \"mils\". The higher the mil, the thicker and more durable the plastic is. 6 mil plastic would be the best choice for this purpose because when the person shifts positions in bed and moves around it causes a certain amount of wear and tear on the plastic and since 6 mil plastic is thicker, it should last longer than thinner plastic. The only drawback of using this material for bedwetting protection is that it makes a \"crinkling\"(i.e. rustling) sound when the person moves around in bed. The crackling sound can be minimized, however, by covering the plastic with fitted mattress pads as described below. The advantages of using 6 mil plastic is that since it is thicker it is more durable and will be less likely to tear.;\n,, Cut the plastic so that it covers the mattress from the top of the mattress to the bottom. Also, make sure the plastic is long enough on both the left and right sides so that in touches the floor, that way you can tuck it under the bed which prevents the plastic from coming loose or bunching up. Do not cover the corners of the mattress with the plastic sheet because when you put the mattress pads and fitted sheet over the plastic it has a tendency to slip off the bed.\n\n,, Run your hand over the fitted top sheet to see how much crinkling noise the plastic makes. If you feel the child, teenager, or incontinent adult can sleep on the bed with just one pad covering the plastic then just use one of the fitted mattress pads. Ask them if the crinkling noise bothers them and if they say that it does, tell them that you'll place another fitted mattress pad over the plastic to lessen the sound. You can buy mattress pads for around $20.00. There are two types of mattress pads-waterproof and non waterproof. You can buy a quilted waterproof mattress pad for added protection, but it's not really necessary since you have plastic over the box spring.\n\n,, Even though this type of plastic is very durable it's still a good idea to check it once in a while just to be on the safe side. If you notice any holes or rips,there are two things you can do. You can either use some type of waterproof tape to put over the hole or tear, or you can replace the plastic with another piece. Hardware stores such as Lowes and Home Depot should have waterproof duct tape(or other types of tape) for this purpose. Make sure the tape is waterproof or when the person wets at night they'll pee through the plastic, ruining the box spring. This is another advantage of having a roll of plastic for this purpose - you can just cut off another sheet to put over the bed or you can repair the plastic, whereas if a vinyl mattress cover rips you have to go back to the store to purchase another one or you have to go online to buy another one. Alternatively, you can cut off several sheets of plastic from the roll and alternate using the different sheets. For instance you can cut off 4 sheets to use during the month. At the end of the first week you can put the second plastic sheet on, at the end of the second week you can put the third sheet on, etc. Rotating the plastic sheets will reduce the wear and tear of any one of the sheets.\n\n, If they can't get used to it you can always purchase vinyl sheeting from fabric stores such as Jo Anne Fabrics. This option is discussed in the next step.\n\n, Vinyl is a type of plastic. Jo Anne Fabrics sells vinyl sheeting in different thicknesses. The vinyl comes in both clear and frosted clear. I've used the 12 gauge frosted clear vinyl as a bedwetting sheet and so far it's worked really well. This plastic also makes a crinkling sound so you'll also have to cover this to muffle the rustling sound. The wikiHow article Purchase Vinyl Sheeting from a Fabric Store to Manage Bedwetting discusses this in greater detail\n\n, For information about the different types diapers available for older children and teenagers with bedwetting problems see the wikiHow articles Choose Pin on Cloth Diapers for Older Children and Teenagers With Bedwetting Problems and Choose Bedwetting Diapers for an Older Child or Teenager If they're embarrassed about wearing diapers the wikiHow article Encourage Older Children and Teenagers to Wear Diapers for Bedwetting offers a number of methods for encouraging them to wear overnight diapers as well as ways to make them feel less ashamed about it.\n\nIf the person finds the cloth diapers and plastic pants too uncomfortable to wear during the warmer times of year, such as spring and summer, switch to disposables during that time. The plastic sheeting is not meant to be the primary form of protection, it's meant to be used as a fail safe in case the urine leaks through the diapers. If the moisture leaks through the diapers you'll have to wash both the fitted mattress cover and the mattress pads. You'll also need to wipe the plastic sheet off and hang it up to dry. You can use a clothesline for this purpose.\n\n",
        "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments.",
        "The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.\n\nLSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it.",
        " Add 5 ounces (142 g) of boneless pork loin that’s been cut in ¼-inch (6-mm) thick strips and 2 teaspoons (10 ml) of dark soy sauce to a bowl. Mix the two together well so the pork is fully coated and let the meat sit for 10 to 15 minutes.You can omit the pork from the soup if you want to make a vegetarian version.;\n, Add 4 small Chinese dried black mushrooms and 12 dried tree ear mushrooms to a large bowl or pot. Pour 3 cups (710 ml) of boiling water over the mushrooms, and let them soak for until they are softened, which should take approximately 30 minutes.You can usually find Chinese dried black mushrooms and dried tree ear mushrooms at Asian grocery stores.\nMake sure that the water completely covers the mushrooms.\nUse a large bowl or pot that has plenty of extra space because the tree mushrooms will expand significantly as they soak.\nWhile the mushrooms are soaking, turn them over every so often to ensure that they soften all over.\n\n, After the mushrooms have softened, use a knife to remove the stems from the black mushrooms. Squeeze the mushrooms over the bowl to extract the excess liquid, and slice them thinly. Set them aside on a plate for the moment.If there are any hard portions of the black mushrooms, remove them with the knife.\n\n, After you’ve removed the black mushrooms from the bowl, lift out the tree ear mushrooms. Use a knife to remove any hard portions, and set them on the plate with the black mushrooms, reserving the soaking liquid.If the tree ear mushrooms are very large, you should cut them into bite-size pieces.\n\n, Once you’ve removed the mushrooms, mix ¼ cup (59 ml) of the liquid that the mushrooms have soaked in with 1 ½ tablespoons (12 g) of cornstarch in a small bowl. Whisk the two together until they’re fully blended and smooth. Set aside the mixture for the moment., Place 12 dried lily buds in a small bowl, and pour 1 cup (237 ml) of warm water over them. Allow the lily buds to soak for 20 minutes to soften them, and then drain the water.Lily buds are sometimes known as golden needles. You can usually find them at Asian markets.\n\n, After you’ve drained the lily buds, use a knife to cut off the tough tips. Next, slice the buds in half lengthwise and then tear each half into two or three pieces., Add ½ cup (115 g) of canned sliced bamboo shoots that have been cut lengthwise into ⅛-inch (3-mm) wide strips to a small saucepan. Next, pour enough cold water into the pan to cover the shoots by about 2-inches (5-cm).A half cup (115 g) of bamboo shoots usually amounts to half an 8-ounce (230 g) can.\n\n, Place the pan with the bamboo shoots on the stove, and heat over medium-high heat. Allow the mixture to just come to a boil, which should take about 5 to 7 minutes.Boiling the bamboo shoots helps remove some of their bitterness.\n\n, Once the bamboo shoots come to a boil, remove the pan from the heat. Pour the bamboo shoots through a sieve or colander to drain, shaking well to remove the excess moisture.After you’ve drained the bamboo shoots, set them aside for the moment.\n\n, Add 2 tablespoons (30 ml) of red-wine vinegar, 2 tablespoons (30 ml) of rice vinegar, 1 tablespoon (15 ml) of light soy sauce, 1 ½ teaspoons (6 g) of sugar, and 1 teaspoon (6 g) of kosher salt to a small bowl. Whisk the ingredients together until they’re fully combined, and set aside.Be sure to use unseasoned rice vinegar for the soup.\n\n, Place a large wok on the stove, and heat it on high until a drop of water vaporizes on contact with it within a couple of seconds. Next, add 2 tablespoons (30 ml) of peanut oil and swirl it in the wok to coat all of the sides.If you don’t have a wok, you can use a large pot for the soup.\n\n, Once you’ve oiled the wok, add the marinated pork loin pieces. Cook the pork until it just changes color, which should take about 1 minute.Be sure to stir the pork with a wooden spoon to ensure that it cooks evenly.\n\n, After you’ve stir fried the pork for a minute, mix the mushrooms, lily buds, and bamboo shoots into the wok. Stir fry the mixture for another minute., Once you’ve stir fried the pork mixture for a couple of minutes, add 4 cups (946 ml) of reduced-sodium chicken broth to the wok. Allow the mixture to come to a full boil, which should take approximately 5 minutes.It’s best to use reduced-sodium broth for the soup because some of the other ingredients, such as the soy sauce, are high in sodium. However, you can use a homemade broth if you prefer.\nIf you want to make a vegetarian version of the soup, you can substitute vegetable stock for the chicken stock. You can even use water in place of the chicken stock, though your soup may not have as rich a taste as a version with the stock.\n\n, After the broth mixture has come to a boil, add 3 to 4 ounces (85 to 113 g) of firm tofu that’s been rinsed, drained, and cut into ¼-inch (6-mm) thick strips to the pot. Allow the soup to return to a boil.You can use semi-firm tofu instead of firm if you prefer.\n\n, When the soup has return to a boil, add the vinegar mixture. Mix well to blend it into the soup, and then bring it back to a boil, which should take about 3 minutes., Once the soup is boiling again, add the cornstarch mixture that you made earlier. Allow the mixture to return to a boil once more, stirring the whole time to help thicken it.Before you add the cornstarch mixture to the soup, make sure to stir it well. It can clump up while it sits.\n\n, When the soup has returned to a boil, lower the heat to medium. Let the soup simmer for 1 to 2 minutes., Crack 2 large eggs into a small bowl and beat well. Add just a few drops of sesame oil and stir together until they’re well combined.You can use egg substitute in place of the eggs if you prefer.\n\n, When the eggs and oil are mixed, pour the mixture into the soup in a slow, steady stream. Stir the soup in a single direction to mix in the eggs.The eggs should cook almost immediately when you add them to the soup.\n\n, Once you’ve mixed the eggs into the soup, stir in 2 teaspoons (10 ml) of sesame oil, 1 ½ teaspoons(3 g) of freshly ground white pepper, and ¼ teaspoon (1 ml) of chili oil. Stir well so the ingredients are fully incorporated.Adding the chili oil is optional. Your soup will have plenty of heat from the white pepper alone.\nTaste the soup after you’ve mixed in the seasonings. Add more pepper, chili oil, and/or salt if necessary.\n\n, When you’re ready to serve the soup, spoon it into 6 to 8 individual bowls. Use 2 tablespoons (13 g) of thinly sliced scallion greens and 2 tablespoons (3 g) of whole cilantro to garnish the soup and serve.",
        "  We present the first multi-epoch study that includes concurrent mid-infrared\nand radio interferometry of an oxygen-rich Mira star. We obtained mid-infrared\ninterferometry of S Ori with VLTI/MIDI at four epochs between December 2004 and\nDecember 2005. We concurrently observed v=1, J=1-0 (43.1 GHz), and v=2, J=1-0\n(42.8 GHz) SiO maser emission toward S Ori with the VLBA at three epochs. The\nMIDI data are analyzed using self-excited dynamic model atmospheres including\nmolecular layers, complemented by a radiative transfer model of the\ncircumstellar dust shell. The VLBA data are reduced to the spatial structure\nand kinematics of the maser spots. The modeling of our MIDI data results in\nphase-dependent continuum photospheric angular diameters between about 7.9 mas\n(Phase 0.55) and 9.7 mas (Phase 1.16). The dust shell can best be modeled with\nAl2O3 grains using phase-dependent inner boundary radii between 1.8 and 2.4\nphotospheric radii. The dust shell appears to be more compact with greater\noptical depth near visual minimum, and more extended with lower optical depth\nafter visual maximum. The ratios of the SiO maser ring radii to the\nphotospheric radii are between about 1.9 and 2.4. The maser spots mark the\nregion of the molecular atmospheric layers just beyond the steepest decrease in\nthe mid-infrared model intensity profile. Their velocity structure indicates a\nradial gas expansion. Al2O3 dust grains and SiO maser spots form at relatively\nsmall radii of 1.8-2.4 photospheric radii. Our results suggest increased mass\nloss and dust formation close to the surface near the minimum visual phase,\nwhen Al2O3 dust grains are co-located with the molecular gas and the SiO maser\nshells, and a more expanded dust shell after visual maximum. Silicon does not\nappear to be bound in dust, as our data show no sign of silicate grains.\n",
        "It has to do with embryology (how the embryo develops into a human).\nAn embryo starts out as a flat disc with 3 layers of cells and then \"folds\" in on itself (imagine wrapping a burrito).\n\nThe innermost layer forms a tube, which will develop into the internal organs, and the outermost layer forms your skeleton, bones and connective tissues (pretty much everything that isn't your organs)\n\nThe outer layer develops symmetrically about the midline. 2 eyes, 2 ears, 2 arms, 2 legs. Interestingly, your brain is also symmetrical about the midline (although the right and left brain do have different functions)\n\nAs the inner layer of cells (the tube) it becomes squashed against the walls of the body cavity. This causes the tube to twist and bend as it develops. Eventually different parts of the tube differentiate into different organs. The lungs, liver kidneys and spleen branch off the midline. The gut does a whole bunch of Gymnastics. Interestingly the \"start\" and \"end\" of this tube (the mouth and anus) ARE symmetrical.\n\nThe heart is a good example of this. It starts out as a symmetrical tube, but bends and folds as it develops, ending up in the 4 chambered configuration of a mature heart.\n\nThe reason the outermost layers are bot symmetrical is partly because they are not space restricted like the internal organs.",
        "If you are talking about inhaling through the nose, there is a nasal structure that affects this.\n\nDeep within the nose there are a pair of spiral structures that are *explicitly designed* to introduce a special type of turbulence. This turbulence can dramatically reduce the effort needed to draw air through the nose. When you have strong gusts of wind impact your nose, it can overwhelm the effect that these structures have, causing your nose to become far less efficient in passing air.\n\nThis is why rhinoplasty that affects these structures can be so dangerous - if something is done to disrupt the ability of these structures to channel air properly, [it can cause a “suffocating” feeling](_URL_1_) that can cause sufferers all sorts of discomfort, to the point of committing suicide just to obtain relief.\n\nEdit: [A bit of a better article](_URL_0_).\n\nEdit2: Ow, my inbox.\n\nEdit3: To head people off at the pass: This is *NOT* a promotion of Intelligent Design - exactly the opposite, in fact. When I am talking about things being “explicitly designed”, I am talking about evolution creating a structure for a specific purpose or extremely limited range of purposes, not a general structure that can do many different things. General structures are “easier” because they come about from many different evolutionary pressures working in concert, so these pressures on an individual level can be much lower. A very specific structure with a single or very narrow range of purposes point to an evolutionary pressure or range of pressures that are highly specific and very precise, and which are either very strong or exist over very long evolutionary periods. The fact that turbinates can be found in mammalian, avian and saurian species (and in a primitive form in reptiles) indicaties that they had a very important use early on in the history of air-breathing animals, and that persistently strong evolutionary pressures have kept them for many hundreds of millions of years.",
        "Marie Antoinette's days began with her own semi-public rising ceremony that paralleled the king's *lever*.\n\nUpon waking up around eight, she would be handed a dressing gown, and then take breakfast (little but coffee or hot chocolate) either in bed or at a small table nearby. Some mornings, a tub would be rolled into the room for her to bathe in while wearing a flannel gown; after getting out, she would be dried off and dressed in another shift and dressing gown. Either way, she would get back in bed to read or embroider for a while; at this point more people were admitted to the room, like her doctor and surgeon, the king's doctors and surgeons, her private secretary. Eventually, the first *femme de chambre* would bring in a book full of swatches of fabric matching all of the gowns in the wardrobe, and the queen would stick a pin into the ones that she intended to wear during the day: one [court gown](_URL_0_) for the pre-midday-meal ceremonies, one [casual outfit](_URL_1_) for the afternoon, and one [formal gown](_URL_2_) for the evening. Servants would bring these out wrapped in silk taffeta, as well as a taffeta-covered basket containing a couple of chemises and neck handkerchiefs for the day. Earlier in her reign, the male viewers would leave and individual pieces of clothing would be handed by a servant to the highest-ranking lady present to give to the queen or help the queen into - this could lead to farcical situations if everyone didn't get to the room on time; her waiting-woman Henriette Campan later wrote a memoir that documented one instance where the queen's shift had to be passed from one woman to the next as new ladies and princesses walked in, while Marie Antoinette stood naked in the middle of the floor. However, once Rose Bertin became an important part of the queen's dressing routine, she would retreat to the closet to dress after the toilette described in a bit: the ladies were not enthusiastic about giving the tradeswoman a place of prominence that implied she outranked them. Around noon, the maids who had been with her were replaced by new ones in full court dress, and more people would come into the room (such as the Princes of the blood, captains of the guard, and other officers). The queen would make her full hair-and-makeup toilette at a table brought into the middle of the room, all of her ladies would join her, and then she would set out of her chambers to meet people who were to be presented to her.\n\nAt this point, she would meet up with the king for mass, normally a small ceremony. The two would then dine in the \"cabinet of the nobility\", a room attached to her chambers, with titled nobles holding specific serving appointments and anyone who could make it to Versailles watching, and then split apart again so that Marie Antoinette could change out of her hoops and train and into something more comfortable. \n\nAfter the early-afternoon dinner (sometimes followed by another dinner with the Duchesse de Polignac, her BFF; perhaps it was necessary, given that the queen was known to have barely touched her food at the public meals), her time was more her own. This was when she might socialize with her ladies, read or be read to, receive more people, embroider, walk around the gardens, etc. Then she would head back to her rooms for yet another change of clothes, into the formal \"*robe parée*\" that was appropriate for the supper and card parties which, like dinner, were fairly public. She might then go on to a more private party with close friends, or go out to the opera.\n\nWhen she was ready for bed, the queen would be undressed in much the same way she'd been dressed in the morning. A basket with her nightclothes (a lace-trimmed shift, loose corsets, and a nightcap) would be brought out, and her clothing taken back to the wardrobe to be meticulously mended, cleaned, and stored. If the king were going to spend the night with her, she would be put to bed first; then he would come in through the door that connected their rooms after his own *coucher* ceremony that put him into his nightclothes. In the morning, before the full *lever*, one of the queen's servants would open the door to put him back into the hands of his male staff.",
        "This paper introduces a new approach to semantic parsing in which the model is\nequipped with a neural sequence to sequence (seq2seq) model (referred to as the\n“programmer”) which encodes a natural language question and produces a\nprogram. The programmer is also equipped with a ‘key variable’ memory\ncomponent which stores (a) entities in the questions (b) values of intermediate\nvariables formed during execution of intermediate programs. These variables are\nreferred to further build the program.                    The model is also equipped\nwith\ncertain\ndiscrete operations (such as argmax or 'hop to next edges in a KB'). A separate\ncomponent (\"interpreter/computer\") executes these operations and stores\nintermediate values (as explained before). Since the ‘programmer' is\ninherently a seq2seq model, the \"interpreter/computer” also acts as a\nsyntax/type checker only allowing the decoder to generate valid tokens. For\nexample, the second argument to the “hop” operation has to be a KB\npredicate. Finally the model is trained with weak supervision and directly\noptimizes the metric which is used to evaluate the performance (F score).\nBecause of the discrete operations and the non differentiable reward functions,\nthe model is trained with policy gradients (REINFORCE). Since gradients\nobtained through REINFORCE have high variance, it is common to first pretrain\nthe model with a max-likelihood objective or find some good sequences of\nactions trained through some auxiliary objective. This paper takes a latter\napproach in which it finds good sequences via an iterative maximum likelihood\napproach. The results and discussion sections are presented in a very nice way\nand the model achieves SOTA results on the WebQuestions dataset when compared\nto other weakly supervised model.\n\nThe paper is written clearly and is very easy to follow.\n\nThis paper presents a new and exciting direction and there is scope for a lot\nof future research in this direction. I would definitely love to see this\npresented in the conference.\n\nQuestions for the authors (important ones first)\n\n1. Another alternative way of training the model would be to bootstrap the\nparameters (\\theta) from the iterative ML method instead of adding pseudo gold\nprograms in the beam (Line 510 would be deleted). Did you try that and if so\nwhy do you think it didn’t work?\n2. What was the baseline model in REINFORCE. Did you have a separate network\nwhich predicts the value function. This must be discussed in the paper in\ndetail.\n3. Were there programs which required multiple hop operations? Or were they\nlimited to single hops. If there were, can you provide an example? (I will\nunderstand if you are bound by word limit of the response)\n4. Can you give an example where the filter operation would be used?\n5. I did not follow the motivation behind replacing the entities in the\nquestion with special ENT symbol\n\nMinor comments:\nLine 161 describe -> describing\nLine 318 decoder reads ‘)’ -> decoder generates ‘)'",
        "Will Ropp is an American actor. He is best known for his role as Kenny Dawes in The Way Back, as well as for roles in films such as Silk Road and The Fallout.\n\nLife and career\nWill was born in Connecticut. After graduating from The Bolles School in Jacksonville, Florida, he went on to earn a Bachelor of Fine Arts degree at the University of Michigan. In 2020, He received his breakthrough role as Kenny Dawes on the sports drama film The Way Back by Warner Bros. Pictures.\n\nFilmography\n\nFilm\n\nTelevision\n\nReferences\n\nExternal links\n \n\nLiving people\n21st-century American male actors\nAmerican male film actors\nAmerican male television actors\nMale actors from Connecticut\nUniversity of Michigan alumni\nYear of birth missing (living people)",
        "Word of mouth was a big source of information the entire time. Slaves that were allowed to go into town and interact with other slaves would have learned about it and passed it on. By the late eighteenth/early nineteenth century, abolitionist literature was making its way to the South in much greater quantity than before. Some slaves who gained access to pamphlets and whatnot shared them with others, and slaves who could read would share the information.\n\nIn order to actually try to escape using the Underground Railroad, there were likely certain slaves, free blacks, or whites in the community known to be connected. You would have to identify one of these people in order to learn the route.\n\nHowever, unless you had a guide (and even then), it was a very very dangerous trek. Keep in mind, these slaves were usually not allowed to travel or wander around so they had no idea what the layout of the land was like. They had no mental map of the area, and likely no physical map either. I'm not sure on the statistics, but most who attempted to flee didn't make it, and punishments were very harsh in most cases.\n\nAlso note that men were much more likely to try to make it north than women. Sometimes if men made it, they would return for their families. But this was all incredibly dangerous.",
        "David Dederer (born October 5, 1964) is an American guitarist and singer.  He was a member of the alternative rock band The Presidents of the United States of America,. An alumnus of Seattle, Washington's The Bush School and Brown University in Providence, Rhode Island, he founded The Presidents with fellow Bush School alumnus Chris Ballew. He has also been a member of The Gentlemen and Loaded with Guns N' Roses bassist Duff McKagan, also a Seattle native, and Subset, a collaboration between The Presidents and Sir Mix-a-Lot.\n\nDederer currently heads programming and editorial at Amazon Music and manages The Presidents' ongoing business interests.\n\nDederer worked for Seattle web/mobile media company Melodeo from 2007 to 2010 as Vice President, Business Development.  Melodeo was acquired by Hewlett Packard in June 2010.  He previously worked for four years as a public affairs consultant at Seattle firm Pyramid Communications.\n\nPrior to The Presidents' success, Dederer taught high school English at Kent Denver School and The Bush School, did public relations work on environmental issues, and attended graduate school in urban planning at the University of Washington. He has 2 daughters who play in the critically acclaimed Jazz Ensemble 1 at Bellevue High School.\n\nDederer is the older brother of writer Claire Dederer.\n\nReferences\n\nExternal links \nDave Dederer on reddit\nThe Melodeo Team\n\n1964 births\nLiving people\nMusicians from Seattle\nAmerican rock singers\nAmerican rock guitarists\nAmerican male guitarists\nThe Presidents of the United States of America (band) members\nLoaded (band) members\nThe Minus 5 members\n20th-century American guitarists\n20th-century American male musicians\nKent Denver School alumni\nUniversity of Washington College of Built Environments alumni\nBrown University alumni",
        "Using an ensemble in the discriminator portion of a GAN is a sensible idea, and it is well explored and described in this paper. Further clarification and exploration of how the multiple discriminators are combined (max versus averaging versus weighted averaging) would be good. The results are fairly strong, across a variety of datasets.",
        " The opening statement of the Bible declares that “In the beginning God created the heavens and the earth.” (Genesis 1:1) Ask, \"Who or what can say who or what did begin the universe?\" Contemporary science -- although not absolutely certain on exactly what, when or how:\n~ State that scientific theories postulate that the known universe did in fact have a beginning with what is called as “The Big Bang”.\nIt makes no logical sense for anyone to assert that \"absolutely nothing\" could have produced that initiating bang or anything else: \"something existed\" and \"caused that\" -- producing those things that we know of as having a beginning.\n\n, about:\n\n\nThere was a \"beginning of things\",\nThere was an outline of \"from-simple-to-complex-development\" in the creation of earth and life in a Genesis account of creation.\nExplains our universe far \"in advance of humanity having theories/developed or technology\" to try to form such concepts and verify scientific assertions. For instance, the Bible says:\n\n\n“He stretches out the north over 'empty' space, and 'hangs the earth on nothing'.” (Job 26:7)\n\nHow would the writer four thousand years in the past (2000BC) have known that the Earth is \"suspended on nothing\" -- not attached to anything, but mountains were called pillars of heaven in the poetry of Job 26:11, \"The pillars of heaven tremble and are astonished at His threatening.\", ie: mountains \"tremble and are astonished\" (a poetic phrase).\n\n\n\n\n\n, The Bible says that, “God is spirit…” (John 4:24), and it says that “God is love…” (1 John 4:8), and that \"perfect love casts out fear\". The awesome interdependence/relationships and amazing elegance pervading the entire universe suggest a planner, maker, superhuman architect having unlimited knowledge and power. The minds of human beings are taking time and, perhaps, eternity to discover what the originating mind (divine mind) actually accomplished in creating the universe and all it contains. The Bible says that God created mankind in his image (Genesis 1: 26-27), and it is logical to realize that the human mind is able to increasingly succeed, make progress, in understanding the universe because the human mind has a resemblance of the divine mind.\n\n, Jesus was recorded to have been born in Bethlehem (Micah 5:2) of the Tribe of Judah (Genesis 49:10), and to have visited the Temple (Malachi 3:1) and to have risen from the dead (Isaiah 53:11). Historical sources and archaeological evidence establish the legitimacy of Jesus of Nazareth as an actual person to an extent comparable to that of any other notable historical figure. The books called the gospels document the life and teachings of Jesus and the existence and expansion of Christianity as a major religion showing Christ's purpose to create free access to the all mighty through His advocacy/mediation.\n\n, While the material universe provides us with the things necessary to sustain, enhance, and enjoy life, there are also exquisite luxuries which are not essential to survival and reproduction such as knowledge: art, music, and ever-evolving technologies. Is this indicative of an indifferent cosmos or more indicative of one which is somewhat friendly to self-teaching humans? It seems reasonable to conclude that the universe materialized due to the presence of an intelligent force/source when one takes the time to learn how so many things had to go right in order for us to be here and how difficult it would be to possibly explain how not one of those interdependent things went wrong without making reference to God pro or con (or beliefs). It also seems as though the universe were designed for people, considering how human beings are the phenomenal creatures with the rare capacity to \"be upwardly-mobile\" to maximize the somewhat hidden potential of creation.\n\n, Accept it instead of doubting it, see it instead of shutting your eyes and you will believe that God exists. Consider how the usefulness and attractiveness of things produced by human beings are not accidental qualities when present but are a result of our intelligent nature and our natural delight in order, balance, and beauty. In a similar way, it is unreasonable to conclude that the surpassing usefulness and attractiveness of things throughout nature are accidental qualities but it is rather more reasonable to conclude that they are the result of the existence of a surpassing intelligence which also delights in order, balance, and beauty. Creativity is at the bedrock and pinnacle of the existence of all living and non-living things and it is up to each person to attempt to explain his or her natural reverence for this illustrious state of affairs. The Bible says that God created everything and was quite pleased with his work.\n\n",
        "  This paper considers broadcast channels with L antennas at the base station\nand m single-antenna users, where each user has perfect channel knowledge and\nthe base station obtains channel information through a finite rate feedback.\nThe key observation of this paper is that the optimal number of on-users (users\nturned on), say s, is a function of signal-to-noise ratio (SNR) and other\nsystem parameters. Towards this observation, we use asymptotic analysis to\nguide the design of feedback and transmission strategies. As L, m and the\nfeedback rates approach infinity linearly, we derive the asymptotic optimal\nfeedback strategy and a realistic criterion to decide which users should be\nturned on. Define the corresponding asymptotic throughput per antenna as the\nspatial efficiency. It is a function of the number of on-users s, and\ntherefore, s should be appropriately chosen. Based on the above asymptotic\nresults, we also develop a scheme for a system with finite many antennas and\nusers. Compared with other works where s is presumed constant, our scheme\nachieves a significant gain by choosing the appropriate s. Furthermore, our\nanalysis and scheme is valid for heterogeneous systems where different users\nmay have different path loss coefficients and feedback rates.\n",
        "  The physics of manganites is often described within an effective two-band\ntight-binding (TB) model for the Mn e_g electrons, which apart from the kinetic\nenergy includes also a local \"Hund's rule\" coupling to the t_{2g} core spin and\na local coupling to the Jahn-Teller (JT) distortion of the oxygen octahedra. We\ntest the validity of this model by comparing the energy dispersion calculated\nfor the TB model with the full Kohn-Sham band-structure calculated within the\nlocal spin-density approximation (LSDA) to density functional theory. We\nanalyze the effect of magnetic order, JT distortions, and \"GdFeO_3-type\"\ntilt-rotations of the oxygen octahedra. We show that the hopping amplitudes are\nindependent of magnetic order and JT distortions, and that both effects can be\ndescribed with a consistent set of model parameters if hopping between both\nnearest and next-nearest neighbors is taken into account. We determine a full\nset of model parameters from the density functional theory calculations, and we\nshow that both JT distortions and Hund's rule coupling are required to obtain\nan insulating ground state within LSDA. Furthermore, our calculations show that\nthe \"GdFeO_3-type\" rotations of the oxygen octahedra lead to a substantial\nreduction of the hopping amplitudes but to no significant deviation from the\nsimple TB model.\n",
        "Oh!  Me!  I'm a research specialist starting up my PhD in motor control with a focus on handedness as we speak.  There are two competing theories.  One is simply \"evolutionary baggage\", where we got better and better with one arm and the other one just sort of tagged along and helped when it can.\n  The other line of thought is that handedness emerges due to lateralization of the brain.  As the brain got bigger, it would be innefficient to have something that needs to be as quick as motor control doing the same thing twice on both sides of the brain.  Instead, some people believe that each side does something else.  Robert Sainburg has a good decade of research on the particular hypothesis that the dominant arm is better at fast, powerful movements, while the non-dominant arm is better at goal monitoring and \"equilibrium point\" (where you wish to settle) tracking.  I don't necessarily agree with everything he says, but I do think he's very correct to assume handedness is better than just a better and worse copy of the same thing.\n\nmost up to date sainburg handedness theory (review of handedness in intro):\n_URL_0_",
        "La Tour-du-Meix is a commune in the Jura department in the region of Bourgogne-Franche-Comté in eastern France.\n\nPopulation\n\nSee also\nCommunes of the Jura department\n\nReferences\n INSEE statistics\n\nCommunes of Jura (department)",
        "  Context: Cool stars, companions to compact objects, are known to show Li\nabundances which are high compared to field stars of the same spectral type,\nwhich are heavily Li depleted. This may be due either to Li production or Li\npreservation in these systems. Aims: To measure the lithium isotopic ratio in\nthe companion star of the neutron star X-ray binary Cen X-4. Method: We use\nUVES spectra obtained in years 2000 and 2004 around the orbital quadratures.\nThe spectra are analysed with spectrum synthesis techniques and the errors\nestimated with Monte Carlo simulations. Results: We measure A(Li)=2.87+-0.20\nand 6Li/7Li = 0.12+0.08-0.05 at 68% confidence level. We also present updated\nsystem parameters with a refined determination of the orbital period and\ncomponent masses i.e. 1.14+-0.45 Msun and 0.23+-0.10 Msun for the neutron star\nand companion, respectively. Conclusions: In our view the low level of 6Li\nfavours Li preservation scenarios, although Li production mechanisms cannot be\nruled out. In the case of preservation, no Li is freshly created in the binary,\nbut the tidally-locked companion has preserved its original Li by some\nmechanism, possibly inhibited destruction due to its fast rotation.\n",
        "  Motivation: Similarity-measure based clustering is a crucial problem\nappearing throughout scientific data analysis. Recently, a powerful new\nalgorithm called Affinity Propagation (AP) based on message-passing techniques\nwas proposed by Frey and Dueck \\cite{Frey07}. In AP, each cluster is identified\nby a common exemplar all other data points of the same cluster refer to, and\nexemplars have to refer to themselves. Albeit its proved power, AP in its\npresent form suffers from a number of drawbacks. The hard constraint of having\nexactly one exemplar per cluster restricts AP to classes of regularly shaped\nclusters, and leads to suboptimal performance, {\\it e.g.}, in analyzing gene\nexpression data. Results: This limitation can be overcome by relaxing the AP\nhard constraints. A new parameter controls the importance of the constraints\ncompared to the aim of maximizing the overall similarity, and allows to\ninterpolate between the simple case where each data point selects its closest\nneighbor as an exemplar and the original AP. The resulting soft-constraint\naffinity propagation (SCAP) becomes more informative, accurate and leads to\nmore stable clustering. Even though a new {\\it a priori} free-parameter is\nintroduced, the overall dependence of the algorithm on external tuning is\nreduced, as robustness is increased and an optimal strategy for parameter\nselection emerges more naturally. SCAP is tested on biological benchmark data,\nincluding in particular microarray data related to various cancer types. We\nshow that the algorithm efficiently unveils the hierarchical cluster structure\npresent in the data sets. Further on, it allows to extract sparse gene\nexpression signatures for each cluster.\n",
        "  We introduce and test a new and highly efficient method for treating the\nthermal and radiative effects influencing the energy equation in SPH\nsimulations of star formation. The method uses the density, temperature and\ngravitational potential of each particle to estimate a mean optical depth,\nwhich then regulates the particle's heating and cooling. The method captures --\nat minimal computational cost -- the effects of (i) the rotational and\nvibrational degrees of freedom of H2, H2 dissociation, H0 ionisation, (ii)\nopacity changes due to ice mantle melting, sublimation of dust, molecular\nlines, H-, bound-free and free-free processes and electron scattering; (iv)\nexternal irradiation; and (v) thermal inertia. The new algorithm reproduces the\nresults of previous authors and/or known analytic solutions. The computational\ncost is comparable to a standard SPH simulation with a simple barotropic\nequation of state. The method is easy to implement, can be applied to both\nparticle- and grid-based codes, and handles optical depths 0<tau<10^{11}.\n",
        "I'm writing my thesis on a related issue right now! It comes down to the fact that humans were forced to build settlements by regional famines, and those didn't start until 14,000 years ago. The deal is that farming is really hard and completely sucks, relative to the life of a hunter-gatherer tribe in an area with plentiful food. However, as hunter-gatherers multiplied, areas eventually (after those hundreds of thousands of years) ran out of local food sources, and they turned to farming and stationary settlements as a last-ditch effort at carbohydrates. It then turned out that the organizational systems inherent in villages advantaged organized settlements over disorganized ones, and that eventually facilitated the rise of civilizations in general.\n\nEDIT:\n\nSource 1: _URL_1_  < - talks about the rise of villages and demographic transitions that the famine theory explains quite nicely\n\nSource 2: _URL_0_  < -this talks about violence (famine-related and otherwise) in ancient societies\n\nEDIT 2: Making my grammar gooder.",
        "Specifically from the military side: \n\nBy the time someone has risen in their career to the point where they're assigned at Groom (or some other \"black\" site), they've been thoroughly vetted, and are generally a 'known quantity' from a security perspective.\n\nMy stepfather was career Air Force, a pilot, and toward the end of his career was assigned to the 117 program (we later found out) at Groom. For several years toward the end of the 80's, he and my mother lived in Vegas, and he commuted every week from McCarran to Groom on 'Janet airlines'. All he could tell my mom was that he'd be home when he got home. There were a few deployments (probably training) where he was gone for several weeks, and all my mom had was an assurance from other wives that if something happened, someone would let her know.\n\nTo this day, he hasn't spoken a word of what he did there; we only figured it out based on stuff that happened afterward. Once the 117 was general knowledge, he deployed with one of the squadrons, and eventually assumed a position of squadron leadership when they moved to their final operational base.\n\nHe's been retired since the late 90's, and the 117 has been mostly declassified (not completely, though most of the technology is outdated and no longer used), but there are very few pieces of information that he's ever revealed or confirmed.\n\nFor instance, every pilot who qualified in the 117 was given a \"Bandit\" number indicating a permanent call-sign. Through research of my own, I discovered his Bandit number (listed in two publicly available books about the program I found at the Air Force museum); while the roster of Bandits has been publicly released, he still refuses to even confirm that that's his number. To this day he still won't answer any real questions about the program, even though the plane has been retired for almost a decade.",
        "Section 4 Continue:\nThe experiments in 4.5 are misleading and together with 4.4 they are contradictory to their theoretical results. \nForemost, the readers need to be aware that the decoders with switch units are very powerful (if you ever trained one you would know it too). Therefore what figure 4 presents is extremely misleading because it *only* takes the very last conv layer and use their proposed linear reconstruction algorithm to recover that layer’s input only.  The rest was propagated with the very powerful decoder plus switch units information (see [6] for how powerful the decoder is).  Visually, we can also see how much info switch units carry: for their conv 5 reconstruction with random activation, there is no meaningful info carried by the activation values but all through switch units and we can still obtain the silhouette of the objects in the original images; in fact, pool-1’s switch units alone carries at least 64x224x224 bit information. Their results back in the appendix (Figure5) also shows their reconstruction algorithm’s weakness. The lower layer has extremely bad reconstruction quality from the proposed algorithms. The higher the layer, the better the reconstruction from their proposed algorithms because decoder together switch units has more chance to correct the reconstruction.What the authors should do, is to use their IHT to reconstruct all the way, following what has been done in [1]’s linear reconstruction algorithm, which the authors here failed to compare against, despite its high relevance. In fact, the author of [1] used to be in charge of this submitted project and proposed this experimental approach, with a slightly different algorithm that is presented in [1]; but current authors of this submission failed in achieving similar reconstruction results with their algorithm and hence can only perform their reconstruction algorithm over a single layer. The biggest difference between the reconstruction algorithm from [1] and this submission is that the former used pseudo-inverse of the weight matrix and the latter used the transpose because in their theory, the weight matrix is approximate orthogonal hence its transpose is approximately its inverse.\n There is another serious issue with using high-up conv layer to verify their theoretical claim: recall that the excuse this submission used to discard ReLU non-linearity is that [1] introduced the pairing phenomenon, however, [1] stressed that it only appeared in the first few layers. Hence the authors should have at least chosen the first few layers for their experiments; yet, they knowingly only presented their analysis of the highest layer. \nTable 3 again presents very misleading results. First their “random activation” has a relative error 1.414 for activation space. But if we simply pick zero vectors instead of “random”, it gives 1 as a relative error, which is much smaller than 1.414. Therefore, this “relative error” in the activation space is a very inappropriate measurement for the reconstruction quality from their algorithm. But measuring the relative error in the input space is also a bad metric because of the powerful effects from the decoders. In fact, the term “relative error” is a paraphrase of the evaluation method used in [1], where it is called “reconstruction ratio”. However, it is a much more appropriate measurement coupling the reconstruction algorithm used in [1] because the reconstruction from [1] will always be a subset of the original input by using the pseudo-inverse.\n Recall that in 4.4, the authors approximate the conv weight matrix for conv(5,2) has approximately 0.05-0.1 distortion constant, which means that the “relative error” based on their Theorem 3.3 is 0.6-0.2, but in reality, the relative error is 1.051, which is not only worse than their theoretical guarantee but even worse than simply choosing zero vector as reconstruction. Keep in mind that these results are built on top of the assumption that ReLU does not exist, if we add it on top, these results in Table3 and Figure5 can get further downgraded. One question why there is such a huge gap. In fact, such bad reconstruction results based on their algorithm is not surprising because after all, unlike random weights, the learned conv weights are not approximately orthogonal so Model-RIP does not fit into realistic CNN models. In fact, the coherence is a very direct measurement of how “orthogonal” the learned weights are, which both this submission and [5] (again this submission took over [5]’s idea) measure this quantity: the higher the coherence the less orthogonal.  \n",
        " Seek a heartfelt \"anointing\".\n\n, Study and pray to seek The Holy Spirit's guidance: be enthused. Usually the basis of the idea should be backed up with Bible scripture. You never will begin to preach without a direction or purpose, if you follow the steps to get it organized.\n\n,\n\n\nA lesson or sermon is usually best, if it is spoken without memorizing it all and not even writing it all down in complete sentences, and then you cannot merely read it, but use a meaningful outline. Make your key words larger so that they stand out in your eye and in your mind. That can be like a map to follow. A lesson or sermon should be better when it is not like a speech or oration that a public speaker (such as a politician) might simply read to an audience, unless you are an extremely effective reader.\nEach sermon may be a whole new topic or one in a \"series\" of multiple sermons or lessons.\n\n,,\n\n\nKnow the outline and the plan so well that you don't need to look at it or your notes more than an occasional glance, or so that you only need the larger key word to make it click in your mind, but you can have them there, open and available.\n\n,, That three part process is given next.\n\n",
        "Maisonette or Maisonettes may refer to:\n\n Maisonette, a type of apartment\n The Maisonette, a former restaurant in Cincinnati, Ohio, United States\n The Maisonettes, an English band\n\nSee also \n Maison (disambiguation)\n Maisonnette (disambiguation)",
        "This paper presents a transition-based graph parser able to cope with the rich\nrepresentations of a semantico-cognitive annotation scheme, instantiated in the\nUCCA corpora. The authors start first by exposing what, according to them,\nshould cover a semantic-based annotation scheme: (i) being graph-based\n(possibility for a token/node of having multiple governors) (2) having\nnon-terminal nodes (representing complex structures â syntactic -: coordinate\nphrases, lexical: multiword expression) and (3) allowing discontinuous elements\n(eg. Verbs+particules). Interestingly, none of these principles is tied to a\nsemantic framework, they could also work for syntax or other representation\nlayers. The authors quickly position their work by first introducing the larger\ncontext of broad-coverage semantic parsing then their annotation scheme of\nchoice (UCCA).              They then present 3 sets of parsing experiments: (i) one\ndevoted to phrase-based parsing using the Stanford parser and an UCCA to\nconstituency conversion, (ii) one devoted to dependency parsing using an UCCA\nto dependency conversion and finally (iii) the core of their proposal, a  set\nof experiments showing that their transition-based graph parser is suitable for\ndirect parsing of UCCA graphs.\n\nI found this work interesting but before considering a publication, I have\nseveral concerns with regards to the methodology and the empirical\njustifications:\n\nThe authors claimed that there are the first to propose a parser for a\nsemantically-oriented scheme such as theirs. Of course, they are. But with all\ndue respect to the work behind this scheme, it is made of graphs with a various\nlevel of under-specified structural arguments and semantically oriented label\n(Process, state) and nothing in their transition sets treats the specificities\nof such a graph. Even the transitions related to the remote edges could have\nbeen handled by the other ones assuming a difference in the label set itself\n(like adding an affix for example). If we restrict the problem to graph\nparsing, many works post the 2014-2015 semeval shared tasks (Almeda and\nMartins, 2014,2015 ; Ribeyre et al, 2014-2015) proposed an extension to\ntransition-based graph parser or an adaptation of a higher-model one, and\nnothing precludes their use on this data set.  Itâs mostly the use of a\nspecific feature template that anchors this model to this scheme (even though\nitâs less influencial than the count features and the unigram one). Anyway,\nbecause the above-mentioned graph-parsers are available [1,2] I donât\nunderstand why they couldnât be used as a baseline or source of comparisons.\nRegarding the phrase-based  experiments using uparse, it could have been also\nvalidated by another parser from Fernandez-Gonzales and Martins (2015) which\ncan produce LCFRS-like parsing as good as Uparse (ref missing when you first\nintroduced uparse).  \n\nBecause this scheme supports a more abstract view of syntaxico-semantic\nstructures than most of the SDP treebanks, it would have been important to use\nthe same metrics as in the related shared task. At this point in the field,\nmany systems, models and data set are competing and I think that the lack of\ncomparison points with other models and parsers is detrimental to this work as\nwhole. Yet I found it interesting and because weâre at crossing time in term\nof where to go next, I think that this paper should be discussed at a\nconference such as ConLL.\n\nNote in random order\n-         please introduce the âgrounded semanticâ before page 2, you use\nthat phrase before\n-         why havenât you try to stick to constituent-tree with rich node\nlabels and propagater traces and then train/parse with the Berkeley parser? It\ncould have been a good baseline. \n-         The conversion to surface dependency trees is in my mind useless: you\nloose too many information, here a  richer conversion such as the one from\nâSchluter et al, 2014, Semeval SDP) should have been used.\n-         Can you expand on âUCCA graphs may contains implicit unit that have\nno correspondent in the textâ  or provide a ref or an example.\n-         You mentioned other representations such as MRS and DRT, this raises\nthe fact that your scheme doesnât seem to allow for a modelling of quantifier\nscope information. Itâs thus fully comparable to other more syntax-oriented\nscheme. Itâs indeed more abstract than DM for example and probably more\nunderspecified than the semantic level of the PCEDT but how much? How really\ninformative is this scheme and how really âparsableâ is it? According to\nyour scores, it seems âharderâ but an  error analysis would have been\nuseful.\n- As I said before, the 3 principles you devised could apply to a lot of\nthings,  they look a bit ad-hoc to me and would probably need to take place in\na much wider (and a bit clearer) introduction. What are you trying to argue\nfor: a parser that can parse UCCA? a model suitable for semantic analysis ? or\na semantic oriented scheme that can actually be parsable?  you're trying to say\nall of those in a very dense way and it's borderline to be be confusing.\n\n[1] http://www.corentinribeyre.fr/projects/view/DAGParser\n[2] https://github.com/andre-martins/TurboParser and\nhttps://github.com/andre-martins/TurboParser/tree/master/semeval2014_data",
        ">  Do they actually memorize the location of every piece? \n\nKind of. Ultimately, they have to memorize enough information to be able to recreate the cube state, but most blindfold methods have you memorize the sequences in which you solve pieces, and the method prescribes that the pieces are to be solved in a distinct way so that each piece you solve exactly sets up the next one appropriately.\n\nFor instance, here's the webpage introducing probably the best-known blindfold method: [the Pochmann method](_URL_0_). In this method, you basically start out by finding a piece you want to move into an appropriate location. You then move it there, careful to affect the rest of the cube as little as possible (of course, you must also move the piece that's currently sitting at the destination, and perhaps a couple more pieces somewhere else, but leave the rest of the cubies as is). The algorithm works in such a way that the piece that was at the destination is now where your other piece started! Therefore, you now repeat this process for the new piece. Repeat this (with minor caveats) for each edge and for each corner, and eventually you're done!\n\nSo what exactly needs to be memorized? Just the destinations of your pieces. A typical example, taken from the site, would be memorizing something like \n\n    FR - UL - FD - DB - DR - UF - LB - UB - DL - FL - BU - stop - UB - RD - UF - UR - DL - RB - FR - LD - BL\n\n(where F = front, B = back, U = up, D = down, L = left, R = right) or some color coded equivalent.\n\nThis is not hard to do using either the *story method* (come up with a story based on the text -- see the link for an example) or simply by lots of practice.",
        "The paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n-\tThe suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n-\tThe paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\n\nWeaknesses:\n1.\tIt is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2.\tExperimental evaluation\n2.1.\tIt is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2.\tIt would be interested if this approach generalizes to other datasets.\n\n\nOther (minor/discussion points)\n-\tThe task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n-\tI am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process.\n-\tPage 6, last paragraph: missing “.”: “… searching This…”\n\n\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n",
        "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). \nAdditionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.\nExperiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.\n\nI have the several comments on the paper:\n- I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. \n- The paper is missing some important references in conditional computation (e.g. ",
        "The paper proposes a new way of transferring knowledge.\nI like the idea of transferring attention maps instead of activations.\nHowever, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section.\nI would consider updating the score if the authors extend the last section 4.2.2.",
        " Consider the color, manufacturer, and options of your car. Some of these might be irrelevant to you, but some might be important. Think about what matters to you in a car and what you’ll be doing with your car before deciding which to buy.\n\n\nCertain features might depend on your intentions for the car. If you intend to haul lumber in your new vehicle, for instance, you should choose a sturdy pickup truck, not a small compact car.\nIf you have several kids you plan on shipping to soccer practice, school, and extracurricular activities, you might want a minivan.\nDecide what parts of your specification are optional or negotiable. If you would prefer slate blue but will consider cobalt or want but don't require a sunroof, write that down, but don't disclose it right away. You can negotiate on these optional points later, if it makes sense to do so.;\n, If you plan on selling your current car, you should include that money in the calculations when setting your budget. Be realistic when setting your budget. Don’t buy a car that is not affordable for you.\n\n\nYou could also think about getting financing for your car. Investigate what kinds of loans you qualify for with your bank or credit union.\nDon’t forget to calculate the costs associated with taxes, licensing, and (potentially) interest.\n\n, Consider cars that have high standards in safety and overall quality in addition to cars that meet the list of specifications you previously drafted. Check both user reviews and professional reviews written by automobile experts.\n\n\nConsumer Reports, Kelly Blue Book, and Auto Trader are useful sources for researching and comparing cars.\n\n, Typically, the final price of a fleet sales car is between $250 and $1,000 above the invoice price (what the dealership paid for the car).The fleet sales manager may talk about the price of the car you’re interested with from this reference point. For instance, if the fleet sales manager tells you a car is “$400 over,” that means it is $400 over the invoice price. A car that was invoiced for $20,000, then, would cost $2,400.\n\n\nEach year, most dealerships get anywhere between $200,000 and several million dollars in volume bonuses (rebates given to dealerships when they move a certain number of vehicles). Therefore, their invoice price is not usually a true representation of the cost of the vehicle to the dealership.\n\n, Knowing the dealer's invoice price and MSRP are the most important things when buying through fleet sales. Most dealerships will not hesitate to give you this information, if you ask. If the price is within your range, proceed with the purchase process. If it is not, find a car that better meets your financial specifications.\n\n\nMany online tools will also help you figure out what a reasonable price for a given car is. For instance, the database at http://www.edmunds.com/tmv.html offers both MSRP and actual prices paid on a variety of vehicles.\n\n, It can be a little tricky, however, because retail salesman may attempt to divert your shopping to their own department and might even pretend to be fleet salesmen. For this reason, it’s best to identify who the fleet manager of the dealership is before calling the dealership.Ask the operator or receptionist for the name of the fleet manager. If you are asked why you wish to know the fleet manager's name, say that you're doing research for your company's upcoming car purchase.\nBe as insistent as you need to be. Once you get the name, ask to speak to the manager and tell him/her that you are interested in buying a car soon. Give a specific time frame (for instance, three days) to show that you're serious.\nCall during regular weekday business hours. Fleet managers and salesmen deal with businesses, which usually operate on that schedule.If the manager asks which business you're affiliated with, give the name of the company you work for.Don't lie, however, and say the car is for business use when it's not. Fleet managers are often allowed to sell to private buyers, and if you handle the process professionally, the manager should have no problem doing business with you.\n\n, Again, be sure it gets to the fleet department by using specific names, phone numbers, and e-mail addresses of fleet department managers or salespeople.\n\n\nNote on the request for bids that you would like their best price and that you are asking other dealers. This should help shorten the process.\nIf you are willing to wait for the car you want to be ordered or transferred from another dealer, state as much on your specification sheet.\n\n, After the various fleet managers get back to you with a bid, make an appointment to meet the ones who have what you’re looking for. If all you get is an invitation to come to the dealership without any kind of bid attached, ignore it; they probably don't have what you want and are hoping to convince you to get something else.\n\n\nIf you receive a bid that doesn't match exactly with your specifications, circle or underline any discrepancies so you can ask about them later.\nIf you're not seeing bids that you like, consider expanding your dealership radius, or shopping for a different car.\n\n, Ask to see the car you're considering.Inspect it carefully. Look for dings in the windows, scratches on the surface, flat or low tires, and other imperfections. Pop the trunk and hood and look for anything that could be out of place. Read the window sticker to ensure the model has all the qualities you’re looking for. Finally, test drive the car.\n\n\nEven if you've test driven that exact model already, sometimes individual cars have manufacturing variations, and it's worth taking a quick spin to make sure everything's in good shape.\nDuring the test drive, listen for odd sounds coming from the engine, inoperable features like power windows, and other problems.\nRemember, you are about to spend a lot of money and a lot of time in this car, so you should be sure that the car has everything you’re looking for.\n\n, Many fleet departments waive these fees without you asking them to do so, but look over the final sale contract to ensure that they do. Ask that manufacturer rebates be factored into the final sales price as well.\n\n\nIf the fleet sales discounts are incompatible with regular retail sales discounts, ask that the more favorable set of sales discounts be applied to your purchase. For instance, if fleet sales offer cars at the invoice price plus $500, but the dealership is running a sale on that particular car that is lower than the fleet sales price, ask the dealership to sell you the car at the lower retail price.\n\n,\n\n\nNext, take your two lowest offers and call the dealership with the higher of the two. Make a firm offer to buy the car at a price that's slightly lower (maybe $200) than the lowest bid; if they say yes, ask them to fax you that bid immediately and make an appointment to fill out the paperwork and buy the car. If they say no, call the dealer with the lowest bid and make an appointment to fill out the paperwork and buy the car.\nStarting this process a week before the end of the month will get you better bids because that's when most dealerships are eager to move inventory.\nBuy under favorable conditions. For instance, you could choose to buy a summer car (a convertible, for instance) during winter. Sometimes a dealership will drop their selling price a few hundred dollars to move a stale car off their lot.\n\n, Find a dealership that offers exactly what you're looking for.Your area, as far as car dealerships are concerned, should be pretty big. If you can save $500 or $1000 or more by going to another dealership, or get something much closer to what you want, it can be well worth a one-time drive to a dealer a few towns away.\n\n\nWait for the various dealerships to respond with a quotation and compare the offers. Read each offer carefully.\n\n, Another unique thing about dealing with fleet sales is that a fleet manager or salesperson can do everything -- sell the car and arrange financing -- without the cryptic consultations between the retail and financing departments that you'd experience otherwise.If you haven't arranged your own financing, the fleet sales department can help you. However, be prepared to make a cash purchase, since many fleet sales departments expect you will.\n\n, The fleet number will enable you to qualify for deeper discounts than you would as a private purchaser. When you get a fleet number from the automaker, you can use it in future purchases to smooth the transaction.\n\n\nContact the auto manufacturer you’re interested in purchasing from in order to apply for a fleet number.\nIf you’re just buying a private car for yourself through fleet sales, you don’t need (and cannot obtain) a fleet number.\n\n",
        "James Cullingham Ph.D (born March 5, 1954) is a documentary filmmaker, historian and journalist with Tamarack Productions based in Nogojiwanong – Peterborough. His documentaries concerning social justice, history and popular culture have been screened around the world. Cullingham was an executive producer with CBC Radio and has been published by Canada’s leading newspapers and magazines.\n\nHis most recent documentary film Jim Galloway – A Journey in Jazz concerns the saxophonist and impresario Galloway who co-created the Toronto Jazz Festival.\n\nCullingham is now producing a film about refugee journalists in Canada.\n\nJames received his doctorate in Canadian and Latin American History from Toronto’s York University in 2014. He was a coordinator of the Journalism programme and professor of Journalism and English at and Liberal Studies at Seneca@York 2002-2018. He is an Adjunct Graduate Faculty Member in Canadian Studies and Indigenous Studies and the PhD program in Canadian Studies at Trent University. Cullingham is also an Instructor at Trent’s Chanie Wenjack School for Indigenous Studies.\n\nIn 1989 Cullingham formed Tamarack Productions to produce Canada's first national documentary series on Aboriginal issues featuring the work of Indigenous and non Indigenous filmmakers. Since then Cullingham has made documentaries in Canada, the United States, Europe, Africa, the Middle East and Pakistan. His films on history, politics, popular culture and social justice have been screened around the world. Cullingham has been published by Canada's leading newspapers and magazines.\n\nCullingham has a doctorate in History specializing in Canada and Latin America from York University in Toronto.\n\nCullingham's next film project is to write a scenario for a proposed documentary The Cost of Freedom - Refugee Journalists in Canada.\n\nHe is finalizing Two Dead White Men - Duncan Campbell Scott, Jacques Soustelle and the Failure of Indigenous Policy, a manuscript based on his doctoral dissertation.\n\nIn addition to English, Cullingham speaks French fluently and has a working knowledge of Spanish.\n\nJames Cullingham is a member of the Documentary Organization of Canada. He is a past National Board Member of the Canadian Association of Journalists.\n\nTamarack Productions \nTamarack Productions is a media production company that Cullingham launched in 1989, releasing its first productions in 1991, As Long as the Rivers Flow,[1] a 5-part documentary series on Aboriginal rights in Canada which has been broadcast and distributed globally in English and French (Tant que coulent les rivières.) Since that time, Tamarack has made films in several countries on themes addressing history, politics, popular culture and social justice.\nMost recently, Tamarack Productions released Jim Galloway - A Journey in Jazz in 2018, a documentary that chronicles the extraordinary career of the Scots Canadian saxophonist, journalist and impresario Galloway (1936-2014.) Cullingham directed and produced the film which had its World Premiere at the 2018 Toronto Jazz Festival and its UK premiere at the 2019 Glasgow Film Festival. In 2102, Tamarack released In Search of Blind Joe Death: The Saga of John Fahey in 2012, a documentary that follows the life and legacy of American guitarist, composer, writer and iconoclast, John Fahey. The documentary was directed, produced and executive produced by Cullingham, and has been screened around the world with much praise.\n\nSelected filmography \n\nJim Galloway - A Journey in Jazz Released: June 2018 Director, producer, writer, co-executive producer\nThe Pass System (2015), executive producer\nIn Search of Blind Joe Death: The Saga of John Fahey\nReleased: Oct. 2012\nDirector, Producer, Executive producer \nDishonour Defied\nReleased: 2007\nDirected by: Azara Rashid\nExecutive Producer, Producer\nLessons In Fear  \nReleased: 2005\nDirector, Producer\nWe Have Such Things At Home\nReleased: October 1997\nDirector, Producer\nDuncan Campbell Scott: The Poet and The Indians\nReleased: 1994\nDirector, Producer\n\nSelected radio productions \n\nEducation on the West Bank \nCBC Radio One \nJune 2005\n\nLessons in Loathing \nCBC Radio One\nApril 2004\n\nNisga'a and the BC Election \nCBC Radio National Network \nMay 1996\n\nThe Comeback of Howie Morenz \nCBC Radio/National Syndication/ Satire Series \nWritten by: Roy McGregor \nPerformed by: Booth Savage\n\nConfessions of a Dead Head \nPrime Time, CBC Radio\nJune 1992\n\nA Forgotten Frontier, Aboriginal Rights in B.C.\nCBC Radio\nJanuary 1986\n\nThe Ghost of Busher Jackson \nCBC Radio\nMarch 1986\n\nBroadcast - journalism career \nCullingham’s career in Journalism began in 1983 as a producer, documentarian and line-up editor for Sunday Morning at CBC Radio. He was promoted  in 1985 to Desk Producer, and was moved to Producer of Morningside for CBC Radio in 1986. That same year, he became Senior Producer of As It Happens (CBC Radio). In 1987, Cullingham moved to Executive Producer of As It Happens until 1989, when he returned to Sunday Morning (CBC Radio), becoming an Executive Producer until 1990.\n\nIn 1989, Cullingham created Tamarack Productions, producing its first project, As Long As The Rivers Flow, in 1991. The television series focuses on Aboriginal rights in Canada, in both modern and historical times, and was broken up into five one-hour episodes - Flooding in Job’s Garden, The Learning Path, Starting Fire with Gunpowder, Tikinagan and Time Immemorial. The series brought together notable directors in this subject area in Canada including Hugh Brody, Gil Cardinal, Boyce Richardson and Loretta Todd. Peter Raymont of White Pine Pictures was series Executive Producer.\n\nCullingham returned to CBC in 1997 to work as a Producer for Canada: A People’s History. He stayed there until 2000, and moved to VisionTV Insight as a Supervising Producer and Story Editor.\n\nIn 2002, Cullingham began his career as a Broadcast-Journalism professor at Seneca College, Seneca@York in Toronto. He was the Journalism program coordinator from 2004 to 2011. Cullingham currently teaches documentary courses in the Journalism program, along with History in English and Liberal Studies program.\n\nCullingham has done guest lectures and screenings  at a number of universities including Concordia University, Queen's University, the University of Western Ontario, University of Toronto, York University, l’École normale supérieure and EFAP Images et médias in Paris. He conducts documentary master classes at EFAP annually.\n\nPrint-journalism \nCullingham contributes frequently to publications including the Toronto Star, The Globe and Mail and the Journal of Wildlife Culture. He began writing in 1984 for various publications such as Aboriginal Voices, Maclean’s, MOJO, NOW Magazine, Ontario Indian, Pollution Probe, Saturday Night and Bulletin of Latin American Studies.\n\nHis publications include articles concerning Aboriginal rights in Canada, Canadian Politics, sports and a variety of others.\n\nEducation \nCullingham was born and raised in Toronto, Ontario. As a youth he also resided in Florida and Switzerland where he completed his high school studies. \n 2014 - received History PhD from York University with his dissertation: \"Scars of Empire: A Juxtaposition of Duncan Campbell Scott and Jacques Soustelle.\" \n 2008 - doctoral research at the L'Université de Paris Sorbonne \n 2005 - received Master of Arts degree in History at the University of Toronto, began PhD in History at York University\n 1980 - graduated Trent University in Peterborough, Ontario, with an Honours B.A. in Native Studies and French\n\nSelected awards \n Worldfest- Houston International Film Festival - Silver Remi Winner for In Search of Blind Joe Death: The Saga of John Fahey - Houston, Texas, 2013. \n International Wildlife Film Festival -  Top prize for Film Dealing with Aboriginal Peoples; Awards of Merit for Balanced Presentation of a Controversial Subject and Script - Missoula, Montana, 1995. \n American Indian Film Festival - Producers Award, Tamarack Productions - San Francisco, 1991. \n Nyon International Documentary Film Festival (Switzerland) - Director: Gill Cardinal - The People's Jury Award - Tikinagan, 1991.\n Two Rivers Native Film and Video Festival - \"New Visionary\" Awards to directors Gil Cardinal, David Poisey and Loretta Todd for their films in As Long As The Rivers Flow - Minneapolis, 1991.\n\nReferences\n\nExternal links\n \n\n1954 births\nCanadian documentary filmmakers\nLiving people\nSeneca College\nCanadian radio producers\nCanadian Broadcasting Corporation people\nCanadian documentary film producers\nPeople from Toronto\nUniversity of Toronto alumni\nJournalism teachers",
        "  In Volborthite, spin-1/2 moments form a distorted Kagom\\'e lattice, of corner\nsharing isosceles triangles with exchange constants $J$ on two bonds and $J'$\non the third bond. We study the properties of such spin systems, and show that\ndespite the distortion, the lattice retains a great deal of frustration.\nAlthough sub-extensive, the classical ground state degeneracy remains very\nlarge, growing exponentially with the system perimeter. We consider degeneracy\nlifting by thermal and quantum fluctuations. To linear (spin wave) order, the\ndegeneracy is found to stay intact. Two complementary approaches are therefore\nintroduced, appropriate to low and high temperatures, which point to the same\nordered pattern. In the low temperature limit, an effective chirality\nHamiltonian is derived from non-linear spin waves which predicts a transition\non increasing $J'/J$, from $\\sqrt 3\\times \\sqrt 3$ type order to a new\nferrimagnetic {\\em striped chirality} order with a doubled unit cell. This is\nconfirmed by a large-N approximation on the O($n$) model on this lattice. While\nthe saddle point solution produces a line degeneracy, $O(1/n)$ corrections\nselect the non-trivial wavevector of the striped chirality state. The quantum\nlimit of spin 1/2 on this lattice is studied via exact small system\ndiagonalization and compare well with experimental results at intermediate\ntemperatures. We suggest that the very low temperature spin frozen state seen\nin NMR experiments may be related to the disconnected nature of classical\nground states on this lattice, which leads to a prediction for NMR line shapes.\n",
        "The GNU Compiler Collection (GCC) is an optimizing compiler produced by the GNU Project supporting various programming languages, hardware architectures and operating systems. The Free Software Foundation (FSF) distributes GCC as free software under the GNU General Public License (GNU GPL). GCC is a key component of the GNU toolchain and the standard compiler for most projects related to GNU and the Linux kernel. With roughly 15 million lines of code in 2019, GCC is one of the biggest free programs in existence. It has played an important role in the growth of free software, as both a tool and an example.\n\nWhen it was first released in 1987 by Richard Stallman, GCC 1.0 was named the GNU C Compiler since it only handled the C programming language. It was extended to compile C++ in December of that year. Front ends were later developed for Objective-C, Objective-C++, Fortran, Ada, D and Go, among others. The OpenMP and OpenACC specifications are also supported in the C and C++ compilers.\n\nGCC has been ported to more platforms and instruction set architectures than any other compiler, and is widely deployed as a tool in the development of both free and proprietary software. GCC is also available for many embedded systems, including ARM-based and Power ISA-based chips.\n\nAs well as being the official compiler of the GNU operating system, GCC has been adopted as the standard compiler by many other modern Unix-like computer operating systems, including most Linux distributions. Most BSD family operating systems also switched to GCC shortly after its release, although since then, FreeBSD, OpenBSD and Apple macOS have moved to the Clang compiler, largely due to licensing reasons. GCC can also compile code for Windows, Android, iOS, Solaris, HP-UX, AIX and DOS.\n\nHistory \nIn late 1983, in an effort to bootstrap the GNU operating system, Richard Stallman asked Andrew S. Tanenbaum, the author of the Amsterdam Compiler Kit (also known as the Free University Compiler Kit) for permission to use that software for GNU. When Tanenbaum advised him that the compiler was not free, and that only the university was free, Stallman decided to work on a different compiler. His initial plan was to rewrite an existing compiler from Lawrence Livermore National Laboratory from Pastel to C with some help from Len Tower and others. Stallman wrote a new C front end for the Livermore compiler, but then realized that it required megabytes of stack space, an impossibility on a 68000 Unix system with only 64 KB, and concluded he would have to write a new compiler from scratch. None of the Pastel compiler code ended up in GCC, though Stallman did use the C front end he had written.\n\nGCC was first released March 22, 1987, available by FTP from MIT. Stallman was listed as the author but cited others for their contributions, including Jack Davidson and Christopher Fraser for the idea of using RTL as an intermediate language, Paul Rubin for writing most of the preprocessor, and Leonard Tower for \"parts of the parser, RTL generator, RTL definitions, and of the Vax machine description.\" Described as the \"first free software hit\" by Peter H. Salus, the GNU compiler arrived just at the time when Sun Microsystems was unbundling its development tools from its operating system, selling them separately at a higher combined price than the previous bundle, which led many of Sun's users to buy or download GCC instead of the vendor's tools. While Stallman considered GNU Emacs as his main project, by 1990, GCC supported thirteen computer architectures, was outperforming several vendor compilers, and was used commercially by several companies.\n\nEGCS fork \nAs GCC was licensed under the GPL, programmers wanting to work in other directions—particularly those writing interfaces for languages other than C—were free to develop their own fork of the compiler, provided they meet the GPL's terms, including its requirements to distribute source code. Multiple forks proved inefficient and unwieldy, however, and the difficulty in getting work accepted by the official GCC project was greatly frustrating for many, as the project favored stability over new features. The FSF kept such close control on what was added to the official version of GCC 2.x (developed since 1992) that GCC was used as one example of the \"cathedral\" development model in Eric S. Raymond's essay The Cathedral and the Bazaar.\n\nIn 1997, a group of developers formed the Experimental/Enhanced GNU Compiler System (EGCS) to merge several experimental forks into a single project. The basis of the merger was a development snapshot of GCC (taken around the 2.7.2 and later followed up to 2.8.1 release). Mergers included g77 (Fortran), PGCC (P5 Pentium-optimized GCC), many C++ improvements, and many new architectures and operating system variants.\n\nWhile both projects followed each other's changes closely, EGCS development proved considerably more vigorous, so much so that the FSF officially halted development on their GCC 2.x compiler, blessed EGCS as the official version of GCC, and appointed the EGCS project as the GCC maintainers in April 1999. With the release of GCC 2.95 in July 1999 the two projects were once again united. GCC has since been maintained by a varied group of programmers from around the world under the direction of a steering committee.\n\nGCC 3 (2002) removed a front-end for CHILL due to a lack of maintenance.\n\nBefore version 4.0 the Fortran front end was g77, which only supported FORTRAN 77, but later was dropped in favor of the new GNU Fortran front end that supports Fortran 95 and large parts of Fortran 2003 and Fortran 2008 as well.\n\nAs of version 4.8, GCC is implemented in C++.\n\nSupport for Cilk Plus existed from GCC 5 to GCC 7.\n\nGCC has been ported to a wide variety of instruction set architectures, and is widely deployed as a tool in the development of both free and proprietary software. GCC is also available for many embedded systems, including Symbian (called gcce), ARM-based, and Power ISA-based chips. The compiler can target a wide variety of platforms, including video game consoles such as the PlayStation 2, Cell SPE of PlayStation 3, and Dreamcast. It has been ported to more kinds of processors and operating systems than any other compiler.\n\nSupported languages \n, the recent 11.1 release of GCC includes front ends for C (gcc), C++ (g++), Objective-C, Fortran (gfortran), Ada (GNAT), Go (gccgo) and D (gdc, since 9.1) programming languages, with the OpenMP and OpenACC parallel language extensions being supported since GCC 5.1. Versions prior to GCC 7 also supported Java (gcj), allowing compilation of Java to native machine code.\n\nRegarding language version support for C++ and C, since GCC 11.1 the default target is gnu++17, a superset of C++17, and gnu11, a superset of C11, with strict standard support also available. GCC also provides experimental support for C++20 and upcoming C++23.\n\nThird-party front ends exist for many languages, such as Pascal (gpc), Modula-2, Modula-3, and VHDL (GHDL). A few experimental branches exist to support additional languages, such as the GCC UPC compiler for Unified Parallel C or Rust.\n\nDesign \n\nGCC's external interface follows Unix conventions. Users invoke a language-specific driver program (gcc for C, g++ for C++, etc.), which interprets command arguments, calls the actual compiler, runs the assembler on the output, and then optionally runs the linker to produce a complete executable binary.\n\nEach of the language compilers is a separate program that reads source code and outputs machine code. All have a common internal structure. A per-language front end parses the source code in that language and produces an abstract syntax tree (\"tree\" for short).\n\nThese are, if necessary, converted to the middle end's input representation, called GENERIC form; the middle end then gradually transforms the program towards its final form. Compiler optimizations and static code analysis techniques (such as FORTIFY_SOURCE, a compiler directive that attempts to discover some buffer overflows) are applied to the code. These work on multiple representations, mostly the architecture-independent GIMPLE representation and the architecture-dependent RTL representation. Finally, machine code is produced using architecture-specific pattern matching originally based on an algorithm of Jack Davidson and Chris Fraser.\n\nGCC was written primarily in C except for parts of the Ada front end. The distribution includes the standard libraries for Ada and C++ whose code is mostly written in those languages. On some platforms, the distribution also includes a low-level runtime library, libgcc, written in a combination of machine-independent C and processor-specific machine code, designed primarily to handle arithmetic operations that the target processor cannot perform directly.\n\nGCC uses many additional tools in its build, many of which are installed by default by many Unix and Linux distributions (but which, normally, aren't present in Windows installations), including Perl, Flex, Bison, and other common tools. In addition, it currently requires three additional libraries to be present in order to build: GMP, MPC, and MPFR.\n\nIn May 2010, the GCC steering committee decided to allow use of a C++ compiler to compile GCC. The compiler was intended to be written mostly in C plus a subset of features from C++. In particular, this was decided so that GCC's developers could use the destructors and generics features of C++.\n\nIn August 2012, the GCC steering committee announced that GCC now uses C++ as its implementation language. This means that to build GCC from sources, a C++ compiler is required that understands ISO/IEC C++03 standard.\n\nOn May 18, 2020, GCC moved away from ISO/IEC C++03 standard to ISO/IEC C++11 standard (i.e. needed to compile, bootstrap, the compiler itself; by default it however compiles later versions of C++).\n\nFront ends \n\nEach front end uses a parser to produce the abstract syntax tree of a given source file. Due to the syntax tree abstraction, source files of any of the different supported languages can be processed by the same back end. GCC started out using LALR parsers generated with Bison, but gradually switched to hand-written recursive-descent parsers for C++ in 2004, and for C and Objective-C in 2006. As of 2021 all front ends use hand-written recursive-descent parsers.\n\nUntil GCC 4.0 the tree representation of the program was not fully independent of the processor being targeted. The meaning of a tree was somewhat different for different language front ends, and front ends could provide their own tree codes. This was simplified with the introduction of GENERIC and GIMPLE, two new forms of language-independent trees that were introduced with the advent of GCC 4.0. GENERIC is more complex, based on the GCC 3.x Java front end's intermediate representation. GIMPLE is a simplified GENERIC, in which various constructs are lowered to multiple GIMPLE instructions. The C, C++, and Java front ends produce GENERIC directly in the front end. Other front ends instead have different intermediate representations after parsing and convert these to GENERIC.\n\nIn either case, the so-called \"gimplifier\" then converts this more complex form into the simpler SSA-based GIMPLE form that is the common language for a large number of powerful language- and architecture-independent global (function scope) optimizations.\n\nGENERIC and GIMPLE \nGENERIC is an intermediate representation language used as a \"middle end\" while compiling source code into executable binaries. A subset, called GIMPLE, is targeted by all the front ends of GCC.\n\nThe middle stage of GCC does all of the code analysis and optimization, working independently of both the compiled language and the target architecture, starting from the GENERIC representation and expanding it to register transfer language (RTL). The GENERIC representation contains only the subset of the imperative programming constructs optimized by the middle end.\n\nIn transforming the source code to GIMPLE, complex expressions are split into a three-address code using temporary variables. This representation was inspired by the SIMPLE representation proposed in the McCAT compiler by Laurie J. Hendren for simplifying the analysis and optimization of imperative programs.\n\nOptimization \nOptimization can occur during any phase of compilation; however, the bulk of optimizations are performed after the syntax and semantic analysis of the front end and before the code generation of the back end; thus a common, even though somewhat contradictory, name for this part of the compiler is the \"middle end.\"\n\nThe exact set of GCC optimizations varies from release to release as it develops, but includes the standard algorithms, such as loop optimization, jump threading, common subexpression elimination, instruction scheduling, and so forth. The RTL optimizations are of less importance with the addition of global SSA-based optimizations on GIMPLE trees, as RTL optimizations have a much more limited scope, and have less high-level information.\n\nSome of these optimizations performed at this level include dead code elimination, partial redundancy elimination, global value numbering, sparse conditional constant propagation, and scalar replacement of aggregates. Array dependence based optimizations such as automatic vectorization and automatic parallelization are also performed. Profile-guided optimization is also possible.\n\nBack end \n\nThe GCC's back end is partly specified by preprocessor macros and functions specific to a target architecture, for instance to define its endianness, word size, and calling conventions. The front part of the back end uses these to help decide RTL generation, so although GCC's RTL is nominally processor-independent, the initial sequence of abstract instructions is already adapted to the target. At any moment, the actual RTL instructions forming the program representation have to comply with the machine description of the target architecture.\n\nThe machine description file contains RTL patterns, along with operand constraints, and code snippets to output the final assembly. The constraints indicate that a particular RTL pattern might only apply (for example) to certain hardware registers, or (for example) allow immediate operand offsets of only a limited size (e.g. 12, 16, 24, ... bit offsets, etc.). During RTL generation, the constraints for the given target architecture are checked. In order to issue a given snippet of RTL, it must match one (or more) of the RTL patterns in the machine description file, and satisfy the constraints for that pattern; otherwise, it would be impossible to convert the final RTL into machine code.\n\nTowards the end of compilation, valid RTL is reduced to a strict form in which each instruction refers to real machine registers and a pattern from the target's machine description file. Forming strict RTL is a complicated task; an important step is register allocation, where real hardware registers are chosen to replace the initially assigned pseudo-registers. This is followed by a \"reloading\" phase; any pseudo-registers that were not assigned a real hardware register are 'spilled' to the stack, and RTL to perform this spilling is generated. Likewise, offsets that are too large to fit into an actual instruction must be broken up and replaced by RTL sequences that will obey the offset constraints.\n\nIn the final phase, the machine code is built by calling a small snippet of code, associated with each pattern, to generate the real instructions from the target's instruction set, using the final registers, offsets, and addresses chosen during the reload phase. The assembly-generation snippet may be just a string, in which case a simple string substitution of the registers, offsets, and/or addresses into the string is performed. The assembly-generation snippet may also be a short block of C code, performing some additional work, but ultimately returning a string containing the valid assembly code.\n\nC++ Standard Library (libstdc++) \nThe GCC project includes an implementation of the C++ Standard Library called libstdc++, licensed under the GPLv3 License with an exception to link closed source application when sources are built with GCC.\nThe current version is 11.\n\nOther features \nSome features of GCC include:\n\n Link-time optimization\n Link-time optimization optimizes across object file boundaries to directly improve the linked binary. Link-time optimization relies on an intermediate file containing the serialization of some Gimple representation included in the object file. The file is generated alongside the object file during source compilation. Each source compilation generates a separate object file and link-time helper file. When the object files are linked, the compiler is executed again and uses the helper files to optimize code across the separately compiled object files.\n Plugins\n Plugins extend the GCC compiler directly. Plugins allow a stock compiler to be tailored to specific needs by external code loaded as plugins. For example, plugins can add, replace, or even remove middle-end passes operating on Gimple representations. Several GCC plugins have already been published, notably:\n The Python plugin, which links against libpython, and allows one to invoke arbitrary Python scripts from inside the compiler. The aim is to allow GCC plugins to be written in Python.\n The MELT plugin provides a high-level Lisp-like language to extend GCC.\n The support of plugins was once a contentious issue in 2007.\n C++ transactional memory\n The C++ language has an active proposal for transactional memory. It can be enabled in GCC 6 and newer when compiling with -fgnu-tm.\n Unicode identifiers\n Although the C++ language requires support for non-ASCII Unicode characters in identifiers, the feature has only been supported since GCC 10. As with the existing handling of string literals, the source file is assumed to be encoded in UTF-8. The feature is optional in C, but has been made available too since this change.\n C extensions\n GNU C extends the C programming language with several non-standard-features, including nested functions and typeof expressions.\n\nArchitectures \n\nGCC target processor families as of version 11.1 include:\n\n AArch64\n Alpha\n ARM\n AVR\n Blackfin\n eBPF\n Epiphany (GCC 4.8)\n H8/300\n HC12\n IA-32 (x86)\n IA-64 (Intel Itanium)\n MIPS\n Motorola 68000\n MSP430\n Nvidia GPU\n Nvidia PTX\n PA-RISC\n PDP-11\n PowerPC\n R8C / M16C / M32C\n RISC-V\n SPARC\n SuperH\n System/390 / zSeries\n VAX\n x86-64\n\nLesser-known target processors supported in the standard release have included:\n\n 68HC11\n A29K\n C6x\n CR16\n D30V\n DSP16xx\n ETRAX CRIS\n FR-30\n FR-V\n IBM ROMP\n Intel i960\n IP2000\n M32R\n MCORE\n MIL-STD-1750A\n MMIX\n MN10200\n MN10300\n Motorola 88000\n NS32K\n RL78\n Stormy16\n V850\n Xtensa\n\nAdditional processors have been supported by GCC versions maintained separately from the FSF version:\n\n Cortus APS3\n ARC\n AVR32\n C166 and C167\n D10V\n EISC\n eSi-RISC\n Hexagon\n LatticeMico32\n LatticeMico8\n MeP\n MicroBlaze\n Motorola 6809\n MRISC32\n MSP430\n NEC SX architecture\n Nios II and Nios\n OpenRISC\n PDP-10\n PIC24/dsPIC\n PIC32\n Propeller\n Saturn (HP48XGCC)\n System/370\n TIGCC (m68k variant)\n TMS9900\n TriCore\n Z8000\n ZPU\n\nThe GCJ Java compiler can target either a native machine language architecture or the Java virtual machine's Java bytecode. When retargeting GCC to a new platform, bootstrapping is often used.  Motorola 68000, Zilog Z80, and other processors are also targeted in the GCC versions developed for various Texas Instruments, Hewlett Packard, Sharp, and Casio programmable graphing calculators.\n\nLicense \nGCC is licensed under the GNU General Public License version 3. The GCC runtime exception permits compilation of proprietary programs (in addition to free software) with GCC. This does not impact the license terms of GCC source code.\n\nSee also \n\n List of compilers\n MinGW\n LLVM/Clang\n\nReferences\n\nFurther reading \n Using the GNU Compiler Collection (GCC), Free Software Foundation, 2008.\n GNU Compiler Collection (GCC) Internals, Free Software Foundation, 2008.\n An Introduction to GCC, Network Theory Ltd., 2004 (Revised August 2005). .\n Arthur Griffith, GCC: The Complete Reference. McGrawHill / Osborne, 2002. .\n\nExternal links\n\nOfficial \n \n GCC Release Timeline\n GCC Development Plan\n\nOther \n Collection of GCC 4.0.2 architecture and internals documents at I.I.T. Bombay\n \n \n From Source to Binary: The Inner Workings of GCC, by Diego Novillo, Red Hat Magazine, December 2004\n A 2003 paper on GENERIC and GIMPLE\n Marketing Cygnus Support, an essay covering GCC development for the 1990s, with 30 monthly reports for in the \"Inside Cygnus Engineering\" section near the end\n EGCS 1.0 announcement\n EGCS 1.0 features list\n Fear of Forking, an essay by Rick Moen recording seven well-known forks, including the GCC/EGCS one\n\n1987 software\nC (programming language) compilers\nC++ compilers\nCompilers\nCross-platform free software\nFortran compilers\nFree compilers and interpreters\nCompiler Collection\nJava development tools\nPascal (programming language) compilers\nSoftware that was rewritten in C++\nFree software programmed in C++\nSoftware using the GPL license\nUnix programming tools",
        "Sounds are usually described by three characteristics - pitch, loudness, and quality (or \"timbre\"). Pitch is determined by frequency and describes how high or low a note is. Loudness is what it sounds like (forgive the pun - I simply mean loudness is self-explanatory). Everything else is generally categorized as \"timbre,\" which is what allows you to tell the difference between a piano and violin playing a note at the same pitch and loudness.\n\nQuite a bit goes into timbre such as harmonic content, attack and decay, and vibrato. I don't really know enough about those to explain them further other than to say they are characteristics of sound waves that affect how you hear, but don't change the pitch or loudness. If you can find an app that lets you play around with a synthesizer, you can usually change those settings directly so you can play around with how they affect sound.\n\nEdit: There are some more technical responses below from people who know more about this than I do. Check them out for more details. And thanks to everyone who provided more details!\n\nEdit 2: Most of the comments below are saying that attack (the beginning of a sound and how long it takes to get to peak volume/loudness) and slight variations in the pitch and loudness are the physical properties that create different timbres, though the area is still being researched. Apparently when you play a note on an instrument the part of the instrument producing the note actually produces multiple frequencies and can have slight variations in loudness. The frequencies not associated with the pitch you hear are called overtones. You generally only perceive the fundamental frequency (the pitch of the note), but the overtones change how you perceive that. To bring it back to your original question, you can tell the difference between a piano and violin playing middle C because they have different attack, overtones, and the loudness during the note will change slightly. And you might not be able to tell the difference if you didn't hear the attack (the beginning of the note). \n\nAs an added bonus, if I understand correctly, the different overtones result in a different spectral flux and the changing loudness over the duration of the note results in a different spectral envelope.",
        " Pack the essentials: undergarments, shoes, a set or two of regular clothes, entertainment, medication and, for longer flights, basic toiletries. Some people fly as if they may never see their luggage again – and that has some merit. Keep in your carry-on the minimum of what you need to survive should you lose your luggage.\n\n\nMake sure to take all your medication and everything you need to be comfortable. Prescription and non-prescription medication are allowed. It’s easier to get extra liquids through security if they’re medically needed, like with saline solution.\n\n\n\n\n\n\nIn order to cut down on the amount of clothes to pack, choose items that are interchangeable. Stick to a few items that all go together, rather than completely separate outfits. Use accessories to spice up an outfit. For example, scarves are small and easy to pack, and can be used as a scarf, headband, or even a belt.\nTake your swimsuit if you are travelling by air, put it with in your vacation gear, especially if you are a woman. If your bags are lost when travelling by air, most items (such as shorts or T-shirts) can usually be purchased at your destination. However, if your bags are lost, swimsuits for women can be hard to shop for. If you do not have your swimsuit you may miss out on the beach, hot tub, or other vacation fun.\n\n\n\n\n\n;\n, Anything valuable should come with you in your carry-on. On the off-chance your luggage gets lost or damaged, your carry on should not leave your possession. If you'd be heartbroken if you lost it, take it in your carry-on if you take it at all.\n\nPack large electronics last, so they are easily accessible. You will not need to go digging around when time is of the essence.\n\n,\nThe TSA requires electronics to be screened – when they are all in the same place and easy for the agents to see, you won’t be the one holding up the line at security.\n\n, In order to get on the plane, you need identification, such as a passport or driver’s license. Do not forget your ATM card and credit card or AAA card. However, it is probably a good idea NOT to take every piece of plastic you own because you run the risk of losing the cards.\n\n\nIn an easily accessible pocket of your carry-on luggage, store your flight information: the airline, the flight number, your confirmation code, and the flight details. This comes in handy at the self-service check-in kiosks that so many airlines provide now at the airport.\n\n, You may not need to pack much, if any. Your Aunt Maria probably has shampoo, for instance, and Peru will likely have toothpaste. It may take an extra stop at a store on your travels, but by avoiding tons of bottles, lotions, and tubes, you save space for other, more important things.\n\n\nIf you do bring toiletries, in the United States the 3-1-1 TSA regulations still applies. You can fill as many 3 oz bottles of toiletries (100 ml) as you want into ‘’one’’ quart-size plastic ziplock bag (limit one per flyer), but you have to take out the bag at security screening.Go to www.tsa.gov for the full rules and regulations.\n\n, Sometimes flights can cause headaches, so have a pack ready just in case this happens to be that one. A few things you may want to pack:\n\n\nPainkillers\nBandages\nA sedative (if you are a nervous traveler)\nAnti-nausea medication\nChewing gum (for air pressure changes)\nTissues\nEarplugs (good for travel in general)\nMedication for anything you are prone to, such as allergies.\n\n, Remember you are not charged for the clothes you wear traveling, so dress with that in mind. Dress in layers so you can bring more with you. Instead of a T-shirt and jacket, wear a T-shirt under a long sleeved top under a sweatshirt, for example. Wear your hiking shoes and pack your flip-flops, especially when you are travelling for business., You can manage, which err n traveling by air, for a three month work trip, without checking any bags if you really want to. Checking luggage, for some, is a pain in the rear. You have to worry about packing it, dragging it with you, meeting weight requirements, likely paying extra fees that you did not know about, and then hoping the airlines do not lose it. If you are traveling for less than two weeks, consider it. It may be a challenge, but it is doable.\n\n\nFlight attendants and crew do it all the time. They can go over a week with just a carry on.If they can do it, so can you. You can then use the extra $50, if applicable, for whatever you please.\n\n, In addition to meeting the weight requirements, it’s just easier to pack lighter – fewer things may get lost (via flying or when you leave them in your hotel room), it is a lighter bag to tote around, and you'll have plenty of room for souvenirs and impulse buys. And it'll take less time to repack.\n\n\nThough you should hold off on bringing too many shoes, you have to bring some. Shoe should be packed in plastic bags to avoid soiling your others goods unless they are brand new. Also, consider packing socks in your shoes rather than waste space.\n\n, Just in case something were to happen to your carry on, you forget to pack your carry-on correctly, or something unfortunate happens on your trip, put copies of important documents in your checked luggage. Scan your passport, visa, and anything else that you may need in the worst of circumstances. If you do it, you won’t need it. But if you don’t, you may.\n\n, If you are bringing toiletries with you, it is likely something will leak. Each item should be wrapped separately and stored in bags to make sure none of it gets on your clothes. Keep these in a separate area in your bag, too.\n\n\nTake the lid off of each bottle and plastic wrap the top; then put the lid back on. This means that even if the lid opens, you should still be fine., If you are not already rolling your clothes, get on the bandwagon. It prevents awkward square-shaped wrinkles and it saves room, so hop to it. Start with the heavier ones on bottom as lighter ones are generally more moldable to the shape of the top of your bag.\n\n\nThe tighter the roll, the more room you save. Even a little more compression here and there goes a long way.\n\n, Some airports are courteous enough to provide plastic bags for you, but if yours is not one of those, take them yourself. They are always useful, especially if you are traveling in a group – someone always forgets. And this way if your first round bags get soiled, you have a backup.\n\n\nThe zipper kind – the kind that literally has a zipper on it. The resealable ones are better than the non-sealable kind, but the zipper kind are best – the resealable kind can open when force is applied.\nHigh-quality zip-lock bags can also be used to pack your bag tighter. You can sometimes get to 1/3 more room if your clothing is put in zip-lock bags, the air forced out, then sealed. It can also protect clothing from getting soaked in outdoor adventures and keep your dirty underwear away from your clean clothes.\n\n, To get the most out of your bag, you’ve gotta pack it according to shape and size of your items. Start with the biggest, heaviest items on bottom and work your way up to the light items – this will make it easier to close your bag when all is said and done. If something is an odd shape, pack clothes around it – make it a point to never pack air.\n\n\nIn general, it's easier to back long, cylindrical items than odd shaped bottles and containers. In the future, to streamline your packing look for items that are of more basic shapes and sizes. They take up less room overall.\n\n, If you are planning to shop at fashionable Parisian boutiques on your travels, do not stuff your suitcase full of ordinary clothes. Leave room for your purchases in your bags., In some cases, it may be easier to ship your items by mail or by a service like FedEx or UPS. This may be very important if you are going on an extended trip or need special equipment, like winter camping gear., Your trip destination will determine the kinds of things to pack, and the length will determine how much of each item is to be packed. What days do you have special events planned? How can you use the same pieces over and over?\n\n\nIf you can, try to avoid needing a checked bag. More and more airlines are charging for that first checked bag, and a cheap flight can turn into an expensive one in a matter of no time. If flight attendants can live out of a carry-on for over a week at a time, so can you.\n\n, Checking before packing can help pinpoint what you really need. For example, Vermont typically has mild summers, but also has \"heat waves\" that can make it semi-tropical. Checking the weather will let you know if you really need to pack that tank top or that umbrella.\n\n\nTake a small amount of multipurpose items to deal with the climate of your vacation destination. For instance, one waterproof windbreaker takes less space than a raincoat and a jacket.\n\n, If you’re going to a different country or overseas, odds are certain things will be different. Will you need an electronics adapter?\n\n, You may not be able to bring your Saudi host a bottle of wine, for example. Or take certain kinds of plant seeds to Australia.",
        "Bruce Almighty is a 2003 American fantasy comedy film directed by Tom Shadyac and written by Steve Koren, Mark O'Keefe and Steve Oedekerk. The film stars Jim Carrey as Bruce Nolan, a down-on-his-luck television reporter who complains to God (played by Morgan Freeman) that he is not doing his job correctly and is offered the chance to try being God himself for one week. The film is Shadyac and Carrey's third collaboration, as they had worked together previously on Ace Ventura: Pet Detective in 1994 and Liar Liar in 1997. It co-stars Jennifer Aniston, Philip Baker Hall and Steve Carell.\n\nWhen released in American theaters on May 23, 2003, Bruce Almighty opened to mixed reviews from critics, but was a box-office success and grossed $85.9 million, making it the top Memorial Day opening weekend of any film in history at the time. The film surprised film pundits when it beat The Matrix Reloaded the following weekend. By the end of its theatrical run, the film had made $242 million domestically and a total $484 million worldwide, making it Carrey's highest-grossing film worldwide, as well as the fifth-highest-grossing film of 2003.\n\nEvan Almighty, a spin-off sequel focusing on Steve Carell's character, with Shadyac and Oedekerk returning to direct and write, and Freeman also reprising his role, was released on June 22, 2007.\n\nPlot\n\nBruce Nolan is a television field reporter for Eyewitness News on WKBW-TV in Buffalo, New York, but desires to be the news anchorman. When Bruce is passed over for promotion by his rival, Evan Baxter, he becomes furious and vents on live T.V.; his actions lead to him being fired from the station. Following a series of misfortunes, Bruce takes his anger out on God and complains that \"He's the one that should be fired\".\n\nBruce receives a message on his pager, which takes him to an empty warehouse where he meets God. God offers to give Bruce His powers to prove that He is doing the job correctly. God only gives Bruce two rules: he cannot tell others he has God's powers, nor can he use the powers to alter free will. Bruce is initially jubilant with the powers, using them for personal gain, such as exposing a woman's panties, getting his job back, and impressing his girlfriend, Grace Connelly. Bruce finds ways of using his powers around Buffalo to cause miraculous events to occur at otherwise mundane events that he covers, such as discovering Jimmy Hoffa's body or causing a meteor to harmlessly land near a cook-off, earning him the name \"Mr. Exclusive\". Bruce then causes Evan to embarrass himself on-air, causing Evan to be fired in favor of Bruce as the new anchor. During this, Bruce continues to hear voices in his head. He later re-encounters God, who explains that the voices are prayers, meant for God, that Bruce must deal with. Bruce creates a computerized email-like system to receive the prayers and respond but finds that the influx is far too many for him to handle—even though God has stated that Bruce is only receiving prayers from a section of the Buffalo area—and sets the program to answer every prayer Yes automatically.\n\nBruce attends a party celebrating his promotion. When Grace arrives, she finds Bruce kissing his co-anchor, Susan Ortega, after she forcefully comes on to him, and quickly leaves. Bruce follows her, trying to use his powers to convince her to stay but cannot influence her free will. As Bruce looks around, he realizes that Buffalo has fallen into chaos due to his actions: parts of the city believe the Apocalypse is nearly upon Earth due to the meteor strikes, while a large number of people, all having prayed to win the multi-million dollar lottery and finding they all won reducing their prize to a few dollars, have started rioting in the streets. Bruce returns to God, who explains that He cannot solve all the problems and Bruce must figure out a way himself. Bruce then goes about helping people without using his powers, including giving Evan his job back. As Bruce reads through the prayers on his computer, he finds several from Grace, wishing for his success and well-being. As Bruce reads it, another prayer from Grace arrives, this one wishing to not be in love with him anymore.\n\nBruce is stunned and walks alone on a highway. Heartbroken, he asks God to take back His powers and letting his fate be in His hands. Bruce is suddenly hit by a truck and regains his consciousness in a white void. God appears, and He asks Bruce what he really wants; Bruce admits that he only wants to make sure Grace finds a man that would make her happy. God agrees and brings Bruce back to life. Bruce wakes up and finds himself in the hospital. Grace arrives shortly after. She and Bruce rekindle their relationship, and later become engaged. Following his recovery, Bruce returns to field reporting, and continues to report offbeat news stories.\n\nCast\n\n Jim Carrey as Bruce Nolan\n Morgan Freeman as God\n Jennifer Aniston as Grace Connelly, Bruce's girlfriend\n Catherine Bell as Susan Ortega, Bruce's co-anchor\n Steve Carell as Evan Baxter (credited as Steven Carell), Bruce's rival\n Philip Baker Hall as Jack Baylor\n Lisa Ann Walter as Debbie Connelly, Grace's sister\n Paul Satterfield as Dallas Coleman\n Nora Dunn as Ally Loman\n Eddie Jemison as Bobby\n Sally Kirkland as Anita Mann\n Micah Stephen Williams as boy on bike\n Tony Bennett as himself\n Carlos Sánchez as Juan Valdez\n John Murphy as himself\n Madeline Lovejoy as Zoe\n Noel Gugliemi as Hector\n\nProduction\n\nFilming of Buffalo was done in the \"New York Street\" at Universal Studios Hollywood. The restaurant with Tony Bennett was filmed at Cicada, in the James Oviatt Building, downtown Los Angeles. The spa scene with Jennifer Aniston was filmed in the Shoin building at The Japanese Garden in Los Angeles.\n\nReception\nOn Rotten Tomatoes, the film has a score of 48% based on 193 reviews, with an average rating of 5.70/10. The site's critical consensus reads, \"Carrey is hilarious in the slapstick scenes, but Bruce Almighty gets bogged down in treacle.\" On Metacritic, it has a score of 46 out of 100, based on 35 critics, indicating \"mixed or average reviews.\"\n\nRoger Ebert of the Chicago Sun-Times gave the film three out of four stars, calling it: \"A charmer, the kind of movie where Bruce learns that while he may not ever make a very good God, the experience may indeed make him a better television newsman.\" Ebert praised Aniston's performance: \"Aniston, as a sweet kindergarten teacher and fiancee, shows again (after \"The Good Girl\") that she really will have a movie career.\"\n\nVarietys Robert Koehler gave the film a mixed review: \"There's remarkably little done with a premise snatched from high-concept heaven, adding yet another file to the growing cabinet of under-realized comedies.\"\n\nThe film was released in the United Kingdom on June 27, 2003, and topped the country's box office that weekend.\n\nThe Los Angeles Times gave it a negative review and called it \"not so mighty.\"\n\nControversies\nThe film was banned in Egypt because of its portrayal of God as an ordinary man. Bans in both Malaysia and Egypt were eventually lifted after the nations' censorship boards gave the film their highest rating (18-PL in the case of Malaysia).\n\nAs God contacts Bruce using an actual phone number rather than one in the standard fictional 555 telephone exchange, several people and groups sharing this number received hundreds of phone calls from people wanting to talk to God, including a church in North Carolina, US (where the minister was named Bruce), a pastor in northern Wisconsin and a man running a sandwich shop in Manchester, England. The producers noted that the number (776-2323) was not in use in the area code (716, which was never specified on screen) in the film's story, but did not check anywhere else. For the home-video and television versions of the film, the number was changed to the fictional 555–0123.\n\nSequel\nA sequel and spin-off, titled Evan Almighty, was released on June 22, 2007, with Steve Carell reprising his role as Evan Baxter and Morgan Freeman returning to his role as God. Although Shadyac returned to direct the sequel, neither Carrey nor Aniston was involved with the film, and Carrey's character, Bruce, is never mentioned in the film. The film was a critical and commercial failure.\n\nAccolades\n\nSoundtrack\n\nThe soundtrack was released on June 3, 2003, by Varèse Sarabande. Tracks 8-13 are from the score composed by John Debney, performed by the Hollywood Studio Symphony (conducted by Pete Anthony) with Brad Dechter and Sandy De Crescent.\nTrack listing\n \"One of Us\" - Joan Osborne\n \"God Shaped Hole\" - Plumb\n \"You're a God\" - Vertical Horizon\n \"The Power\" - Snap!\n \"A Little Less Conversation\" - Elvis vs. JXL\n \"The Rockafeller Skank\" - Fatboy Slim\n \"God Gave Me Everything\" - Mick Jagger featuring Lenny Kravitz\n \"AB Positive\"\n \"Walking on Water\"\n \"Seventh at Seven\"\n \"Bruce Meets God\"\n \"Bruce's Prayer\"\n \"Grace's Prayer\"\n\nAdaptations\n Arai En 305-il Kadavul\n God Tussi Great Ho\n Evan Almighty, a follow-up film starring Steve Carell instead of Carrey\n The Story of God with Morgan Freeman, a spin-off documentary television series starring Morgan Freeman\n\nReferences\n\nExternal links\n\n \n \n \n \n \n \n\n2003 films\n2003 comedy films\n2003 fantasy films\n2000s fantasy-comedy films\nAmerican films\nAmerican fantasy-comedy films\nEnglish-language films\nFilms about Christianity\nFilms about God\nFilms about television\nFilms about wish fulfillment\nFilms directed by Tom Shadyac\nFilms scored by John Debney\nFilms set in Buffalo, New York\nFilms shot in Los Angeles\nFilms with screenplays by Steve Oedekerk\nReligious comedy films\nSpyglass Entertainment films\nUniversal Pictures films\nCensored films\nReligious controversies in film",
        "The 5.45 mm Kalashnikov Avtomat and Handheld Machinegun manual (РУКОВОДСТВО ПО 5.45-мм АВТОМАТУ КАЛАШНИКОВА (АК74, АКС74, АК74Н, АКС74Н) И 5.45-мм РУЧНОМУ ПУЛЕМЕТУ КАЛАШНИКОВА (РПК74, РПКС74, РПК74Н, РПКС74Н) says:\n\nIn the observation of the battlefield section:\n\n >  When indicating a target with tracer bullets, fire one or two short bursts in its direction.\n\nTracers are loaded as every fourth round. So in order to perform fire correction effectively, the shooter must fire in bursts. As an aside on tracers: it is not recommended to use them in clear weather (you can't see them) or use only tracers (they wear out the barrel). \n\nIn \"Firing on stationary or disappearing targets\", the manual says:\n\n > A singular, clearly visible target is fired upon in short or long bursts depending on the importance of the target, its size, and the range. The longer the range, the longer the burst. Fire until the target is destroyed or it disappears.\n...\nA disappearing target must be destroyed with bursts, where one burst quickly follows another.\n...\nA group target, consisting of several clearly visible figures, must be fired upon in bursts, moving fire from one figure to the next.\n...\nWhen the enemy attacks at a range of 200 m or closer, fire in long bursts, spreading fire along the front line.\nThe spreading of fire is achieved by shifting your assault rifle (machinegun) along the horizon. The speed of the shift is dictated by the range and required density of fire. In any case, the density of fire must be at least two bullets per each meter of the target.\n\nIn \"Firing on moving targets\"\n\n > When firing using the method of waiting for the target (fire assault), the rifleman (machinegunner) aims at a point selected in front of the target, and as soon as the target is within 1.5-2 points in the table *[a table of offsets is attached to this page]* fires a long burst. If the target is not hit, he selects a new point in front of the target at the correct offset, etc.\n\nThe same technique works when shooting at low flying aircraft and parachutists, except the table of offsets is different.\n\nI think you get the idea by now, so I'll summarize the rest. The tendency is to fire in short bursts when you have a good view of the target and a good idea of where it will be, long bursts when the target is poorly visible, moving, etc. The exception is that when you're running, you fire in short bursts, but you fire in long bursts when riding a truck. \n\nA helicopter is a special case. When firing from a helicopter, you fire in very long bursts (10-15 rounds) and use tracers more extensively.",
        "We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:\n\n-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.\n\n-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.\n\n-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space. \n",
        "Life After 30 is a studio album by the American singer Freddie Jackson. It was released by Orpheus Music on September 14, 1999. Jackson's only album with the label, it peaked at number 81 on the US Top R&B/Hip-Hop Albums.\n\nTrack listing\n\nCharts\n\nReferences\n\n1999 albums\nFreddie Jackson albums",
        "IIRC, the penis is more or less split open and stuffed inside a man-made orifice. The skin holding the testicles is used to create a faux labia and the head and shaft of the penis is used to make the clitoris and vaginal opening. The opening has to be dilated every day* to prevent it from closing in on itself and healing. I can't say how familiar it is to a natural vagina because I've never had a penis. Many mtfs are able to orgasm after surgery.\n\nFor female to males there are a couple of options. One is enlarging the clitoris as much as you can with hormones and then surgically pulling it out, resulting in a small but somewhat functioning and sexually pleasing penis. The other is to take skin from other parts of the body and surgically implant it on to the groin, resulting in a less satisfying but more aesthetically pleasing penis. Obviously with the latter the patient is not able to produce an erection on his own; generally there is an air pump located somewhere close by. There are also operations where fake testicles can be implanted, and I think that these can sometimes be used as air pumps to get an erection going.\n\nMEDICINE CRAY\n\n\nedit: my first gold and my first obligatory gold edit i'm gonna poop thanks guise :'^)",
        " This is towards the very end of the chapter.;\n, You must open this gate to progress further in the level so you can't miss it.\n\n, They will be above ground height and in shade. The left outlet's bars will be bent at the bottom with enough room to enter through.\n\n, It must be close enough to the cliff to jump on and climb through, although the cliff has built in stair-like bumps for you to hop onto.\n\n, It may help to turn on your flashlight to gather your surroundings, but do not waste too much charge as you will need it soon as the sewer is filled with radioactive slime.\n\n, The toxic slime will sap your health, so be quick. It is possible to walk through the sewer if you have a full armour rating and health, but it is easier to sprint (to sprint hold down the Left Bumper on your Xbox 360 controller).\n\n, Although it is hard to see where the gap is in the dim light of the sewer, failing to make the turn will cost valuable health if you're stuck at the end of the sewer.\n\n, Congratulations! You have just found the hidden singing vortigaunt cave! He will be standing around a head crab roasting on a fire, singing to himself and saying philosophical thoughts. If you are on the Xbox 360, simply being there is all that's needed to unlock the Vorticough achievement.\n\n, It is represented by the vortigaunt with musical notes coming from it.\n\n, You will have to sprint back the way you came again through the radioactive slime, so make sure you have enough health and suit power to sprint through.\n\n",
        " Visit http://www.amazon.com from any web browser.;\n, At the upper-right corner of the page are some quick access links. Hover over the “Hello. Sign in Your Account” menu, and click on the yellow “Sign in” button that will appear. You will be directed to the Sign In page.\n\n, Enter your Amazon account username and password on the corresponding fields, and click on the yellow “Sign in using our secure server” button to proceed. You will be signed in and brought back to Amazon’s main page.\n\n, From the home page, click on the “Today’s Deals” link found at the upper left part of the page, right near the Amazon logo. This will bring you to the current deals page that displays all currently running deals and promotions on Amazon.\n\n\nCheck the Gold Box and Lightening Deals on the first page. These are extremely limited offers that are usually valid between 1 and 24 hours from the time they are posted. If you click on an “Add to Cart” box, you will not need to type in a promotional code to receive the discount as long as you purchase the item within the specified deadline.\n\n, From the page’s submenu, click on the “Coupons” link. This will bring you to the Amazon Coupons page.\n\n\nThis page lists all the coupons, properly categorized, available on Amazon.\nYou can also reach this page directly by visiting http://www.amazon.com/Coupons/.\nCoupons are products that include Amazon special offers, such as a percent discount or a buy one, get one free offer.\n\n, Browse through the page and see all the available coupons. There are coupons for grocery items, electronics, books, and a lot more.\n\n, To know more about a coupon, click on it. You will be brought to a page that lists all applicable items on Amazon on which this coupon can be applied to.\n\n\nIf you want to avail of the coupon you’ve just selected, just buy any of the items listed by clicking on the “Add to Cart” button.\n\n, Click “Clip this Coupon” to add the code to your cart. Make sure to add the number of items needed to validate the coupon.\n\n, Make sure the discount is reflected in the order page before you pay. If it does not show, you may not have fulfilled the requirements for the promotion.\n\n\nMake sure to checkout soon after adding the coupon. You may want to do some comparison shopping with a search engine to ensure the coupon is an exceptional deal.\nThe clicked button will be changed to a text reading “Coupon Clipped” and will appear under the “Your Recently Clipped Coupons” section of the page.\n\n, Search these sites at the beginning of the month, since new promotional codes are often posted at the beginning of the month.\n\n,,, You may see an expiration date and a success rate next to the coupon.\n\n, Most sites will bring you directly to Amazon, because they receive an advertising fee for sales that originate at their site.\n\n\nThe advantage of navigating straight to the site is that you may not need to enter the promotional code at checkout; however, you can look for deals with codes you can write down and navigate to Amazon using a different browser window, if you choose.\n\n, Add the items to your cart.\n\n, Enter the promotional code underneath the order totals in your shopping cart. Click “Apply” to activate the code.\n\n, Most promotional codes will not allow you to ship to more than 1 address.\n\n,, Once there, you can vote on whether or not the promotional code worked for you.\n\n, Click on the link that says “Shop by Department.”\n\n,, Look for a banner that says “Get 20 percent off your next purchase” or a similar promotion.\n\n, Click the button and sign in to your account.\n\n, Look for the promotional code to click on or enter in order to receive a discount on your next purchase.\n\n, Use these offers according to their deadline and terms.\n\n, Complete your shopping by adding items to your cart. Click on the “Add to Cart” buttons of all the items you’ll be purchasing. The coupons will only be applied on all applicable items on your cart once you checkout.\n\n, Before proceeding with your purchase, read the promotion details tagged with every coupon. Just hover on the coupon and click on the “Terms and Conditions” link; a window will pop up with the relevant promotion details.\n\n, All the items you’re about to purchase are placed in your shopping cart. View your cart by accessing its page.\n\n\nClick on the “Cart” button from the upper-right corner of the page. Your Shopping Cart will be displayed with all the items you’ve just added. Items with applicable coupons are properly identified with a small coupon box right underneath the item price. At this point, the coupons you’ve clipped haven’t been applied yet.\n\n, Finalize your purchase; click on the “Proceed to checkout” button found at the right side of the page. Complete the rest of the checkout process.\n\n\nAll applicable discounts carried by the clipped coupons will now be applied. Your final total should reflect the discounted prices.\n\n",
        "Reviewers are currently in discussion. Please post a rebuttal to any comments or questions in their reviews.\n\nThanks!",
        "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.",
        "  We present a comprehensive mass reconstruction of the rich galaxy cluster Cl\n0024+17 at z~0.4 from ACS data, unifying both strong- and weak-lensing\nconstraints. The weak-lensing signal from a dense distribution of background\ngalaxies (~120 per square arcmin) across the cluster enables the derivation of\na high-resolution parameter-free mass map. The strongly-lensed objects tightly\nconstrain the mass structure of the cluster inner region on an absolute scale,\nbreaking the mass-sheet degeneracy. The mass reconstruction of Cl 0024+17\nobtained in such a way is remarkable. It reveals a ringlike dark matter\nsubstructure at r~75\" surrounding a soft, dense core at r~50\". We interpret\nthis peculiar sub-structure as the result of a high-speed line-of-sight\ncollision of two massive clusters 1-2 Gyr ago. Such an event is also indicated\nby the cluster velocity distribution. Our numerical simulation with purely\ncollisionless particles demonstrates that such density ripples can arise by\nradially expanding, decelerating particles that originally comprised the\npre-collision cores. Cl 0024+17 can be likened to the bullet cluster 1E0657-56,\nbut viewed $along$ the collision axis at a much later epoch. In addition, we\nshow that the long-standing mass discrepancy for Cl 0024+17 between X-ray and\nlensing can be resolved by treating the cluster X-ray emission as coming from a\nsuperposition of two X-ray systems. The cluster's unusual X-ray surface\nbrightness profile that requires a two isothermal sphere description supports\nthis hypothesis.\n",
        "- Strengths:\n I find the idea of using morphological compositionality to make decisions on\nsegmentation quite fruitful.\n\nMotivation is quite clear\n\nThe paper is well-structured\n\n- Weaknesses:\nSeveral points are still unclear: \n  -- how the cases of rule ambiguity are treated (see \"null->er\" examples in\ngeneral discussion)\n  -- inference stage seems to be suboptimal\n  -- the approach is limited to known words only\n\n- General Discussion:\nThe paper presents semantic-aware method for morphological segmentation. The\nmethod considers sets of simple morphological composition rules, mostly\nappearing as 'stem plus suffix or prefix'. The approach seems to be quite\nplausible and the motivation behind is clear and well-argumented.\n\nThe method utilizes the idea of vector difference to evaluate semantic\nconfidence score for a proposed transformational rule. It's been previously\nshown by various studies that morpho-syntactic relations are captured quite\nwell by doing word analogies/vector differences. But, on the other hand, it has\nalso been shown that in case of derivational morphology (which has much less\nregularity than inflectional) the performance substantially drops (see\nGladkova, 2016; Vylomova, 2016). \n\n The search space in the inference stage although being tractable, still seems\nto be far from optimized (to get a rule matching \"sky->skies\" the system first\nneeds to searhc though the whole R_add set and, probably, quite huge set of\nother possible substitutions) and limited to known words only (for which we can\nthere exist rules). \n\n It is not clear how the rules for the transformations which are\northographically the same, but semantically completely different are treated.\nFor instance, consider \"-er\" suffix. On one hand, if used with verbs, it\ntransforms them into agentive nouns, such as \"play->player\". On the other hand,\nit could also be used with adjectives for producing comparative form, for\ninstance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\".\nMore over, as mentioned before, there is quite a lot of irregularity in\nderivational morphology. The same suffix might play various roles. For\ninstance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are\nthey merged into a single rule/cluster? \n\n No exploration of how the similarity threshold and measure may affect the\nperformance is presented.",
        " Check out your area, find things that motivate you, look for things to do constantly.;\n, Use photo manipulation to emphasize the parts that contain the base emotion of the picture. Like loneliness in an abandoned house. Creativity is a great way to enjoy yourself.\n\n, Having a good conversation with friends is always great fun. You don't need money for that.\n\n, Nature is full of change and energy, and in many cases, it's inexpensive or free. Take friends and your camera for the best experience. Go swimming and fishing. Climb trees. Make a swamp bed, or a fort. After observing restrictions, make a campfire and roast marshmallows or the fish you caught over the fire. If you have a tent or even just sleeping bags, camp out (or sleep in the car). If you have access to a water source, purify your own water. Go foraging or hunting, if you know how (this is not the time to experiment) or bring canned and dried food with you.\n\n,, You really don't need a lot of money for those.\n\n, And if you live together, you could make double the money, so paying the bills will be much easier.\n\n, You can make decorations for any occasions, or even seasons. Making presents by hand will make others happy too.\n\n, This will save money. Or just get off the grid, you'll help to save your environment this way too.\n\n, Countries with bad economies often don't utilize environmentally friendly ideas. Recycle your garbage, get together with your neighbors and clean your immediate area. Doing something good is always fun!\n\n, Money isn't what people are looking for anyway. Everyone is looking for ways to enjoy their life. Some have bigger perspectives others don't. Having bigger goals in life often require having more money, but it depends on you to achieve your goal not your countries economy.\n\n, Countries are not physical boundaries, they should not contain you. We all are at home here on the whole planet.\n\n, Spending money can be enjoyable, but there are so many things you can do without having a lot. You can always try to work your way out of a bad economy anyway, make enough money to move to another country and start a life there. It's never too late to change.\n\n, For many people around the world who struggle with a bad economy every day, song and dance provide a way to come together and have fun without spending anything. If you don't have the means to play music or instruments, you can make your own! Learn to sing and beatbox. Any flat surface can become a drum. You can even make a rubber band guitar from a cardboard box! If you're not musically inclined, now is a great time to learn. Get some family and friends together, and start making music. Sing and dance along. Learn some new songs and dance moves together. Get everyone involved.\n\n, What did people do before they had electricity, running water, and even houses? How did they live? What did they eat? How did they spend their days? People are remarkably adaptive. Research your local history and ask: What were people doing 50 years ago? 100 years ago? 1000 years ago? Try to replicate some aspects of their lifestyle--odds are, it won't cost you a dime, and you'll gain valuable insights into people of the past. Share these interests with your family and friends; kids are especially curious about low-tech, do-it-yourself projects.\n\n\nhunting and gathering (e.g. ricing, yucca)\ngrowing your own food\nmaking clothes & textiles\nmaking shelter\nfood processing (canning, cider making, wine making)\n\n, During hard times is when many people explore their spirituality; some people renew it, some people refine it, and some people change their views altogether. Whatever is your journey, take the time to dwell on your purpose in life, and be thankful for what you have. Pray, meditate, or do whatever it takes to remind yourself that you're lucky to be alive. Live in the moment.\n\n",
        "  The Dicke model describes an ensemble of N identical two-level atoms (qubits)\ncoupled to a single mode of a bosonic field. The fermion Dicke model should be\nobtained by changing the atomic pseudo-spin operators by a linear combination\nof Fermi operators. The generalized fermion Dicke model is defined introducing\ndifferent coupling constants between the single mode of the bosonic field and\nthe reservoir. In the thermodynamic limit, the fermion Dicke model can be\nanalized using the path integral approach with functional method. The system\nexhibits a second order phase transition from normal to superrandiance at some\ncritical temperature with the presence of a condensate. We evaluate the\ncritical transition temperature and present the spectrum of the collective\nbosonic excitations. There is quantum phantum critical behavior when the\ncoupling constants satisfy an especific condition. Two particular situations\nare analyzed. First, we present the spectrum of the collective bosonic\nexcitations in the case using the rotating-wave approximation, recovering the\nwell know results. Second, the case only considering virtual processes. In the\nlast case, it is possible to have a superradiance phase when only virtual\nprocesses are introduced in the interaction Hamiltonian. Here also appears a\nquantum phase transition at some critical coupling, and for larger values for\nthe critical coupling, the system enter in this superradiant phase with a\nGoldstone mode.\n",
        "There are two important facets of both Midway and Japanese air operations that need to be foregrounded for this answer. First, although Japan did lose four aircraft carriers at Midway, they did not lose them all at once. *Kido Butai* (the IJN carrier force) lost three carriers (*Akagi*,*Kaga*, and *Soryu*) in a series of dive-bombing attacks over the course of a few minutes at 1025. The fourth carrier, *Hiryu*, was unmolested and launched a counterattack against the USN. This  aggressive and somewhat foolish move guaranteed *Hiryu*'s later destruction during a USN counterstrike later that evening at 1700. The second point is that the majority of the *Kido Butai*'s  planes aloft at 1025 were the combat air patrol (CAP) A6M2s. These fighter planes were tasked with the defense of the Japanese fleet and had already given a decent account of themselves in decimating the USN torpedo attacks on *Kido Butai*. The general estimate from Jonathan Parshall and Antony Tully's *Shattered Sword* was that the IJN CAP at 1010 was thirty-six fighters which would soon be augmented to forty-two. \n\nWhat the above means for the OP's question was that there was plenty of time for a number of CAP fighters to land on the intact *Hiryu*. *Shattered Sword* estimates that *Hiryu* took on twenty-seven refugees, all of them fighters save for one strike aircraft straggler from the morning's attack on Midway atoll. This ensured that *Hiryu* had a fairly robust CAP through the evening but a somewhat anemic striking arm. \n\nThe fate of other sixteen CAP aircraft that did not land on *Hiryu* is a bit more murky. No doubt the USN shot down some of the fighters  and the aircraft went missing. Both USN fighter escorts and rear gunners did make claims of A6Ms and although such claims cannot be taken at face value due to the confusion of air combat, the USN undoubtedly did shoot down some of the Japanese CAP. *Soryu*'s fighter ace Fujita Iyozo had to bail out because of damage and he drifted in the water in his kapok lifevest  for some four hours until a spotter on the destroyer *Nowaki* sighted him. Some of CAP also ditched next to smaller Japanese ships like destroyers. This was a standard IJN practice and *Kido Butai*'s carriers usually had a destroyer detached as a plane guard on operations. A number of the damaged aircraft from the morning's strike on Midway ditched rather than try to make a landing. *Hiryu*'s post-1700 CAP of around fourteen A6M2s likewise ditched. There are records of nine fighters ditching and being rescued by surface ships, ironically none of the rescued crew were from *Hiryu*.   \n\nContrary to the popular historical memory that Midway devastated IJN aircrew, the IJN personnel losses from Midway were severe, but not catastrophic. Surviving manifests from *Kido Butai* show that only *Hiryu* and *Kaga*'s air groups suffered from massive losses among pilots and aircrew. IJN carrier doctrine was different than the USN in that pilots were seldom manning their planes until they had been spotted on the deck. While Fuchida Mitsuo relayed the tale of *Kido Butai* aircraft awaiting takeoff prior to the 1025 attack, this was a fabrication. Photographic evidence from the B-17 attacks two hours prior to the dive-bombing attack show nearly-empty flight decks and it would have taken a herculean effort for the Japanese to arm, fuel, and spot a strike package in that amount of time between the B-17 attacks and the 1025 dive-bombings. The 1025 bombings did extract a heavy toll among fitters and other maintenance personnel, which were losses the IJN found difficult to replace, but a good many of *Kido Butai*'s aviators were away from the conflagrations and escaped their carriers' fate. Some like Egusa Takashige were injured from the attacks, but others like Fujita were not. \n\nAgain, contrary to popular stereotypes, the IJN initially filtered the Midway survivors into training or instructor positions. The aggressive Fujita lamented this policy as it denuded frontline units of experienced personnel. The real problem was the material losses at the front outstripped Japan's ability to replace them, so many of these survivors had to be filtered back into combat, often as the replacement chiefs for other units. Fujita for example became *Hiyo*'s  divisional officer and Egusa went from leading dive bombers to commanding the 521 *Kokutai*, a shore-based unit of twin-engined Yokusuka P1Y1 strike aircraft, after his recovery. This was the general pattern for other Midway survivors in which they usually were promoted on the basis of experience to reconstituted or new units, but their experience could not make good on the growing qualitative and quantitative inferiority of Japanese airpower vs-à-vis the American enemy that was apparent by the end of 1942. The majority thus did not survive the war  even if they managed to survive the Midway debacle.",
        "Cornelius Anthony (born July 3, 1978) is a former American football linebacker. He was originally signed by the San Francisco 49ers as an undrafted free agent in 2002. He played college football at Texas A&M.\n\nAnthony also played for the Calgary Stampeders and Hamilton Tiger-Cats.\n\nHigh school\nStarred at Elkins High School in Missouri City, Texas where he earned District 16-5A Defensive MVP honors after posting 163 total tackles (25 for a loss), three interceptions and three fumble recoveries in addition to playing running back on offense. Also earned All-Greater Houston honors, was a finalist for the Houston Touchdown Club Defensive Player of the Year, and was an honorable mention all-state selection.\n\nCollege\nAnthony attended Texas A&M University, where he was a three-year starter. He finished his career with six sacks, 288 tackles, and a forced fumble. In a game against Nebraska during his senior year, he posted 18 tackles.\n\nProfessional\n\nNFL\nAnthony was signed as a free agent to the Washington Redskins, but did not appear in a game with the club. He was assigned to NFL Europe, where he played for the Barcelona Dragons.  He finished the year with two sacks, a team-leading 50 tackles, a forced fumble, and an interception.  He then returned to the U.S., where he played in 17 games over the 2003 and 2004 seasons for the San Francisco 49ers.  After being released by the 49ers, he was signed and subsequently released by the Denver Broncos.\n\nCFL\nSigned as a free agent on May 10, 2005 with the Calgary Stampeders. Appeared in 5 games during the 2005 season and recorded 3 special teams tackles. Became a starter during the 2006 season for Calgary. Finished the season with 40 defensive tackles, six quarterback sacks, two special team tackles, two tackles for a loss, one fumble recovery and one interception. Had his best professional season in 2007, making 56 tackles, recording 8 sacks, and recovering a fumble.\n\nExternal links\nJust Sports Stats\nCornelius Anthony at SI.com\nCalgary Stampeders bio\nA&M Bio\n\n1978 births\nLiving people\nPeople from Pineville, Louisiana\nAmerican football linebackers\nCalgary Stampeders players\nCanadian football linebackers\nSan Francisco 49ers players\nBarcelona Dragons players\nHamilton Tiger-Cats players\nTexas A&M Aggies football players",
        "  We re-discuss the evolutionary state of upper main sequence magnetic stars\nusing a sample of Ap and Bp stars with accurate Hipparcos parallaxes and\ndefinitely determined longitudinal magnetic fields. We confirm our previous\nresults obtained from the study of Ap and Bp stars with accurate measurements\nof the mean magnetic field modulus and mean quadratic magnetic fields that\nmagnetic stars of mass M < 3 M_sun are concentrated towards the centre of the\nmain-sequence band. In contrast, stars with masses M > 3 M_sun seem to be\nconcentrated closer to the ZAMS. The study of a few known members of nearby\nopen clusters with accurate Hipparcos parallaxes confirms these conclusions.\nStronger magnetic fields tend to be found in hotter, younger and more massive\nstars, as well as in stars with shorter rotation periods. No evidence is found\nfor any loss of angular momentum during the main-sequence life. The magnetic\nflux remains constant over the stellar life time on the main sequence. An\nexcess of stars with large obliquities beta is detected in both higher and\nlower mass stars. The obliquity angle distribution as inferred from the\ndistribution of r-values appears random at the time magnetic stars become\nobservable on the H-R diagram. After quite a short time spent on the main\nsequence, the obliquity angle beta tends to reach values close to either 90 deg\nor 0 deg for M < 3 M_sun. The evolution of the obliquity angle beta seems to be\nsomewhat different for low and high mass stars. While we find a strong hint for\nan increase of beta with the elapsed time on the main sequence for stars with M\n> 3 M_sun, no similar trend is found for stars with M < 3 M_sun. However, the\npredominance of high values of beta at advanced ages in these stars is notable.\n",
        "This is a well written paper that attempts to craft a practical program synthesis approach by training a neural net to predict code attributes and exploit these predicted attributes to efficiently search through DSL constructs (using methods developed in programming languages community). The method is sensible and appears to give consistent speedups over baselines, though its viability for longer programs remains to be seen. There is potential to improve the paper. One of the reviewers would have liked more analysis on what type of programs are difficult and how often the method fails, and how performance depends on training set size etc. The authors should improve the paper based on reviewer comments.",
        "Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference.",
        "1) Appendix H - Visualization of Attention Weight Evolution - (Asked by AnonReviewer4)\n2) Appendix I - Partial Expert Positive Source Task A2T experiment - (Asked by AnonReviewer3 WRT base network learning complementary skills and AnonReviewer5 pointing out a con that experiments relied on source tasks containing the optimal policy for the target task.)\n3) Appendix J - Sparse Pong Target Task Added - (Asked by AnonReviewer5 for illustrating transfer learning in a case where the target task performance is limited by data availability)\n4) Appendix A - Revised to give precise details of network architecture (Asked by AnonReviewer5 specifically WRT comparing # of parameters in A2T vs Learning from scratch when source tasks contain an optimal policy for the target task, where it is better if there are fewer parameters for A2T). \n5) Mentioned in the Conclusion Section about extensions of A2T to HRL and Lifelong Learning. (Pointed out by AnonReviewer5)\n",
        " In order to be eligible for the Nutrition Assistance Program, applicants must meet several eligibility requirements. Since the program is meant to assist low-income individuals, Arizona evaluates resources available to the program’s applicants and sets a ceiling on the amount of assets one can have and still be eligible for the program.\n\n\nHouseholds can have $2,250 in countable resources, such as a bank account, or $3,250 in countable resources if at least one person is age 60 or older, or is disabled.\nA household is defined as all of the people who live together and purchase and prepare meals together.\nCountable resources do not include a person’s home or lot, or the resources of people who receive: Supplemental Security Income (SSI), Temporary Assistance for Needy Families (TANF), and most retirement (pension) plans.;\n, Your gross and net income is considered as part of the eligibility determination. Gross income includes a household’s total income. Net income is defined as the household’s gross income minus any allowable deductions. You can calculate the gross income of your household by adding together all of your sources of income for every member of the household for the month. To calculate net income, take the gross monthly income and subtract all of the available deductions for which you qualify. The remainder is your net income. Below is a list of allowable deductions:\n\n\nTake a 20% deduction from your total earned income.\nTake a standard deduction of $155 for household sizes of 1 to 3 people and $168 for a household size of 4 (higher for some larger households);\nTake a dependent care deduction when you need care so that you can work or seek training or education.\nDeduct medical expenses for elderly or disabled members that are more than $35 for the month if those expenses are not paid by insurance or someone else.\nDeduct any legally owed child support payments.\nHomeless households can deduct $143 for shelter costs and any excess shelter costs that are more than half of the household's income after the other deductions., Once you have calculated your gross and net monthly income, you must satisfy the income tests for both gross and net income. The tests are based on the number of members of your household.\n\n\nFor a household of 1, you cannot have a gross income of more than $1276 and a net income of more than $981.\nFor a household of 2, you cannot have a gross income of more than $1726 and a net income of more than $1328.\nFor a household of 3, you cannot have a gross income of more than $2177 and a net income of more than $1675.\nFor a household of 4, you cannot have a gross income of more than $2628 and a net income of more than $2021.\nFor a household of 5, you cannot have a gross income of more than $3078 and a net income of more than $2368.\nFor a household of 6, you cannot have a gross income of more than $3529 and a net income of more than $2715.\nFor a household of 7, you cannot have a gross income of more than $3980 and a net income of more than $3061.\nFor a household of 8, you cannot have a gross income of more than $4430 and a net income of more than $3408.\nFor each additional member add $451 to the gross income limit and $347 to the net income limit., In order to be eligible for nutrition assistance, you must show that you are registering for work, not voluntarily quitting a job or reducing hours, taking a job if offered, or participating in employment and training programs provided by the State.\n\n\nAble-bodied adults without dependents must work or participate in a work program for at least 20 hours a week.\nCertain people are exempt from the work requirements, including: children, seniors, pregnant women, and people who are exempt for physical or mental health reasons., You are eligible for benefits if you are a legal immigrant and have lived in the country for five years; or are receiving disability-related assistance or benefits; or have children under 18 years old.\n\n\nSome non-citizens, like those admitted for humanitarian reasons or those admitted for permanent residence may also eligible for the program.\nEligible household members are still able to get nutrition assistance even if there are other members of the household that are not eligible., If you are unsure whether or not you meet the eligibility requirements, you should still apply for the Nutrition Assistance Program. Those reviewing your application, discussed below, will make a final determination on whether or not you meet the eligibility requirements for the program., You may be required to provide a number of documents to help the Arizona’s Department of Economic Security make a determination about your eligibility. These documents may assist you in completing your application and will be required during your eligibility interview (discussed below). You should gather the following documents:\n\n\nIdentification for the person submitting the application (the applicant).\nProof of citizenship and identity for everyone in the household who is applying for the program.\nAlien Registration Cards, if there are non-U.S. citizens in your household.\nSocial security numbers for each person applying to the program or proof that you have applied.\nBirth certificates for everyone.\nName and contact information for your landlord or neighbor.\nA statement verifying your address and the names of everyone living with you. The statement must be made by a non-relative who doesn’t live with you. The statement must be signed, dated and include that person's address and telephone number.\nProof of all money that your household received from any source in the previous month.\nProof that your employment ended and last date paid.\nA statement from banks or credit unions from the previous month, for each member of the household if they have different accounts.\nProof of savings bonds, securities, retirement plans and life insurance.\nProof of rent/mortgage and utility bills (electric, water, gas, etc.) for the most recent month.\nProof of the cost of childcare expenses for the most recent month.\nProof of all medical expenses for those who are age 60 or older or receive disability benefits if they are applying for Nutrition Assistance.\nProof of any medical insurance other than Arizona Health Care Cost Containment System (AHCCCS) Health Insurance programs., There are several ways that you can apply for the Nutrition Assistance Program. As set forth below, each application method requires the same information from the applicants. You can apply for the program in the following ways:\n\n\nYou can complete an online application that you can locate at: https://www.healthearizonaplus.gov/Default/Default.aspx. This is the fastest method for completing the application.\nYou can download a paper application from https://des.az.gov/sites/default/files/media/FA-001_08-15_English%5B1%5D.pdf and deliver it to one of the local Department of Economic Security offices.\nYou can locate local Department of Economic Security offices by calling 1-855-432-7587 or 1-855-HEAplus or by using the online office locator at: https://eol.azdes.gov.\nYou can also request a paper application by calling the above number or by visiting a local office., On your application you will be asked to provide contact information for an adult member of your household. Specifically, you will need to provide:\n\n\nApplicant’s name, address, and phone number and identify whether the address refers to a shelter.\nThe preferred spoken and written languages of the household.\nThe preferred method of contact – email, phone, or by leaving a message., You must provide the name and contact information for someone who knows you well and the name and contact information for your landlord or neighbor. The Department of Economic Security (DES) may contact these people in order to verify that the information regarding your application is correct. When DES contacts this person, the DES caller provides his or her name and title with DES. DES requests the following in the application:\n\n\nName, address and daytime phone number of someone who knows you well.\nYour relationship to the person who knows you well.\nName, address and daytime phone number of your landlord or neighbor.\nWhether you are related to your landlord or neighbor.\nThe application does not indicate that you are prohibited from listing a relative. However, you may not want to list another person who lives in your home and for whom you are applying for Nutrition Assistance., In order to determine your household’s eligibility, you must provide the following information for each person living in your house:\n\n\nFull name, including maiden, alias, suffix and other names.\nIdentify the persons in your household for whom you are applying for nutrition assistance.\nFamilial relationship to the main contact for the application.\nMarital status, date of birth, social security number, and sex., You only need to provide citizenship information for each person applying. This information may include:\n\n\nWhether the person is a U.S. citizen or U.S. national.\nIf not a citizen, identify his or her immigration status., You must provide the following federal tax information from your next year’s tax submission:\n\n\nWhether you plan to file a return and your filing status.\nWhether you will claim any dependents or whether you will be claimed as a dependent on someone else’s return., You must describe how your household buys and prepares food. You must also provide information about anyone in your household who buys and prepares his or her food separate from the rest of the household and whether they pay for their own expenses., You must state whether anyone applying for nutrition assistance also needs assistance with medical bills or Medicare premiums in the next three months., Since your eligibility is based on the number of household members, you are asked to explain any temporary absences by any of your household members. Specifically, you must state:\n\n\nWho will be absent.\nFor how long.\nTheir expected return date.\nReason for the absence., You must identify the residency of all of the applicants for the program and specifically whether they are applying for benefits as an Arizona resident. You may need to provide proof of residency., All persons applying for the program must state whether or not they are currently incarcerated, whether they have been convicted and their expected release date.\n\n\nYou will also need to provide information regarding any applicants felony convictions., You must provide the amount that anyone pays for childcare so that a member of the household can work, look for work or attend job training or school., For those in the household who work, you must state where they work, how often and the amount they are paid., You must provide all sources of income other than your employment. This may include:\n\n\nSocial Security Benefits;\nSupplemental Security Income (SSI Cash);\nUnemployment benefits;\nChild support;\nVeteran’s benefits;\nPayments from natural resources, farming, ranching, fishing, or leases; and/or\nTribal money.\nYou will also need to explain any changes in income that you expect in the next 12 months. This may include the end of a contract position or seasonal work., For all persons applying for the program, you must describe:\n\n\nWhether an adult is unable to work because of a medical or mental condition that has lasted or may last 12 months, or might result in death.\nYou must identify that adult and provide previous employment information.\nIdentify anyone applying under age 65 that has a disability expected to last at least 12 months and is working.\nIdentify anyone who needs help with activities of daily living (bathing, eating, dressing, etc,).\n\n, You must set forth any resources that anyone in the household has, such as uncashed checks, money on a pre-paid debit card or retirement accounts., All parties who are applying for the program must sign the document. These signatures may include: the applicant or the applicant’s designee or the applicant’s spouse, if married and living within the same household; or the parent/legal guardian of a minor child., Submit your application online or in hardcopy to: Arizona Department of Economic Security Family Assistance Administration P.O. Box 19009, Phoenix, Arizona 85005-9009., You may be required to participate in an eligibility interview with a local DES employee. You may be required to bring all or some of the documents that you gathered (as described above). At the end of the interview, you will be told whether DES requires any additional proof of eligibility.\n\n\nMake sure you bring copies of your documents and not the originals.A DES employee will try to contact you to set up an interview by the 12th calendar day after the date of your application.\nDES will work with you to set up an interview time that is convenient for you, including during a lunch break or in the evenings.\nIf necessary, they may be able to conduct the interview via telephone.\nInterviews must be scheduled in a timely fashion so that you can receive a decision within 30 days from the date that your application was received by DES., You will be notified of DES’ decision on the following days:\n\n\nWithin seven days from the application date if you qualify for expedited benefits.\nNo later than 30 days from the date of the application.\nYou can monitor the status of your application on-line at: https://www.healthearizonaplus.gov/Default/Default.aspx, If you feel that you were unjustly denied Nutrition Assistance, you can file an appeal and ask for a legal review of your case. Under this program, an appeal is called a Fair Hearing. When you request a Fair Hearing, an Administrative Law Judge reviews the agency’s decision to make sure Arizona law and Family Assistance Administration (FAA) policy is correct. In order to request a Fair Hearing, you must:\n\n\nComplete the Fair Hearing Request Form, located at: https://des.az.gov/sites/default/files/FA-100-PD.pdf.\nYou must submit the completed Fair Hearing Request Form within 90 days of the mailing date of the decision notice.\nYou can submit your request form to your local DES office by mail, fax or in person. You can locate your local office at: https://eol.azdes.gov.",
        "The authors propose a method to investigate the predictiveness of intermediate layer activations. To do so, they propose training linear classifiers and evaluate the error on the test set.\n\nThe paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.\n\nThe two main reasons for why the authors decided to use linear probes seem to be:\n- convexity\n- The last layer in the network is (usually) linear\n\nIn the second to last paragraph of page 4 the authors point out that it could happen that the intermediate features are useless for a linear classifier. This is correct and what I consider the main flaw of the paper. I am missing any motivation as to the usefulness of the suggested analysis to architecture design. In fact, the example with the skip connection (Figure 8) seems to suggest that skip connections shouldn't be used. Doesn't that contradict the recent successes of ResNet?\n\nWhile the results are interesting, they aren't particularly surprising and I am failing to see direct applicability to understanding deep models as the authors suggest.",
        "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.\n\nPros:\n- Paper is well-motivated, exceptionally well-composed\n- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation\n- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.\nCons:\n- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.\n- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.\n\n\n",
        " What you will need first, is a clear idea of where your money is going, then you can look at ways to cut fluff and lower the cost of your required living expense. Always keep in mind that it is not just about cheaper; it is about efficiency. Analyze your needs and do the math. Most importantly, understand that reducing expenses is a lifestyle change and a change in your thinking patterns. Never let yourself believe that pennies do not count.;\n, If you do not know where your money is going, you are most likely spending too much. You can get a solid idea in as little as one month and as you continue, you will see patterns develop that you can address. Write down everything you buy down to the last dollar. Do not stop at the obvious expenses like rent, utilities, gasoline and food. Include the ancillary items like sodas and snacks as well as gum or tobacco. Use a Row-by-Column ledger, spreadsheet, or other software to keep track every month. If you only use a debit card for your purchases, the bank will do this for you.\n\n, While it is likely this will not be the largest savings area, it is important and easy. Is the coffee shop on your way to work necessary? How critical are the three sodas or snacks a day you buy from the office vending machine at $1.50 each? A cup of coffee that you made at home is only 25-35 cents, as is a soda you bought in a store as part of a 12-pack. Do you seriously have to rent all those movies (and pay those late fees) each month? Have you checked to see whether your library has movies, or calculated the cost of switching to Netflix and BlockBuster Online? Those ten lottery tickets… the odds against you are astronomical. This is quick and most of it is habit. There will be some psychological pain at first but when you add up the dollars you’ll see a big difference instantly.\n\n\nMake a shopping list before you go to the store and stick to it. This is especially helpful to impulse buyers. Did you ever go in for a carton of eggs and come out with a basket of 15 items? Did you need 2 for 1 bags of marshmallows or the jumbo box of cereal just because it was on sale? No. You probably did not need half of those extra things but ended buying them anyway. A shopping list gives you a clear idea of what you need and eliminates unnecessary purchases.\n\n,\n\n\n\nHeating and cooling (gas or electric): When you leave the house, set your thermostat to an \"away\" setting. Do not set it so far from comfortable that it takes too much of time to return to comfort when you get home: 65°F or 18°C in the winter and 80°F or 27°C in the summer might be reasonable numbers to use. A programmable thermostat will do this for you automatically.\n\nSet it to adjust the thermostat a while before the heating or cooling is needed, such as in the pre-dawn hours to wake to comfortable temperatures and in the mid-afternoon to arrive home to them, and to provide less heating or cooling immediately when it will not be needed.\nConsider investing in ceiling fans – you can get these for as little as US$20 and they dramatically reduce cost of heating and cooling by circulating the air more efficiently. If your expenses are already low, and you will not be staying where you are for long, you may not save enough to pay for the fan, however. Also, consider electric blankets and mattress pads.\n\n\n\nElectric: Lighting is expensive. When you leave a room, turn off the light. The idea that it takes more energy to turn on a light than to keep it on is false, as turning on a light only burns as much electricity as burning it for fractions of a second.Energy efficient bulbs work. This is an investment that will pay off over time, but there is a significant savings to be gained. (this energy calculator can help). Turn off your computer/laptop when you are not using it – (probably) the only reason you leave it on is convenience. Any voltage adapters (including the ones in stereo components) use electricity, even if they are not charged or plugged in to the device. Of the total energy used to run home electronics, 40 percent is consumed when the appliances are turned off. Either pull the plug on the devices when not in use or buy a device to do it for you, such as a Smart Power Strip ($31 to $44). If you have a digital box with an auxiliary AC outlet, plug your TV into it, and program the box to shut off the outlet when the box is turned off. For stereo components, plug them all into a power bar that can easily be switched off when not in use.Open the drapes during the day for light instead of burning electricity. Only use electricity when you need it.Clean the radiator on the back of your refrigerator, if it is dirty, this will improve the efficiency of one of the larger consumers of energy in your household.\n\nWater: Save water, save money. Invest in a shower-reduction kit – they cost nearly nothing and will start saving you money immediately. They work by reducing the flow to the showerhead and the change is barely noticeable. Learn to take quicker showers – an inexpensive egg timer is a good way to help. Repair leaky toilets and faucets – this is an enormous waste of water and easy to fix. Reduce your lawn watering to minimum needs. If you have a pool, keep it covered when it’s not in use to reduce evaporation – also, if it’s heated that will dramatically increase evaporation as well (only heat your pool to keep it from freezing, and invest in a thermal blanket). Also, if you are not using the faucet turn it off -- e.g., when brushing teeth do not leave faucet running. Do not buy bottled water except in rare and unusual circumstances; excess chlorine can be removed from tap water by letting it stand in a pitcher in the refrigerator for a few hours, and the fluoride in tap water makes teeth stronger, reducing dental problems and bills.\n\nGas and miscellaneous: Do laundry as often as necessary but as little as possible – for many people this is a pleasant step. Reduce the temperature of your shower by a couple of degrees; the less work your water heater does the more money you’ll save. Also keep your water heater thermostat as low as practical; 120 degrees Fahrenheit is often recommended to minimize power consumption and risk of burns. (Turn off the house power before opening any panel to adjust it.) Use the microwave instead of the oven whenever possible – the cost just to preheat an oven is more than the cost to cook a meal in the microwave. Open the windows when it’s nice outside to reduce heating (and cooling) costs.If you live where natural gas is used only in the winter months, arrange with your local utility to do a seasonal shutoff so that you are not saddled with fixed monthly service charges for the \"privilege\" of being connected to the gas service even though you are not using it. With one supplier, it is $17/month. In the 8 months that you do not need the service, you're charged $17 X 8 = $136, but the season shutoff and turn on costs $54.\n\nCable and telephone: Do you need a thousand channels and every single premium channel available including the high definition packages? You can save the full $100+ on your cable bill every month by watching TV free online, and save most of it while avoiding time-wasting and unnecessary-purchase-inducing commercials by renting DVDs instead, for instance through the vending machines Redbox or through the mail-order company Netflix. However, if you have cable internet, it may actually be cheaper to keep basic cable than to pay for internet alone. If you want to save money, take a close look at your priorities. For your telephone, shop around based on your use. If you make many long-distance calls to family and friends, perhaps one of the unlimited plans would save money. If all your calls are local, you probably can get by with a bare-bones option. Consider that your cell phone may have free long distance; therefore, removing the need of having long distance on a landline. Look into the Voice-Over-IP (phone over the internet) for your telephone solutions. Some services, such as Skype, gChat (from Google), and Windows Live! allow you to make free video calls to other users as well as make low cost calls to cell phones and land lines from your computer - including international calls. Other VoIP services, such as Vonage, are not an option for people with DSL, which is tied to their landline.\n\nCell phone: Text messages cost money. \"Oh no, I have unlimited text!\" Oh? How much does that option cost you? Do you even need a cell phone? Does everyone in your family need a cell phone? Parents should place rules on cell phone use. Another thing to consider is if you need a cell phone then do you need a landline at home? Consider consolidating. If your cell phone use is occasional only, consider a pay-as-you-go plan. Do consider, however, that a cheap unlimited data and navigation plan can sometimes save money by allowing instant price comparisons and quality checks.\n\nCell phone saving plans: Some mobile phone plans are genuinely good and money-saving; but make sure that you shop around first for the deal that best suits you. Many companies offer either contract or PAYG plans based on the cell phone habits of the user, for example, someone who texts an awful lot or someone who prefers to call. For example, some companies reward you for topping up for a little bit a month with hundreds of free texts, which can prove handy and are much cheaper than calls. Remember, calls to networks other than yours, and landlines, are often pricier. Avoid \"traps\" in cell phone plans such as inordinate per-kilobyte or per-message rates, often over some threshold. Look for a plan with modest, if any, overage charges. For instance, Sprint smartphone data is unlimited.\n\n, There are some cheap solar lights on the market that work very well. There are also more expensive ones that have other functions as well. The best thing is that solar lighting functions just as well and is just as bright as those lights that you plug into the wall and you can leave it all night and recharge it in the daytime when you're ready.\n\n, When gas was rationed during World War II, a popular slogan was \"Is this trip necessary?\" Ask yourself that every time you get in your car.\n\n\nMake a list before you go to the store so you do not have to make extra trips.\nDo not go for a drive for pleasure – walk instead or choose other forms of entertainment (for example, reading or exercising).\n\n, Convertibles get better mileage with the top up (although the slight pleasure for the mile or two per gallon sacrificed with the top down is cheap entertainment, assuming one has already paid the considerable extra money for a convertible). A poorly running engine is a huge waste – even a spark plug change can make a big difference, as can clean oil. In addition, the less you drive, the less often you will change tires, oil, or need maintenance. That is a savings-over-time, of course, but it will mount up.\n\n, Another way to save gas (and money) is to change your driving habits. By simply driving more slowly, or less aggressively, you can save significant amounts of money (calculate for yourself at this web site ). Take particular care to avoid driving in heavy traffic, which causes no joy and little efficiency gain over public transportation, and to avoid where parking will be expensive. Public transportation mapping and schedules online, often provided by transit agencies, can make that a great alternative in cities.\n\n, It is astounding how many people complain about money then describe the latest release of a movie with the cost of theater popcorn. In addition, professional sports event, a music concert, or tickets to a play can run hundreds of dollars for a couple on a date. Seriously, can you tell the difference (blindfolded) between a $30 bottle of wine and a $9 bottle of wine? When you do dine out, think about the prices on the menu first. Consider a meal share if the restaurant offers that option. Never, ever order delivery of expensive food; you are wastefully enjoying only the food and not the atmosphere when you could cook for yourself far more cheaply. Look for vacation bargains – consider taking the kids camping instead of one of the expensive amusement parks.\n\n\nMost people, except for serious athletes, actors and musicians (as the case may be) cannot tell any difference between a great and good performance. Even if they can, most will enjoy increased variety and frequency much more. Enjoy local high school and non-prestigious college sports, community theater and orchestras conveniently, for little money (with typically nice but inexpensive meals in the vicinity), and socialize and contribute to community spirit while you are at it.\n\n, Rediscover and show off old ones \"lost\" in storage or the back of a closet, and organize your wardrobe (or whatever you keep them in) and habits to prevent \"losing\" them again.\n\n, The only real difference between a $1.99 can of corn and a $0.63 can of corn is $1.36, and the satisfaction of knowing you're not overpaying largely to feed a cycle of ads to make yourself and others fret about not paying more. (Sure there are exceptions; people on low-sodium restricted diets will often have to pay more). The grocery store is a place you can save big.\n\n\nLook for foods that are marked \"WIC\" for savings. Those have been approved for the Women, Infants and Children program by the USDA Department of Food and Nutrition Services… healthy, nutritious and inexpensive. That ring of cooked shrimp is on sale and sure looks tasty. Would you prefer a grilled chicken breast with green beans and rice? Make dining in an experience instead of just a convenience. It is possible to spend as much on home food as you would by eating out if you are wasteful.\nBuy foods that are on sale, especially meats. Most supermarkets regularly cycle through various meats for specials; you will get to try them all just the same. The difference between expensive beef and other beef is just extra fat and tenderness compatible with not-thorough cooking in the expensive beef.\nInvest $10 in a coffee pot, or $100 in an espresso machine (pump-driven is best, but expensive ones can die just like cheap ones). Making your coffee at home instead of purchasing your $1, $3, or $7 custom latte at the coffee shop will save you money.\nWhen purchasing meat items, aim for pieces where you can identify the body part from which it came. Ground beef, although cheap, is processed which increases its price. Tougher cuts of meat can be slow-cooked and made incredibly tender. Also larger pieces can be cooked in bulk and used for several different recipes. (Cook one large piece and when tender, tear it up for use in enchiladas, sandwiches, stews or soups, etc. Simply store in individual portions, labeled with the type of meat and date, for later use.) Organ meat (chicken hearts and gizzards, beef hearts, tripe) are often far cheaper than normal cuts, and can be used to make tasty and filling stews.\nAvoid large packages of fresh produce to avoid spoilage; frozen produce will extend the shelf life of all your fruit and vegetables.\nBuy fresh foods that are in season. They will cost less than the fresh food flown in from the other side of the world; the customer has to pay for the fuel that got the food there.\n\n, Even an inexpensive lunch out is several dollars a day – do the math.\n\n, Make sure these are on items you would normally eat so you do not buy groceries that will be wasted by sitting in your cupboards forever or spoil in your refrigerator. Also use buy store specials and use store customer cards when possible toward food purchases. Consider, however, that store brands are just as good as and often cheaper than name brands with coupons.\n\n, The price of the membership is usually made up in the first shopping. They carry name-brand products and take coupons. In addition, by not having to shop as often, you spend less money by not being in the store every week and risking impulse purchases. Warehouse club shopping must be done with discretion or you will not save money.\n\n, For example, soap powder, flour, dishwasher detergent or cereal. Do not be wasteful with the product just because it comes in a large container.\n\n, Are you going to enjoy that box of cereal that is not your regular brand, or is it going to sit on your shelf uneaten?\n\n, Do your best to track this. If a promotion is luring you in, ask yourself if you normally use the product. If not, ask yourself what possible benefit it will be to start using it now. If you only want it because it is stuck in front of your face and seems appealing, that is not enough of a reason to purchase it.\n\n, Alcohol has all of these adverse consequences.\n\n, Companies that sell those are incredibly competitive. Get some bids from different companies. When you do this, bear in mind that lower initial premiums will not always be the most cost efficient!\n\n\n\nAuto Insurance: Look at your deductible. Avoid jumping to increase your deductible – analyze the entire plan based on your needs and expectations; do a risk analysis first. If you have an inexperienced driver in your house and you do not have savings, having a high deductible might not be the best choice. If your car is financed, you may have minimum insurance requirements. However if you have a long history of good driving and you own your car, outright, you might consider a high deductible to save on premiums.\n\nHealth Insurance: Explore alternatives. Shop around for plans that are consistent and cost-efficient with your lifestyle. Consider your needs versus what you have. A single man in perfect health in his mid-30’s might choose a plan with a higher co pay or co-insurance and lower premiums, whereas a married couple wanting to start a family might do better with higher premiums but more extensive coverage. In other cases, prescription benefits might be the most important. The point is to look at what you must have .\n\nAnother alternative: Join a health sharing initiative. Members of health sharing groups pay for each other's health care with significant reductions in costs and a fraction of the out-of-pocket costs (such as the deductible found in most health insurance plans). They also qualify for ACA requirements. The best part about them is that you get to choose your own doctor (yes, really!). Most, but not all, health sharing groups are faith-based and require specific faith affirmations.\n\n\n\nLife Insurance: There is no question that this is important – for many people. The rule of thumb for someone with a family is three to five years' replacement income. However, if you are a 20ish single consider carefully and determine if you are over-insured. If you’re married in your mid-60’s have you looked at comparative plans from places like AARP? If you are most interested in \"burial policies\" then, again, these companies are incredibly competitive. We would like to leave our loved ones wealthy if our demise, but not at the expense of your quality of life right now.\n\nHome (and Renter’s) Insurance: This can be a large expense and many home owners have no idea how much they’re paying because it comes out of their house payments – out of sight, out of mind. Review your plan with your agent. Are your personal possessions worth the $250,000 you have on the policy? Also look for areas that are lacking. Is water damage covered; snow damage; hail damage? Think whether you will need those. Is anything important excluded? Is anything irrelevant included? Yes, Great-Aunt Martha’s rocking chair has sentimental value, but do you need a special rider to cover it?\n\n, This is a great way to save significant amounts of money while recycling! If you absolutely must buy something, there are options other than a mall anchor store or a big-box superstore. There are large thrift stores (e.g. Goodwill) and smaller church-run stores that have some incredible bargains on everything from home knickknacks to appliances to clothing. It is beyond imaginable how fast a 4-year old will outgrow shoes (when that happens, re-donate them so somebody else can benefit). Look for garage sales – your neighbors will not think less of you because you bought the winter jacket they are trying to sell. Have your own garage sale and they might want what you no longer need. There are online sites that often have bargains (like Craigslist.org, Overstock.com and eBay.com).\n\n, If you shave compare razors for durability. Some shave satisfactorily for many, many times longer than others, making their cost of up to a few dollars each cartridge relatively insignificant.\n\n, Some of these items, such as printers and suits, though rarely vehicles, are helpful to get rid of even if they are not broken. Culprits include:\n\n\nInkjet printers (a laser printer can cost as little as $100 and cost about 3 cents each page to print, rather than 25 cents or more, with fast waterproof output.) Color laser printers can be cost-effective if much color printing is needed, though they are not great for photos. Online or in-store photo printing is a better deal than the high-quality ink-jet printers suited to photos.\nWool suits and iron-requiring cotton clothing, unless important to creating an impression needed to earn money in one's occupation. Iron-free cotton shirts with a fine pattern to hide residual wrinkles look great, save over a dollar each time plus time, and gas in laundering. Synthetic pants save multiple dollars each laundering and do not feel odd on legs because they are less sensitive than arms.\nMost television and, to a lesser extent, movies. The purpose of television, from a financial perspective, is to cause you to watch advertisements to become unhappy that you do not own things you would not mind not owning. Few of these things are more than frivolous. More insidiously, there is a purpose to keep you watching, which drains efforts from possibly more enjoyable or educational (and thus potentially income-producing) activities. Many movies are about focusing on lavish and extravagant living and create a mindset incompatible with frugality.\nFancy cars. The fastest cars available accelerate about twice as fast as, corner about a third harder than, and have slicker, shinier seats than the cheapest. The differences are much more subtle. Mass-market cars such as family sedans and minivans and professional-driver-market vehicles such as town cars, vans and pickups have enormous companies optimizing them for things such as cost, comfort, fuel consumption, safety durability, and ease of maintenance. More expensive cars, even when not driven beyond the abilities of any others, often make large sacrifices in some of these things for small improvements in others. They also involve much higher overhead because of smaller sales volumes. If many people in your area replace good cars unnecessarily, a well-maintained and carefully inspected used car can save a great deal.\nVideo game consoles and other devices, electronics, with vendor lock-in. These may seem inexpensive and can be a good deal if one is sure one wants only few games or other accessories to play often. However, adapting them to various games or other uses requires paying an excessive markup each time. In contrast, a computer has many games available inexpensively once they've been out for a year or two and many made available free by their creators much like wikiHow, such as Nexuiz.\n\n, A poor credit score costs tens of thousands over the years in increased interest rates and insurance costs. You may even lose your job or miss a job application. Pull all three reports; challenge everything that appears incorrect. Pay all bills on time or early. Pay off revolving debt (credit cards) and put those cards away.\n\n, An overdraft may seem like a good idea, but it is simply one-step nearer to a pricey pitfall. Even if the bank you are with does not charge you for using the overdraft, they will charge you if you even conform over it. The good thing about debit cards is that you are not using money that you do not have, and an overdraft will most weaken your discipline over your finances. Do not do it! If you must currently have credit card debt or an overdraft do not forget to compare interest rates, on all your cards and overdraft. Consolidate loans to the most inexpensive one while paying off the debt.\n\n, A safe area and, if you have children, one with a school where they will be permitted to learn in peace is important. If you enjoy a big yard and big windows, or convenient regular access to varied shopping (itself not helpful in frugality, like neighbors living extravagantly and often beyond their means), recognize, and pay for that. However, a house sits in the rain and rots slowly as it is being (hopefully) enjoyed, and can be replaced or copied on months' notice in ways that are being made more efficient continually. There is plenty of empty space to build them, and less-densely-developed areas can be expected to compete to make money from increased development over time if that is demanded. As recent history shows, it is not a great \"investment\" though it does have significant residual value and some people do make money with them.\n\n",
        "  The near infrared (1-2um) and the thermal infrared (3-25um) trace many of the\nenvironments in which masers are thought to reside, including shocks, outflows,\naccretion disks, and the dense medium near protostars. After a number of recent\nsurveys it has been found that there is a higher detection rate of mid-IR\nemission towards masers than cm radio continuum emission from UC HII regions,\nand that the mid-IR emission is actually more closely cospatial to the maser\nlocations. A high percentage of water and methanol masers that are not\ncoincident with the UC HII regions in massive star forming regions are likely\nto be tracing outflows and extremely young high mass stars before the onset of\nthe UC HII region phase. After a decade of groundwork supporting the hypothesis\nthat linarly distributed class II methanol masers may generally trace accretion\ndisks around young massive stars, compelling evidence is mounting that these\nmasers may generally be associated with outflows instead. Substantiation of\nthis claim comes from recent outflow surveys and high angular resolution mid-IR\nimaging of the maser environments.\n",
        " Driving in a highly populated metropolitan area is very different from those dirt roads out in the country where no one is around. Try to notice how people drive, what habits people seem to have, and what people do in response to certain situations.;\n, Unfortunately bumper-to-bumper traffic is often stop-and-go instead of moving along at a stead slow pace.\n\n, First of all, go the speed of traffic. It is very dangerous to be going 35 mph (56 km/h) on the interstate when everyone else is going 70 -- plus it blocks traffic up, and people won't thank you for it.\n\n, Other drivers can't read your mind -- they have to rely on things like your turn signal to know what you're going to do. If you are turning, put on your signal about 10 car lengths before you plan to turn. Always use your signal when you're changing lanes, and check, double-check, and triple-check to make sure there is a spot open before you change lanes.\n\n, This is common sense; it is safer to go on another block and turn around, and it's not going to cost you more than a minute or two.\n\n, Not only is it rude to try to speed up to pass them, it can also be dangerous.\n\n, It is EXTREMELY rude, and you could find yourself in a rear-end collision, which would be YOUR fault.\n\n, This is common sense and it can mean the difference between a preventable wreck or a safe commute.\n\n, Everyone is trying to get to some different place, and people are usually more preoccupied with their own lives than looking at other drivers.\n\n, If they do, try to figure out what you may have done that prompted it and try to change this behavior. Realize that the person may have just been having a bad day, and let it go.\n\n,",
        " If you drive on the right you will likely involved in a serious collision, or killed, and will be arrested for dangerous driving which is a criminal offense and may end up with a custodial sentence.;\n, Bonnet, boot, windscreen, tyre, bumper, gearstick, mobile, car hire, roundabout, motorway, A-road, B-road, RTA, slip road, dual carriageway ...make sure you know what these mean before driving on UK roads.\n\n, If you cannot drive a manual car, you will have to hire an automatic car because you will not be qualified to drive or insured to drive a manual car. If you have an accident whilst driving a manual that you cannot operate, you may be arrested for dangerous driving which can result in a heavy fine and maybe a custodial sentence.\n\n,, Often, there are adhesive or static shields that can be applied to the headlamps. Some cars even have a simple mechanism under the bonnet for adjusting the beam pattern.\n\n, The UK has very strict drink and drugs regulations and many police officers pull over cars to carry out drink and drug tests on random drivers. The view of the police and the courts is that alcohol immediately makes you guilty in an accident, even if it was not your fault. Refusing to provide a breath or urine sample when asked to do so is an offence itself, and will result in your arrest.\n\n, Always do as you're told without a bad attitude and never assume that if you are polite you will get off any offense you have committed. Always provide ID when asked. You should always carry a valid drivers licence and proof of vehicle insurance when you drive. It is a criminal offense to refuse or give false details which will result in arrest. Unlike in the USA and maybe other countries, the UK police do not need permission or a reason to stop and search you, your belongings (handbag, rucksack or pockets) or your vehicle if stopped while driving. Do not attempt to stop them as you will be detained or arrested.\n\n, In the UK, the left hand lane is considered the \"slow lane\" and some drivers may pull over without checking properly which can cause an accident. Be aware that using the hard shoulder of a motorway to undertake vehicles will also result in a possible arrest.\n\n, These are fixed cameras, mobile speed traps and are also fixed to most police vehicles. These automatic systems in police cars can also confirm whether or not the vehicle is insured or not.\n\n, Always make sure you have the correct motor insurance policy, failure to do so will result in your vehicle being seized and you being arrested. Make sure the vehicle is road legal to UK standards, failure to do so will void any insurance policy and result in the vehicle being seized. If driving a friend's vehicle always get their permission and check your own insurance policy to make sure you are permitted to drive other vehicles.\n\n, The speed limits on the motorways are 60mph (96km/h) for goods vehicles over 3.5 tonnes, 60mph (96km/h) for buses and coaches and 70mph (112km/h) for cars. , Image\n\n, Only public buses, taxis, motorcycles, bicycles and emergency service vehicles are permitted to use this lane. Anyone else will be photographed (automatic cameras) and fined £60 for using this lane. (Approximately $100 USD).\n\n",
        " There are many factors to consider when finding a suitable location on which to build your home. Think about a place you'd like to live long-term and keep in mind things like:\n\n\n\nClimate. Special considerations must be made for building in flood, hurricane, intense heat, frigid cold, and other extreme weather and climatic conditions.\n\nGround stability. Houses built on shifting sand, mucky soil, or other unstable earth will likely fail over a short period of time unless they are built on special foundations or pilings.\n\nAvailability of utilities. If you intend to have electric power, potable water, telephone, and other conveniences, make sure these utility providers offer them at your location.\n\nCommunity infrastructure. If you plan to raise children or have kids, make sure good quality schools are available. Check to see if you are in a police jurisdiction to protect you from crime, look at the distance you will have to travel to acquire basic commodities, and whether medical help is nearby.;\n, This may be a hurdle, depending on the cost, and your available funds. Building a house is an expensive process, but purchasing suitable property is also a major investment just as important as homebuilding. Decide how you're going to pay for your building project going forward and start that process with the land.\n\n\nSome home builders will elect to get a construction loan to purchase the land and secure funding for the building project. This requires that you enter into a contract with a builder or a contractor, and the loan must reference that builder's resume and serve as a contract between you and the builder, as well as a source of funding for the project. To do this, you'll need to wait until you've hired and vetted a builder before purchasing the land., This is not absolutely necessary, especially if you are building on a large parcel of land, but if there is any doubt about the property lines, have this done to assure you are not encroaching on a neighbor's property, or the city's. This will be useful as you move forward with the building process.\n\n, On large parcels, especially, you will need to ascertain the route for a usable driveway if you depend on a car for transportation. Look at any low area that would become impassable in winter mud or heavy summer rain, how installing driveway will affect the landscape, and whether a driveway will be in conflict with underground utilities.\n\n\nPay particular attention to the way surface water will drain off the property. Every effort should be made so that water drains off and away from the driveway. This may require the placement of culverts or pipes under the driveway to avoid puddling along its sides.\n\n, Architects and engineers have special training and years of experience in designing houses, and are necessary for most building and zoning jurisdiction code requirements. Regardless of whether you contract their services or elect to design your own, the house you build will be built for you, so you should be involved closely in the design process.\n\n\nBefore you hire or consult an architect, find out what management services the firm may or may not provide. Some architecture firms will help hire contractors they know and trust, as well as consult and inspect the contractor's work as it progresses, making necessary revisions and additions as the work progresses. This can be a significant headache relief in the process.Before building, you'll need to submit plans to the city or county building commission for approval. Unless you're an experienced architect, it'll be very difficult to produce the necessary to-scale production drawings and engineering specs necessary for approval. To save time, energy, and money, it's recommended that you consult a professional and work alongside them to design the home you want.\n\n, The fun part of designing a home is imagining your new life in your new space. Spend some time researching pre-drawn floor plans for inspiration and consider using them as a guide for your own space. Home building guides are commonly available for free online.Give lots of thought to what kind of rooms you want, the number of bedrooms that will be necessary for your family, and what sort of a style you want in the rooms you'll be spending the most time in.\n\n\n\nBedrooms: For a family house where the possibility of additions exists, remember it is simpler to add a room during initial construction than to remodel or build an addition later. If you only need 2 bedrooms at present, an extra room might be used for an office, storage, or even left unfinished and unfurnished until such time as it is needed.\n\nBathrooms: In practical terms, one bathroom can suffice in almost any circumstances, but if the house is for multiple people, two makes life much easier. Having two or more bathrooms will also increase the resale value in the convenience minded home buyer's mind.\n\nSpecial function rooms: Consider if your lifestyle requires rooms suitable for special functions, such as formal dining, office space, a den, or a play room.\n\n, For family life, having a laundry room, and possibly even a garage can be a real help in managing day to day chores. Planning out the necessary house-running spaces is a critical part of the design process. It's also important to design them to be as easy-to-wire and outfit with plumbing as possible, making it important to consult an architectural engineer when designing the home. Carefully design your:\n\n\nKitchen\nGarage\nLaundry room\nStorage areas\n\n, Part aesthetics and part energy-efficiency, designing your home with an eye toward sunlight will ensure that your home stays full of warm radiant light when it needs it the most. If you are building a home with large windows in a living room, consider facing these toward the most appealing view and at an angle that maximizes the natural lighting when you'll want it most.Kitchens may benefit the most from exterior light, so think about what time a ray of sunshine in the kitchen will offer the best results. Late afternoon may be cooking and dish washing time, so it may be best to orient the kitchen towards the west to take advantage. Larger windows on the north/south face of your house will also help heat the house through solar gain in colder climates.\nIf you live on the northern hemisphere, build your windows facing the south. If you live on the southern hemisphere, build your windows facing the north.\n\n, Be aware of how surface water (rain, snow melt, drainage from seasonal springs) moves across the building site. It is critical to keep water away from your home, especially in colder climates. Freezing pipes and foundation damage can result from a failure to plan at this stage. You want to keep your basement dry and lessen the chance that you will have damp wood, which invites termites in any climate. Simple swales or grassy ditches will go a long way in controlling surface water drainage.\n\n, If you haven't already started this process when securing the land, you'll need to figure out a way to fund the project and a construction loan is the most recommended method of doing so. Apply for a construction loan by filling out a loan application, called a 1003, and submitting it to a loan officer along with a credit report.The completed loan application will need to include information like:\n\n\nThe type of loan requested\nThe amount of money requested\nYour current living situation\nYour social security number\nW-2 info\n\n, To engage in a homebuilding project, you'll need three types of construction-related insurance, some of which may be provided by the builder, some of which won't, depending on the state you live in and the nature of the contract you've signed. Typically, it's required that you provide:\n\n\n\nCourse of Construction Insurance to cover against unforeseen loss including damages from fire, accident, vandalism and malicious mischief.\n\nGeneral Liability Insurance is sometimes provided by the builder and sometimes not. It is a comprehensive liability coverage against accident on the workplace. You should only hire builders who provide this insurance themselves, as it can be quite expensive and could be indicative of shoddy workmanship if a builder doesn't provide it.\n\nWorkman's Compensation Insurance is necessary if your builder hires their own workers. If the work is subcontracted (a common practice) you'll need to provide workman's comp and the builder must write a letter acknowledging that they do not have employees and will not provide compensation.\n\n, A building permit is a basic requirement in many areas, particularly for permanent construction. To obtain this, you'll need to provide detailed architectural diagrams, engineering load specs, and other materials to your state's Department of Housing. It's likely you'll also need the following to comply with local codes and zoning requirements by obtaining:\n\n\nA septic tank permit\nAn electrical permit\nA plumbing permit\nA mechanical (HVAC, or air conditioning) permit\nYou may also find you are required to apply for and receive an environmental and/or impact permit. Having the house location marked prior to obtaining your permits will help to work details out in the environmental permitting process.\n\n, This is the breakdown of each particular cost of construction of the home. The foundation, lumber, framing, plumbing, heating, electrical, painting, and builder's profit, etc. When you hire a builder, they will usually complete this form to show you exactly what it will cost to build your new home.\n\n\nPrice building materials in the area. How much is the cost of wood in the prospective area? Labor? Vinyl? It's helpful to give some though to how much the process will end up costing, aside from the land purchase itself. Try to get a rough estimate of how much it'll cost to build the kind of house you want to build at the location you're considering.\n\n, Building a house involves several specific trades to ensure quality work, so it is usually better to have trained craftsmen perform things you're unable to do at an expert-level. You can probably paint the house and put up drywall yourself, but maybe you want to hire those jobs out. Try to find an economic and practical balance between doing projects yourself to save money and hiring out more intricate and difficult work. Consider hiring out:\n\n\n\nSite workers to clear and grade the land, prepping it for building\n\nBricklayers to lay the foundation\n\nFramers to perform rough carpentry, frame up the walls, and install the trusses or stick-framed rafters\n\nRoofers to install the roof and insulate the house\n\nElectricians, plumbers, and HVAC workers to do the difficult interior work of outfitting the home for living\n\nTrim and finish carpenters for interior design work\n\nFlooring installers to lay the carpet, hardwood, or tile\n\n, Everything will be made easier if you hire an experienced builder to oversee the project. You won't need to worry about doing everything yourself, hiring out particular tasks, and securing the permits yourself. It's also much easier to secure a construction loan if you're working with an experienced builder who can include a statement, resume, banking and experience references, a line item cost breakdown of expected costs (an ECB), a materials list, and a construction contract.The contract should include:\n\n\nThe individual responsibilities of each party\nThe expected date of the beginning and ending of the project\nThe payment expected by the builder\nA completed Estimated Cost Breakdown (ECB), signed and dated\nProvisions for changes\n\n, After a site crew excavates the plot, you'll begin the work of laying the foundation. The type and design of the foundation will depend upon the size of your house, the ground in which its laid, local building codes, and whether or not your home will have a basement. The most recommended and strongest type of foundation is concrete block.\n\n\nThe excavation crew should first survey and stake the dimensions of the foundation and excavate it to the desired depth, then smooth it out to a workable surface, sometimes overlaying dirt or gravel to build upon., These are used to distribute the weight evenly and should be somewhat wider than the foundation walls, forming the perimeter of the home.\n\n\nBuild the form work and fill in with concrete. The form work is basically a mold for concrete, used to pour into and remove after the concrete has set. Alternatively, a block foundation can be laid which won't be removed, in which case you'll inlay rebar into the block and fill in the gaps in the block with concrete.\nThe thickness of the foundation should be determined carefully by a structural engineer, taking into consideration the height of the wall and the load it will be required to bear, both in terms of the building itself as well as the forces of gravity, wind, and earth that affect the structure.\n\n, This means putting either batter boards or corner stakes at each corner of the house foundation to level and square up the foundation. Use a transit or building level to make sure the building lines are level and square, and check by measuring corner to corner, diagonally, to make sure the walls and corners are square.\n\n, There are two common floor types, called \"slab on grade\" or \"pier and beam/joist\" floors. Before pouring the slab floor, you need to make sure you've installed rough plumbing lines so that they are accurately placed. After the slab is poured, it'll be too late to adjust.\n\n\nFor a slab-on-grade floor, form up the footing to the proper specs and lay rebar. Generally, these floors are made on concrete block foundations. After installing your plumbing rough-ins, backfill around the foundation with dirt and gravel, compacting it appropriately. At this point, you may also want to pre-treat for termites and install moisture barrier.\nFor off-grade or above-grade floors, lay out and install wooden flooring piers and install your floor joist framing system to the proper specifications. Install subfloor/finish floor decking.\n\n, You will need to lay out the wall lines on the floor, beginning at one corner, marking your bottom plate (called the rat sill) to attach to anchor bolts.\n\n\nAs you work, mark the location of doors, windows, and interior wall corners on the sill. Be sure to use special metal connectors/straps at the floor and tops of walls as required by code for storm and earthquake proofing.Use tees at wall intersections, substantial headers for openings in load bearing walls, and allow space at each rough opening for the feature to be installed.\n\n, Install sheathing if required. Otherwise, use sheet metal straps to diagonally brace all exterior wall corners. Make sure all studs (vertical framing members, usually 2 inch by 4 inch (5 cm by 10 cm) nominal lumber, graded standard or better) are securely nailed in place, straight and square to the wall line.\n\n, You may want to stick frame your roof, cutting and installing rafters and ceiling joists yourself (especially if you want a usable attic space). Prefab trusses, however, are engineered with lighter, smaller lumber for maximum strength. There are some trusses for attics with high-pitched roofs and dormers, as well as more traditional roofs. Research your options and choose something that works well for your home.\n\n, Generally, this means 24 inches (61.0 cm) apart from one another, sometimes 16 inches (40.6 cm) for stick-bracing structures. Attach hurricane clips or other connectors to secure them, plumb the center of each truss, and temporarily support them with a rat run bracing near the peak.\n\n\nInstall diagonal gable bracing for a roof with gable ends to prevent the roof frame from leaning when you install the roof decking. For a hip roof, install king rafters and hip rafters, being careful to keep the adjacent plane of the roof consistent and straight., Build outlookers to support the gable overhang and gable facia boards, if used. Deck the trusses or rafters with plywood, oriented strand lumber, or nominal lumber such as 1 x 6 inch (2.5 cm x 15 cm) tongue and groove boards.\n\n\nIn areas where high winds or snow-loading (accumulation) is possible, make sure the roof decking is secured and structurally able to withstand these severe forces and conditions. Use appropriate bracing and fasteners for this scope of work.\n\n, To make sure the elements don't set you back as you're working, it's important to install a moisture barrier on your roof even before it's completed. Use 15 or 30 pound (6.8 or 13.8 kg) roofing felt tar paper and simplex nails, roofing tacks, or plastic capped felting tacks to secure it. Begin felting the decking at the lower edge, allowing it to hang over slightly, and overlap subsequent layers to keep water from getting under this moisture barrier.\n\n, Many locations require some type of metal flashing to prevent water from penetrating the edges and the gables, but you may be able to seal them sufficiently with caulking if it is permitted and you are able.\n\n, You may choose painted sheet metal panels, rolled steel formed to lengths needed on site, or shingles, terra cotta tiles, or other materials, depending on your preference, costs, and products available at your location. Consider ridge vents, attic exhaust fans, vented dormers, and other architectural details which can increase the comfort of your house while decreasing cooling costs in hot climates.\n\n, These can be capped off to trim out after the walls are finished, especially if the local codes require pressure testing before finishing may be done.\n\n, Stub out your ductwork for return air and supply air registers. Insulate the ductwork if it is not pre-insulated, and seal all joints. Fasten ductwork as needed to prevent movement and ensure the your conduits are flush.\n\n, Most likely, there will be electrical outlets, light fixtures, and special wiring required for large appliances like water heaters, stoves, and air conditioning that will be necessary to do as soon as possible. Install the main electrical panel box, and any sub-panels your design requires, and install wiring from these to each device.\n\n\nCommonly, #12 Romex cable is used for ordinary lighting and outlet circuits, and nail-in electrical boxes are attached to the wall studs, with the front edge protruding to allow for the finished wall material to be flush.\n\n, Insulate walls where it is required. Depending on the climate, you will want to get location-specific guidelines for this area of work, as warmer climates will use substantially less insulation in the walls than warmer areas. Insulate the spaces between ceiling joists and walls.Walls are usually insulated with a minimum R-value of 13, and ceilings with a minimum of 19, but as much as 30, or even more for lowering fuel and utility usage.\n\n, Gypsum wallboard made of drywall or sheetrock is a common material used for this application, but there are other products including acoustical ceiling tiles, beaded plywood paneling (to simulate planking), and even natural wood lumber that are commonly used for creating solid ceilings.\n\n, Install the bathtub, shower enclosure, and any other large plumbing fixtures which will interface with finished walls. Make sure plumbing rough-ins are correctly located, and pipes are protected and securely anchored.\n\n, Traditionally, builders will use gypsum wallboard, wood, or masonite paneling for this purpose. Panels are generally jacked 3⁄8 inch (1.0 cm) above the floor to avoid moisture from floor spills and regular mopping when you clean the house. There are many interior wall products available, so the installation process will depend on the material used. Apply finish to gypsum wallboard, taping and skimming/floating all joints to an acceptable level of finish. Finish/texture any ceilings during this step if applicable.\n\n, Put up any trim you are using for baseboards, crown mouldings, and corners, and install your interior doors and jambs. If you are using natural wood trim and mouldings, you will want to paint the walls prior to this step. Pre-finishing the trim before installing will make the final finish easier, but any nail-holes will probably still need attention after installation.\n\n, Most likely, you will want to prime wallboard, then apply a finish coat. Use a paint roller where possible, cutting-in with brushes around appurtenances and in corners.\n\n\nBe sure to trim out the electrical devices, install lights and other fixtures, and install breakers in panel boxes if they were not pre-installed.\n\n, You will probably need at least basic kitchen storage cabinets and a bathroom vanity cabinet for a sink, other cabinets may include a bar, upper storage cabinets, and lower units with drawers for kitchen utensils and supplies.\n\n, Note that for carpet floors, base boards are installed prior to flooring, leaving 3⁄8 inch (1.0 cm) for the carpet to tuck underneath it. For hardwood or composite floors, this trim is installed after the floor is finished.\n\n, To start checking to make sure everything is working appropriately, activate the water and electricity to start experimenting with your handiwork. Adjust the jobs as necessary and work on finishing the house to a state at which you'll want to move in and start enjoying your new home.\n\n",
        "This paper applies HDP-HMM to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation. Although the technique itself is not novel, the application of this data-driven method to bioacoustics segmentation is quite challenging, and may yield some scientific findings, and this is a valuable contribution to the bioacoustics field. My concern for this paper is that it does not have fair comparison of the other simple methods including BIC and AIC, and it is better to provide such comparisons. Especially, as the authors pointed out, the computational cost of HDP-HMM is a big issue, and the other simple methods may solve this issue.",
        "Kroshka Island is the smaller of two ice-covered islands lying close together in the Fimbul Ice Shelf, along the coast of Queen Maud Land, Antarctica. The feature was first mapped by the Soviet Antarctic Expedition in 1961 and named \"Kupol Kroshka\", which means, which means \"crumb dome\" in Russian.\n\nSee also \n List of antarctic and sub-antarctic islands\n\nReferences\n\nIslands of Queen Maud Land\nPrincess Martha Coast",
        " This may include not just the raw cost, but also the difference between how much your coverage costs the company and how much it would cost you to get the same coverage. This alone can represent the majority of value when you calculate benefits.;\n, Some examples include vision, dental, long-term disability, short-term disability and life insurance. Although the individual cost of these benefits is often relatively low, the value comes from the often sharply discounted rates as compared to buying these policies as an individual.\n\n, If working this out to determine actual costs or value, use matching for funds that were actually contributed and not the maximum matching you actually did.\n\n,, These might include deposits into a flexible or health spending account, the cost of professional licenses, education expenses and child care expenses. Some companies also reimburse the cost of parking or a gym membership.\n\n, This can be tricky, as the real value often isn't clear until the stock is sold years later.\n\n, If this amount varies considerably, take an average of recent payments.\n\n,,, If you're not sure, check with your accountant or tax lawyer.\n\n,, Add these tax costs, and calculate the benefits value for this third category of benefit.\n\n,",
        "From August 22 to August 24, 1992  violent xenophobic riots took place in the Lichtenhagen district of Rostock, Germany; these were the worst mob attacks against migrants in postwar Germany. Stones and petrol bombs were thrown at an apartment block where asylum seekers lived. At the height of the riots, several hundred militant right-wing extremists were involved, and about 3,000 neighbourhood onlookers stood by, applauding them.\n\nThe initial response of authorities and politicians was heavily criticised. For some days prior to the riots, veiled warnings of impending trouble had been posted in some newspapers. Police and politicians seemed reluctant to respond, and, when they did, their responses were considered inadequate. Outside the building where the refugees were housed, several hundred asylum seekers had been camping for days with little or no access to basic facilities. This was contributing to escalating tensions in the neighbourhood.\n\nBetween 22 and 26 August 1992, there were 370 provisional arrests and 408 preliminary investigations related to the riots. Among the arrested were 110 people from former West Germany; 217 from the state of Mecklenburg-Vorpommern, including 147 from Rostock; and another 37 from the former East Germany. During the riot, 204 police officers were injured. No one was killed.\n\nBackground\n\nThe Zentrale Aufnahmestelle für Asylbewerber für Mecklenburg-Vorpommern (ZAst M-V), or \"Central Refugee Shelter\" for the state of Mecklenburg-Vorpommern, was in an 11-storey plattenbau apartment complex known as the \"Sunflower House\" or \"Sunflower Tower\", because of the large sunflowers decorating one side. The building was notorious for the inhumane conditions under which the asylum seekers there were living and the lack of support (if any) provided for them. The authorities had ignored  numerous complaints from citizens and residents over the filthy and deplorable conditions in the apartment building.\n\nRioting\nThe shelter was originally intended to house 300 refugees a month, but by summer 1992 it was averaging 11,500 refugees per month. Primarily Roma from Romania, they were left by overstretched shelter staff to camp out in front of the building for days at a time. The municipal government refused to provide portable toilets and there was no water or garbage collection. Clashes between the homeless migrants and the Lichtenhagen residents increased. Neither the city nor the state government took action.\n\nFor days prior to the riots, the newspapers Norddeutsche Neueste Nachrichten and Ostsee-Zeitung had been calling for a \"Lichtenhagen interest group\". There were anonymous warnings that if by the weekend, the refugee shelter was not \"cleaned up,\" order would be made. This gave young gang members from every corner of Rostock, normally enemies, a date, place and purpose to congregate. One 19-year-old skinhead said, \"The police know the Rostock Skins and 'Hools' [hooligans]. When something like this is announced, we're there!\"\n\nThe first day's riot started with young people from the neighbourhood throwing stones. This was contained by the police, but media coverage encouraged neo-Nazis to roam the area. This led to a situation where a xenophobic mob outnumbered the police by day three. The original target, the asylum accommodation, was evacuated on the second day, whereupon the mob stormed a neighbouring building in which 115 Vietnamese immigrants, a social worker and a ZDF television crew had mistakenly been left behind. While the building burned, they barricaded their doors against rioters, who were climbing over the balconies armed with baseball bats and Molotov cocktails. Below, a mob of over 3,000 spectators eagerly watched and applauded.\n\nCharges of police and political incompetence were levelled from the beginning. One explanation cited for the lack of effective action by the police was that they were reluctant to take any action which might have been reminiscent of the recently cast-off communist police state. There were also charges that police and politicians were privately sympathetic to the anti-migrant sentiment.\n\nThe first major conviction relating to the riots was on 4 March 1993, though 24 convictions on lesser charges had already been handed down. A 22-year-old man was convicted of throwing a firebomb at police, of seriously disturbing the peace, violating weapons laws and attempted bodily harm. An attempted murder charge was dropped for lack of evidence. Critics complained that no one was convicted of assaulting a foreigner, only of assaulting the police or of disorderliness. It took almost ten years to prosecute 408 people.\n\nThe following timeline was reconstructed by the \"Legislative Committee to Investigate the Refugee Shelter Incident\" (\"Parlamentarischer Untersuchungsausschuss zu den Ereignissen um die ZAst\").\n\nTimeline\n\nAugust 22, Day 1\nFrom about 6:00 p.m. a large crowd assembled in front of the refugee shelter. At 8:02 p.m., thugs started attacking the shelter and violence escalated quickly. By 10:46 p.m., police were forced to retreat from the area. At 11:02 p.m. riot police arrived on the scene and were attacked with Molotov cocktails. At 11:24 p.m. another police unit arrived from Schwerin. At 1:34 a.m. water cannons were set up and put to continuous use. Between 1:34 and 2:34 a.m., the rioters were pushed towards the autobahn. At 2:25 a.m. a water cannon vehicle was set on fire by a Molotov cocktail. At 2:30 a.m. Rostock police command declared a police emergency and the armoury was opened. Officers were issued tear gas and fired at the crowd. The situation calmed down by 5:30 a.m.\n\nDay 1 Statistics: 160 police officers, 300 rioters, 13 police officers injured, nine arrests.\n\nAugust 23, Day 2, Part 1\n\n100 people gathered in front of the shelter. At 11:15 a.m. Rostock police department requested back-up from other police departments. Squads responded from Schwerin, Anklam, Stralsund and Güstrow. The Landespolizei force of the state of Mecklenburg-Vorpommern sent two additional water cannons. Two reserve units (Einsatzhundertschaften) from the Federal Border Patrol (Bundesgrenzschutz) were activated. At 2:15 p.m. plainclothes officers reported the arrival of 30 known neo-Nazis in the area.\n\nAugust 23, Day 2, Part 2\n\nAt 6:45 p.m. about 400 rioters started attacking the shelter. At 7:18 p.m. the rioters started throwing Molotov cocktails. 8:00 p.m. The police used water cannons to clear the area. At 8:30 p.m. police resorted to firing live ammunition. At 10:00 p.m. the police officer in charge reported that without reinforcements, within 30 minutes, the situation would be impossible to control. At 10:30 p.m. a police car was set on fire. At 10:41 p.m. state police declared a state of emergency (Landespolizeialarm).\n\nThe state level of emergency allowed for additional federal brigades to be called in.  Hamburg sent out its SWAT (SEK and MBK) units. These riot police units of about 100 officers each were reinforced by two police dog squads from Kiel, a reserve unit from Lübeck, and helicopters from the federal police.\n\nAt 2:55 a.m. The second Hamburg unit arrived on the scene. At 3:45 a.m. the first Hamburg unit arrived. At 4:10 a.m. the situation quieted down. The Hamburg units took over the night watch.\n\nDay 2 Statistics: 850 police officers, 500 rioters, 70 police officers injured, 130 arrests.\n\nAugust 24, Day 3, Part 1\n\n2:00 p.m. Under the protection of the Hamburg units (now 16 hours in action), the shelter was evacuated. The large crowd of onlookers gave notice of a melee at 4:00 p.m. The police learned of a telephone network that hooligans were using to organize the melee, which was to attack the police exclusively if the shelter was cleared out.\n\nAugust 24, Day 3, Part 2\n\n7:45 p.m. Reinforcements from the 4th Brigade, Mecklenburg-Vorpommern arrived to replace the Hamburg riot units, now in action for 21 hours. 7:55 p.m. Ten minutes after the replacements arrived, an order was given to withdraw all protection for the shelter.\n\nAt 8:00 p.m. during the retreat from the building, Federal Border Patrol units reinforcing the second Hamburg unit came under attack. At this point, the crowd of cheering onlookers had grown to about 3,000. At 8:05 p.m. squads from the second Hamburg unit, which had already left the scene, were ordered back to reinforce the Border Patrol unit under attack. In order to push through the crowd, they resorted to using batons. The commanding officer of the first Hamburg unit reported that the threat of violence was higher than what he had seen in his five years of experience in Hamburg's rioting hotspots, Hafenstraße and Flora.\n\nAt 8:15 p.m. The first Hamburg unit and the 4th Brigade MV reached the other units. Water cannons were used and police cordons were formed. Rail transport police were radioed for back-up. The alarm was \"Officers in distress.\" The first Hamburg unit also provided support.\n\nAt 8:40 p.m. a technical problem knocked out one water cannon. At 9:20 p.m. the retreating first Hamburg unit sought cover from the water cannon of the 4th Brigade MV. At 9:34 p.m. The water supply of the second cannon ran out. The 4th Brigade MV of about 100 men was up against 800 rioters. At 10:37 p.m. the 4th Brigade MV formed a police cordon and aimed the water cannon at the crowd to allow the fire department to get through. At 10:55 p.m. the first Hamburg unit was sent back to Hamburg after 25 hours of duty. At midnight the 4th Brigade MV began clearing out the remaining roughly 300 hooligans, while the second Hamburg unit was sent back to Hamburg after 26 hours of duty. At 12:30 a.m. the area settled down.\n\nAt 2:00 a.m. 400 hooligans again began to storm the refugee shelter, using every means possible. The police had a strong presence and up to 7 water cannons were used to clear the streets surrounding the apartment complex. About 1,000-1,200 rioters took part in the melee with the police. By 3:00 a.m., the situation was under control.\n\nDay 3 Statistics: 2050 police officers, 2000 rioters, 117 police officers injured, 58 arrests.\n\nCopycat acts\n\nIn the week after the riots in Rostock, neo-Nazis attacked 40 residences with firebombs and stones, and fought street battles with the police.  In Mecklenburg-Western Pomerania in the following few days, the asylum centers in Wismar, Rostock-Hinrichshagen, Lübz, and Neubrandenburg were attacked, and there were three such incidents in Greifswald. In Wismar there was a six-day riot between 15 and 20 September in front of the asylum center, whereas in Lichtenhagen there was applause from local residents. Even after that, there were attacks almost daily. On one weekend between Friday 18 September and Sunday 20 September, asylum centers in Güstrow Ueckermünde Kröpelin, Schwarzendorf (in the district of Malchin), Schwerin, Wismar, and Retschow were repeatedly attacked with Molotov cocktails.\n\nLegal proceedings\n\nThe attacks led to 370 arrests and 408 preliminary investigations. Prosecutions proved very difficult, as there was little reliable evidence. Overall, the legal process was judged to have been remarkably slow and consequences mild. \n\nCases were brought before the Regional Court of Rostock against 257 persons, most of which were dropped. Only 40 young people in 1993/94 were charged with rioting and arson. Most were given fines and suspended sentences. Eleven of those convicted were sent into youth custody ranging from seven months to three years, but only four of them were actually incarcerated between two and three years; the other seven sentences were suspended. It took ten years after the riots for the last three cases to be concluded. The sentences were for 12 to 18 months in juvenile detention, or probation, although the then 17-, 18- and 19-year-olds convicted of assault were sentenced not only for arson, but for attempted murder. The vast majority of those involved in the rioting remained anonymous and unpunished, despite the whole three days of rioting having been filmed by national German television, by the BBC, and other foreign news broadcasters.\n\nAn investigation against Rostock police chief Siegfried Kordus was discontinued in 1994. A case was made against the leader of the police operation, Chief Superintendent Jürgen Deckert for criminally negligent arson by omission, but the case was dropped in 2000.\n\nMedia\n Mark Saunders. \"The Truth Lies in Rostock (AVI, 698,7 MB)]\" (BRD, Great Britain, 78 min.) /  1993 von Mark Saunders & Siobhan Cleary - 121 min\n Wir Sind Jung. Wir Sind Stark (We Are Young. We Are Strong), a 2014 film based on the riots\n\nSee also\n Riot of Hoyerswerda, 1991 xenophobic riots of neo-Nazis attacking Vietnamese\n Solingen arson attack of 1993\n 1993 Hădăreni riots of Romanians and Hungarians against Roma\n 2006 Ferentari riot\n\nReferences\n\nExternal links\n \"Pogrome in Rostock-Lichtenhagen\" Upheaval Picture Archive. Accessed Feb. 20, 2010 \n \"Recommended Resolution and Interim Report\" (PDF) Legislative Board of Inquiry of Mecklenburg-Vorpommern, June 16, 1993 ( Untersuchungsausschuss des Landtags Mecklenburg-Vorpommern: Beschlussempfehlung und Zwischenbericht) \n Liane von Billerbeck. \"I was part of the pack\" Die Zeit, No. 25 (2002) \"Ten Years after the Lichtenhagen Pogrom: Offenders who are turned into martyrs, a murder charge and an uncertain verdict\". Accessed Feb. 20, 2010 \n Christoph Koch. \"The Sunflower House: Rostock-Lichtenhagen 15 years after the riots, searching for traces\" (originally published as a Stern/NEON report) October 26, 2007. Accessed Feb. 20, 2010 \n Dieter Wulf. \"Applause for incendiary acts\" (Rich Text Format) Transcript of radio broadcast, Deutschlandfunk, April 9, 2002. (\"Beifall für Brandsätze,\" Deutschlandfunk) \n\nProtests in Germany\n1992 in Germany\nRiots and civil disorder in Germany\n1992 riots\nEthnic riots\nRacism in Germany\nRomani-related controversies\nAntiziganism in Europe\nNeo-Nazi attacks in Germany\nAugust 1992 events in Europe\n1990s in Mecklenburg-Western Pomerania",
        "General surgeon here.\n\nLarger people have larger organs in general for the reasons listed above, I’ve never read any studies on it, it’s just what I see at work.  The cavities in your bodies adjust to the need for the organs IN MOST SITUATIONS.\n\nSymptomatic Pectus excavatum is a good example of what happens when it doesn’t.   \n\nAnd what happens when you have more space that needed?  Well most spaces have atleast once size that is soft tissue that will decrease in size.  But, for instance, in brain atrophy like in old dementia patients or alcoholics, the brain is actually a little “loose”, and can slosh more.  Fluid will fill around it.  You’re body never fills empty cavities with air, it’s always fluid if anything.\n\nHere’s another example.  In the abdomen sometimes we have to do really big surgeries like remove half the organs for a big cancer.  At the end all we do is close the abdomen as normal.  The the abdominal cavity will slowly shrink down some, all air will be absorbed (can take a month if open air, just a few days if laparoscopic).  They may get a little extra fluid in their abdomen.\n\nIn terms of “making space”. People have lots of extra space inside them, especially in the abdomen.  As people get fat, they can store so much fat inside their abdomen.  So, so much.  It can make my job very difficult.\n\nEven on fat people, I’m talking like BMI of 60, I can still put around 3-4 liters of air in the abdomen to do laparoscopic surgery.  The inside of their abdomen will expand with their need for space as long as the need for space happens slowly (over months, not over days.  Google abdominal compartment syndrome’ for what happens if they need a lot of space over days).\n\nHope that makes sense.  I added paragraphs out of order so hopefully didn’t repeat myself too much \n\nEdit: to the dude that says that air is also a liquid and I should use the word “fluid”.......get a life.   Good lord",
        "  A solution to the equivalence problem in three-dimensional gravity is given\nand a practically useful method to obtain a coordinate invariant description of\nlocal geometry is presented. The method is a nontrivial adaptation of Karlhede\ninvariant classification of spacetimes of general relativity. The local\ngeometry is completely determined by the curvature tensor and a finite number\nof its covariant derivatives in a frame where the components of the metric are\nconstants. The results are presented in the framework of real two-component\nspinors in three-dimensional spacetimes, where the algebraic classifications of\nthe Ricci and Cotton-York spinors are given and their isotropy groups and\ncanonical forms are determined. As an application we discuss Goedel-type\nspacetimes in three-dimensional General Relativity. The conditions for local\nspace and time homogeneity are derived and the equivalence of three-dimensional\nGoedel-type spacetimes is studied and the results are compared with previous\nworks on four-dimensional Goedel-type spacetimes.\n",
        "  Boolean Networks and their dynamics are of great interest as abstract\nmodeling schemes in various disciplines, ranging from biology to computer\nscience. Whereas parallel update schemes have been studied extensively in past\nyears, the level of understanding of asynchronous updates schemes is still very\npoor. In this paper we study the propagation of external information given by\nregulatory input variables into a random Boolean network. We compute both\nanalytically and numerically the time evolution and the asymptotic behavior of\nthis propagation of external regulation (PER). In particular, this allows us to\nidentify variables which are completely determined by this external\ninformation. All those variables in the network which are not directly fixed by\nPER form a core which contains in particular all non-trivial feedback loops. We\ndesign a message-passing approach allowing to characterize the statistical\nproperties of these cores in dependence of the Boolean network and the external\ncondition. At the end we establish a link between PER dynamics and the full\nrandom asynchronous dynamics of a Boolean network.\n",
        "Three knowledgable reviewers recommend rejection and there was no rebuttal. The AC agrees with the reviewers.",
        "Holiday Report (German: Urlaubsreport - Worüber Reiseleiter nicht sprechen dürfen) is a 1971 West German comedy film directed by Ernst Hofbauer and starring Sybil Danning, Astrid Frank and Werner Abrolat. It was produced by Wolf C. Hartwig's Rapid Film, in an attempt to capitalise on the success of the company's hit Schoolgirl Report series.\n\nCast\n Sybil Danning as Ina, die Anhalterin \n Astrid Frank as Andrea \n Werner Abrolat as Wieland \n Laurence Bien as Miguel \n Josef Fröhlich as Ignaz Schneider, der Urlaubspfarrer \n Max Grießer as Xaver \n Wolf Harnisch as Fred \n Hans Hass Jr. as Maxl \n Helen Vita as Gitta Mitterer \n Ralf Wolter as Horst-Dieter Mitterer \n Harald Baerow as Steinlechner \n Nadine De Rangot as Karla \n Karin Götz as blonde Nichte von Tante Paula\n Josef Moosholzer as Wirt des Gasthofs, wo Ina mit Jürgen übernachtet \n Gernot Möhner as Franz \n Juliane Rom-Sock as Helga \n Marianne Sock as Helga \n Véronique Vendell as Tisch-Tänzerin bei Après-Ski Party \n Horst Heuck as Jürgen \n Monika Rohde as Püppi \n Evelyne Traeger as Renate Wiesbeck\n Margot Mahler\n Elisabeth Volkmann\n Jochen Mann\n Oliver Domnik\n Michael von Harbach\n\nReferences\n\nBibliography \n Bergfelder, Tim. International Adventures: German Popular Cinema and European Co-Productions in the 1960s. Berghahn Books, 2005.\n\nExternal links \n \n\n1971 films\n1970s sex comedy films\nGerman sex comedy films\nWest German films\n1970s German-language films\nFilms directed by Ernst Hofbauer\nFilms about vacationing\nConstantin Film films\n1971 comedy films",
        "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format stating in the header \"submitted\" instead of \"published\" for your submission to be considered. Thank you!",
        "Komlan Agbeko Assignon (born 21 January 1974) is a Togolese former professional footballer who played as a midfielder.\n\nAssignon has played with a number of French clubs, including AS Cannes, AS Beauvais Oise and US Créteil-Lusitanos. His final year of play was with Al Jahra in Kuwait.\n\nAssignon capped for the Togo national team at the 1998, 2000 and 2002 African Cup of Nations. In total he got 20 caps and four goals.\n\nPersonal life\nAssignon's son, Lorenz Assignon, is a professional footballer for Rennes.\n\nReferences\n\n1974 births\nLiving people\nSportspeople from Lomé\nAssociation football midfielders\nTogolese footballers\nTogo international footballers\n1998 African Cup of Nations players\n2000 African Cup of Nations players\n2002 African Cup of Nations players\nAS Cannes players\nAS Beauvais Oise players\nUS Créteil-Lusitanos players\nAl Jahra SC players\nLigue 1 players\nLigue 2 players\nKuwait Premier League players\nTogolese expatriate footballers\nExpatriate footballers in France\nTogolese expatriate sportspeople in France\nExpatriate footballers in Kuwait\nTogolese expatriate sportspeople in Kuwait\n21st-century Togolese people",
        " You will need to enter your first and last name, your email address, your birthday, and your gender. You will also need to come up with a password. Facebook accounts are free.\n\n\nYou will log in to Facebook with the email address you provide.;\n, You will need to enter the captcha phrase to prove you’re human. After clicking Sign Up again, the account creation process will begin.\n\n, Before you begin creating your profile, Facebook will suggest people to add as friends. If you don’t recognize them, feel free to skip to the next step of the process. You will then be asked if you want to scan your email accounts for any other Facebook users. Again, you can skip this process if you’d like and just add friends through Facebook itself later.\n\n, This is completely optional, but will help others find and connect with you.\n\n, It’s called Facebook for a reason! You can upload a photo from your computer or take a photo of yourself using your webcam.\n\n, Once you’ve finished the account creation process, you will be taken to your Newsfeed. At the top of the page, you will see a message notifying you that an activation email has been sent to the address you signed up with.Open the email and click on the verification link. You will need to enter the code that is included in the message.\nOnce your account is verified, you can begin using Facebook.\n\n, Click the “Edit Profile link underneath your picture in the top-left corner. Add the information that you want to share. You can add a lot of information to your Facebook account. Some profile options include:\n\n\nRelationship Status\nHometown\nCurrent Home\nFavorite Quotes\nReligious Views\nPolitical Views\n\n, When you add information to your profile, you can click the Privacy button to the right of the entry. This will allow you to choose who can see what you’ve shared. You can share with your Friends, the Public, and specific lists (groups) of friends.\n\n, Click the “Privacy Settings” option to open the Privacy Settings and Tools page.\n\n, This menu will allow you to change the basic privacy settings that affect your whole Facebook account. Because of the social nature of Facebook, people tend to post a lot of personal information. Customizing your settings can help you keep personal information private. Be sure to review this section carefully so that you know who can see what.\n\n\nYou can set the default for who can see any future posts you make (this can be changed for individual posts when you make them).\nYou can change who can send you a friend request. By default, this is set to Everyone, but you can change it to Friends of Friends, which means that the person who sends the request has to be Facebook Friends with someone on your Friends list.\nYou can change who can look you up by email address and phone number.\nYou can tell search engines to not link to your timeline. This will prevent your timeline from showing up in web searches.\n\n, In the left menu underneath the Privacy option, click Timeline and Tagging. This menu will allow you to control who can post to your timeline, who can see things on your timeline, and how you want tagging to be managed.\n\n\nTagging is the act of adding links to other people in photos and posts.\n\n, Facebook is all about making connections. Whether it’s keeping in touch with your cousin across the country or catching up with an old schoolmate on the other side of town, your Facebook experience will improve by adding connections.\n\n\nEnter a person’s name, email address, or phone number into the search bar at the top of the page. This will return a list of results. Click the result from the list, and then click the “Add friend” button. This will send a message to the person asking to confirm your friend request. You can include a personalized message with your friend request.\nWhen the other person accepts the request, you will become friends with them and will have access to their timeline.\n\n, When you click the Search bar, the first option will be “People I may know”. Click on this to open a list of people that you may know, based on people your friends know. This tool becomes more useful after you have a few friends added.\n\n, You can create custom lists, or groups, of friends that let you easily share with specific people. In the left menu, hover your cursor over the Friends heading and click MORE. This will open a list of your Friends Lists.\n\nClick the “+Create List” button to create a new list. You can create lists to help separate family from friends, and business from both.\n\n\n\n\n\n\nEnter the list name and add some members. As you type names, you will see suggestions for matches. This will let you quickly add multiple people.\n\n\n\n\n\n\nYou can add new and existing friends by clicking the Friends button at the top of their profile and selecting the list you want to add them to.\n\n, As you start connecting with other people on Facebook, you’ll start receiving friend requests from other users. You can see your pending requests by clicking on the Friends icon on the top of the Facebook screen.\n\nNext to each request you will see the number of mutual friends you have with that person.\n\n\n\n\n\n\nYou can click the person’s name to see their basic profile.\n\n\n\n\n\n\nClick the Confirm button to add the person as a friend, or the Not Now button to ignore the request.\n\n\n\n\n\n\n\n, If you no longer want to stay connected with someone, you can unfriend them so they no longer have access to your information and posts. To do this, click on the friend to open their profile, click the Friends button at the top of their page, and then select Unfriend.\n\n, To make a post, click the field at the top of your Newsfeed or Profile. You can start typing what you want to share. When you are ready to post, click the blue Post button.\n\n\nClick the Privacy button next to the Post button to choose who can see your post.\n\n\n\n\n\n\nTyping in a “@” symbol followed by a friend’s name will bring up the option to tag that person in your post. They will receive a notification that you’ve tagged them in your post.\n\n\n\n\n\n\nAdd a location to your post by clicking the Pin icon at the bottom of the post box.\n\n\n\n\n\n\n\n, In your newsfeed, you will see a list of your friends’ posts. Underneath each post will be a box with your profile picture and a text field that says “Write a comment”. Click the field to add your comment. You can tag other users in your comment just like you would a regular post.\n\n\nYou can “Like” a post to show that you think it is a good post worthy of being seen. Posts that get Liked end up at the top of your friends’ newsfeeds more often, and can help posts spread across Facebook.\n\n, You can link to material all over the web by using your status update. Simply paste the URL for the site you want to link to and you will see a preview appear below the text field. You can then delete the link and the preview link will remain.\n\n\nYou can link just about anything, including YouTube videos, blog articles, images, and more.\n\n, You can search for artists, musicians, businesses, organizations, and more using the Facebook search bar. You can then “Like” these pages to receive updates on your newsfeed when they are posted. A lot of times, you can receive deals and early releases by being a Facebook fan.\n\n, This will show two options: Upload Photos/Video and Create Photo Album. Clicking on either of these will open a file browser where you can browse for photos and videos on your computer.\n\n\nYou will be given the option to add additional photos after you select your first one.\nYou can tag friends and write a comment or description of the photo(s).\nThis method makes your photos a post on your timeline.\n\n, If you choose the Photo Album option, you will be able to upload multiple photos into one album. The album editor will open and you will be able to give the album a name and a description.\n\n\nYou can also add a location for the album.\n\n\n\n\n\n\nIf you want to add the photos at their original quality, click the High Quality box at the bottom of the album editor. Your uploads may take longer.\n\n\n\n\n\n\nClick the “+Add More Photos” button to find more photos on your computer to add to the album.\n\n\n\n\n\n\n\n, If you want to remove a photo that you’ve uploaded, you can do so from the photo manager. Click the Photos link in the left menu underneath your profile picture. This will open up all of your photos and albums.\n\n\nNavigate to the photo that you want to delete.\nClick the pencil button in the top-right corner of the photo preview. This will open a menu. Click the “Delete This Photo” option to remove the photo from your uploads.\n\n, Facebook allows you to create events which you can then send out invitations for or make public. To create an event, click the Events link in the left menu. At the top-right corner of the Events page, click the “+Create Event” button.\n\n\nSet a name for the event.\nAdd details about your event. Let guests know what to expect or bring.\nSet the date for the event. You can use the calendar button to open a calendar and choose a date. You can also specify a start and end time.\n\n, Before you click Create, set the privacy to the settings that best suit your needs. If you’re throwing a birthday party you probably want to keep it to invite only, but if you’re throwing a concert you’ll probably want to set it to Public.\n\n\nIf you choose Invite Only, you can choose to allow guests to invite others.\n\n, If your event is Invite Only, then it’s time to send out the invitations. Click the link to “Invite Friends” and a window with all of your Facebook friends will appear. Check the box for each friend that you want to send an invite to.\n\n\nYou can invite people after the event is created as well.\n\n, Once your event has been created, you can visit in in the Events menu. On your Event page, you can post messages to the people that are attending, change the date and time, invite more people, and more.\n\n, If you’ve received an event invite, you will see it in your Notification window. You will have a few different options on how you’d like to respond to the invite. You can say you’re going, say “Maybe”, or decline the invite.\n\n, This will open a small chat window at the bottom of the page. You can send messages to other Facebook members, even if they are not online.\n\n, If you want to create a group chat, click the Friends icon with a “+” sign at the top of the chat window. You can then designate additional people that you want to add to the chat.\n\n, Click the Gear icon and select Add Files. This will open a file browser where you can browse for files on your computer. The file size limit is 15 MB.\n\n, Click the Messaging icon at the top of the Facebook page and then click “See All” at the bottom of the list. This will open your Inbox. Use the left frame to scroll through all of your past messages.\n\n\nTo archive a conversation, hover your cursor over it in the left frame and then click the “X” icon that appears. This will remove the conversation from your Inbox. The conversation will still exist on the Facebook servers.\n\n\n\n\n\n\n\n, Remember not to give out personal information to strangers, and make sure you treat everyone with the courtesy and respect they deserve.\n\n, If you feel like you have a problem spending too much time on Facebook, this article can help you get it under control., Facebook can be a massive time sink, and is especially damaging to productivity at work. Follow this guide to stay on task. If you can’t stay away, you can block access to your account for the times you are supposed to be working.\n, Have you been playing too much Farmville? This article will help wean you off those addicting Facebook games., If you decide if Facebook is no longer right for you, this article will show you how to permanently delete your profile., If you have a Facebook page for your business, organization, art, or anything else, follow this guide to gain more fans. More fans will mean more exposure for your work., Facebook is a powerful tool that you can use to advertise your business to millions of potential customers. This guide will show you how to get started., If you use the Blogger blog service, you can add a Facebook like button to your blog, which can help you drive more traffic to your blog. Follow this guide to learn out how.",
        "  Quantum key distribution (QKD) offers an unconditionally secure means of\ncommunication based on the laws of quantum mechanics. Currently, a major\nchallenge is to achieve a QKD system with a 40 dB channel loss, which is\nrequired if we are to realize global scale QKD networks using communication\nsatellites. Here we report the first QKD experiment in which secure keys were\ndistributed over 42 dB channel loss and 200 km of optical fibre. We employed\nthe differential phase shift quantum key distribution (DPS-QKD) protocol\nimplemented with a 10-GHz clock frequency, and superconducting single photon\ndetectors (SSPD) based on NbN nanowire. The SSPD offers a very low dark count\nrate (a few Hz) and small timing jitter (60 ps full width at half maximum).\nThese characteristics allowed us to construct a 10-GHz clock QKD system and\nthus distribute secure keys over channel loss of 42 dB. In addition, we\nachieved a 17 kbit/s secure key rate over 105 km of optical fibre, which is two\norders of magnitude higher than the previous record, and a 12.1 bit/s secure\nkey rate over 200 km of optical fibre, which is the longest terrestrial QKD yet\ndemonstrated. The keys generated in our experiment are secure against both\ngeneral collective attacks on individual photons and a specific collective\nattack on multi-photons, known as a sequential unambiguous state discrimination\n(USD) attack.\n",
        "From my original comments:\n\nThe results looks good but the baselines proposed are quite bad.\n\nFor instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...\n\nIn CIFAR-10 experiments, i do not understand  why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. \n\nAgain i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. \n\nIn fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.\n\n----\n\nThe authors reply still does not convince me.\n\nI still think that the same technique should be applied on more challenging scenarios.\n\n",
        "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It’s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing.",
        "  We present the studies of the decays $B\\to a_1(1260) \\pi$ and $a_1(1260) K$\nwithin the framework of QCD factorization. Due to the G-parity, unlike the\nvector meson, the chiral-odd two-parton light-cone distribution amplitudes of\nthe $a_1$ are antisymmetric under the exchange of quark and anti-quark momentum\nfractions in the SU(2) limit. The branching ratios for $a_1 \\pi$ modes are\nsensitive to tree--penguin interference. The resultant ${\\cal B}(B^0 \\to\na_1^\\pm \\pi^\\mp)$ are in good agreement with the data. However, using the\ncurrent Cabibbo--Kobayashi--Maskawa angles, $\\beta=22.0^\\circ$ and\n$\\gamma=59.0^\\circ$, our results for the mixing-induced parameter $S$ and\n$\\alpha_{\\rm eff}$ differ from the measurements of the time-dependent CP\nasymmetries in the decay $B^0\\to a_1^\\pm \\pi^\\mp$ at about the $3.7\\sigma$\nlevel. This puzzle may be resolved by using a larger $\\gamma \\gtrsim 80^\\circ$.\nFor $a_1 K$ modes, the annihilation topologies give sizable contributions and\nare sensitive to the first Gegenbauer moment of the leading-twist tensor\n(chiral-odd) distribution amplitude of the $a_1$ meson. The $B\\to a_1 K$\namplitudes resemble the corresponding $B\\to \\pi K$ ones very much. Taking the\nratios of corresponding CP-averaged $a_1 K$ and $\\pi K$ branching ratios, we\ncan extract information relevant to the electroweak penguins and annihilations.\nThe existence of new-physics in the electroweak penguin sector and final state\ninteractions during decays can thus be explored.\n",
        "This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.\n\nThis is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.\n\nMy second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.\n\nOverall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)\n\nRelated work:\nI think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.\n\n“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear. Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that. “distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.\n\nSection 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.\n\nThe use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).\n\nFinally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).\n\nThe relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.\n\nThe section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.",
        "Ptychopseustis eutacta is a moth in the family Crambidae. It is found in Australia, where it has been recorded from Queensland.\n\nThe wingspan is about 13 mm. The forewings are whitish-ochreous with some brownish-ochreous suffusion and a paler median area. There are some dark-fuscous scales on the costal edge, as well as a broad dark-fuscous transverse line containing some whitish scales. The hindwings are whitish with a\nfaint fuscous subterminal line.\n\nReferences\n\nCybalomiinae\nMoths described in 1908",
        " Options range from smartphone cameras to professional camcorders. Choose one that is appropriate for your channel. If your channel gets a lot of views, you might want to use a higher quality video recorder. Similarly, if your channel is not very popular, it might not be worth buying a professional camcorder.\n, Editing is everything. Higher quality videos lead to higher viewership, so choose a good quality editing suite., Tripods, zoom lenses, and other equipment can improve the quality of your video, but are not necessities., The topic of your video is a very important factor determining your viewership. Choose a topic you are knowledgeable in., Date and time is very important. Choose a time which suits you., Venue generally depends on what kind of video you are shooting. If you are shooting a video on how to be a good gardener, the venue could be a garden. If you are shooting a video on how to cook, you could shoot the video in your kitchen., No one wants background sound interfering with their video. It can result in your channel losing viewership., Choose somebody trustworthy, and good with a camera., Seek help from the cameraman and friends or family on this crucial aspect of your video., Don't harbor any feeling of tension, or anxiety.,, Place the camera on a tripod, if you are using one, set up the lighting, and set up any other equipment you are using., This can enhance the quality of your video., Don't shoot your entire video in one sequence. Switch between scenes to make a high quality educational clip., Exhaustion can lower your videos quality., Imagine that the video you are shooting is like a book, with multiple sections and pages. Move step by step, filming the different sections of the video, like the things intro section, things you'll need section, and the steps section., Ask the viewer a question in the end of the video, and ask them to reply in the comments. This can bring back viewers., Take all your equipment back inside your studio, and begin the next phase, which is editing the video., Move it to your computer to being editing it with the video editor you chose.,, You should have good vision of your finished video in your head at this point, so work towards it., This can brand your videos and impress viewers. You can choose either an intro or an outro, or both., Remove any accidental bloopers or goof ups., Captions can help your viewers clearly understand what you are saying. You can either add your captions directly into the video, or upload them as a text file later, which has the added benefit of letting the viewer turn off the captions., You shot your video in sections, so arrange the sections in order., This prevents reuse of your video without credit., This is optional, but is a nice way to get your name out there., Remember to choose a YouTube recognizable format. This can take quite some time, so remember to be patient. After you are done, move on to the next section, which is uploading your video., If you don't have a channel yet, create one., Depending on the quality of your video and internet connection, this can take a long time., It is best to choose the shortest and most accurate way of describing your video's content., The first few lines of your video's description are the most important, as they are seen by every viewer. The rest of the lines can contain the not so important information, such as the credits, since they are hidden under the 'Show more' button. Your description should contain call to action text, and links to any sources or citations., Your thumbnail is what shows up next to your video's name in search results, and is what viewers see. It should be something that makes viewers eager to explore the rest of your video. Set something which is different, since your video will show up next to many others in searches., The category must be the most specific category possible, and it is what helps users discover your video while browsing YouTube., Tags are what link your video to others, so set ones that are relevant., Fine tune other details and publish your video, and then move on the next section., Act as your own critic, and see what areas you can improve on. This helps enhance the quality of your next video., This can help you earn money from your hard work., Social media can help boost your video's views, due to shares and re-shares. If your video is good enough, it might even go on to become the next big thing!, Make a note of how many people are viewing your video and from where, and other details. This can greatly help in creating your next video., This helps link your video to your website, and bring in more viewers., wikiHow allows editors to embed high quality, instructional videos on related articles. Embedding your video on wikiHow can greatly increase viewership., If your video gets popular, make another how to video!",
        "Well, the reason why it's called the \"Third Reich\" is that Hitler was modeling it after the first two reichs. That's a German word that means \"empire.\" \n\nThe [Second Reich](_URL_2_) was run by an emperor called a Kaiser, which is a German word that means \"Caesar.\" \n\nThe First Reich sprang from the original Caesars themselves - the Romans, who (more or less) unified Europe into one body, which then carried on through the [Holy Roman Empire](_URL_1_). (If you want to get technical, the Holy Roman Empire was the First Reich - it merely inherited a sense of self-importance or entitlement from the original not-so-holy Roman Empire.)\n\nWhat's important to understand for all this is that \"Germany\" didn't really exist as Germany for most of its history - it was a collection of often warring kingdoms, duchies and principalities with names like \"Saxony\" and \"Prussia.\" There were also enclaves of ethnic German aristocrats who ruled towns or counties across Eastern Europe where most of the people were Slavs or Poles or something similar. \n\nSo one of Hitler's main political aims was to bring all these scattered places together into one country - Greater Germany - and to get the world to accept that these enclaves and bits and pieces of land that were nominally other countries actually belonged to Germany. That's why when Germany annexed the [Sudetenland](_URL_0_), there was enough international confusion that not everyone accepted it as an invasion. \n\nSome people there spoke German... and some of those German people were traditionally \"in charge.\" \n\nThe big plan was to create and consolidate a \"Greater Germany\" that would become the most powerful political body in Europe - a new European Empire (or something empire-like) that would rival the former world-controlling empires (like Britain, or Rome) or the upstart world powers (like the United States).",
        "Yes, absolutely.\n\nTo begin with, don't forget that the romanticized Western image of samurai as hyper honor focused warrior monk types is pure exoticism with no real historic backing. \n\nMore to the point, like with the knights of Europe, while there was an official ideal of honor it was more prescriptive than descriptive and when you have a large group of heavily armed men some are going to be scumbags.\n\nFurther, \"samurai\" simply meant \"person from the caste permitted to carry weapons\", towards the end of the Tokugawa period (1600-1868) a great many samurai class men had no real weapon training, a minimal pension from the government, and generally survived by running up debts which were nullified every few years by government edict. \n\nThe Seven Samurai takes place earlier, in the Sengoku period (aka the Warring States Period), at a time of chaos and general confusion.  There was no centralized government, no rule beyond what the local warlord decreed and could enforce, and samurai (again, meaning \"people who carried weapons\", not \"super highly trained and deeply honorable warrior monk types\") were thugs enforcing the will of their local warlord, which usually meant stealing whatever they could from the peasants and calling it taxes.\n\nOr, worse, they were ronin.  When a warlord was defeated his soldiers (samurai) often just wandered off and turned to banditry to survive. There's a lot of mythology and several stories involving deeply honorable ronin seeking adventure and vengeance for the people who betrayed their lords, but mostly in real life they were just armed and trained men who took whatever they could from the people least likely to fight back.\n\nYou might check out [State of War](_URL_1_), it's more about the somewhat earlier times than the Sengoku period, but most of what it covers applies to the later periods as well.\n\nFor an interesting, often funny, first hand, primary source, account of daily life for a poor man of samurai class during the mid Tokugawa period check [Musui's Story](_URL_0_), it's a very quick read, an autobiography written by Musui himself, who lived a quite disreputable life and busts a lot of myths of the noble honorable samurai.\n\nTL;DR: even at the best of times, samurai were just soldiers, and historically soldiers weren't what you'd call very nice.  In the worse times they were just bandits.  The idea of samurai as super honorable warriors is just a myth.",
        "The reviewers provided detailed, confident reviews and there was significant discussion between the parties. \n \n Reviewer 2 and 3 felt quite strongly that the paper was a clear reject. Reviewer 1 thought the paper should be accepted.\n \n I was concerned with the two points raised by R3 and don't feel they were adequately addressed by the author's comments:\n \n - Dependence of the criteria on the learning rate (this does not make sense to me); and\n - Really really poor results on CIFAR-10 (and this is not being too picky, like asking them to be state-of-the-art; they are just way off)\n \n I engaged R1 to see how they felt about this. In reflection, I side with the majority opinion that the paper needs re-work to meet the ICLR acceptance bar.",
        "So lets say you own a company that sells clothes.  You get all the proper paperwork saying that your company is authorized to operate in your home country.  So you are free to sell it's clothes within your own country.  While there might be some minor regulation differences between regions, normally countries take steps to make these \"domestic\" sales as easy as possible. \n\nBut when you want to expand that company into a new country, you'll face a series of problems.  First is that shipping the product itself into the new country will likely incur a kind of tax.  This is known as a tariff or duty, aka an import tax. \n\nThe next problem has to do with regulations.  Different countries have different regulations on products.  So my home country might allow the use of red die #1357 but the country I'm trying to expand into might not.  Just different rules for different countries.\n\nA long time ago the people of Europe saw some problems with this system.  European countries tend to be fairly small so companies were always looking to expand into the country next door, and due to the above 2 things this was a giant pain in the ass. \n\nThe idea of the \"single market\" was that they would do away with the import/export taxes.  Second they should take steps so that a product that is legal to sell in Germany is also legal to sell in Spain. This makes it easy for companies to expand their customer base from one country to another.\n\nIn a related way, the same idea was applied to the movement of people.  A, Employee is just someone who sells their labor and they should be allowed to sell it in Germany, Spain or the UK. Just like any company can sell it's goods, any person can sell their labor. \n\nSo what does leaving this do to the UK.  It means that the citizens of the UK will require special permission (a visa) to live and work in other European countries.  So a Londoner wanting to work in Berlin would face the same challenges as a Londoner who wanted to work in New York. \n\nNext, UK companies would face additional challenges when it comes to selling their products to the rest of Europe.  This would likely mean the prices of those products raise, and sales fall.  This is expected to cause some economic problems for the UK.\n\nForeign companies would also face higher costs when selling to the UK.  So a product made in Germany would cost more.  This would make it easier for local companies to compete with those foreign companies. So in general there would be a few more jobs, but the costs of almost all imported goods would raise so everyone would be spending a little more of their income on those same goods. In general it is expected that the economic boost from the new jobs will be fairly minor, wheres the damage from increased prices would be fairly large, so a net negative.",
        "The short answer here is yes, definitely.\n\nThe current best theory is that our Moon formed from a [giant impact](_URL_1_) very early in the solar system's history. It's a little difficult to say exactly how far away it formed, but it was certainly closer to the Earth than it is now.\n\nThe moon steals angular momentum energy from the resulting torque generated by the tides in combination with Earth's rotation. Without rotation, tides are generated at the point on Earth directly facing the Moon, and the exact point on the opposite side of the Earth. Earth's rotation shifts these tides from these points, generating a net torque that gives a velocity kick to the Moon, [like this](_URL_0_).\n\nNote that this angular momentum of an ever-widening orbit had to come from somewhere, namely Earth's rotation rate. The Moon's increasing distance also means that the Earth's rotation has slowed down considerably - it's likely that a few billions years ago, one \"day\" was only a few hours long. This is also one of the reasons we need to insert a \"leap second\" every so often, as the Earth continues to slow down ever so slightly.\n\nNow, to get back to your original question - just how much larger? Tidal forces are formed from the net *difference* of the gravitational force. Since gravity goes as 1/r^2 , the difference means differentiating with respect to r, so tidal forces go as 1/r^3 . In other words, if the Moon were 2 times closer, tidal forces would be 8 times stronger. \n\nThere's some evidence that the Moon formed around 10 Earth Radii away (though this is heavily debated, with some saying it formed only 70% of its current distance). That's 10 radii distance is about 6 times closer to Earth, resulting in tides that were 216 times larger. Big. Note that if it did form at this distance, it would not have stayed that close for long - the tidal forces would be so large as to generate really whopping torques, moving the Moon outwards quite quickly.\n\n**tl;dr:** A little over 200 times larger.\n\nInteresting side note: This also means that the moon would've been 6 times larger in our sky, subtending 3 degrees instead of its current 0.5 degrees. Eclipses, both solar and lunar, would've been much more common.\n\nEDIT: added a tl;dr.",
        " You can also give stocks and mutual funds to the Red Cross, or leave the organization money in your will.\n\n\nThe American Red Cross also benefits when you purchase medical supplies and T-shirts through its website. In addition, the organization hosts special fundraising drives when a special circumstance arises or a disaster strikes.;\n,, Or better yet, host a blood drive through your school or employer.\n\n\nIf you can't give blood, offer your time at a blood drive. Volunteers help register donors, work the canteen area, and help navigate donors through the process. There's also a need for blood donor recruitment, which involves scheduling blood drives in the community.\n\n,,, Safety and health classes include CPR, first aid, babysitting, life guarding and swimming, just to name a few.\n\n, Volunteers with disaster services can also help with clerical work and dispensing safety information to those in affected areas.\n\n, Volunteers are also needed to go with elderly individuals to their health appointments.\n\n,,, Whether it's a workshop or a fund-raising gala, much behind-the-scenes help is needed for these events to be successful.\n\n,,,",
        " There are many reasons why you might want a relationship with a boy that's older than you. Most of the time, we see older people as more sophisticated and mature. This is most often the case, but not always. Some older boys will be just as immature as they ever were.;\n, There's no point in dating an older boy because you think it will make you appear cooler. Before committing to a relationship, you should make sure you're ready for a relationship. Especially if you're looking to date someone older than you, there's that much less\n\n, If two people are at different stages in their lives, it can bring up issues in communication. For instance, if you want to go out and party with your older boyfriend, he might be at a part of his life where he's had his fun and prefers a night indoors. In some cases, it can be hard to have a sense of equality in a relationship when one person has considerably more life experience than the other. Keep these potential problems in mind before you commit.\n\n\nOn the other hand, some maintain that age is just a number.There's no concrete reason why people of different ages can't connect on a deep, meaningful level.\n\n, Even if your relationship is perfect, other people are prone to make nasty assumptions about an age gap relationship. There's only so much you can do to fend off these nay-sayers. Ultimately, its your relationship, not theirs, and you shouldn't put their feelings before your own.\n\n\nIf you're under the age of 18, you should consider the legal consequences of someone who is older than you. If the age gap is big enough to make a sexual relationship illegal, you should not go through with it.\n\n, In virtually all cases of trying to make someone your boyfriend, you should be his friend first before anything romantic happens. Go out of your way to spend time with him. Ask him to hang out as you normally would with a regular friend. If there is chemistry and the two of you are emotionally available, there's a good likelihood that things will begin to take shape in time. For the meantime however, you should simply enjoy the experience of hanging out as friends.\n\n, Unless the older boy in question is incredibly shy, the fact that he has more life experience will probably mean that he'll be the one to instigate something. Some guys can be made to feel uncomfortable by a girl that is very forward, especially someone who is younger than them.\n\n\nIf it doesn't look like he's going to make a move out of shyness, you should think about making a move yourself. This should occur if he has shown signs of interest but hasn't acted on it over the course of a few hangouts.\n\n, This is where it can difficult for some girls who try to attract older guys. While your first instinct may be to be completely submissive towards the boy, this will only go to show your age. Instead, you should take age out of the equation when it comes to talking to him. If it makes you feel more comfortable, pretend he is the same age as you. Relationships usually need a sense of balance between the two people in order to succeed.\n\n, If you're attracted to a guy, you can say a lot simply by making eye contact with him and holding his gaze. Flirting covers a wide range of different behaviours, but most flirting begins with steady eye contact.\n\n\nThis kind of gaze is best combined with a smile. That way, it will tell the boy that you're looking at him for good (as opposed to negative) reasons.\n\n, If you smile at the boy you're interested in, you'll be projecting positive vibes in his direction. If he notices your smile and smiles back at you, it's a sign he is probably interested in you.\n\n, Saying something nice about someone can be interpreted as a form of flirting. Complimenting a boy based on his looks or other traits you find attractive will let him know you're interested. Make sure your compliments are genuine, however. People are usually able to tell when a compliment is sincere or not.\n\n, Because the boy you are interested in is older than you, it's possible he has different ideas of what he wants in a relationship with a girl. Although you may feel pressured to give in, you should only ever do what feels comfortable with you.\n\n, Relating emotionally to people older than you can be hard if you're not used to spending time with you. This is why it's recommended that you try to surround yourself with people that are older than you. Make friends with people in the boy's age range and get comfortable with hanging out with people with more experience than you. This can be hard to do if you don't already have older friends. If you have older siblings, you might try tagging along with some of their hangouts.\n\n\nIf you want to make older friends, you won't be able to depend on your classes. Extracurricular clubs are a place where people of different ages will get together with a common interest. If you work a job, there will likely be older co-workers you can befriend as well.\n\n, Regardless of the age you're trying to attract, it will work to your benefit if people are able to see that you're a girl of your word. If you make a promise, do everything you possibly can to follow through with it. Don't flake out on plans you make. When you follow through with the things you say, everything you say will have a greater sense of weight in the eyes of others. This includes any older guys you may fancy.\n\n, Self-esteem is a major sign of maturity. It's natural for young boys and girls to feel a lot of insecurity as they grow up. If you want to catch the attention of an older boy, you 're going to need to match the sort of self-assurance that is common with his age range. Challenge any negative thoughts that enter your mind, and replace them with positive thoughts about yourself. Even if you have to fake it until you make it at first, you'll eventually start to believe it sincerely.\n\n\nRemember that there is no right or wrong answer when it comes to self-esteem. There's no objectively wrong reason to have confidence in yourself.\n\n, You might be surprised how much you'll find out about yourself if you make a regular habit of keeping a diary. Buy any book of lined paper, or keep a Word file on your computer where you can add thoughts at the end of each day. If you become aware of the things you're thinking and feeling on a daily basis, you'll get a stronger grasp of the way other people see you.\n\n, Much like a healthy sense of self-confidence, getting past feeling jealous of other people is an important indicator that someone has matured. Most teenage girls tend to be very jealous at that stage in their lives. If you want to stand out from others your age, you'll need to let go of your jealousies towards other people. This is easier said than done for many people, but it is possible. Anytime you are beginning to feel jealous towards someone, remind yourself that there is no one who can do you better than you. Comparing yourself to anyone else is a waste of time.\n\n, Even if you're trying to become mature beyond your years, your current circle of friends might make it difficult to break free. Less mature groups of friends will often fall into spurts of drama or gossip. The best thing for you to do is to avoid this stuff entirely. It doesn't add anything constructive to your life, and you won't be made to look good if you're associated with that crowd.\n\n\nThis does not mean abandoning your friends. Rather, you should spend time with them, but take a step back if bickering starts. Of course, if a friend of yours is hurt and wants to talk it through with you, you should do so. Being mature can also mean being a reliable friend.\n\n, Nothing can make people look at your twice than a new hairstyle. Getting your hair cut or permed will have a major effect on your appearance. You should use this opportunity to spruce up your look and turn some heads. If you're trying to appeal to older groups, you are going to want to steer away from fashion trends that are reserved for juveniles. Aim to make your hair attractive but classy.\n\n\nIt's a good idea to pick a hairstyle that suits your face shape. Ask your hairdresser if you have any confusion., Depending on your age, there may be things you wear that might seem juvenile in the eyes of someone older. While you should always ultimately find your own sense of fashion, impressing an older guy can be easier if you look at the fashion expectations for his age group.\n\n\nIf the age gap is big enough, don't wear the boy's age range's fashion if it will look weird on somebody your age.\nIt helps to take a look at some current fashion outlets. This will give you some insight into the latest trends.\n\n, If you're young girl, don't try to compensate for your age by using a lot of makeup. Doing so only brings more attention to how young you are. In addition, it also gets the impression across that you are insecure with the way you look. You'll have a much higher chance of getting a guy interested if you stay as true to your real sense as possible. Some makeup to highlight your natural features should be more than enough.\n\n\nIt's a fact that younger girls aren't often as skilled at makeup as they will be once they're older. With that in mind, it's good to start small and work your way up from there.\n\n, Whether you're standing up or sitting down, make an effort to keep your back straight and your shoulders even. If you're not used to having good posture, this will feel a bit strange to do at first, but it will eventually feel natural if you stick with it.\n\n\nMaturity and confidence are the two major ways you can get across the age gap, and posture is a way of expressing both of them.\n\n",
        "  We utilize a recent formulation of a spherically symmetric spacetime endowed\nwith a general decomposition of the energy momentum tensor [Phys. Rev. D, 75,\n024031 (2007)] to derive equations governing spherically symmetric\ndistributions of electromagnetic matter. We show the system reduces to the\nReissner-Nordstrom spacetime in general, spherically symmetric coordinates in\nthe vacuum limit. Furthermore, we show reduction to the charged Vaidya\nspacetime in non-null coordinates when certain equations of states are chosen.\nA model of gravitational collapse is discussed whereby a charged fluid resides\nwithin a boundary of finite radial extent on the initial hypersurface, and is\nallowed to radiate charged particles. Our formalism allows for the discussion\nof all regions in this model without the need for complicated matching schemes\nat the interfaces between successive regions. As further examples we consider\nthe collapse of a thin shell of charged matter onto a Reissner-Nordstrom black\nhole. Finally, we reduce the entire system of equations to the static case such\nthat we have the equations for hydrostatic equilibrium of a charged fluid.\n",
        "KZLB (92.1 FM, \"92 Rock\") is a radio station that broadcasts out of Fort Dodge, Iowa airing an active rock format. The station is owned by Alpha Media, through licensee Alpha 3E Licensee LLC.\n\nOn October 3, 2019, KZLB changed their format from classic rock to active rock, branded as \"92 Rock\".\n\nReferences\n\nExternal links\n92 Rock official website\nThree Eagles Communications\n\nZLB\nFort Dodge, Iowa\nRadio stations established in 1990\nAlpha Media radio stations\nActive rock radio stations in the United States",
        "TLDR: Circumstantial evidence suggests tigers would win. \nEdit: added TLDR. Edit, spelling mistakes.\n\nSeems like no one has answered your question. I studied Roman animal games in college. I was more interested in how the animals were procured and transported instead of what animal would win. \n\nThe question is who would win between a tiger and lion in an ancient Roman animal game. Circumstantial evidence suggests that tigers are the favored animal, but direct evidence seems scarce. \n\nRomans loved animal games, called *venatios*. Large cat games were probably high quality games that attracted many spectators since tigers were harder to source than lions. Lions predominantly came from North Africa. Lions became common combatants in Roman games after they first appeared in Rome in 186 BCE. (Livy, 39.22.1-2). According to Livy, lions and leopards were first debuted in Rome in the same year. There are numerous Roman mosaics depicting lions in great and accurate detail suggesting they were familiar with the animal. Many attribute the disappearance of the North African lion to *venatios*. \n\nTigers seem to be the favored animal because they are slightly more aggressive. The reasons are two fold: 1) tigers are mostly solitary animals while lions live and fight in packs/prides; 2) tigers are more aggressive, going straight for the kill, where as lions tend to pounce and exhaust their prey. \n\nIn 1899, the Gaekwar of Baroda in Hindostan, hosted a battle between a lion and a tiger. He had a specially prepared amphitheatre and hosted the games for local and European guests. Apparently, he set the odds at 1 to 37,000 rupees, against the tiger. The tiger won; the Gaekwar of Baroda lost 37,000 rupees. (*Lion Against Tiger*, Baltimore Sun, Jan. 26, 1899). \n\nIn 2011, a Bengal tiger killed a lion in a Turkish zoo in Ankara. The tiger broke into the lion's cage through a gap in the fence. Allegedly, \"the tiger severed the lion's jugular vein in a single stroke with its paw, leaving the animal dying in a pool of blood.\" (*Tiger kills lion in Turkish zoo*, BBC, Mar. 7, 2011). \n\nFurthermore, Smithsonian Zoo biologist Craig Saffoe also favors the tiger. In an interview with Live Science, Saffoe suggested a tiger would win because \"[w]hat I've seen from tigers, they seem to be more aggressive; they go for the throat, go for the kill ... [w]hereas lions are more 'I'll will just pound on you and play with you.\" (*What Would Happen If a Lion Fought a Tiger?*\n , LiveScience, Jul. 16, 2012). There are, however, mitigating circumstances. If a lion is older, he is more likely experienced in combat. But, if he is older, he is probably more used to going along unchallenged in a pride. Younger lions, especially lions without prides, are less experienced fighters (ergo, no pride). \n\nFrom what I studied in college, I will add that a tiger raised in captivity would probably be more aggressive than a lion in captivity. One of the ways big cats were captured and sold was by a process called *cubbing*. Cubs were abducted because they were easier to capture, easier to transport, and less dangerous to handle. Tigers are generally solitary animals and have little inclination to work in a group. On the other hand, lions are \"pack animals.\" Lions can coexist in a social unit. If a lion was raised in captivity, it is less likely to be a dominant animal because a human would attempt to curb any dominant behaviors. A tiger, however, would be less indomitable. (This is my speculation). \n\nHowever, a lion may have been favored against a leopard. Some direct Roman evidence of big cat fights is found in the House of the Doves in Pompeii. Mosaic VIII.2.34 shows a snarling lion pinning a wounded leopard to the floor. The leopard's blood gushes unto the floor from its neck and from a wound on it's flank. \n\n_URL_5_\n\n_URL_0_\n\n_URL_4_\n\n_URL_3_\n\n_URL_1_\n\n_URL_2_",
        "Big Boy Records also known as the Boot Camp Clicc, was an independent record label established in late 1992 by Charles \"Big Boy\" Temple; Robert \"Big Rob\" Shaw\";and his producer Leroy \"Precise\" Edwards. For several years in the 1990s, Big Boy Records ruled the  hip hop music industry in New Orleans until No Limit Records and Cash Money Records became powerhouse labels.\n\nHistory\nBig Boy's first signee was pioneering New Orleans rapper Sporty T (Terence Vine). He had previously been a founding member of The Ninja Crew, New Orleans' first rap group to record. In the early 1990s, inspired by hits by Juvenile and Everlasting Hitman's bounce hits, he moved in that direction as well. The label's first single was \"Sporty Talkin' Sporty.\" Though bounce, it had an uncharacteristically heavy sound for the genre. After it sold 4,000 copies, Big Boy sought out more talent. In that same year Big Boy Signed Partners-N-Crime, a rapping duo from Hollygrove, New Orleans. When they dropped their single \"Let the Good Times Roll\" in 1994 it became an instant hit. Months later their debut album PNC 3 hit the stores. Later on in that year G-Slimm's Fours Deuces & Trays sold 200,000 copies. However, the label's first huge success came from a recently returned Gulf War veteran, Mystikal (Michael Ernest Tyler). The 12th Ward native had originally been with Parkway Pumpin as Mystikal Mike. Then he was poached by Big Boy, who released \"I'm Not That Nigga\" and a self-titled debut which sold 300,000 copies. Apparently bolstered by this considerable windfall, Big Boy made most of their promotional videos in the wake. After Mystikal signed with No Limit and the death of G-Slimm, Big Boy records slowly began to fall apart. In 2004 they released Big Boy Greatest Hits compilation album featuring all the greatest hits from 1992-2000.\n\nIn 2015, Charles \"Big Boy\" Temple died.\n\nArtists\n G-Slimm\n Mystikal\n Black Menace (J-Dawg & Threat) \n Ghetto Twiinz\n Partners-N-Crime\n Tim Smooth\n Sporty T\n Silky\n Rated X\n M.V.P.\n Elaté\n Insane\n Lil Lipp\n Fiend\n G-Quikk\n 17th Survivors\n Ghetto Slaves\n\nReferences\n\nHip hop record labels\nGangsta rap record labels\n1992 establishments in Louisiana",
        " Use a fabric measuring tape to determine the circumferences of your natural waist. This number will correspond with the backpack belt.;\n, Start at the base of your neck at the seventh cervical vertebra and continue along your spine until you reach the iliac crest. The iliac crest is the top front of your hip, and you will need to show your helper where the level of the iliac crest is on your spine with your fingers.\n\n\nFind the seventh cervical vertebrae by standing up straight. Tuck your head forward. The neck vertebra that sticks out the farthest is your seventh cervical vertebra.\nThe iliac crest is a bump at the side of your hip. It is not the top of your hip on your back. This bump is farther forward in women, and usually at the direct side of the hip in men.\nPlace your hand on your hip, between your thumb and forefinger, to mark the line of the iliac crest for the helper., Men with slender frames can get a better fit with a woman’s pack than with a men’s pack.\n\n, Women with broader chests or shoulders can get a better fit with a unisex pack, because women’s fit packs tend to have a narrower shoulder construction.\n\n, You may need to get a replacement shoulder harness, so look for a pack that allows you to replace belts and harnesses.\n\n, It is unlikely standard packs will adjust to accommodate you comfortably.\n\n,,, A waist 28 inches or smaller needs a small or extra small belt. A waist over 36 inches will need an extra large belt.\n\n\nYou may want to test medium and large belts for comfort if you are in between this range.\n\n, Pick a pack and ask the associate to help you test it.\n\n, (9 kg) of weight into the pack. Most outdoor stores have sandbags that will allow you to test weight without taking too much time packing individual gear in the pack., You don’t want them to be cinched up until you start putting the pack on your back., Lean forward and allow the pack to spread across your back. Tighten the shoulder straps slightly., Cinch it tightly. You want the majority of the weight to rest on your hips.\n\n\nTighten the hip belt stabilizers, if the pack has them. They are smaller straps that fit the belt more snugly.\n\n, There should be no gaps above or behind the shoulders.You can stand sideways in a mirror to assure of good fit.\n\n\nIf you can’t get the shoulder straps to be tight and comfortable, the torso is likely too long for you. If the torso length is adjustable, take the pack off again and change the length.\nIf the shoulder straps lift the weight off your hips, they are either too tight or your torso length is too small., It is usually located on the chest between the front of the shoulder straps. They should be at a 45-degree angle with the shoulder strap.\n\n\nTighten the load lifters as needed.\n\n, Lean slightly forward as you walk, just as if you were on the trail. If the straps dig in or feel off-balance, try another pack.\n\n",
        "You need a setting where the \"Line at Infinity\" is actually a thing. If we use coordinates, then the ordinary plane is the collection of all points (x,y), where x and y can be any real numbers. In this setting, a line is the solution set of an equation of the form Ax+By=C, where A,B and C are real numbers, and at least on of A or B is nonzero. Two lines, Ax+By=C and Dx+Ey=F are parallel if AE=DB. We can clearly see that if AE=DB and C and F are not equal, then there are no simultaneous solutions to these equations and so the lines do not intersect. \n\nThis is kinda inconvenient because we have theorems like \"All distinct pairs of lines intersect exactly once, unless they are parallel, which don't intersect\". Almost all pairs of lines have this nice property, and there are very few exceptions, but the existence of exceptions means that this is not a wholly reliable property that pairs of lines have. We essentially have two options, either accept that this is not always true and that there are these messy pairs of lines that don't intersect. Or, we can conclude that all pairs of lines *should* intersect, it's just that the plane itself is too small to contain the place where parallel lines intersect. We'll investigate this latter case, as it cleans up our understanding of lines and is flexible enough to apply to other situations.\n\nThe way we'll do this is find a simple way that kinda does the trick, but is unsatisfying. We'll then use this as inspiration to make a somewhat usually unmotivated construction that allows us to do these things more naturally.\n\nTo do this, we need to extend the plane. But we don't want to extend it too much. For instance, one way to extend the plane is to go into 3D space, and we can then say that the line Ax+By=C is actually just the plane Ax+By=Cz in 3D coordinates (x,y,z) where the plane z=1 corresponds to the original plane. In this case, if the lines Ax+By=Cz and Dx+Ey=Fz are parallel, then they will actually intersect in the plane z=0 along the line Ax+By=0 in that plane. You can interpret this as follows: In if we have two lines Ax+By=C and Dx+Ey=F, then the corresponding planes Ax+By=Cz and Dx+Ey=Fz will intersect along a line in 3D. This line always intersects (0,0,0) and, in the case when they are not parallel, some point in the plane (x,y,1). This is because this line is oblique to this plane. As the lines get more and more parallel, the point of intersection, (x,y,1), goes out further and further from the origin and the 3D line of intersection begins to flatten. When Ax+By=C is parallel to Dx+Ey=F, then their 3D intersection line is parallel to the plane (x,y,1), and the point of intersection on this plane has been pushed out \"off\" the plane. This extension is fine, it offers some intuition. It works. Instead of the theorem \"All non-parallel lines intersect once\", we get the new theorem \"All lines correspond to planes that intersect along a line\". We've gotten rid of the non-parallel condition, but at the cost of the singular intersection point. Moreover, when we extend this theorem to higher dimensions, we still have things like parallel planes to deal with. If we extend the \"All non-parallel lines intersect once\" to higher dimensions, we get the theorem \"All non-parallel planes intersect along a line\" or \"All non-parallel 3D spaces in 4D space intersect along a plane\" etc. In all of these cases, the intersection set has dimension 1 lower than the object in question, which we lose when we use this higher dimension model. Moreover, it's a fairly unsatisfying setup and seems pretty ad hoc. We want a more universal way to extend flat, Euclidean space so that can take care of all of these cases, in a way that is *intrinsic* to this space, that doesn't lose the 1-lower dimension, and doesn't force us to continually work in higher dimensions.\n\nTo resolve this, we can take inspiration to what we did above. After all, we *did* manage to get parallel lines to \"intersect\". The key was the third coordinate z, as we could set it to zero and the line of intersection would be Ax+By=0. But we want this to be a single point. We note that, in this case, the solution set in 3-dimensions to Ax+By=0 and z=0 are the points (Ar,-Br,0) where r is any real number or, more pointedly, (Ar,-Br,0\\*r). That is, all of these points are the same up to scaling by r. What we want is for this \"plane\" of intersection to actually be a line, so we're going to do something very drastic and just say that this line is really only *one* point. \n\nWe're going to use three coordinates (x:y:z) to describe our space, but instead of having each triple of number correspond to their own point, we're going to say that two triples (x:y:z) and (u:v:w) correspond to the same point if they are the same point up to scaling by some value r. That is, if u=rx and v=ry and w=rz. For instance, we have (1:2:3)=(3:6:9), they're the same point because they are the same up to a scaling factor of 3. This set of points is called the [Real Projective Plane](_URL_2_).\n\nNow, just what does this new projective plane look like? Note that if z is nonzero, then we can multiply the point (x:y:z) by the scaling factor of 1/z to get the point (x/z:y/z:1) and, moreover, the only way that two points (X,Y:1) and (U:V:1) are equal is if (X,Y)=(U,V) in the normal, traditional sense. That means that the ordinary, traditional plane lives inside the projective plane and is equal to the set of \"projective coordinates\" (x:y:z) where z is nonzero. This accounts for almost all points in the projective plane. So what about the points of the form (A:B:0)? Well, we can see where they live by \"tracking\" the point (A:B:Z) as we send Z to zero. This means that we'll be seeing where the point (A/Z,B/Z) goes on the plane as we send Z to zero. When we do this, the point will travel along the line Bx=Ay in the plane out towards infinity. This means that we can thing of the points (x:y:0) as the points \"at infinity\" or, as we usually call it, the \"line at infinity\".\n\nSome may have noted something off here. When we track the point (A/Z,B/Z) as Z goes to zero, it's fine that it goes along the line Bx=Ay, but there are *two* directions we can go to infinity with. If Z goes to zero from the positive numbers, it will travel along Bx=Ay in one direction and if it goes to zero from the negative numbers, then it will travel along Bx=Ay in the opposite direction. This process simultaneously sends (A/Z,B/Z) to completely opposite infinity points. But, for this to work, we need (A:B:0) to be a single point. And this is what we do. If we imagine the entire euclidean plane surrounded by a circle \"at infinity\", then to make the real projective plane, we'll need to take points on this circle that are opposite each other and glue them together into a single point. In this context, for instance, opposite infinities are the same. This is why visualizations of the real projective plane[look weird](_URL_0_). \n\nThis is good, this real projective plane is, essentially, the ordinary real plane but with a circle weirdly wrapped around it. It's and extension, but not too much of an extension. Note that each point on this line at infinity corresponds to a *slope*. The point (A:B:0) corresponds to the slope of the line Bx=Ay. Intuitively, then, parallel lines should intersect at the point at infinity corresponding to their shared slope. A line on the real projective line is the set of all projective coordinates (x:y:z) that solve an equation of the form Ax+By=Cz and the solution to Ax+By=0 ( ie (-B:A:0)) corresponds to where it intersects the line at infinity. We can check and we find that non-parallel lines still only intersect at one point. But now, if Ax+By=Cz and Dx+Ey=Fz are parallel (which means that there is some nonzero real r so that D=Ar and E=Br), then they don't intersect off the line at infinity and, moreover, they intersect the line at infinity at the point corresponding to their shared slope: (-B:A:0) = (-Br:Ar:0) = (-E:D:0).\n\nThis is much more satisfying. We've only included the new points that we actually need. These new points allow us to say that \"All pairs of distinct lines intersect exactly once\". It makes more natural many other theorems in classical geometry, eg [Pascal's Theorem](_URL_1_). Furthermore, there are natural generalizations to this in higher dimensions that make everything nice and analogous to this. Moreover, the interpretation of their intersection makes sense of being \"at infinity\" while also having the natural understanding of they intersect \"at\" their slope, which is the same.\n\nWe can look at other objects in the projective plane, in particular we can look at conics. In the projective plane, we can conclude that all non-degenerate conics are of the same type. That is, there is nothing to distinguish between ellipses, parabolas and hyperbolas intrinsically, and the only way to classify them is how they interact with the line at infinity. An ellipse/circle is a conic that does not intersect the line at infinity. A parabola is a conic that intersects the line at infinity once, ie is tangent to it. And a hyperbola is a conic that intersects the line at infinity twice. The hyperbola intersects the line at infinity at the two slopes corresponding to the its two asymptotes, so we can think of their asymptotes as being the \"points at infinity\". In fact, if you trace along a hyperbola and when you get to an asymptote, you cycle around you'll trace both sides of the hyperbola nicely.",
        " The number of t-shirt logos you want to use will determine the size of your blanket. However, if you want a bigger quilt and you don't have enough t-shirts, you can make blocks out of the plain fabric on the back of your t-shirt or buy patterned cotton fabric.\n\n\nApproximately 12 t-shirts (3 by 4 blocks) will make a throw sized blanket. 20 shirts (4 by 5 blocks) will make a twin bed sized quilt. 30 shirts (5 by 6 blocks) will make a double sized quilt. 36 shirts (6 by 6 blocks) will make a queen sized quilt and 42 shirts (6 by 7 blocks) will make a king sized quilt.;\n, Treat any stains before you throw them in the wash. It will be hard to remove deep stains later on in the process.\n\n, The amount you need will depend upon the size of t-shirt quilt you would like. You want to have 2.5 inch (6.4 cm) borders and 2 inch (5.1 cm) pieces for sashing.\n\n\nChoose something that either matches your décor or matches your t-shirts. You may also want to use the same fabric for the back of your blanket.\n\n, Use cold water and low heat in the washer and dryer to ensure dark colors do not fade.\n\n, Decide if your block designs will fit in a 12 by 12 inch block or if they need to be 14 by 14 inch blocks. All the blocks need to be the same size.\n\n, You will adhere this to the back of your t-shirt squares to keep your blanket squares from stretching. Buy enough to have approximately 17 inch (43.2 cm) squares of interfacing for all of your blanket blocks.\n\n, Center the logo and cut 15 inch (38.1 cm) squares with a rotary cutter, if your final blocks will be 12 inches (30.5 cm). Cut 17 inch (43.2 cm) squares if your final blocks will measure 14 inches (35.6 cm).\n\n,, Lay the t-shirt square with the logo down. Lay the fusible interfacing with the resin side down, on the back of your t-shirt square.\n\n, Follow the package instructions to ensure you attach the fusing correctly.\n\n, Use a rotary cutter or fabric scissors. This will give enough room for seam allowance.\n\n, Determine how you want your quilt to be arranged. Try to put intricate designs next to simple ones and light colors next to dark ones.\n\n, They should be either 15 inches (38.1 cm) by 2 inches (5.1 cm) or 13 inches (33.0 cm) by 2 inches (5.1 cm), depending upon the size of your blocks. Pin the sashing to the bottom of the blocks, with a 1⁄4 inch (0.6 cm) seam allowance.\n\n\nThe bottom blocks of the blanket do not need horizontal strips, because they will be next to the border.\n\n, Use a 1/4 seam allowance for the entire project. Sew 4, 5, 6 or 7 columns together, depending upon the size of your quilt.\n\n, You do not need to measure and cut vertical sashing strips for the outside edges, because you will have borders. Sew 1 sashing strip to the right of each column.\n\n, Cut off any excess fabric around your blanket top. Once your columns are sewn, it is time to begin your borders.\n\n, The strips should be 25 inches (12.7 cm) wide. Pin the borders to your blanket top.\n\n, Measure a layer of batting that is the length and width of your blanket. Set the layer of batting on top of your quilt top.\n\n, It should be the length and width of your completed quilt top. Cut it with a rotary cutter or fabric scissors.\n\n, Stitch around the outside of your quilt with a 1⁄4 inch (0.6 cm) seam allowance. Leave 1 side of the quilt open so that you can turn the quilt right side out.\n\n, Pin the remaining open side closed by turning the edges under. Hand sew the remaining side with a needle and thread.\n\n, Alternately, you can hand stitch down the sashing on your quilt. This will keep your batting and quilt blocks from moving around and bunching on the inside of your blanket.\n\n",
        "Short answer: no one knows. \n\nLonger answer:\nFirst of all, REM is a bit of a misnomer. The eye movements during REM are actually slower than while you're awake - they're just much faster than in non-REM sleep.\n\nThere are two main ideas as to what's going on.\nThe things you see in dreams are obviously not based on images from your eyes. These images are built in the visual centres of your brain though. During normal vision, there's constant feedback to the eyes from the visual centres to help with following something you're interested in, focus, and so on. Even though the eyes aren't sending much information to the brain, the brain doesn't stop sending feedback to the eyes based on the images it's made up. \n\nThis might be the cause of REM. It makes sense, but it's not perfect. People who were born blind still move their eyes (if they physically can) during REM, in a way that doesn't match their eye movements when awake. Keep in mind that your eye nerves are directly plugged into the visual centres of your brain. Most of the rest of your body goes via the spinal chord and brain stem.\n\nThere's also the idea that the REM phase of sleep might be for actively processing memory. In this view, your brain is playing back memories kind of like a tape, and your eyes are playing back the movements that went with those memories. This could be a side-effect, or it could actually help in processing.",
        "{{Infobox election\n| election_name = 1991 Mississippi gubernatorial election\n| type = presidential\n| country = Mississippi\n| flag_image = Flag of Mississippi (1894-1996).svg\n| previous_election = 1987 Mississippi gubernatorial election\n| previous_year = 1987\n| next_election = 1995 Mississippi gubernatorial election\n| next_year = 1995\n| ongoing = no\n| election_date = November 5, 1991\n| registered = \n| turnout = \n| image1 = \n| nominee1 = Kirk Fordice\n| party1 = Republican Party (United States)\n| popular_vote1 = 361,500\n| percentage1 = 50.83%\n| image2 = \n| nominee2 = Ray Mabus\n| party2 = Democratic Party (United States)\n| popular_vote2 = 338,459| percentage2 = 47.59%\n| map_image = 1991 Mississippi gubernatorial election results map by county.svg\n| map_size = 150px\n| map_alt = \n| map = \n| map_caption = County results Fordice:Mabus:   \n| title = Governor\n| before_election = Ray Mabus\n| before_party = Democratic Party (United States)\n| after_election = Kirk Fordice\n| after_party = Republican Party (United States)\n}}\n\nThe 1991 Mississippi gubernatorial election' took place on November 5, 1991, in order to elect the Governor of Mississippi. Incumbent Democrat Ray Mabus unsuccessfully ran for reelection to a second term. This election marked the first time a Republican was elected Governor of Mississippi since Adelbert Ames in 1873.\n\nThis is the last gubernatorial election where the Democratic candidate carried any of three counties (Hancock, Harrison and Jackson) along the Mississippi Gulf Coast. \n\nDemocratic primary\nIncumbent Democrat Ray Mabus won the Democratic primary, defeating former U.S. Representative Wayne Dowdy and George \"Wagon Wheel\" Blair. According to The New York Times,'' Mabus had to fend off charges that he was \"arrogant and out of touch with Mississippi politically\", and was perceived as a \"Porsche politician in a Chevy pickup state\".\n\nResults\n\nRepublican primary\nNo candidate received a majority in the Republican primary, so a runoff was held between the top two candidates. The runoff election was won by businessman Kirk Fordice, who defeated State Auditor Pete Johnson.\n\nResults\n\nRunoff\n\nGeneral election\n\nResults\n\nReferences\n\n1991\ngubernatorial\nMississippi\nNovember 1991 events in the United States",
        "Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions.  This avoids overfitting.\n\nI generally like this idea.  Regularizing the behavior of the model often makes more sense than regularizing its parameters.  After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior.  So one might be able to choose a more sensible prior over the behavior.  In other words, prefer parameters not because they are individually close to 0 but because they jointly lead to a distribution that is plausible or low-risk a priori.\n\nPro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5).\n\nPro: It's possible that this hasn't been well-explored yet in neural networks (not sure).\n\nPro: The experimental results look good.  So maybe everyone should use this kind of regularizer. \n\nCon: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning.  There are many, many papers that include a scaled entropy term in the optimization objective!  It's not just for reinforcement learning.  Please see the long list of connections in my pre-review questions / comments.  \n\nCon: Experimental results should always be accompanied by significance tests and error analysis.  Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?  Are the improvements robust across many different training sets?  What errors does your model fix, and what errors does it introduce?  \n\nSummary recommendation: Revise and resubmit.  ICLR has lots of submissions.  I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it.  Otherwise, there's a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships.",
        "  We present evidence for the formation of dust grains in an unusual Type Ib SN\nbased on late-time spectra of SN 2006jc. The progenitor suffered an LBV-like\noutburst just 2 yr earlier, and we propose that the dust formation is a\nconsequence of the SN blast wave overtaking that LBV-like shell. The key\nevidence for dust formation is (a) the appearance of a red/near-IR continuum\nsource fit by 1600 K graphite grains, and (b) fading of the redshifted sides of\nHe I emission lines, yielding progressively more asymmetric blueshifted lines\nas dust obscures receding material. This provides the strongest case yet for\ndust formation in any SN Ib/c. Both developments occurred between 51 and 75 d\nafter peak, while other SNe observed to form dust did so after a few hundred\ndays. Geometric considerations indicate that dust formed in the dense swept-up\nshell between the forward and reverse shocks, and not in the freely expanding\nSN ejecta. Rapid cooling leading to dust formation may have been aided by\nextremely high shell densities, as indicated by He I line ratios. The brief\nepoch of dust formation is accompanied by He II 4686 emission and enhanced\nX-ray emission. These clues suggest that the unusual dust formation in this\nobject was not due to properties of the SN itself, but instead -- like most\npeculiarities of SN 2006jc -- was a consequence of the dense environment\ncreated by an LBV-like eruption 2 yr before the SN.\n",
        "  We propose a subtraction scheme for a massive Yang-Mills theory realized via\na nonlinear representation of the gauge group (here SU(2)). It is based on the\nsubtraction of the poles in D-4 of the amplitudes, in dimensional\nregularization, after a suitable normalization has been performed. Perturbation\ntheory is in the number of loops and the procedure is stable under iterative\nsubtraction of the poles. The unphysical Goldstone bosons, the Faddeev-Popov\nghosts and the unphysical mode of the gauge field are expected to cancel out in\nthe unitarity equation. The spontaneous symmetry breaking parameter is not a\nphysical variable. We use the tools already tested in the nonlinear sigma\nmodel: hierarchy in the number of Goldstone boson legs and weak power-counting\nproperty (finite number of independent divergent amplitudes at each order). It\nis intriguing that the model is naturally based on the symmetry SU(2)_L local\ntimes SU(2)_R global. By construction the physical amplitudes depend on the\nmass and on the self-coupling constant of the gauge particle and moreover on\nthe scale parameter of the radiative corrections. The Feynman rules are in the\nLandau gauge.\n",
        "This paper is about introducing eye-tracking features for sentiment analysis as\na type of cognitive feature.  I think that the idea of introducing eye-tracking\nfeatures as a proxy for cognitive load for sentiment analysis is an interesting\none.  \n\nI think the discussion on the features and comparison of feature sets is clear\nand very helpful.  I also like that the feasibility of the approach is\naddressed in section 7.\n\nI wonder if it would help the evaluation if the datasets didn't conflate\ndifferent domains, e.g., the movie review corpus and the tweet corpus.             \nFor one\nit might improve the prediction of movie review (resp. tweets) if the tweets\n(resp. movie reviews) weren't in the training.              It would also make the\nresults\neasier to interpret.  The results in Table 2 would seem rather low compared to\nstate-of-the art results for the Pang and Lee data, but look much better if\ncompared to results for Twitter data.\n\nIn Section 3.3, there are no overlapping snippets in the training data and\ntesting data of datasets 1 and 2, right?  Even if they come from the same\nsources (e.g., Pang & Lee and Sentiment 140).\n\nMinor: some of the extra use of bold is distracting (or maybe it's just me);",
        "This paper explores the performance-area-energy-model accuracy tradeoff encountered in designing custom number representations for deep learning inference. Common image-based benchmarks: VGG, Googlenet etc are used to demonstrate that fewer than1 6 bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy.\n\nQuestions:\n\n1. Does the custom floating point number representation take into account support for de-normal numbers? \n2. Is the custom floating point unit clocked at the same frequency as the baseline 32-bit floating point unit? If not, what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory\n\nComments:\n\n1. I would recommend using the IEEE half-precision floating point (1bit sign, 5bit exponent, and 10bit mantissa) as a baseline for comparison. At this point, it is well known in both the ML and the HW communities that 32-bit floats are an overkill for DNN inference and major HW vendors already include support for IEEE half-precision floats. \n2. In my opinion, the claim that switching to custom floating point  lead to a YY.ZZ x savings in energy is misleading. It might be true that the floating-point unit itself might consume less energy due to smaller bit-width of the operands, however a large fraction of the total energy is spent in data movement to/from the memories. As a result, reducing the floating point unit’s energy consumption by a certain factor will not translate to the same reduction in the total energy. A reader not familiar with such nuances (for example a typical member of the ML community), may be mislead by such claims. \n3. On a similar note as comment 2, the authors should explicitly mention that the claimed speedup is that of the floating point unit only, and it will not translate to the overall workload speedup. Although the speedup of the compute unit is roughly quadratic in the bit-width, the bandwidth requirements scale linearly with bit-width. As a result, it is possible that these custom floating point units may be starved on memory bandwidth, in which case the claims of speedup and energy savings need to be revisited.\n4. The authors should also comment on the complexities and overheads introduced in data accesses, designing the various system buses/ data paths when the number representation is not byte-aligned. Moving to a custom 14-bit number representation (for example) can improve the performance and energy-efficiency of the floating point unit, but these gains can be partially eroded due to the additional overhead in supporting non-byte aligned memory accesses.\n",
        " The taskbar is the bar at the bottom of the screen. Doing so invokes a pop-up menu.\n, It's near the bottom of the pop-up menu.\n\nAlternatively, press Ctrl+⇧ Shift+Esc.\n\n, This tab is at the top of the Task Manager window., It's a link near the top of the window. Doing so will reset the usage clock for each app on your computer to zero., Click the Start menu\n\n\n\n\n\nand type file explorer into Start, and then click\n\n\n\n\n\nFile Explorer at the top of the Start window., This tab is in the top-left section of the File Explorer window., It's a box-shaped icon in the upper-right side of the File Explorer window. Doing so opens the Folder Options window., You'll see this option in the upper-left corner of the Folder Options window., It's in the \"Privacy\" section toward the bottom of the window. Doing so will remove your recent searches from the File Explorer.\n\nIf you pinned any folders or files to the File Explorer, they won't be cleared.\n\n, To do so, uncheck both the Show recently used files in Quick Access and Show recently used folders in Quick Access boxes in the \"Privacy\" section. Doing so will prevent future searches from appearing in the File Explorer., It's at the bottom of the Folder Options window. Your File Explorer history should now be clear., It's on the left side of the taskbar, just right of the Windows logo. The Cortana window will pop up.\n\nIf you don't see this option:\n\nRight-click the taskbar.\nSelect Cortana.\nClick Show search box.\n\n\n\n, It's on the left side of the Cortana window. Doing so will open Cortana's settings in the window., It's below the \"My device history\" heading. Doing so clears your on-device search history., This link is below the \"My search history\" heading. Clicking this link opens a Bing page with all of your searches listed in chronological order.\n\nYou must be connected to the Internet in order to view this page.\n\n, It's a link near the top of the Bing page., This button is in the \"Clear search history\" section near the top of the page., Doing so will clear Cortana's search history entirely., It's a dark-blue app with a white \"e\" on it., This option is in the top-right corner of the Edge window. Clicking it invokes a drop-down menu., It's at the bottom of the drop-down menu., This button is beneath the \"Clear browsing data\" heading., You can check other items here as well, but these two items must be checked in order to clear your Edge history., It's in the middle of the menu. Doing so will delete your Edge browsing and downloads history., It's a light-blue \"e\" icon., This option is in the top-right corner of the Internet Explorer window. Doing so invokes a drop-down menu., You'll see this icon near the top of the drop-down menu. Clicking it will prompt a pop-out menu to appear., It's at the top of the pop-out menu., These two options will ensure that you clear all aspects of Internet Explorer's history., It's at the bottom of the window. This will clear your Internet Explorer browsing history., It resembles a red, green, yellow, and blue sphere., This icon is in the top-right corner of the Chrome window. Clicking it will prompt a drop-down menu., It's near the bottom of the drop-down menu. Selecting it invokes a pop-out menu., It's near the top of the pop-out menu., This is the drop-down menu at the top of the \"Clear browsing data\" window. Make a selection for how far back you'd like to delete the browsing history (for example, selecting the beginning of time will delete all the recorded browsing history)., Both of these categories make up your browsing history., This is a blue button at the bottom of the window. Clicking it will promptly clear your browsing and download history., This app resembles a blue globe with an orange fox wrapped around it., It's in the top-right corner of the Firefox window., You'll see this option in the middle of the drop-down menu., It's a tab on the left side of the Firefox window., This link is below the \"History\" heading on the right side of the page.\n\nIf you don't see this option, click the box next to \"Firefox will:\", then click Remember history.\n\n, To do so, click the box to the right of \"Time range to clear:\" at the top of the Clear Recent History window, then click a time (e.g., Today or Everything)., These items will ensure that your Firefox usage history is targeted when you click Clear Now., It's at the bottom of the window. Doing so will immediately clear your Firefox history.",
        " Firstly, you need two pans for this, one for the veg/chili/garlic and one for the meat.\n\n\nPut only a teaspoon or two of oil (preferable groundnut oil, but it doesn't matter) in the veg pan, once hot, add the chopped chili first, this infuses the oil with the chili heat and flavor. The first thing you put in the pan is essential.\n\n\n\n\n\n;\n, if you don't have the seeds, and you only have the powder, then add a touch of this now instead, not all at this stage.\n\n,,\n\n\nYou don't need a lot of oil at all, as the meat gives off some good juices. The meat starts to sweat the fat off itself as it starts to brown, once this has happened, pour 95% of the fat juice from the meat into the veg pan, so essentially, the veg is now cooking in meat fats. They soak up that flavor and make them really really tasty!!\n\n\n\n\n\n\nNext put some more of the cumin into the veg, not all of it, and a little bit of salt and black pepper.\n\n\n\n\n\n\nYou want the lamb (or if you really want to use beef, go ahead) to be brown but not burnt, keep it moving, sprinkle a touch of cumin on the meat, stir it round, remember, you don't need it 100% cooked, as you're still gonna be stewing it a little later on.\n\n\n\n\n\n\nMaking sure you don't overcook it here is key to keeping the tenderness of the lamb.\n\n\n\n\n\n\nOnce the meat is at a point you're happy with, and the veg is nicely softened a little (the peppers won't soften so much but they are now packed full of flavor and burst in your mouth when you eat them), add the meat to the veg, or veg to the meat, which ever pan is bigger.\n\n\n\n\n\n\nAdd your chopped tomatoes and your chili kidney beans stir around and add all the cumin now, don't be shy with it, it's what gives a good chili real flavor. Add a decent amount of black pepper and salt to taste.\n\n\n\n\n\n\n\n, At this point, depending on how confident you feel you can try experimenting with a few different flavors, you could grate a little nutmeg in now, add a touch of paprika or even cinnamon. Feel free to experiment.\n\n, remember do not think you've put in too little of this, as you can overdo it easily. 10drops max.\n\n,,,,, Also works well with tortilla chips, sour cream , and garlic bread.\n\n,, The grated carrot and onion give it texture too and the soft lamb is delicious, and because of the RED beans and general red color of the dish, and the YELLOW peppers, and the GREEN peppers, that's why I've named it \"Traffic Light Chili\".\n\n,",
        "Because muscle building isn't *just* micro tears and repairs of muscle fibers. Actually, it's not totally clear that those are directly correlated with muscle growth; some lifts, like the deadlift, the Olympic lifts, box squats, and partial squats or presses done from pins or a box, have either a vastly reduced eccentric component or none at all, and therefore they do not cause significant microtrauma; however, they nonetheless lead to increased size and strength. Microtrauma has also been shown to decrease with training; hence why your first workout after a layoff will produce unimagined levels of soreness, but your tenth or eleventh will produce almost none. My understanding is that satellite cells are able to contribute to the growth of muscle cells even if there is no microtrauma.\n\nThe actual process by which the body \"figures out\" that new muscle tissue is required isn't fully understood. A great deal of it definitely has to do with the flood of anabolic hormones post-workout. Strength gains also come from not just the synthesis of new muscle tissue but also from increased neuromuscular efficiency; likewise, size comes not just from myofibrillar hypertrophy, wherein the actual muscle fibers increase in diameter, but also from sarcoplasmic hypertrophy, wherein the muscles basically store more fluid. The continuum here explains why Olympic weightlifters can be very explosive but not particularly large and why bodybuilders can be very large but not particularly strong.",
        " This might take a while and you shouldn't force yourself to like someone right away. In this position of depression, you are very vulnerable to emotional attacks. Just go with the flow and avoid her. Go different ways.;\n,, She has her boyfriend because she likes him!\n\n, You'll just be digging a deeper hole for yourself.\n\n, Go for it. There's plenty of fish in the sea.\n\n, Don't just stay at home thinking about what she's doing with the boyfriend.\n\n,, There is always someone better.\n\n, Seeing her and her boyfriend there will just hurt.\n\n, She'll just be suspicious.\n\n,,, That's just not cool.\n\n,,,, After all, your degree is more important.\n\n,,,,, It will only make the situation between you and the girl worse. If you fight, one of you might get injured whether or not you fought. It may lead to having the authorities called and/or a restraining order, Be Careful!\n\n,,,,,,, Maybe she's not lucky enough to have a guy like you.\n\n,,, If you find this difficult, consider relocating if the option is available.\n\n",
        "Hi,\n\nI have a question about eq. 3 and 4.\n\nAs far as I understood from the beginning of section 3, f_{w}^{l} (x) is essentially activations at layer l, which means that value of f_{w}^{l} (x) is in R^{d_l} space where d_l is dimensionality of output of layer l.\nIn eq. (3) and (4) you \\eta compute gradient of f_{w}^{l} (x) over input x, which I would expect to be Jacobian matrix with size d_l*n (where n - dimensionality of input x). So looking at eq (3) and (4) I would expect that \\eta and x has different dimensionality. At the same time in section 4 you add s*\\eta to x.\nCould you explain this discrepancy in dimensionality and how \\eta should be calculated?\nMaybe you meant that numerator of eq (3) and (4) contain norm of f_{w}^{l} instead of it's value?\n\nThanks,\nAlex",
        "If you are asking about the United States, citizens can claim self-defense against officers.  The legal standard varies from state to state- are you interested in one in particular?\n\nIn some states there is an elevated standard if the assailant is an officer, requiring actual imminent danger rather than a reasonable fear or belief (the standard for self-defense). Here are a few examples I found in responding to a similar question:\n\n*State v. Kraul*, 90 N.M. 314, 318 (N.M. Ct. App. 1977) (\"The right of self-defense is not barred simply because the other person in the affray is a police officer.\")\n\n*State v. Hutchinson*, 959 P.2d 1061 (Wash. 1998) (self-defense instruction in case involving shooting of two police officers)\n\n*Brown v. Commonwealth*, 497 S.E.2d 527, 530 (Vir. 1998) (\"It has long been held in Virginia that where an officer attempts an unlawful arrest, the officer is an aggressor which gives the arrestee the right to use self-defense to resist so long as the force used is reasonable.\")\n\n*Boyd v. State*, 406 So. 2d 824 (Miss. 1981) (reversing conviction for assault on a police officer because the trial court failed to instruct the jury on self-defense)",
        "While the core ideas explored in this paper are quite limited in algorithmic novelty (e.g., the direct sparse convolutions), the reviewers largely feel that the paper is well written, experiments are carefully done on multiple architectures and system issues are discussed in-depth. Given the interest in the ICLR community around performance characterization and acceleration of CNNs in particular, this paper offers an interesting perspective.",
        "The answer lies in our conception of magic. To most people in the modern world the first image that comes to mind is Harry Potter making things fly around the room, shooting big, violent spells everywhere. Historically, this is not how witches were seen. \n\nMagic was almost always related to a relationship with the Devil, which made it inherently evil. The witches gained their power by worshipping Satan. By doing his bidding on Earth, he in turn granted them with extraordinary powers. \n\nTheir resulting magic was much more subtle. Most reports from Europe and the Americas allege that a certain person cast some magic upon a cow and killed it or caused some crops to fail. Magic was used to harm others, but not in the direct way that we often see in popular culture. \n\nTake the Salem Witch Trials, perhaps the most famous example in American history. When a few girls started acting in a strange manner, screaming and writhing to draw attention, it was assumed that people had cast spells upon them to make them suffer. The results of this case aren't really important for your question, but this would be an example of a way that people believed magic took a direct and tangible effect.\n\nArresting and executing the witches was simply reasserting God's will on Earth. The witches were under guard and were never expected to bust out riding a broom while breathing fire. The most they could do was, in a rather lengthy time, slowly poison one's soul or cause incremental physical ailments.  \n\nSo, since most of the time charges instead focused on abstract allegations of sabotage and rarely human violence, they were not too worried. Most mass-hysteria episodes coincided with difficult times economically, politically, or environmentally, but it was always easier to say, \"My cow died and I hate that girl. She's a witch!\" The girl could take revenge, but it'd be rather difficult for her to find the time to slowly implement her incremental magic if she's constantly under surveillance and then burned to death. \n\nEdit: Sorry for the lack of sources and formatting, I'm a little bit new here. \n\n* Dr. Brian Pavlac's book *Witch Hunts in the Western World: Persecution and Punishment from the Inquisition through the Salem Trials* is a good overview of the topic. If you want a quick version, his website lays it out pretty well with a few FAQs. \n\nFor example, regarding their conception of magic he writes, \"Usually the danger was seen in an organized conspiracy led by the Devil. Or the concern was witches causing harm (maleficia) through spells: raising storms, killing people or livestock, and/or causing bad luck.\" As people became more and more hysterical, the government almost always stepped in to counteract the Devil's influence, so it was very much an institutionalized phenomenon. \n\nHe also briefly comments on why outbreaks occurred in some places more than others. He writes, \"Historians are still trying to explain the reasons for this great variety in witch hunting.  Important factors could have been:  the power of the central government; the independence of local authorities; tensions created by war, failing economies, or famine; and uncertainties about religious conformity.\" \n\nFor more info on the Salem witch trials... \n\n* The rather famous Cotton Mather left a firsthand account of the trials. He describes how New England culture understood magic and its effects throughout. For example, he wrote the following of the first case that triggered the trials. \n\n > It was not long before one of her Sisters, an two of her Brothers, were seized, in.Order one after another with Affects' like those that molested her. Within a fe weeks, they were all four tortured every where in a manners very grievous, that it would have broke an heart of stone t have seen their Agonies. Skilful Physicians were consulted for their Help, and particularly our worthy and prudent Friend Dr. Thomas Oakes,' who found himself so affronted by the Dist'empers of the children, that he concluded nothing but an hellish Witchcraft could be the Original of these Maladies. \n\nAs you can see they didn't believe witchcraft worked anything like we do today. Because of this their fear of a witch locked up in a jail cell was naturally much different than our's would be. \n\nThere are many more books on the Salem Witch Trials, and it really is fascinating to look at why the entire thing happened. \n\n* I'd recommend *The Salem Witch Trials: A Day-By-Day Chronicle of a Community Under Siege* by Maryilynne K. Roach. It's essentially a timeline of the whole thing with the historical context. It's kind of long, but very informative, and not overly academic. \n\n* For a more scholarly take try *Entertaining Satan: Witchcraft and the Culture of Early New England* by John Demos.",
        "This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).\n The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:\n - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity\n - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the \"state-of-the-art\" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.\n In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.\n Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, ",
        " Don't worry, they are REALLY slow. Simply knife every zombie once for a cool, easy 130 points each. To maximise the amount of points you receive per zombie, you should shoot them in the leg four times and then knife them. An easy 170 points a kill. At this rate you should have your first grand in no time.;\n, This round is also very easy, if you are good with a pistol. You will want to go for chest shots, and drop 4 rounds into each zombie (same strategy as before, but chest shots do more damage). A knife will then kill them.\n\n\nEvery level thereafter, it will take an additional 4 shots and the stab to kill them, so a whole clip and a stab on level 3 (note that it is possible to kill them with only a clip with occasional headshots).\nAfter round 3, it is slightly dangerous to keep with the pistol, as you'll need to either get consistent headshots or reload for each zombie, however it does still get points like crazy.\nAt the end of round 2, you should have the points to open the top door.\n\n, This leads you to the MP40 and Stakeout. One person takes the top by the window and the table, and the other can watch the two windows on the bottom. All the zombies will walk toward the person on the top, so the bottom person can use the pistol to rack up points. The person on top is recommended to take the MP40, but nobody should grab the Stakeout.\n\n, DO NOT TURN IT ON. This is the best place to rack up points in the level.\n\n,, When you save claymores, it allows you to use more than two when the situation calls for it.\n\n,, It all depends on how you prefer to play.\n\n,\n\n, The reason the Bowie Knife is a necessity is because the crawlers will not explode when you kill them with it. Plus, it make the Ballistic knife ridiculously powerful when stabbing.\n\n, This should get you to a high round by itself. However, if you dislike crawlers enough to merit buying the bowie knife, you can get rid of them entirely here.\n\n,, This is crucial.\n\n, One player (with monkey bombs and a thundergun) sits by the door and shoots the zombies coming down the alley. This player RUNS THEIR GUN ALL THE WAY OUT OF AMMO TO RELOAD. Failing to do so will open the door, causing crawlers to pour in and zombies spawning behind your group. The other two stand facing down the alley in line with chain link fence.\n\n, you can permanently rid yourselves of the crawlers. Using these simple methods, you can have fun, help your team, and not seem like a total douche in online zombies. Read on for more helpful tips on how to be an efficient, number crunching zombies player.\n\n",
        "This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.\n\nI think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.\n\nIn addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).\n\nFinally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?",
        "The paper presents a method for pruning filters from convolutional neural networks based on the first order Taylor expansion of the loss change. The method is novel and well justified with extensive empirical evaluation.",
        " of kefir grains into a jar. Add ½ to ¾ cup of raw or non-homogenized milk and let sit in a dark environment for 24 hours.\n,,\n\nIf it’s thick and creamy, drain the yogurt from the Kefir grains using a strainer.\n\n, Stir in ¼ to ½ cup of kefir yogurt., Always test the temperature while stirring to obtain the exact temperature. While waiting for the milk to warm, dilute ¼ tsp. of liquid animal rennet into ¼ cup of purified water., Stir for 30 to 60 seconds and cover for 1 hour., If you have a clean break, it should like Jell-O and will leave a hole. If it is still liquid like, cover for another 30 minutes and try this step again., Resist the urge to keep checking on the milk; it needs to retain the heat for the fermentation process to begin.\n\nAfter 4 hours, the curds should have sunken to the bottom of the whey and will have connected back together.\n\n, Try your best to make one solid level piece similar to a pie. Poke several holes on the top of the curd. You will then see whey accumulate in the holes that you created., This should be done right away to get a base line of the pH. Normally, it will be around 6.2 at this point, but it will vary., As a rule of thumb, the desired pH is 5.2. However, this is subjective. For a tougher and denser mozzarella, a desired pH would be 5.3 to 5.4. For a softer mozzarella, a desired pH would be 5.0 to 5.1.\n\nThe fermentation process can range from 2 to 18 hours depending on the amount of cultures present in the kefir and the climate that you live in. Test periodically after the initial test and then every hour once the pH has reached 5.5 to 5.6 range.\n\n, It should be one solid dense piece at this point. Cut the curd into half inch squares., Place the chopped-up curds into the first bowl, fill the second bowl with cool water and the third with ice and water.,, Do not pour directly onto the curds, but rather around the rim of the mixing bowl., Sprinkle 1 tbsp. of salt onto the cheese curds and then dump 2 to 3 more cups of hot water into the bowl and let sit for 1 more minute., If they are ready for stretch, they should stick together and not fall apart. Scoop the curds up and let gravity do its work. The cheese should stick to the spoon and begin to stretch on its own.\n\nYou can assist the stretching by grabbing the bottom of the cheese and pulling it down. If it is too hard to stretch, simply place all the cheese back in the hot water and let it sit a little longer. Stretch into a long 2-inch-wide strip and fold over 3 to 4 times.\nDo not over work the stretching because doing so will make for a tough and dry mozzarella cheese. Once you have repeated this process 3 – 4 times and the mozzarella is smooth, it’s time to form the fresh mozzarella balls.\n\n,, Squeeze the bottom of the ball and break it off., Repeat this step until you have made all of the mozzarella balls and let them sit in the cool water for 10 minutes or until they have all cooled down., This preserves the shape of the fresh mozzarella.\n\nThere you have it: you have made fresh mozzarella using cultures. It is ready to eat at this point and is always best right after the stretch.\n\n",
        "**Obligatory thanks for the /r/bestof, the /r/DepthHub, and the gold!! :D** Just a note to all those new to the subreddit - it's rather strictly moderated, [so be sure to check out the rules of the sub.](_URL_3_) They're not that bad to read, and reading them is the best way to not get your comment deleted! If you guys are interested in some other cool posts, [check out the user profiles some of us have made!](_URL_1_) Thanks again, and hope you enjoy the read!\n\n\n---\n\nUgh, I hate hitting f5 in the middle of writing :P Throws me off. Sorry in advance! \n\nOkay, so Caesar in battle is a REALLY interesting topic that I don't get to talk about nearly enough. I'll discuss the first part of your question as WELL as your second - in that order, but it will be a bit of a read! Buckle up :D\n\nCaesar was, by birth, a member of the patrician class- and they were pretty much the upper crust of Roman society. And Roman society was inherantly martial - all the generals were politicians as well (Even if not all the politicians were generals), which caused LOADS of problems. Caesar himself was many things before he had himself appointed *governor* of Gaul. Note that I said *governor*...because governors were also generals of the legions in the region they were governing. (Hope that made sense)\n\nI'll start you off with Caesar's martial prowess! From Adrian Goldsworthy's *Caesar: Life of a Colossus:*\n\n >  [...] young aristocrats learned how to run, swim in the Tiber and fight with weapons, most particularly the sword and javelin. They were also taught to ride, and Varro, a near contemporary of Caesar's, tells us that at first he rode bareback rather than with a saddle. Much of the instruction in all these skills was supposed to be given by the father or another male relative. \n\n >  [...]\n\n >  Caesar was slightly built and not particularly robust, but his great determination seems to have made up for this. Plutarch tells us that he was a natural horseman and we also read that he accustomed himself to riding with his arms folded behind his back, guiding the trotting horse with his knees. In later life his skill at arms was also praised and the Romans believed that all good commanders should handle sword, javelin and shield as well as they controlled whole legions.\n\nThat's certainly not the most tedious quote I've typed out! But basically what he's saying is that **all** aristocrats of Caesar's social rank were required to at the very least train in weaponry - and although he was slightly built (as a slightly built man, I can sympathize), he had so much sheer *willpower* that he just didn't give a shit. One thing that Caesar was particularly famous for is his legendary *speed.* He pushed his men hard and moved fast enough to always be where his allies never expected him to be. Granted, he constantly outran his supply lines, and Barry Strauss jokingly states that Caesar had no understanding of logistics compared to other generals that we consider to be \"great.\" And yet, that agility won him engagements constantly, and he smashed Gallic army after Gallic army on the shields of his legions. \n\nAnother aspect about Caesar that I have to emphasize - he was an extraordinarily charismatic individual! And as such, he was able to win over his legions with the same skill that he won over women (And he was THE player of his time). To emphasize on that a bit...here's a song that his legions sang during one of his triumphs! (Remember - soldier humour!)\n\n >  Men of Rome, keep close your consorts, here’s a bald adulterer.\nGold in Gaul you spent in dalliance, which you borrowed here in Rome. \n\nTL;DR, he fucked a lot of married ladies. But that's another story! :D\n\nI go [into detail on the loyalty that Caesar's men had for him here.](_URL_0_) But! I'll only quote the relevant part  < 3\n\nQuoting me!\n\n---\n\nSo, the Battle of Pharsalus. One thing you HAVE to remember about Caesar is that he was a brilliant battle commander, and he's especially renowned for two things: His INSANE speed in pushing his men (He was always two steps ahead of his opponents, appearing places faster than anyone could ever expect), and the INSANE loyalty they had for him. Seriously, when I say insane...he could quell mutinies amongst them with ONE WORD. The battle that had immediately preceded Pharsalus, [Dyrrhachium](_URL_2_) was a devastating defeat for Caesar's forces. After it, his men were so ashamed that they apparently begged for decimation, the most infamous punishment of the ancient world. Another example of Caesar's men's INSANE devotion to him and his fame was exactly how far his soldiers would go for him in battle. Here's Plutarch on that:\n\n >  Such a man, again, was Cassius Scaeva, who, in the battle at Dyrrhachium, had his eye struck out with an arrow, his shoulder transfixed with one javelin and his thigh with another, and received on his shield the blows of one hundred and thirty missiles. In this plight, he called the enemy to him as though he would surrender. Two of them, accordingly, coming up, he lopped off the shoulder of one with his sword, smote the other in the face and put him to flight, and came off safely himself with the aid of his comrades.\n\n---\n\nSeriously, that last story is hilariously crazy. It literally sounds like a scene from a movie - and considering that Caesar's *Gallic Wars* were meant as propoganda, that might well have been the desired effect! But let's head to Gaul with this for more context on Caesar as a general - and we'll head to one of the greatest battles that the man fought. The Battle of Alesia. \n\nSo Alesia was a crazy situation. Caesar had been putting down rebellion left and right in Gaul, when all of a sudden, this new leader comes up out of the woodwork, and his name was Vercingetorix. Stop for a sec. That's a fucking BADASS name. Seriously, if I didn't care about what happened to my future children in high school, I would name one of them Vercingetorix. Because that dude sounds like some sort of dinosaur - and (unfortunately, we don't have a description of him in *particular*) the Gauls were known for being tall, blonde haired barbarians, and the leader of them would not only have to have the strength of will, but also the strength as warrior. Anyways, this war leader starts causing loads of trouble, so Caesar catches up with him in the town of Alesia. Now Caesar, wily old fox that he is, doesn't feel like losing half his men to storming Alesia's strong walls. So he besieges the town. And by besieges, I mean he pulled a Caesar and set his army to building an 11 mile wall surrounding the city. Unfortunately for Caesar, the Gauls were able to get messengers out by cavalry to all the surrounding tribes - who knew that Caesar was just as pinned down as Vercingetorix was. Unfortunately for Alesia, it was a **big fucking town.** And a town that big has lots of people. And lots of people need lots of food - hence the point of a siege ;) Anyways! So as soon as the legions finished building *that* wall, Caesar told them to build another one. Facing out. You can just imagine how much manpower that took. Nevertheless, the number that Caesar gives for the army that came to relieve Alesia was about 260,000. And Alesia had at least 80,000 **fighting** men inside. And Caesar had about 60,000. Yeah, he was a TINY bit outnumbered here. But he was CAESAR dammitall. And here is the man's firsthand account of the battle. His...\n\n >  arrival was known through the colour of his cloak, which he always wore in battle as a distinguishing mark and the troops of cavalry and the cohorts which he had ordered to follow him were also visible, because from the higher parts of the hill these downward slopes and dips could be seen. Then the enemy joined battle: both sides cheered, and the cry was taken up by a shout from the men within the fortifications and rampart. Our troops threw their *pila* [spears] and got to work with their swords. Suddenly [the Gauls] spotted the cavalry behind them; other cohorts approached. The enemy turned around and were caught as they fled....\n\nIn other words, Caesar was a mobile morale machine. Wherever he went, he held the line, and his men fought all the harder for his presence. Well, that and the reinforcements who were with him. But he was still a HUGE presence, not least because (remember earlier?) his men fucking LOOOOOOOVED their bald adulterer. \n\nNow, onto the last example. This one is years later - and in this battle, Caesar is said to have said that he 'had fought many times for victory, but at Munda, he fought for his life.'\n\nCaesar ordered his men to advance, and the opposing legions met them in combat. The fighting was INCREDIBLY fierce, despite the Pompeiian (they were fighting under Pompey the Great's son) forces being completely green recruits - and Caesar's legions were pushed back, which was a HUGE shock to everyone there. His line all but collapsed - but Caesar, being Caesar, matched the crisis with his casual brilliant insanity. Yes, I called Caesar insane. You will too. Here's a quote from Goldsworthy again, cause it's the only source I have handy. \n\n >  He is said to have advanced to within 10 paces of the enemy line [alone. He grabbed one of his men's shields.]. At first he was alone, dodging the missiles or catching them in his shield, but he was then joined by the nearest officers, and finally by the legionaries. \n\nNeedless to say, he won that battle, cause his troops were NOT going to break after that stunt.\n\nYeah. That's one HELL of a fucking commander.",
        "- Strengths:\n\n1) This paper proposed a semi-automated framework (human generation -> auto\nexpansion -> human post-editing) to construct a compositional\nsemantic similarity evaluation data set.\n\n2) The proposed framework is used to create a Polish compositional semantic\nsimilarity evaluation data set which is useful for future work in developing\nPolish compositional semantic models.\n\n- Weaknesses:\n\n1) The proposed framework has only been tested on one language. It is not clear\nwhether the framework is portable to other languages. For example, the proposed\nframework relies on a dependency parser which may not be available in some\nlanguages or in poor performance in some other languages.\n\n2) The number of sentence pairs edited by leader judges is not reported so the\ncorrectness and efficiency of the automatic expansion framework can not be\nevaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs\nneed further post-editing is worrying. \n\n3) There are quite a number of grammatical mistakes. Here are some examples but\nnot the complete and exhaustive list:\n\nline 210, 212, 213: \"on a displayed image/picture\" -> \"in a displayed\nimage/picture\"\n\nline 428: \"Similarly as in\" -> \"Similar to\"\n\nA proofread pass on the paper is needed.\n\n- General Discussion:",
        "This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. \n\n-This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.\n\n-The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.\n\n-The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.\n",
        "Solitary bees are other species than social bees, such as the honeybee. They make it on their own by definition.\n\nAnd about being adopted by another nest, I've got no idea how easy it is for a bee to locate an unknown hive. If your \"lost bee\" would find one, she would most likely be chased away by the guard bees because her smell was different, but if she had the foresight to load up on nectar or pollen first, she might be admitted.\n\nWe did an experiment where we marked 100 honeybees with tiny number stickers on their backs. Newborn adult bees are relatively easy to handle, they don't fly yet and their stingers aren't very effective. We put the bees in the hive from which the brood had been removed. We checked the hive for the numbered bees daily. Once they were old enough to leave the hive to forage for nectar and pollen, some would end up in neighboring hives. In the end, about 10% migrated. Since these hives were pretty close together in a bee stand, maybe they just landed at the wrong address by mistake.",
        "Just some feedback on your idea:\n\nThe criteria for determining a filter that is not important was the focus of my ICLR16 paper: RandomOut: Using a convolutional gradient norm to win The Filter Lottery ",
        ",,,\n\nIn effect, the union may be voted out of your workplace.\nElections of this type occur under the laws in the NLRA, which governs the general U.S. national labor relations of most private sector workers.\n\n\n\n,,\n\n\nInstead of relying on the power of rules in the union-state to gain financial support for the union -- perhaps, including firings for refusing or failing to follow union-contract rules -- union officials may have to sell the benefits of union membership to each individual employee in the open union shop.\n\n, It can be worth it, only if you, the petition signers, agree that you could win.\n\n, You can find a directory of the regional NLRB offices in your area on the website., You must fill in the names of the union and the employer in the blank spaces above on the petition form before you collect signatures.\n\n\nUse no employer help to collect signatures of 30% or more of employees, with no employer resources to get votes (no employer motivation, promises and not on company work time), or it will be thrown out, if proven.\n\n, This one sheet of paper is easy to fill out, and is available from any Regional Office of the NLRB.If the number of valid signatures are more than 50% of the eligible employees of that particular bargaining unit represented by the union, then the signed employees can request that your employer immediately withdraw recognition from the union, because it does not enjoy the support of a majority of employees in the bargaining unit.\n\n,",
        " to the little piece of fruit behind the broken machine and under the tree. If you can't find it, go to the map and click somewhere in the top right corner. The villager will pick up the fruit and plant it. You will either have bananas, coconuts, or mangoes, depending on the fruit you start out with. When your tree is finished growing, a farmer can harvest fruit from it.;\n,,, They will start a fire.\n\n, They will probably run away. So, drag them to the blackened sticks (torches) next to the beehive. They will start Trying an idea! They will light the torches and hold them in front of the beehive. The bees will fly away and your farmer can start harvesting honey.\n\n, They will try on the cloak. The cloak will fit a villager and the villager will become the Tribal Chief! They help, but they don't work.\n\n, He or she will start to repair it.\n\n, The builder will start removing the leaves.\n\n, He will begin Making magical food!\n\n, Make sure they research! If you don't have anybody left to research... wait until the food supply (on the side of the screen) is at least 80. When it is, drag the farmer to the research table until your research is at 200. When it is, drag the farmer-turned-scientist back to farm until the food supply is 160. Then drag him/her back to research until it's 300, and so on.\n\n, He will, hopefully, Go inside with her. If she doesn't have a baby in her arms, drag her back. If they come out and she still doesn't, drag her back. Do this over and over again until the woman is fertile.\n\n,",
        " With experience, the process will take less than 5 minutes, but allow extra time if this is your first time changing the bulbs. You don't need help from anybody. This is a very light job. You may wish to change both HID light bulbs at the same time simply because if one of them failed the other may fail soon.;\n, 2006-2009 Priuses use D4R HID bulbs. The stock bulbs are made by Phillips and have a color temperature of 3400K. These can be found on eBay for $35 per bulb rather than the $150 charged by Toyota. Other brands can be found for $50 per pair.\nOther color temperatures are also available. The higher the color temperature, the more blue the lights will be (example: 8000K is much bluer than the stock 4300K (bright yellowish)).\nWhile it may be a good idea to replace both bulbs at the same time, if you choose not to do this, make sure to get the same color temperature bulb as the existing one or your headlights will appear different. In some places, it may be illegal to use higher kelvin (more blue or purple) bulbs. Some users note that the standard 4300K bulbs provide the best visible light for nighttime driving, and do not recommend going above 5000K.\n\n,\n\n\nPhillips head screwdriver\nFlathead screwdriver\nSmall flashlight\nTelescopic mirror (helps to take a peek before and after removing parts that you can't see well)\nRubber or latex gloves (for grip and to avoid getting oil on the new bulbs)\n\n,, (You do not need to remove all the fasteners or the entire cover). This fastener is not a regular screw. Turn it with a phillips screwdriver without pushing down, and the center will pop up. You can then use a flathead screwdriver to gently pry the clip out of its hole.\n\n, This is a black plastic piece that looks like a snorkel, and is blocking your access to the rear of the passenger side headlight assembly. Once you have removed the fastener in the previous step, you can gently lift the plastic radiator cover to reveal the plastic fastener holding the vent tube. The fastener has two indentations that will receive a flathead screwdriver. Gently pry the fastener out, and remove the vent tube. As you remove the tube, it is a good idea to note how it seats on the opening below so you'll know how to replace it.\n\n, It has a pop-out screw or clip on the left and one threaded screw towards the firewall on top with a bolt/washer combo that holds it down. Also, there is an electrical wire that pops free from the right side. Take off the two electrical connectors at the bottom of the front of the reservoir by squeezing them and pulling. Now the container will come out with a little manipulation. The front plastic tab that goes to that threaded screw hold down will need to be bent just a tad to release from under the front lip and the whole thing will come out. You can probably let the container rest right there lifted up and out of the way; if you have very large hands, you can remove the “slide off” rubber hose connectors \"on bottom\" that will allow you to completely remove it and sit it on the ground.\n\n,, These are the two that are closest to the headlight assembly, and will create a bit more room for your hand to reach behind the headlight assembly. These relays have several pins and pull straight out of the fusebox. Apply steady pressure and a slight wiggle and they will pull out.\n\n, This is the round plastic part that is about 4 inches (10.2 cm) across and has little fins around the outside. You need to turn it about an eighth of a turn counterclockwise. It has a very sticky O-ring to keep out moisture and may be very reluctant to turn. If this is the first time this part has been removed, then this step is by far the most time consuming of the entire process. The little fins to give you traction when attempting to turn... don't hit them with a screwdriver nor should you use pliers or tools, as you \"will\" break them off if you do!!! Using some garden gloves to improve grip and grab and turn counterclockwise. If it resists wiggle it a bit to loosen the large O ring underneath and try again. \"Constant\" tension seems to be the key... if you look very close you will see it turning very slowly. It takes about an eighth of a turn before it stops and you can wiggle it free. Once you've done this once, it’s much easier should you need to do it again.\n\n, You can gently move the cover out of the way without disconnecting the connector... just be careful not to pull on the wires.\n\n, Turn the chrome connector at the top (with a wire mesh connection) about an eighth of a turn counterclockwise and you will hear a click. Then pull it off. It may be a good idea \"not\" to touch the inside of this connector as it comes from the ballast with high voltage. (One user reported putting it near metal to discharge it and on one occasion actually hearing a pop from the arc).\n\n, Gloves are not required but may be useful. Just handle the bulb at the socket end.\n\n, Use a mirror and flashlight to see how the wire retaining spring latches \"before\" you remove it. They pull in, and back \"towards the front of the car\" and then out away from the base to release to let it fall forward \"towards the engine\" to release the bulb.\n\n, Two things to note: The HID bulb fits only in the right orientation -- see grooves in its base. The solid wire on the bottom of the bulb goes to the bottom when positioned properly in the lens. If you look at the outside of the headlight, you will be able to see the bulb as you remove it, and note its orientation. Replace it the same way, or you will have a nasty shadow on the ground as it shines.\n\n, Be careful not to touch the glass. It helps to watch from the outside of the car through the front of the headlight lens while you insert the bulb to insure correct orientation and seating.\n\n, If you have it fastened correctly, the bulb will be firmly seated and will not move when gently wiggled. Use a mirror again to verify that both tabs are seated properly.\n\n, Remember that the connector will need to be turned 1/8th of a turn clockwise to fasten it, so give it a slight turn in the opposite direction before connecting it to the bulb so you can then turn it to lock it in place. It goes on very gracefully and easily.\n\n,, The rubber seal prevents moisture from destroying the HID bulb inside. Turn the black cover 1/8th of a turn clockwise, making sure it fits well.\n\n, Turn on the car and test the lights, and verify that it lights properly on low and high beams, and that the light is in alignment with the headlight on the other side. If the bulb seems out of alignment, the bulb may be seated improperly. Remove the back of the assembly again and re-insert the bulb to make sure it is seated correctly. Both headlights should be aimed at the same level.\n\n,, Make sure to seat it properly, and then insert the plastic fastener into the hole on top.\n\n, Replace the single fastener that we removed to access the vent tube.\n\n, If you optionally removed the relays, replace them. View the pins to align them properly into the fuse box and gently press them into place until they seat.\n\n,,",
        "First things first: we cannot discuss Tolkien in the context of midcentury academia without recognizing that he is not \"that Tolkien fellow who wrote about hobbits.\" He is the Oxford professor who almost singlehandedly revitalized the study of Anglo-Saxon *literature* with his lecture series \"Beowulf: The Monsters and the Critics.\" While British (and to a good extent American) academia had of course become rather obsessed with 'Old English' texts in the late 19th century, for nationalist reasons verging on racist, the pride was found in the *language*, not the *literature*. \"Beowulf: The Monsters and the Critics\" is as much manifesto as scholarship. Tolkien argues passionately that critics have spent too much time deriding *Beowulf* for not being what they want it to be (pagan/pre-Christian, historically accurate, sidelining the 'childish' monsters in favor of a realistic hero, etc). He argued for a *literary*, not just historical-critical, understanding, appreciation, analysis of the poem. \"Monsters and Critics\" is still considered one of the most important, influential, and even best literature essays of all time.\n\nI begin here partially for the reminder of just how awesome Tolkien was (this is the dude who, *in high school*, proposed the debate topic \"That the Norman conquests were a deplorable event\"). But also because it introduces the central question of writing about Tolkien and Middle-Earth: what are the boundaries of \"scholarship\" and \"criticism\"?\n\nThe publication of LOTR, in particular, in the 1950s spawned a significant amount of attention in the \"book review\" sense. To be certain, these are eminent literature scholars and authors: Naomi Mitchison (Scottish political activist and novelist, considered one of the best historical novelists of the century) helped proofread LOTR and declared it a future classic; Edwin Muir (chaired professor in English at Harvard) trashed them. In fact, Muir trashed them along the lines for which Tolkien had strived to redeem *Beowulf*: \"All the characters are boys masquerading as adult heroes.\" W.H. Auden, a big fan, commented, \"I rarely remember a book about which I have had such violent arguments...I can only suppose that some people object to Heroic Quests and Imaginary Worlds on principle.\"\n\nBut certainly the most entertaining bad review came from Edmund Wilson, in his other life quite skilled at bringing psychoanalytic and Marxist criticism to texts, here reviewing LOTR as an exasperated father having read LOTR and *The Hobbit* countless times with his daughter (nb: his *daughter*):\n\n >   Now, how is it that these long-winded volumes of what looks to this reviewer like balderdash have elicited such tributes as those above? The answer is, I believe, that certain people-especially, perhaps, in Britain-have a lifelong appetite for juvenile trash. They would not accept adult trash, but, confronted with the pre-teen-age article...they bubble, they squeal, they coo.\n\nTolkien, a high-profile figure, attracted the attention of high-profile literary figures *as critics* from the beginning. As for scholarship in the sense of the literary analysis that he had brought to bear on *Beowulf*, or that we might consider today? You can find glimmerings in the more positive and thoughtful reviews of the 1950s, such as Auden's. For the most part, late 1950s/early 1960s scholarship tended to consider LOTR in the context of Tolkien's life and writings, or Tolkien in his literary context.\n\nBut the skyrocketing popularity of LOTR in the mass market in the 1960s had its impact in academia, too (both in terms of scholars paying more attention to \"popular culture\", and being consumers of said popular culture themselves). The first academic conference on Middle-Earth was organized in 1966, and many of the papers considered Tolkien's novels and poetry through literary analysis: good and evil in LOTR, the heroes of LOTR as a commentary on the genres of epic versus fairy tale, and so forth.\n\nIf you're looking for a good overview of these earliest years of Tolkien criticism/scholarship/investigation of the person/study of the early reception (this is pre-*Letters*, pre-*Simarillion* publication, pre-Marxist/feminist criticism, and pre-Tom Shippey scholarship), I'd recommend wrangling a copy of:\n\n* John Ryan, *Tolkien: Cult or Culture?* (1969)\n\nanother collection of essays resulting from a conference, although in scope ranging far beyond the 1966 one (which you can also track down papers from, in Mankato State University publications). Ryan had previously (1967) finished his *PhD dissertation* on the fiction of Tolkien and C.S. Lewis as mythology at Cambridge, which seems like a pretty good marker for the study of something being \"acceptable\" scholarship.\n\nBut it's important to recognize that the scholarly enthusiasm for Tolkien was not necessarily scholarly *praise*. The 1970s rise of feminist and related schools of literary scholarship in academia brought with it both new and revised takes that criticized Middle-Earth in a much more rigorous and less dismissive way than Muir's \"juvenile trash.\" Even Naomi Mitchison, Tolkien's close friend and proofreader, came to criticize the (lack of) presence and role of women in LOTR. The point of critics like Mitchison and Catharine Stimpson was not simplistic 'there should be women,' but rather, the way in which an overwhelmingly monolithic, masculine presence tilted and twisted the narrative in light of the idea of [crafting an English mythology](_URL_0_). The study of Anglo-Saxon literature had once been rooted in a nationalist, exclusionary, and superior motive; Stimpson argued that LOTR's gender and class politics celebrated that ideal.\n\nThe publication of *The Silmarillion* in 1977 (which was almost universally disliked/puzzled at) and then two crucial works of scholarship, Tom Shippey's *The Road to Middle-Earth* (1982) and  Verlyn Flieger's *Splintered Light: Logos and Language in Tolkien’s World*, are generally considered the beginning of the next (modern?) phase of Tolkien scholarship, opening up cultural/linguistic/contextual analysis in addition to the traditional markers of literary analysis (quest, epic, good and evil) and political perspectives. \n\n\"Tolkien studies\" as a catchphrase/subfield is generally dated to the mid-1990s, and the *Tolkien Studies* academic journal launched in 2004. Those probably have more to do with trends in academia than anything Middle-Earth specific, because it's pretty clear that Tolkien scholarship, wrapped up in but flavored differently than Tolkien *criticism*, had already enjoyed several healthy decades.",
        " To create a fire charge, you must have one piece of coal, one gunpowder, and one blaze powder.\n\n\nMine coal using a pickaxe, craft a block of coal into nine pieces of coal, or smelt coal using coal ore and any type of fuel. Coal can also be obtained from chest minecarts in abandoned mine shafts and stronghold storeroom chests.Obtain gunpowder by killing Creepers, Ghasts, or Witches, or search for gunpowder in dungeon chests.\nCreate blaze powder by crafting a Blaze Rod picked up from a Blaze. A Blaze is a mob with yellow skin and black eyes that dwells in the Nether.\nCreate a crafting table by gathering wood, constructing wood planks, and using the wood planks to build the table.\n\n,, The instructions for accessing the crafting grid vary depending on your gaming system.\n\n\nPC version: Right-click on the crafting table to open the crafting grid.\nPE: Tap on the crafting table to open the crafting grid.\nXbox 360 / Xbox One: Press the X button on the controller to access the crafting grid.\nPS3 / PS4: Press the square button on the controller to access the crafting grid.\n\n,,,, A fire charge is a round, black ball featuring gray and orange swirls.\n\n, The fire charges can now be placed in the dispenser.Skip to step #15 to add the fire charges to the dispenser if you already own a dispenser. If your character does not own a dispenser, proceed with the following steps to craft a dispenser.\n\n, To create a dispenser, you must own seven cobblestones, one bow, and one redstone wire.\n\n\nObtain cobblestones from dungeons, strongholds, villages, or jungle temples, or mine cobblestone using a pickaxe.Craft a bow using three sticks and three strings in a 3x3 crafting grid.Place redstone wire by selecting redstone dust in the hotbar and right-clicking on the surface of a block.\n\n,,,,, The dispenser is a gray box that features a hole on the left side of the box., This will bring up the dispenser’s inventory menu.\n\n, The fire charge will be placed inside the dispenser.\n\n, The hole in the dispenser must be facing the space on which you want the fire to start.\n\n, Fire will shoot from the dispenser, and start a fire on the adjacent space.",
        "Section 4 Continue:\nThe experiments in 4.5 are misleading and together with 4.4 they are contradictory to their theoretical results. \nForemost, the readers need to be aware that the decoders with switch units are very powerful (if you ever trained one you would know it too). Therefore what figure 4 presents is extremely misleading because it *only* takes the very last conv layer and use their proposed linear reconstruction algorithm to recover that layer’s input only.  The rest was propagated with the very powerful decoder plus switch units information (see [6] for how powerful the decoder is).  Visually, we can also see how much info switch units carry: for their conv 5 reconstruction with random activation, there is no meaningful info carried by the activation values but all through switch units and we can still obtain the silhouette of the objects in the original images; in fact, pool-1’s switch units alone carries at least 64x224x224 bit information. Their results back in the appendix (Figure5) also shows their reconstruction algorithm’s weakness. The lower layer has extremely bad reconstruction quality from the proposed algorithms. The higher the layer, the better the reconstruction from their proposed algorithms because decoder together switch units has more chance to correct the reconstruction.What the authors should do, is to use their IHT to reconstruct all the way, following what has been done in [1]’s linear reconstruction algorithm, which the authors here failed to compare against, despite its high relevance. In fact, the author of [1] used to be in charge of this submitted project and proposed this experimental approach, with a slightly different algorithm that is presented in [1]; but current authors of this submission failed in achieving similar reconstruction results with their algorithm and hence can only perform their reconstruction algorithm over a single layer. The biggest difference between the reconstruction algorithm from [1] and this submission is that the former used pseudo-inverse of the weight matrix and the latter used the transpose because in their theory, the weight matrix is approximate orthogonal hence its transpose is approximately its inverse.\n There is another serious issue with using high-up conv layer to verify their theoretical claim: recall that the excuse this submission used to discard ReLU non-linearity is that [1] introduced the pairing phenomenon, however, [1] stressed that it only appeared in the first few layers. Hence the authors should have at least chosen the first few layers for their experiments; yet, they knowingly only presented their analysis of the highest layer. \nTable 3 again presents very misleading results. First their “random activation” has a relative error 1.414 for activation space. But if we simply pick zero vectors instead of “random”, it gives 1 as a relative error, which is much smaller than 1.414. Therefore, this “relative error” in the activation space is a very inappropriate measurement for the reconstruction quality from their algorithm. But measuring the relative error in the input space is also a bad metric because of the powerful effects from the decoders. In fact, the term “relative error” is a paraphrase of the evaluation method used in [1], where it is called “reconstruction ratio”. However, it is a much more appropriate measurement coupling the reconstruction algorithm used in [1] because the reconstruction from [1] will always be a subset of the original input by using the pseudo-inverse.\n Recall that in 4.4, the authors approximate the conv weight matrix for conv(5,2) has approximately 0.05-0.1 distortion constant, which means that the “relative error” based on their Theorem 3.3 is 0.6-0.2, but in reality, the relative error is 1.051, which is not only worse than their theoretical guarantee but even worse than simply choosing zero vector as reconstruction. Keep in mind that these results are built on top of the assumption that ReLU does not exist, if we add it on top, these results in Table3 and Figure5 can get further downgraded. One question why there is such a huge gap. In fact, such bad reconstruction results based on their algorithm is not surprising because after all, unlike random weights, the learned conv weights are not approximately orthogonal so Model-RIP does not fit into realistic CNN models. In fact, the coherence is a very direct measurement of how “orthogonal” the learned weights are, which both this submission and [5] (again this submission took over [5]’s idea) measure this quantity: the higher the coherence the less orthogonal.  \n",
        "- Strengths:\ni. Well organized and easy to understand\nii. Provides detailed comparisons under various experimental settings and shows\nthe state-of-the-art performances\n\n- Weaknesses:\ni. In experiments, this paper compares previous supervised approaches, but the\nproposed method is the semi-supervised approach even if the training data is\nenough to train.\n\n- General Discussion:\nThis paper adopts a pre-training approach to improve Chinese word segmentation.\nBased on the transition-based neural word segmentation, this paper aims to\npre-train incoming characters with external resources (punctuation, soft\nsegmentation, POS, and heterogeneous training data) through multi-task\nlearning. That is, this paper casts each external source as an auxiliary\nclassification task. The experimental results show that the proposed method\nachieves the state-of-the-art performances in six out of seven datasets. \n\nThis paper is well-written and easy to understand. A number of experiments\nprove the effectiveness of the proposed method. However, there exist an issue\nin this paper. The proposed method is a semi-supervised learning that uses\nexternal resources to pre-train the characters. Furthermore, this paper uses\nanother heterogeneous training datasets even if it uses the datasets only for\npre-training. Nevertheless, the baselines in the experiments are based on\nsupervised learning. In general, the performance of semi-supervised learning is\nbetter than that of supervised learning because semi-supervised learning makes\nuse of plentiful auxiliary information. In the experiments, this paper should\nhave compared the proposed method with semi-supervised approaches.\n\nPOST AUTHOR RESPONSE\n\nWhat the reviewer concerned is that this paper used additional\n“gold-labeled” dataset to pretrain the character embeddings. Some baselines\nin the experiments used label information, where the labels are predicted\nautomatically by their base models as the authors pointed out. When insisting\nsuperiority of a method, all circumstances should be same. Thus, even if the\ngold dataset isn’t used to train the segmentation model directly, it seems to\nme that it is an unfair comparison because the proposed method used another\n“gold” dataset to train the character embeddings.",
        "The paper presents a theoretical analysis of the convergence of the training error. The presented result is rather general and can potentially apply to many neural networks. \n \n Reviewers pointed out several important concerns regarding the mathematical rigor and precision of the claims. The authors' response partially addressed concerns about the scope of the claims and unclear arguments of the proofs. \n \n We invite the authors to submit a revision of the paper, where all statements and proofs are mathematically clear, to the workshop track.",
        "We highly appreciate the time and feedback from all the reviewers, all of which we will take into serious consideration in our revision. We will particularly strengthen and clarify the empirical experiments. Below we address some of the major points: \n\n[Testing Accuracy Score]\nWe agree with the reviewers' point on the \"testing accuracy\" score, but think that it still provides some valuable insight about the dataset. Its blindness to the background can be a good thing in that it captures more information about the \"effective amount\" of objects the dataset contains.  The problem is that it is very difficult to obtain a *perfect* score, and reporting more than one metrics (in an objective fashion) can help to gain more comprehensive understandings. \n\n[Repulsive Term in High Dimension]\nOur repulsive force works due to two tricks: 1) scaling the bandwidth with the data diversity using the median trick, which alleviates the exponential decay of RBF kernel. 2) define kernel on the feature space instead of the raw pixels of the images, which allows us to respect the manifold structure of the images. The framework of SVGD allows us to use any positive definite kernels and change it adaptively during iterations, because the kernel only defines the \"tangent space\" for improvement. \n\nSteinGAN without kernel corresponds to Viterbi training of the energy model and we find it work well with careful tuning of parameters, but tend to converge to a small number of bad-looking images after running a large number of iterations; adding the kernel under the same setting helps prevent this problem. Our current results on CIFAR10 shows that SteinGAN without kernel gives an inception score of 6.34, while that SteinGAN with kernel gives 6.76. \n\n[Amortized is slower than non-amortized]\nAlthough the amortized algorithm has the overhead of updating $\\xi$, it stores the information in a generative network, and allows us to simulate as many images as we need. By using the one-step gradient update we proposed, the update of $\\xi$ is the same as standard backpropagation except replacing the Dlogp with the SVGD gradient. ",
        "Sassey () is a commune in the Eure department in Normandy in northern France.\n\nPopulation\n\nSee also\nCommunes of the Eure department\n\nReferences\n\nCommunes of Eure",
        "  We present a comprehensive and self-consistent modelling of the D' type\nsymbiotic star (SS) HD330036 from radio to UV. Within a colliding-wind\nscenario, we analyse the continuum, line and dust spectra by means of SUMA, a\ncode that simulates the physical conditions of an emitting gaseous cloud under\nthe coupled effect of ionization from an external radiation source and shocks.\nWe find that the UV lines are emitted from high density gas between the stars\ndownstream of the reverse shock, while the optical lines are emitted downstream\nof the shock propagating outwards the system. As regards with the continuum\nSED, three shells are identified in the IR, at 850K, 320 K and 200 K with radii\nr = 2.8 10^13 cm, 4 10^14$ cm, and 10^15 cm, respectively, adopting a distance\nto Earth d=2.3 kpc: interestingly, all these shells appear to be circumbinary.\nThe analysis of the unexploited ISO-SWS spectrum reveals that both PAHs and\ncrystalline silicates coexist in HD330036, with PAHs associated to the internal\nshell at 850 K, and crystalline silicates stored into the cool shells at 320 K\nand 200 K. Strong evidence that crystalline silicates are shaped in a disk-like\nstructure is derived on the basis of the relative band strengths. Finally, we\nsuggest that shocks can be a reliable mechanism in activating the annealing and\nthe consequent crystallization processes. We show that a consistent\ninterpretation of gas and dust spectra emitted by SS can be obtained by models\nwhich accounts for the coupled effect of the photoionizing flux and of shocks.\nThe VLTI/MIDI proposal recently accepted by ESO aims to verify and better\nconstrain some of our results by means of IR interferometric observations.\n",
        "This paper explores a variational autoencoder variant.\n \n ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.\n \n This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready.",
        "One of the postulates of special relativity is that the speed of light is invariant across reference frames. (The first evidence for this postulate was provided by the Michelson-Morley experiment.) The postulate means the following. Suppose you are travelling at 0.99*c* with respect to me, and you emit a light signal. You measure the speed of that light signal to be *c*. But I *also* measure the speed of that same light signal to be *c*.\n\nThis is in stark contrast to how velocities add in Galilean relativity, which would say that you measure the light to move at *c*, but I measure it to move at 1.99*c*. This peculiarity that *c* must remain invariant implies that *c* is, in fact, a cosmic upper speed limit to relative speeds. (Technically speaking, to conclude that *c* is the upper speed limit, we also need to assume that causality cannot be violated, but no one legitimately believes otherwise.)\n\nThe invariance of *c* means that a lot of the physics we knew before relativity have to be changed, because now all of the physical laws have to somehow mix together to keep anything from traveling faster than *c* and for *c* to remain invariant. One such change is that it takes an infinite amount of energy to reach *c*. If it only took a finite amount of energy to reach *c*, we could always switch reference frames (since kinetic energy is dependent on reference frame) to impart enough energy to a particle to make it reach *c*. But that's impossible.\n\nThere did not have to be a finite upper speed limit to all relative speeds. The value of *c* could very well have just been infinite, in which case Galilean relativity would be correct. Whatever the value of *c* (whether it turned out to be finite or infinite), it would always take an infinite amount of energy to reach *c*. A better question is why the upper speed limit is actually finite. Well, it just is.\n\n---\n\n**edit:** This is in response to the growing number of incorrect responses about the nature of light. I want to emphasize that photons have no valid reference frame. The postulate of SR that says that *c* is invariant across reference frames is equivalent to the fact that single photons have no rest frame. Statements of the ilk \"photons experience no time\", \"the speed of light from the perspective of a photon is infinite\", \"photons travel from point A to point B instantly from their perspective\", \"from the perspective of photons...\", etc. are all wrong. Any statement which requires the existence of the rest frame of a photon is in direct violation with the second postulate. In the same spirit, asking questions about what photons experience and making hypothetical statements about photons experience are also meaningless because such questions and hypotheses are formulated within the framework of a theory whose postulates must be violated even to accept the premise of such questions or hypotheses.\n\nI am aware that it is extremely common in pop-sci articles, pop-sci YouTube channels, and subs like /r/explainlikeimfive to make such statements about a photon's rest frame, but they are wrong. Invariably, the statements are made with some vague or misleading or incomplete or incorrect reference to the effect of time dilation, which describes how coordinate time between two events is different for different observers. At no point in the analysis are you ever allowed to consider a reference frame which moves at speed *c* with respect to some other frame. Such a frame does not exist. Also, at no point can you conclude that time passes at a different rate for you if you have some velocity with respect to some other observer. You always experience 1 second as 1 second, no matter how fast you move. Time always feels the same for you. Hence the problem of a statement like \"photons experience zero time\" is made even clearer.",
        "This looked like an exciting though mathematically dense paper, so I spent a couple of days working through it. The presentation in the paper is not very clear and is in places incomplete, so I hope the author will correct me if I misunderstood some of the details of the proposed method.\n\nAs far as I can tell, the gradients of the variational objective w.r.t. the parameters phi of the variational posterior q(z, zeta) are incorrect, due to some of the probabilistic dependences being ignored. Most significantly, the gradient w.r.t parameters of q(z|x) seems to contain only the term obtained by differentiating -KL(q||p), but no contribution from the term E_q[log P(x|zeta)] in the objective. This likelihood term is considerably harder to deal with than the KL term as it does not factorize, so it is important to explain how it is taken into account when updating parameters of q(z|x). Ignoring this term amounts to training q(z|x) to match the prior p(z), teaching it to ignore the current observation x (assuming a factorial q(zeta|z) and r(zeta|z) that does not depend on x). This makes q(z|x) a completely ineffective recognition model.\n\nNote that making r dependent on x or using a hierarchical posterior does allow q(z|x) to learn some dependence on x as z_i will depend on zeta_{j",
        "  A synfire chain is a simple neural network model which can propagate stable\nsynchronous spikes called a pulse packet and widely researched. However how\nsynfire chains coexist in one network remains to be elucidated. We have studied\nthe activity of a layered associative network of Leaky Integrate-and-Fire\nneurons in which connection we embed memory patterns by the Hebbian Learning.\nWe analyzed their activity by the Fokker-Planck method. In our previous report,\nwhen a half of neurons belongs to each memory pattern (memory pattern rate\n$F=0.5$), the temporal profiles of the network activity is split into\ntemporally clustered groups called sublattices under certain input conditions.\nIn this study, we show that when the network is sparsely connected ($F<0.5$),\nsynchronous firings of the memory pattern are promoted. On the contrary, the\ndensely connected network ($F>0.5$) inhibit synchronous firings. The sparseness\nand denseness also effect the basin of attraction and the storage capacity of\nthe embedded memory patterns. We show that the sparsely(densely) connected\nnetworks enlarge(shrink) the basion of attraction and increase(decrease) the\nstorage capacity.\n",
        "This paper proposes a new model for sentence classification. \n\nPros:\n- Some interesting architecture choices in the network.\n\nCons:\n- No evaluation of the architecture choices. An ablation study is critical here to understand what is important and what is not.\n- No evaluation on standard datasets. On the only pre-existing dataset evaluated on a simple TFIDF-SVM method is state-of-the-art, so results are unconvincing.",
        "A few reasons. Mostly:\n\n-The brain uses about 30% of the energy stored in the human body, and the head produces about an equivalent amount of heat despite being of comparatively low mass/volume (typically the head is only 12.5% of the mass of a body)\n\n-The head, neck, and face have a very high density of blood vessels close to the surface. Cuts to the head and face tend to bleed a LOT even when they're just superficial \n\n-The face has such a high surface area with the nose and ears and lips, these allow for heat to dissipate faster\n\n-Blowing cold air at the face ensures that cold air can be taken in by the lungs to cool the core\n\nThese four things combine to make the face a beautiful heat sink, just like the one in your PC.\n\nAdditionally: applying cool fluids (typically water but air could work too) to the face will engage your body into preparing to go under water, reducing your heart rate and allowing you to take slower, deeper breathing. Dipping your face and whole head if possible into cool (not cold) water is one of the fastest ways to mitigate the onset/symptoms of heat stroke.",
        " This is the fastest way to make a dog bed and does not require any sewing.\n\n, The size will depend on the size of your dog. You will want to err on the side of extra fabric in order to leave enough room for the stuffing. For a small dog, two one-yard pieces of fabric will be enough. For a medium to big dog, two, two-yard pieces of fabric will do the trick.\n\n\nFleece tie blankets are extremely easy to make because they don't require any sewing and fleece is a relatively cheap material. You can buy yards of patterned or solid colored fleece at any craft store.\nYou can mix and match patterns and solids by using a single color on one side of the blanket and a patterned print on the other.\n\n, In other words, the rougher sides of the fabric will be touching one another.\n\n, Make sure to use the lines on your template for a straight cut. You do not need to cut the other edges unless they are uneven or you are using a rotary cutter with a patterned cut.\n\n, Repeat for the remaining three sides of the fleece.\n\n, It's helpful to pin the tape measure down so that it doesn't move.\n\n, Only cut to just below the tape measure line.\n\n, You should now have fringes along all sides of the fleece that are one-inch thick and four-inches long.\n\n, For this style dog bed, a lightweight stuffing made of polyester or egg crate foam works best.\n\n,, Continue to tie each fringe together until you have made your way around the entire bed.\n\n,",
        "This paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them. The results of the paper show that adversarial examples in fact can be easily detected. Moreover, such detector generalizes well to other similar or weaker adversarial examples. The idea of this paper is simple but non-trivial. While no final scheme is proposed in the paper how this idea can help in building defensive systems, it actually provides a potential new direction. Based on its novelty, I suggest an acceptance.\n\nMy main concern of this paper is about its completeness. No effective method is reported in the paper to defend the dynamic adversaries. It could be difficult to do so, but rather the paper doesn’t seem to put much effort to investigate this part. How difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper. Such investigation may essentially help improve our understanding of adversarial examples.\nThat being said, the novelty of this paper is still significant.\n\nMinor comment:\nThe paper needs to improve its clarity. Some important details are skipped in the paper. For example, the paper should provide more details about the dynamic adversaries and the dynamic adversary training method. \n\n",
        "  The structures of globular clusters (GCs) reflect their dynamical states and\npast histories. High-resolution imaging allows the exploration of morphologies\nof clusters in other galaxies. Surface brightness profiles from new Hubble\nSpace Telescope observations of 34 globular clusters in M31 are presented,\ntogether with fits of several different structural models to each cluster. M31\nclusters appear to be adequately fit by standard King models, and do not\nobviously require alternate descriptions with relatively stronger halos, such\nas are needed to fit many GCs in other nearby galaxies. The derived structural\nparameters are combined with corrected versions of those measured in an earlier\nsurvey to construct a comprehensive catalog of structural and dynamical\nparameters for M31 GCs with a sample size similar to that for the Milky Way.\nClusters in M31, the Milky Way, Magellanic Clouds, Fornax dwarf spheroidal and\nNGC 5128 define a very tight fundamental plane with identical slopes. The\ncombined evidence for these widely different galaxies strongly reinforces the\nview that old globular clusters have near-universal structural properties\nregardless of host environment.\n",
        "Fairbank may refer to:\n\nPeople\n Fairbank (surname)\n\nPlaces \n 67235 Fairbank, asteroid\n\nUnited States\n Fairbank, Arizona\n Fairbank, Iowa\n Fairbank Township, Buchanan County, Iowa\n Fairbank Island (Michigan)\n\nCanada\n Fairbank, Newfoundland and Labrador\n Fairbank, Toronto, Ontario - a neighbourhood in Toronto in which the following are located:\n Fairbank Memorial Park\n Fairbank Middle School\n Fairbank station - an underground light-rail station on Line 5 Eglinton\n Fairbank Island (Fairbank Lake), Northern Ontario\n Fairbank Lake, Northern Ontario\n\nOther uses \n Fairbank's disease, or multiple epiphyseal dysplasia, a genetic disorder\n\nSee also \n\n \n \n Fairbanks, Alaska\n Fairbanks (disambiguation)",
        "No, there was no science of economics or really much intellectual understanding of the topic. Gary Young in *Rome's Trade with the East* even goes so far as to say that all of the policies that seem to show economic planning and foresight, such as the roads between the Nile and the Red Sea that allowed for much easier transport and protection, were purely reactive in nature. That is, the Roman didn't think \"Hmm, if I build this road merchants might be more willing to make this journey\" but rather \"there are merchants going to the Red Sea and unless we do something to help them we will lose all that tax revenue\". I personally wouldn't go quite as far as Young, but it can be very difficult to see specific measures taken to promote trade.\n\nWithin the empire it gets a bit more complicated. There is enough incidental evidence supporting it that we can guess that different cities had particular \"market days\" in a manner not unlike Medieval markets, but those markets were bound up in expressions of royal or lordly power, as they were granted to individual cities. There is no real evidence of something similar in Roman times with one partial exception: the Senate could use its taxation powers to favor particular ports. This is exactly what happened with Rhodes and Delos during the Republic, as the Senate granted privileges to the latter which almost instantly destroyed the position of the former (a terrible move, all in all). But it doesn't seem to be more fine grained than that.\n\nFinance gets even more complicated, but it also has perhaps the most prominent example of real economic understanding on the part of Roman authorities. Early in Tiberius' rule through a rather complicated series of events many of the banks suffered an acute crisis as they could not repay their loans, and so they called in their debts and in a classic banking run pattern their debtors could not repay them. Tiberius saw this situation spinning out of control, so offered loans from the Imperial treasury at sharply reduced rates, which restored stability to the financial system in a way that would be approved by most economists today. But this moment is exceptional, at least within our sources. Beyond that there does not seem to have been much in the way of financial regulation outside of Roman contract law.\n\nSo on the whole, there was no individual position of finance or economics minister. Actions taken were largely ad hoc and never really amounted to a coherent policy. That being said, these administrative actions could still have major effects on the economy.",
        "William Pinkney (born 1935) is an American sailor and retired executive. In 1992, he became the first African American to sail around the world solo via the Cape of Good Hope and Cape Horn.\n\nEarly life\nPinkney was born and raised on the South Side of Chicago. His parents, Marion Henderson and William Pinkney Sr, divorced when he was young. He attended Tilden Technical High School and graduated in 1954.\n\nNavy and later career\nPinkney joined the United States Navy in 1956, serving as a hospital corpsman. He left the Navy in 1964, and moved to Puerto Rico for a few years, where he learned how to sail.\n\nAfter returning to the mainland in 1961, Pinkney worked as a marketing manager for Revlon and later the Johnson Products Company. He started planning to sail around the world in 1985, after being made redundant from his job at the Department of Human Services, and fundraised throughout the late 1980s.\n\nVoyage\nPinkney's voyage around the world lasted 22 months. He traveled approximately . He departed from Boston on August 5, 1990, sailing first to Bermuda, then along the eastern South American coastline, across the Atlantic Ocean to Cape Town, South Africa, across the Indian Ocean to Hobart, Tasmania, across the South Pacific Ocean, around Cape Horn, and up the eastern South American coastline, finally ending up back in Boston.\n\nPinkney sailed on a Valiant 47, a 47-foot cutter named The Commitment. The expedition cost around $1 million.\n\nOn June 9, 1992, he arrived at the Charlestown Navy Yard in Boston Harbor where he was greeted by over 1,000 school students and 100 officers from the Navy, Coast Guard, and National Park Service.\n\nThe story of his trip was told in the documentary The Incredible Voyage of Bill Pinkney, based on Pinkney's own footage. The film won a 1992 Peabody Award. He also wrote a children's book about his experiences called Captain Bill Pinkney's Journey.\n\nAmistad replica \nFrom 2000 to 2002, Pinkney served as the first captain of the replica of the Amistad. As captain, he took a group of teachers to Africa as part of a trip that traced the route of the Middle Passage crossing from Senegal to the Americas.\n\nAwards and honors\nHe was named Chicago Yacht Club’s Yachtsman of the Year in 1992. In 1999, he was named one of the Chicagoans of the Year by Chicago magazine.\n\nHe is a member of the National Sailing Hall of Fame.\n\nReferences\n\nExternal links\n Capt. William \"Bill\" Pinkney, interview archive with The HistoryMakers (African American video oral history archive)\n\n1935 births\nLiving people\nPeople from Chicago\nAfrican-American sailors\nUnited States Navy corpsmen\n21st-century African-American people\n20th-century African-American people\nAfrican-American United States Navy personnel",
        "In geology, continental collision is a phenomenon of plate tectonics that occurs at convergent boundaries. Continental collision is a variation on the fundamental process of subduction, whereby the subduction zone is destroyed, mountains produced, and two continents sutured together. Continental collision is only known to occur on Earth.\n\nContinental collision is not an instantaneous event, but may take several tens of millions of years before the faulting and folding caused by collisions stops. The collision between India and Asia has been going on for about 50 million years already and shows no signs of abating.  Collision between East and West Gondwana to form the East African Orogen took about 100 million years from beginning (610 Ma) to end (510 Ma). The collision between Gondwana and Laurasia to form Pangea occurred in a relatively brief interval, about 50 million years long.\n\nSubduction zone: the collision site\nThe process begins as two continents (different bits of continental crust), separated across a tract of ocean (and oceanic crust), approach each other, while the oceanic crust is slowly consumed at a subduction zone. The subduction zone runs along the edge of one of the continents and dips under it, raising volcanic mountain chains at some distance behind it, such as the Andes of South America today. Subduction involves the whole lithosphere, the density of which is largely controlled by the nature of the crust it carries.  Oceanic crust is thin (~6 km thick) and dense (about 3.3 g/cm3), consisting of basalt, gabbro, and peridotite. Consequently, most oceanic crust is subducted easily at an oceanic trench. In contrast, continental crust is thick (~45 km thick) and buoyant, composed mostly of granitic rocks (average density about 2.5 g/cm3). Continental crust is subducted with difficulty, but is subducted to depths of 90-150 km or more, as evidenced by ultra-high pressure (UHP) metamorphic suites. Normal subduction continues as long as the ocean exists, but the subduction system is disrupted as the continent carried by the downgoing plate enters the trench. Because it contains thick continental crust, this lithosphere is less dense than the underlying asthenospheric mantle and normal subduction is disrupted. The volcanic arc on the upper plate is slowly extinguished. Resisting subduction, the crust buckles up and under, raising mountains where a trench used to be. The position of the trench becomes a zone that marks the suture between the two continental terranes. Suture zones are often marked by fragments of the pre-existing oceanic crust and mantle rocks, known as ophiolites.\n\nDeep subduction of continental crust\nThe continental crust on the downgoing plate is deeply subducted as part of the downgoing plate during collision, defined as buoyant crust entering a subduction zone.  An unknown proportion of subducted continental crust returns to the surface as ultra-high pressure (UHP) metamorphic terranes, which contain metamorphic  coesite and/or diamond plus or minus unusual silicon-rich garnets and/or potassium-bearing pyroxenes.  The presence of these minerals demonstrate subduction of continental crust to at least 90–140 km deep.  Examples of UHP terranes are known from the Dabie–Sulu belt of east-central China, the Western Alps, the Himalaya of India, the Kokchetav Massif of Kazakhstan, the Bohemian Massif of Europe, the North Qaidam of Northwestern China, the Western Gneiss Region of Norway, and Mali.  Most UHP terranes consist of an imbricated sheets or nappes.  The fact that most UHP terranes consist of thin sheets suggests that much thicker, volumetrically dominant tracts of continental crust are more deeply subducted.\n\nOrogeny and collapse\n \nAn orogeny is underway when mountains begin to grow in the collision zone. There are other modes of mountain formation and orogeny but certainly continental collision is one of the most important. Rainfall and snowfall increase on the mountains as these rise, perhaps at a rate of a few millimeters per year (at a growth rate of 1 mm/year, a 5,000 m tall mountain can form in 5 million years, a time period that is less than 10% of the life of a typical collision zone). River systems form, and glaciers may grow on the highest peaks. Erosion accelerates as the mountains rise, and great volumes of sediment are shed into the rivers, which carry sediment away from the mountains to be deposited in sedimentary basins in the surrounding lowlands. Crustal rocks are thrust faulted over the sediments and the mountain belt broadens as it rises in height. A crustal root also develops, as required by isostasy; mountains can be high if underlain by thicker crust. Crustal thickening may happen as a result of crustal shortening or when one crust overthrusts the other. Thickening is accompanied by heating, so the crust becomes weaker as it thickens. The lower crust begins to flow and collapse under the growing mountain mass, forming rifts near the crest of the mountain range. The lower crust may partially melt, forming anatectic granites which then rise into the overlying units, forming granite intrusions. Crustal thickening provides one of two negative feedbacks on mountain growth in collision zones, the other being erosion. The popular notion that erosion is responsible for destroying mountains is only half correct - viscous flow of weak lower mantle also reduces relief with time, especially once the collision is complete and  the two continents are completely sutured.  Convergence between the continents continues because the crust is still being pulled down by oceanic lithosphere sinking in the subduction zone to either side of the collision as well as beneath the impinging continent. \n\nThe pace of mountain building associated with the collision is measured by radiometric dating of igneous rocks or units that have been metamorphosed during the collision and by examining the record of sediments shed from the rising mountains into the surrounding basins.  The pace of ancient convergence can be determined with paleomagnetic measurements, while the present rate of convergence can be measured with GPS.\n\nFar-field effects \nThe effects of the collision are felt far beyond the immediate site of collision and mountain-building. As convergence between the two continents continues, the region of crustal thickening and elevation will become broader. If there is an oceanic free face, the adjacent crustal blocks may move towards it. As an example of this, the collision of India with Asia forced large regions of crust to move south to form modern Southeast Asia. Another example is the collision of Arabia with Asia, which is squeezing the Anatolian Plate (present day Turkey). As a result, Turkey is moving west and south into the Mediterranean Sea and away from the collision zone. These far-field effects may result in the formation of rifts, and rift valleys such as that occupied by Lake Baikal, the deepest lake on Earth.\n\nFossil collision zones\n\nContinental collisions are a critical part of the supercontinent cycle and have happened many times in the past. Ancient collision zones are deeply eroded but may still be recognized because these mark sites of intense deformation, metamorphism, and plutonic activity that separate tracts of continental crust having different geologic histories prior to the collision. Old collision zones are commonly called \"suture zones\" by geologists, because this is where two previous continents are joined or sutured together.\n\nReferences\n\nExternal links \nWhere Continents Collide\nDynamics of Continental Collision Zones\nThe Wilson Cycle\n\nPlate tectonics",
        "  We conducted a preliminary field study to understand the current state of\npersonal digital archiving in practice. Our aim is to design a service for the\nlong-term storage, preservation, and access of digital belongings by examining\nhow personal archiving needs intersect with existing and emerging archiving\ntechnologies, best practices, and policies. Our findings not only confirmed\nthat experienced home computer users are creating, receiving, and finding an\nincreasing number of digital belongings, but also that they have already lost\nirreplaceable digital artifacts such as photos, creative efforts, and records.\nAlthough participants reported strategies such as backup and file replication\nfor digital safekeeping, they were seldom able to implement them consistently.\nFour central archiving themes emerged from the data: (1) people find it\ndifficult to evaluate the worth of accumulated materials; (2) personal storage\nis highly distributed both on- and offline; (3) people are experiencing\nmagnified curatorial problems associated with managing files in the aggregate,\ncreating appropriate metadata, and migrating materials to maintainable formats;\nand (4) facilities for long-term access are not supported by the current\ndesktop metaphor. Four environmental factors further complicate archiving in\nconsumer settings: the pervasive influence of malware; consumer reliance on ad\nhoc IT providers; an accretion of minor system and registry inconsistencies;\nand strong consumer beliefs about the incorruptibility of digital forms, the\nreliability of digital technologies, and the social vulnerability of networked\nstorage.\n",
        "  We investigate the Gross-Pitaevskii equation for a classically chaotic\nsystem, which describes an atomic Bose-Einstein condensate confined in an\noptical lattice and driven by a spatiotemporal periodic laser field. It is\ndemonstrated that the exact Floquet states appear when the external\ntime-dependent potential is balanced by the nonlinear mean-field interaction.\nThe balance region of parameters is divided into a phase-continuing region and\na phase-jumping one. In the latter region, the Floquet states are\nspatiotemporal vortices of nontrivial phase structures and zero-density cores.\nDue to the velocity singularities of vortex cores and the blowing-up of\nperturbed solutions, the spatiotemporal vortices are unstable periodic states\nembedded in chaos. The stability and instability of these Floquet states are\nnumerically explored by the time evolution of fidelity between the exact and\nnumerical solutions. It is numerically illustrated that the stable Floquet\nstates could be prepared from the uniformly initial states by slow growth of\nthe external potential.\n",
        "Otradnoye District  () is an administrative district (raion) of North-Eastern Administrative Okrug, and one of the 125 raions of Moscow, Russia. The area of the district is .\n\nEducation\n22 comprehensive secondary schools are located in this district, including School No. 263.\n\nSee also\nAdministrative divisions of Moscow\n\nReferences\n\nNotes\n\nDistricts of Moscow\nNorth-Eastern Administrative Okrug",
        "The Västergötlands Fotbollförbund (Västergötland Football Association) is one of the 24 district organisations of the Swedish Football Association. It administers lower tier football in the historical province of Västergötland.\n\nBackground \n\nVästergötlands Fotbollförbund, commonly referred to as Västergötlands FF, is the governing body for football in the historical province of Västergötland, which partly corresponds with the area now covered by Västra Götaland County. The Association was founded on 17 March 1918 and currently has 279 member clubs.  Based in Skövde, the Association's Chairman is Magnus Gunnarsson.\n\nAffiliated Members \n\nThe following clubs are affiliated to the Västergötlands FF:\n\nAlingsås FK\nAlingsås IF\nAlingsås Internationella FF\nAlingsås Kvinnliga IK\nAlvhems IK\nAmbjörnarps IF\nAnnelunds IF\nAplareds IF\nArdala GoIF\nArentorp Helås FK\nArentorps SK\nAxvalls IF\nBergdalens IK\nBjörketorps IF\nBjörsäters IF\nBK Spark\nBK Trix\nBollebygds IF\nBorås AIK\nBorås GIF\nBorås Kings Idrottsförening\nBorgstena IF\nBorgunda IK\nBosna FC\nBrämhults IK\nBredareds IF\nByttorps FC\nByttorps IF\nDalsjöfors GoIF\nDalstorps IF\nDannike IK\nDardania IF\nDIF Holmalund\nDIK Friscopojkarna\nEdsvära/N.Vånga FF\nEkedalens SK\nElmer-Fåglum FK\nEssunga IS\nFagersanna IF\nFåglums IF\nFalköping United FC\nFalköpings DIK\nFalköpings FK\nFalköpings Kvinnliga IK\nFC Corner\nFC Gauthiod\nFC Kabel Åttio\nFC Lockryd\nFC Södra Ryd\nFC Trollhättan\nFK Yugo\nFloby IF\nFolkabo IK\nForsviks IF\nFrämmestads IK\nFristads GoIF\nFritsla IF\nFröjereds IF\nFutsal Club Tranan\nGällstads AIS\nGällstads FK\nGällstads IF\nGånghester SK\nGerdskens BK\nGöta BK\nGötene IF\nGrimsås IF\nGrolanda IF\nGrönahögs IK\nGullspångs IF\nHåcksviks IF\nHajoms IF\nHåkantorps IS\nHällekis IF\nHällstads IF\nHalvorstorps IS\nHangelösa IF\nHärlunda IF\nHassle-Torsö GoIF\nHåvens IF\nHedareds BK\nHemsjö IF\nHerrljunga SK FK\nHestrafors IF\nHillareds IF\nHjärtums IS\nHögvads BK\nHolmalunds IF Alingsås\nHols IF\nHolsljunga IF\nHörnebo FC\nHörnebo SK\nHorreds IF\nHössna IF\nHova IF\nHudene GoIF\nHyssna IF\nIF Elfsborg\nIF Heimer\nIF Knallen\nIF Olsfors\nIF Tymer\nIF Weimer Lyrestad\nIFK Emtunga\nIFK Falköping FF\nIFK Hjo\nIFK Mariestad\nIFK Örby\nIFK Öxnevalla\nIFK Skövde FK\nIFK Tidaholm\nIFK Trollhättan\nIFK Värsås\nIgelstorps IK\nIK Elmer\nIK Friscopojkarna\nIK Gauthiod\nInlands IF\nJärpås IS\nJula BK\nJung/Kvänum  IF\nKållandsö GoIF\nKällby IF\nKindaholms FF\nKinna IF\nKinnahults IF\nKinnarp-Slutarps IF\nKinne-Vedums FK\nKinne-Vedums IF\nKorsberga IF\nKronängs IF\nLångareds BoIS\nLänghems IF\nLarvs FK\nLerdala IF\nLevene/Skogslunds IF\nLidköpings IF\nLidköpings FK\nLidköping United\nLilla Edets IF\nLimmareds IF\nLjungsarps IF\nLödöse Nygård IK\nLundsbrunns IF\nMagra IS\nMålsryds IF\nMånstads IF\nMarbäcks IF\nMariedals IK\nMariestads BK\nMariestads BoIS DFF\nMariestads BoIS FF\nMellby IK\nMjöbäcks GOIF\nMoholms SK\nMölltorp/Breviks AIF\nMullsjö IF\nNittorps IK\nNorra Fågelås IF\nNorra Härene BK\nNorrby Futsal Club\nNorrby IF\nNorrmalms IK\nNossebro IF\nOds FF\nPars IF\nRackeby IK\nRackeby KIK\nRåda BK\nRåda DBK\nRångedala IK\nRedvägs FK\nRoasjö IF\nRydals GIF\nRydboholms SK\nRyrs Allmänna IF\nSaleby IF\nSandareds IF\nSandhems IF\nSandhults SK\nSätila SK\nSävens BK\nSerbisk-Svensk KIF\nSexdrega IF\nSils DIF\nSils IF\nSjömarkens IF\nSjötofta IF\nSjuntorps IF\nSK Mjörn\nSkara FC\nSkara IF\nSkara Kvinnliga Idrottsklubb\nSkarke KIK Varnhem\nSkene IF\nSkene IF 09\nSkepplanda BTK\nSkoftebyns DIF\nSkoftebyns IF\nSkogslunds IF\nSkövde AIK\nSkövde Kvinnliga IK\nSkultorps IF\nSödra Härene IF\nSödra Vings IF\nSollebrunns AIK\nSomali FF\nSöne SK\nSparsörs AIK\nStenstorps IF\nStora Levene IK\nStora Mellby SK\nSvenljunga IK\nSvenska Suryoyo Föreningen\nTeam Ted Futsal Club\nTibro AIK FK\nTidaholms GoIF\nTidans IF\nTidavads IF\nTigris IF\nTimmele GoIF\nTöllsjö IF\nTomtens IF\nTöreboda IK\nTorestorp/Älekulla FF\nTorestorps IF\nTorsö Bygdegårds och IF\nTrädets IF\nTranemo IF\nTranscom FC Borås\nTrässbergs BK\nTråvads IF\nTrollhättans BoIS\nTrollhättans FF\nTrollhättans FK\nTrollhättans IF\nTrolmens SK\nTuns IK\nTvärred/Vegby FC\nTvärreds IF\nUbbhults IF\nUddebo GoIF\nUlricehamns IFK\nUlvåkers FC\nUlvåkers IF\nUndenäs IF\nUpphärads IS\nValtorps IF\nVåmbs IF\nVänersborgs FK\nVänersborgs IF\nVänga BK\nVara SK\nVårgårda IK\nVärings GoIF\nVarnhems IF\nVartofta SK\nVästerlanda GOIF\nVedums AIS\nVegby SK\nVilske-Kleva BK\nVinninga AIF\nViskafors IF\nVretens BK\nWargöns IK\nWästerhov IK\nÅsaka SK\nÅsarps IF\nÅsarp-Trädet FK\nÅsunden FK\nÄlekulla IF\nÄlgarås SK\nÄspereds IF\nÖrby FC\nÖrslösa IF\nÖstadkulle SK\nÖstra Frölunda IF\nÖxabäcks IF\n\nLeague Competitions \nVästergötlands FF run the following League Competitions:\n\nMen's Football\nDivision 4  -  three sections\nDivision 5  -  six sections\nDivision 6  -  six sections\n\nWomen's Football\nDivision 3  -  two sections\nDivision 4  -  three sections\nDivision 5  -  five sections\n\nFootnotes\n\nExternal links \n Västergötlands FF Official Website \n\nVastergotlands\nSport in Västra Götaland County\nSports organizations established in 1918\n1918 establishments in Sweden",
        "  This paper presents a study of quasi-steady spherical accretion in the early\nUniverse, before the formation of the first stars and galaxies. The main\nmotivation is to derive the basic formulas that will be used in a companion\npaper to calculate the accretion luminosity of primordial black holes and their\neffect on the cosmic ionization history.\n  The following cosmological effects are investigated: the coupling of the gas\nto the CMB photon fluid (i.e., Compton drag), Hubble expansion, and the growth\nof the dark matter halo seeded by the gravitational potential of the central\npoint mass. The gas equations of motion are solved assuming either a polytropic\nor an isothermal equation of state. We consider the cases in which the\naccreting object is a point mass or a spherical dark matter halo with power-law\ndensity profile, as predicted by the theory of \"secondary infall''. Analytical\nsolutions for the sonic radius and fitting formulas for the accretion rate are\nprovided.\n  Different accretion regimes exist depending on the mass of the accreting\nobject. If the black hole mass is smaller than 50-100 Msun, gas accretion is\nunaffected by Compton drag. A point mass and an extended dark halo of equal\nmass accrete at the same rate if M>5000 Msun, while smaller mass dark halos\naccrete less efficiently than the equivalent point mass. For masses M>3 x 10^4\nMsun, the viscous term due to the Hubble expansion becomes important and the\nassumption of quasi-steady flow fails. Hence, the steady Bondi solutions\ntransition to the time-dependent self-similar solutions for \"cold cosmological\ninfall\".\n",
        "**No, they did not think they were oceans.**\n\nSamuel Champlain's 1615 adventures in Ontario represent the first written account we have of a European regarding the Great Lakes. If you consider Lake Champlain to be the \"sixth Great Lake\", then Champlain's \"discovery\" would be sooner, but it wasn't until [his 1615-1616 travels](_URL_0_) that he reached Lake Huron and Lake Ontario.\n\nHe came to the lakes from what is (to a modern eye, looking at a modern map) an odd direction. Traveling up the St. Lawrence River to the modern site of Montreal, he explored the Ottawa River to the west, past the site of the city that would later be built on its banks. He continued west, reaching Lake Nipissing and then downriver to Georgian Bay in Lake Huron. Traveling southeast along the eastern shore of Georgian Bay, he navigated a series of tributaries and rivers between lakes Huron and Ontario before reaching and crossing that latter lake.\n\nChamplain wasn't traveling alone, of course. His voyages were an attempt to make allies and ensure the fur-trading routes along the upper St. Lawrence and Ottawa rivers stayed open. As a friend of the Huron confederacy, he traveled in a fairly large party, contributing his European firearms to campaigns against the Iroquois and others. I'll use [the 1907 translation on _URL_2_](_URL_1_) for this, so you can read the account in full if you want.\n\nChamplain had kind words of the Nipissing people, whom he encountered around the lake of the same name and the northeastern portion of Lake Huron:\n\n > \"These people are very numerous, there being from seven to eight hundred souls, who live in general near the lake. This  contains a large number of very pleasant islands, among others one more than six leagues long, with three or four fine ponds and a number of fine meadows ; it is bordered by very fine woods, that contain an abundance of game, which frequent the little ponds, where the savages also catch fish. The northern side of the lake is very pleasant, with fine meadows for the grazing of cattle, and many little streams, discharging into the lake.\"\n\nThis passage in Champlain's account refers to Lake Nipissing, which is connected by French River to Lake Huron. \n\nIn his account, Champlain refers to Georgian Bay of Lake Huron as \"Lake Attigouautan\" or the lake of the Attigouautan, a term given by the Huron to Champlain.\n\n > \"All this region (of Lake Huron) is still more unattractive than the preceding, for I saw along this river only ten acres of arable land, the rest being rocky and very hilly. It is true that near Lake Attigouautan we found some Indian corn, but only in small quantity. Here our savages proceeded to gather some squashes, which were acceptable to us, for our provisions began to give out in consequence of the bad management of the savages, who ate so heartily at the beginning that towards the end very little was left, although we had only one meal a day. But, as I have mentioned before, we did not lack for blueberries and strawberries; otherwise we should have been in danger of being reduced to straits.\"\n\nAgain and again in Champlain's account, he discusses the quality of the soil and the food/game available in the area. This is because Champlain's journal isn't just a travelogue; it's supposed to serve as a letter in support of settlement and colonization of the area. \n\nSo our first account of the lakes is effectively written by a land developer, someone encouraging others to \"buy\" into the area.\n\nAllow me to jump ahead almost 200 years. That's nearly how long it took for Europe to learn that the lakes definitively did not connect to the Pacific Ocean. Up until the end of the 18th century, there was a thought that the lakes could, from their western end, connect to a bay or inlet of the Pacific by river or other watercourse.\n\nWarren Cook's *Flood Tide of Empire* is my favorite work when it comes to the Spanish presence in the Pacific Northwest, and it does a great job of explaining why the Spanish had a vested interest in keeping the Pacific Northwest unexplored as long as there was the possibility of a Northwest Passage. \n\nIn 1592, there was a rumored voyage by a man named Juan de Fuca, who allegedly found a route eastward. There's been no definitive evidence that there was such a voyage or even that de Fuca existed. The sole account of Fuca's voyage was in a 1596 account written by Michael Lok in Venice, who told it to another man, who had it published as part of a larger account.\n\nRegardless of the truth, de Fuca's account was carried forward as authentic or at least partially authentic until the 1790s, when Spanish and British voyages of exploration began poking into Puget Sound. That the myth survived so long is attributable to the existence of the Columbia River, which wasn't definitively explored until Americans did so until the last decade of the century.",
        "*** Paper Summary ***\n\nThis paper formalizes the properties required for addressing (indexing) memory augmented neural networks as well as how to pair the addressing with read/write operation. It then proposes a framework in which any Lie group as the addressing space. Experiments on algorithmic tasks are reported.\n\n*** Review Summary ***\n\nThis paper brings unity and formalism in the requirement for memory addressing while maintaining differentiable memories. Its proposal provide a generic scheme to build addressing mechanisms. When comparing the proposed approach with key-value networks, the unbounded number of memory cells and the lack of incentive to reuse indexes might reveal impractical. \n\n*** Detailed Review ***\n\nThe paper reads well, has appropriate relevance to related work. The unified presentation of memory augmented networks is clear and brings unity to the field. The proposed approach is introduced clearly, is powerful and gives a tool that can be reused after reading the article. I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.",
        "  $B\\to\\phi K^*$ ($\\btos$) is three separate decays, one for each polarization\nof the final-state vector mesons (one longitudinal, two transverse). It is\nobserved that the fraction of transverse decays, $\\fT$, and the fraction of\nlongitudinal decays, $\\fL$, are roughly equal: $\\fTfL \\simeq 1$, in opposition\nto the naive expectation that $\\fT \\ll \\fL$. If one requires a single\nexplanation of all polarization puzzles, two possibilities remain within the\nstandard model: penguin annihilation and rescattering. In this paper we examine\nthe predictions of these two explanations for $\\fTfL$ in $\\btod$ decays. In $B\n\\to \\rho\\rho$ decays, only $\\bd \\to \\rho^0\\rho^0$ can possibly exhibit a large\n$\\fTfL$. In B decays related by U-spin, we find two promising possibilities:\n(i) $B^+ \\to K^{*0} \\rho^+$ ($\\btos$) and $B^+ \\to \\Kbar^{*0} K^{*+}$ ($\\btod$)\nand (ii) $\\bs \\to K^{*0} \\Kbar^{*0}$ ($\\btos$) and $\\bd \\to \\Kbar^{*0} K^{*0}$\n($\\btod$). The measurement of $\\fTfL$ in these pairs of decays will allow us to\ntest penguin annihilation and rescattering. Finally, it is possible to\ndistinguish penguin annihilation from rescattering by performing a\ntime-dependent angular analysis of $\\bd \\to \\Kbar^{*0} K^{*0}$.\n",
        "Let’s first establish timeframe here, because obviously different periods in Chinese history vary from one another, and also because Mulan is set in a particular time within that spectrum. Most of the telling put her in the state of Northern Wei, during the Northern and Southern Era between 420 and 589 CE. As the name suggests, this was a period of disunion for the Chinese empire, falling in between the collapse of the Han c. 220 CE, and the formal reunification of the imperium under the Sui c. 589.\n\nNorthern Wei was, like virtually the whole of China northern of the Yangtze and Huai Rivers, under the political and military control of non-Chinese people, collectively known as Turks. And when I say “Turks”, don’t start thinking of Istanbul and kebabs, these are in virtually every respect the analogue to what the Mongols under Genghis Khan would become 8 centuries later: a powerful confederation of central Asian steppe-riders. Northern Wei in particular was dominated by a subset called the Tuoba Xianbei, which had seized the throne of the largest territory in the north by successfully rebelling against its overlord state, Later Yan, in 399 and their clan leader enthroning himself as Emperor Daowu.\n\nSo that puts us in a time and place, let’s get to your question: how did military conscription take place in such a society?\n\nIt’s worth pointing out that, were we talking about a “real” Chinese dynasty – one controlled and run by the ethnic Han people – Like Han, Tang, or Song… we’d have a very different answer to this – they wouldn’t have conscripted; they’d have outsourced their military issues to other border “barbarians” to play them off of their own neighbors. But no, in this time and place – with both the north and the south embroiled in conflicts lasting centuries – conscription was a very real and necessary thing. The *Ballad of Mulan* makes this very clear: \n\n >  可汗大点兵。军书十二卷，卷卷有爷名。阿爷无大儿，木兰无长兄。愿为市鞍马，从此替爷征。\n\n >  trans: The Khan is calling many troops. The army list is in twelve scrolls, On every scroll there’s Father’s name. Father has no grown up son, Mulan has no elder brother. I want to buy a saddle and horse, And serve in the army in Father’s place.”\n\nHere already, though, we see the fictive license taken with this poem: this sort of personalization – individual names slated – would’ve been completely untenable, at least at the level of the khan/emperor. There would’ve just been far too much paperwork for that sort of thing, and far too expensive… throughout virtually the entire span of Chinese  imperial history, the central government itself could not manage anything below the county-level. Thus, authority had to be devolved to the local level. This had been done for centuries via the *chaiyi fa*, the “draft service system.” Villages were assigned as groups of households, most typically as a unit of 110 households called a *li*. The *li* was then further subdivided into eleven units of 10 households called *jia*, according to their tax-brackets – high, middle, or low. Of those 11 *jia*, the richest was assigned by the census-taker each decade as the “administrative” *jia*, and each household took a 1-year shift as head of the li. From the remaining 10 *jia*, they’d likewise take one-year-long shifts as the “levy labor unit” for their *li*. From Ray Huang: “Under the direction of the *li* chief of the year, it performed the local tax collection and delivery, and met all material and labor requisitions on behalf of the entire *li*. The other units paid their regular taxes, but were not liable to service obligations that year. Thus in a decennial period all households took a one-year turn at discharging their service obligations. After the ten-year cycle a new census was taken and all *lijia* were re- organized in accordance with the changes that had occurred during the decade. With certain variations, the wards and precincts in the cities were organized along similar lines.”\n\nServices rendered by the *jia* called up for their year-long spate of duties could expect to perform tasks for the government such as providing office attendants for the administration, from the county all the way up to the imperial government... in role as varied as doormen, guards, messengers, litter-bearer, cooks, buglers, boatmen, patrolmen, jailers, stable grooms, receiving men in warehouses, operators of canal watergates, and clerical assistants. As mentioned, this wasn’t necessarily confined to one’s home region, but instead wherever the government deemed a vacancy needed to be filled. \n\nThis same kind of locally-based conscription rotation was in effect for the military, as well. And while what I’m directly pulling from here in this next part actually occurred in the 11th century during the Northern Song, it was heavily inspired by and meant to emulate the conscription practices of the Period of Disunion. It was laid out as followed: in every locality, a group of households – first 10, but later reduced to 5 – were organized into a “small guard unit”, from which two mature males (or more depending on extenuating circumstances) could be called up for active duty. A level higher than that, the process largely repeated, with these small guards organized by 5s into “large guards” headed by the wealthiest landowner from among them. A level up, and again the large guards were groups by 5s into a single Superior Guard, headed by the two wealthiest landowners. All other households in the area not eligible for service as guardsmen – those unlanded peasants and urbanites – were considered to be eligible to be called upon as auxiliaries to the guard corps. Guardsmen and auxiliaries alike were permitted to train with bow and arrow, as well as any weapon not specifically forbidden by law. Probably the most significant difference between the *baojia* and the older security systems, though, was that service was compulsory rather than voluntary – and to keep track of who had supplied what in terms of manpower, each unit was required to write up and maintain a list of each household in their region and who was expected to report for duty when called upon. These local militias’ duties ran the gamut of what you’d expect: nightly patrols of their township, pursuit of thieves, and informing on bandits, murderers, arsonists, rapists, and cultists to government higher-ups, as well as those suspecting of harboring them.\n\nSo, all of this to say that, *had* Hua Mulan been a real person, her father wouldn’t have been summoned to don his family armor and march out with a bum leg to the frontlines – and certainly not by some faraway faceless imperial decree. It would have been done on the level of the township, by families and individuals who had lived together their whole lives. They would have *known* Father Hua was crippled and had no adult sons, and they would have ensured that he and his family were, when their rotation to “active duty” within the *lijia* system came up, shunted off into other, more suitable and equally useful positions within the bureaucratic system… anything from doorman, to groom, to jailer. Mulan wasn’t saving her father from anything like death on the front lines – the girl just wanted to fight.\n_________\n\nFrankel, Han H. *The Flowering Plum and the Palace Lady: Interpretations of Chinese Poetry.*  \n\nHuang, Ray. *Taxation and Governmental Finance in Sixteenth Century Ming China.*  \n\nSmith, Paul Jakov. “Shen-tsung’s Reign and the New Policies of Wang An-shih, \n1067–1085” in *The Cambridge History of China, vol. 5: The Sung Dynasty and its Precursors.*  \n\nStanden, Naomi. “The Five Dynasties” in *The Cambridge History of China, vol. 5: The Sung Dynasty and its Precursors.*",
        "Great and very interesting work. I have some questions.\n\n1. It seems that the introduced scale parameters for different bit-rates are also trainable. How do you train and update the scale parameters for different bit-rates? Could you explain it in more details? Also, could you elaborate more about Figure 3A? What does the x-axis of the graph means? \n\n2. What was the intuition for incremental training? Do you have some insights why incremental manner is beneficial for training? Also, for figure 3B, it seems that the loss are not quite stable before 100 thousand iterations but suddenly the loss become very stable. Is this due to decreased learning rate at that point? \n\n3. Regarding the GSM modeling of quantized coefficients, how many mixtures did you use and how did you train GSM (e.g., EM algorithm) ? Also, isn't it a burden to train GSM to fit the quantized coefficients at every iteration? \n\nThanks.",
        "The thing is, your body doesn't really measure temperature - it measures a little something known as heat flux.\n\nHeat flux is pretty much the rate at which heat enters or leaves your body. For us, the most comfortable state is when we have a little bit of heat leaving our body. \n\nIn all of these cases, heat is moving from the body to the surrounding environment, since your body is always warmer, but the important part here is the rate. Different materials touching have different willingness to transfer heat to each other, as it turns out, water transfers heat easier than air does. The farther the temperatures between the two materials is, the faster the heat transfer will occur as well.\n\nSo, 90 degree air is uncomfortable because your body wants to move more heat from your body to the surrounding environment, but water, which conducts heat better, will transfer the heat from your body to it much faster, making it a comfortable feeling.\n\nThe opposite is true. 65 degree air pulls quite a bit of heat from you, but still it is within normal comfortable range. Water on the other hand will pull much more heat from you, making it cold.",
        "We would like to thank all the reviewers and commenters for their time, feedback, and ultimately for making our paper better.\n\nWe updated the paper to address the questions raised in the reviews.\n\nWe found out that the backtracking algorithm is beneficial only for the small-scale setting when the mini-batch size is of the order of the full dataset (e.g. UCI experiments). Otherwise, backtracking is (locally) tuning the learning rate too well and overfits to each mini-batch, which stagnates the convergence process.\nFor this reason, we removed the backtracking from the paper text and from all the experiments.\nWe also found out that after removing the backtracking, the dropout becomes unnecessary, and we removed it as well.\n\nOther changes:\n1) We added the complexity of each step of the Riemannian gradient descent.\n2) Investigated the effect of the proper initialization on the model and proposed a random initialization that works on par with the initialization from the solution of the linear model.\n3) Added the validation loss for the comparison of optimizers on the UCI datasets.\n4) Added the convergence plots to compare Riemannian optimization vs SGD baseline on the synthetic dataset.\n5) Added 3 suggested papers to the related work section.\n6) Plotted the TT-rank vs. accuracy graph on the MovieLens 100K dataset.\n7) Restructured the experiments section to make it easier to assess different aspects of the model on different datasets.\n8) Added a feedforward neural network baseline for the synthetic dataset.\n9) Fixed typos and made a few clarifications. \n",
        " Decide what hotel you will be staying at, how much gas you will need for the trip, etc.\n, Buy them directly at AAA or The Disney Store. Other sellers can sell you fake or expired tickets. It also helps to buy your tickets ahead of time, to avoid waiting in a line in the hot Florida sun with a screaming child., Nothing is worse than to finally get to your hotel and they don't know you reserved something., Tell everyone who is going! Your children should be happy! If not, buy them a book about Disney World or Walt Disney. They might learn to appreciate the park., See \"Things You'll Need\" for more information. Keep in mind it can get very HOT, especially during the months of June-August., See \"Things You'll Need\" for more information on what to pack.,,, Also, clean it up. Vacuum the seats and floor, pick up trash, etc. Some people don't like travelling for long distances in a trash pit., It sounds early, but you need all of the sleep you can get!,, Sleep!!! In this case, curfew is 5:15pm., If you wake up any earlier than 6:00am (or 2 Hours before your flight), nap until then., This is a fun part! Run up and down the hallway and scream \"We're going to Disney World!\" Then, make some loud noises., Wear comfy clothing, you will be in the car for a bit., This will avoid any extra stops., Eat quick meals, like oatmeal, cereal (Cheerios brands recommended), grits, bagels, leftover waffles or pancakes, Pop Tarts (or any other toaster tart brand) or some fruit.,(Or go to the airport and do the airport check-in, security, and get on the plane.) This might take a while, so be prepared for kid tantrums and complaining., Lake Buena Vista is the real town Walt Disney World is in, not Orlando., Go to your room, unpack, and take a cold, short shower., Try microwaving mac & cheese, eating a sandwich, or anything else that suits your fancy., This will be loads of fun; it is the best way to cool off., By now it should be 8:00 if you enjoyed yourself at the pool. Put on your pajamas and get in the car with everyone else.,,, Make sure to rest well because you have big days ahead of you!, Take a cold shower. Eat breakfast. Do everything you would normally do in the morning.,,, There are many signs to get you there, so don't worry. They might even tell you where to park!,,, Enjoy! Stay until 12 noon or until your kid starts screaming., Eat something, do your business, go for a little swim, etc., See the parades and fireworks.,,, There is a LOT to do at Epcot, so plan ahead for your day., Hollywood studios is made up of more shows than rides, so make sure you plan ahead for any shows., Animal kingdom tends to close earlier than most parks, so when you get back to the hotel, take the opportunity to relax and maybe watch tv or swim again.,,,,,,,,,,, Also, go back to the hotel room and check everywhere to make sure you didn't leave anything.,,,",
        ";\n,,,, The site will prompt you to create a provider account.\n\n, Printable versions of the forms are available on the department's website. Mail those to your local Child Care Licensing office. Contact information for your local office is available on the department's website, as well.\n\n, Visit the Texas Department of Family and Protective Services website to obtain the latest fee schedule.\n\n, Schedule yourself for a daycare home pre-application class if you are attempting to become a licensed or registered child care home.\n\n, You should be familiar with licensing law and rules and the application process and administrative procedures.\n\n, The department will inspect your home to ensure that you comply with the minimum standards.\n\n\nThe standards vary depending on the type of daycare you operate, and are subject to change with Texas law.\n\n, The records from these inspections and tests must remain on file at your daycare.\n\n, A listing or registration will be mailed to you if your license is approved., Your operation can be inspected at least every 12 months and your license can be revoked if you are found to be in violation of the standards.",
        "Nice paper! I have one orthogonal question for the mini-imageNet dataset:\n\n\"we create our own version of the Mini-Imagenet dataset by selecting a random 100 classes from ImageNet and picking 600 examples of each class\". \n\nI am curious how sensitive with random selected classes and examples? It would be great if the exact split or the whole dataset is shared publicly, so others can repeat experiments and make comparison on a fixed benchmark dataset.\n\n",
        "Vjera Petrović-Njegoš, Princess of Montenegro (Serbian Cyrillic: Вјера Петровић-Његош; 22 February 1887 – 31 October 1927) was a member of the Petrović-Njegoš dynasty as the eleventh (of twelve) child of Nicholas I of Montenegro and Milena of Montenegro.\n\nVjera and her sister Xenia was not educated at the Smolny Institute in Russia like her eldest sisters had been, but educated at home. She was described as pretty and elegant but more sensitive and timid, and not as energetic or strongwilled, like her elders sisters.  \n\nShe was interested in painting, but are foremost remembered because of the effort she made helping the injured victims of an explosion in the harbor of Bar, for which she was awarded a medal. She left Montenegro when her father was deposed in 1918 and settled with her parents and her sister Xenia in France. She participated in humanitarian work in France as well. \n\nShe died in France. She was buried with her parents and sister in San Remo, but like them, her remains were reburied in Cetinje in 1989.\n\nNotes\n\nReferences\n\n1887 births\n1927 deaths\nPeople from Cetinje\nMontenegrin princesses\nEastern Orthodox Christians from Montenegro\nPetrović-Njegoš dynasty\nBurials at Serbian Orthodox monasteries and churches\nBurials in Montenegro",
        "  A general and basic model of primordial evolution--a soup of reacting\nfinitary and discrete processes--is employed to identify and analyze\nfundamental mechanisms that generate and maintain complex structures in\nprebiotic systems. The processes--$\\epsilon$-machines as defined in\ncomputational mechanics--and their interaction networks both provide well\ndefined notions of structure. This enables us to quantitatively demonstrate\nhierarchical self-organization in the soup in terms of complexity. We found\nthat replicating processes evolve the strategy of successively building higher\nlevels of organization by autocatalysis. Moreover, this is facilitated by local\ncomponents that have low structural complexity, but high generality. In effect,\nthe finitary process soup spontaneously evolves a selection pressure that\nfavors such components. In light of the finitary process soup's generality,\nthese results suggest a fundamental law of hierarchical systems: global\ncomplexity requires local simplicity.\n",
        "We would like to thank the reviewers for their comments and for providing us with valuable feedback. We already updated the paper with new results on ImageNet, and are also going to further update the paper based on the reviewer comments as explained in our responses below.\nWe also plan to release the code for our experiments next week.",
        "Miles O'Connor (born 20 April 1982 in Mississauga, Ontario) is a Canadian soccer player who plays for AAC of Toronto.\n\nCareer \nHe previously played for Italia Shooters, Brampton Lions and in Belgium for RRFC Montegnée.\n\nInternational \nHe is a former member of the Canada U-20 men's national soccer team and scored in 6 games, one goal.\n\nCoaching \nO'Connor leads Step Up Soccer Summer Camp in Mississauga, which coached youth talents and the U-18 B of Mississauga SC.\n\nHis brother Matthew O'Connor also plays football.\n\nReferences \n\n1982 births\nLiving people\nBrampton United players\nAssociation football midfielders\nCanada men's youth international soccer players\nCanadian expatriate soccer players\nCanadian people of Irish descent\nCanadian soccer players\nR.R.F.C. Montegnée players\nSoccer players from Mississauga\nYork Region Shooters players",
        "This paper was submitted to arXiv last week:\n\n",
        "  We combine Hubble Space Telescope images of a sample of 20 Seyfert galaxies\nat z=0.36 with spectroscopic information from the Keck Telescope to determine\nthe black hole mass - spheroid luminosity relation (M-L), the Fundamental Plane\n(FP) of the host galaxies and the M-sigma relation. Assuming pure luminosity\nevolution, we find that the host spheroids had smaller luminosity and stellar\nvelocity dispersion than today for a fixed M. The offsets correspond to Delta\nlog L_B,0=0.40+-0.11+-0.15 (Delta log M = 0.51+-0.14+-0.19) and Delta log sigma\n= 0.13+-0.03+-0.05 (Delta log M = 0.54+-0.12+-0.21), respectively for the M-L\nand M-sigma relation. A detailed analysis of known systematic errors and\nselection effects shows that they cannot account for the observed offset. The\ndata are inconsistent with pure luminosity evolution and the existence of\nuniversal and tight scaling relations. To obey the three local scaling\nrelations by z=0 the distant spheroids have to grow their stellar mass by\napproximately 60% (\\Delta log M_sph=0.20+-0.14) in the next 4 billion years.\nThe measured evolution can be expressed as M/ M_sph ~ (1+z)^{1.5+-1.0}. Based\non the disturbed morphologies of a fraction of the sample (6/20) we suggest\ncollisional mergers with disk-dominated systems as evolutionary mechanism.\n",
        " If you find something like a baby picture or a souvenir, or something from your history, show it to them. It will help you get closer if you see into each other's personal life, even if it's through objects.;\n, Go on a huge roller coaster together, or do something like skydiving or zip-lining. Doing something like this will help you associate amazing memories with your friend.\n\n, If you are too clingy, it will make you grow apart. If your friend goes on a month long trip, have fun emailing her, but make it occasional. The space will help you bond, because you will actually begin to miss the fun you usually have together, and it will make you want to stay friends more.\n\n, Make a craft together. You can make build something, record a duet, make a two person film, choreograph a dance, make a website, make custom t-shirts, start a business, make some food, start a club, anything. If you have the know-how, teach your friend and you can do it all together.\n\n, If you have a fear of heights and they have a fear of the dark, go on a huge roller coaster as late at night as you can. Anything to help you get over your fears, or at least face them.\n\n, Find a time when everybody is out of the house where you can just be alone together. This way, you can say and do what you want without people hovering over you. Being alone does wonders for a friendship.\n\n, If they are talking, don't space out. It greatly helps your friendship if you know as much about what's going on as they are willing to share. This is a two way thing, though. They should be listening to you, too.\n\n, Call, text, or email them. Get Facebook, Twitter, Myspace, Youtube, Instagram, Pinterest, Skype, or Vine. Any of these ways insure that you will be updated electronically during your time without them.\n\n, Try some trust exercises, physical or mental. You want to know you can trust them, right? Well, they probably want to do the same.\n\n, Find a time to go buy a scrapbook, scrapbook paper, stickers, etc. One you have all of those things, find and/or print out any pictures of you together that you have. Glue and label them, and make the scrapbook fun. Periodically add pictures and update it. Don't be afraid to add pics of you separately. It's still you and them, just no together. Pics of you without them also make a great conversation piece if you forgot to tell them about what happened.\n\n, Find all the videos you have of each other together and apart, and put them together into a big compilation on a program like iMovie. Add transitions, labels, and effects, and always update it. Make sure you both have access to and can watch it at any time.\n\n, Hang out, play games, eat stuff, watch shows and movies, have your mom make a scavenger hunt, pull an all-nighter, fall asleep as soon as you see it's morning, beg your parents to let you stay over later, whatever you want. Sleepovers are a great way to bond.\n\n, Go to a concert, party, pool, amusement park, dance, beach, haunted house, theater, city (like NYC...if you're old enough), local fair, sports game, or mall. Anything you both enjoy can be amazing.\n\n, Arguments can be the death of a friendship if you don't fix it immediately. Try being the bigger person and apologizing first. If you take that step, they might too. Plus, if they try being the bigger person too, the argument will be over as soon as it started. Even better, try ending the argument before it starts by agreeing to disagree.\n\n, If you are jealous of your friend, nothing will ever be fun. Be happy for them, and hope that they are happy for you. They are luckier than you sometimes, and sometimes you're luckier than them. There will always be someone better and worse than you at just about everything. Accept it and move on.\n\n, Laugh, giggle, and smile, or rant, scream, and cry. It's okay to express yourself. Let them know what you do and don't agree with or like, however...expressive you get. It's okay, you guys are close already, so you shouldn't hold back. It's not a turn off if they really are your friend.\n\n, Don't only have one friend. This can make you clingy, and in the end, you will end up making them have only one friend too. Be open to others. Also, don't exclude them in what you do with others, because it will make them feel bad.\n\n, Ask questions. You should know as much about them as they are willing to tell. Just don't intrude upon what they aren't willing to share. Somethings are too personal to tell, even if you are close.\n\n, Sometimes you shouldn't have to ask. Tell them everything. Secrets shouldn't be kept from best friends. Don't hold back with them. If you can really trust them, they won't tell a soul.\n\n, For things like birthdays and Christmas, or for the little things like a good job on something/no reason at all. Anytime you feel like giving a gift, give one.\n\n, Comfort them, celebrate with them, whatever they need. They need you, whether they admit it or not. For the moment, forget how you feel and just think about how they feel. And remember, don't make a joke out of the problem unless it's appropriate.\n\n, Help them get adjusted to moving, or help them with homework. Whatever they need help with, you should help them to the best of your abilities.\n\n, Two good ideas would be Dunkin' Donuts or Starbucks, since you can meet there whenever just for a snack. It's a good idea just to be able to eat breakfast every Saturday with them, just to have a set time and place to meet in case you haven't seen each other in forever.\n\n, Some friends even meet every day right off the bus if they can. It's a great system to have you you can relax with a friend right after school.\n\n, That only creates a barrier that make you grow apart whether you like it or not. Always tell the real truth, the whole truth, and nothing but the truth.\n\n, It's always nice to have something that only you can laugh at together. It's also a great way to cheer each other up when you're down.\n\n, Even in bed, have a phone near you in case they need to call you at 2am. Always be there for them. If they need you to come over, come over! It's okay to miss an activity once or twice if your BFF is about to break into tears. Emergencies are emergencies, and sometimes convenience can't matter.\n\n, It's always nice to hear a compliment from someone you are close to. And besides, there gotta be a reason you're friends, so use that as a compliment.\n\n, Virtual relationships never work out if you don't meet in person a few times. In-person meetings are vital, even if they only happen every other year.\n\n, If you like acting, do the play together. If you like singing join choir. No matter what you love doing, if they love it too, it can become a vital part of the glue that holds you together.\n\n, If you don't have fun together, it will never work. Friendship is about the ups and downs, so you have to make sure there are always abundant ups, tainted only by occasional downs.\n\n, Some friends will form a band. Others will move in together. No matter what, find a way to make your bond last for life.\n\n, If you start dating, make time for both people, and if dating gets in the way, stop dating them. Best friends before boy/girlfriends. Also, avoid the \"dating my BFF\" situation. If you break up, it will make it REALLY awkward between the two of you.\n\n, By now you should be like family. It's time you got to know their real family, like brothers, sisters, and parents. Your friendship won't work if you don't know the family.\n\n, That puts your friend in a seriously awkward position where they feel like they have to make you happy. The thing is, you are responsible for your happiness, and even though your friendship makes you happy, your friend shouldn't have to all the time. Be positive and your relationship will grow.\n\n, Looks don't matter. No one should have to say that, but this is just a reminder.\n\n, We've all been bullied, and no one likes it. So make sure to stand up for your friend. That way, they won't have to experience the bullying anymore.\n\n, Don't let jealously, anger, etc get in the way of your relationship. You want to be close, so don't let ANYTHING get in the way.\n\n, Don't forget what you know. That could make your friend feel like they are unimportant in your eyes. That creates a barrier that is hard to take down.\n\n, Be yourself around them. You are close, so you don't have to hold back. You can be sad, mad, happy, surprised, or whatever around them.\n\n, If something happens, don't let it ruin your relationship. Forgive each other, and let it go. Let it be a learning experience for you, and don't let it get in the way.\n\n, Say they can't make it to your party. Just remember that they might not be able to help it. Don't be mad at them if you have no reason.\n\n, Do homework together. Take walks. Eat meals. Whatever you regularly do, do it with your friend. It will make them feel loved and included.\n\n, If you both really want to go somewhere, and one of you gets to go, see if you can go together. Vacations are a great way to bond.\n\n, If your mom is making you go to an antique show, invite your friend and put the clothes on, taking pictures in black and white. Family outings are always more fun with your \"extended family\".\n\n, You are best friends, but are you like one person. Try to practice finishing each other's sentences, saying things in synchronization, knowing when the other is acting differently, etc. Once you are this close, there's hardly a chance at all that you two will stop being friends.\n\n",
        "Here's the answer I usually give when I hear this question:\n\nMost of the modern advances around us are the result of us understanding how smaller things work. \n\nSomething as simple as us trying to study light and finding that it is made up of a component particle (photon) and how it behaves revolutionized things. Visible light is really just a small spectrum of a whole spectrum of radiation emitted from the sun. And we studied every part of the spectrum and found uses for it with the microwave, radiowaves, x-rays, ultraviolet waves, etc. From this, we have advanced communication, can detect and treat a whole slew of diseases and cancers, can reheat our food so it tastes rubbery and nasty, etc.\n\nWait, we get sick because microscopic things are attacking the microscopic things in our body!? Thanks modern medicine. So I can kill the bad microscopic thing by lightly boiling things? Thanks pasteurization for saving millions of lives. People aren't playing russian roulette with beverages and food anymore (at least in first world countries :()\n \nWe studied the electron and now we understand and can harness electricity and we have light, television, monitors (all electron interaction emitting visible light), computers, etc etc. We studied the atom and have nuclear fission.\n\nStudying how small things work and how they interact with other small things has always trickled down to real world, big impact, application. The things I listed above are only a sliver of the pie.\n\nEven without going into what's actually happening in a LHC, this type of answer I think is very important for the casual person to understand.\n\nHope this helps.",
        "This paper introduces an actor-critic deep RL approach with experience replay, which combines truncated importance sampling and trust region policy optimization. The paper also proposes a new method called stochastic duelling networks to estimate the critic for continuous action spaces. The method is applied to Atari games and continuous control problems, where it yields performance comparable to state-of-the-art methods.\n\nAs mentioned in the beginning of the paper, the main contributions of this work lies in combining 1) truncated importance sampling with retrace, 2) trust region policy optimization, and 3) stochastic duelling networks. These improvements work well and may be beneficial to future work in RL.\n\nHowever, each improvement appears to be quite incremental. Moreover, the ACER framework seems much more complex and fragile to implement compared to the standard deep q-learning with prioritized replay (which appears to perform just as well on Atari games). So for the Atari domain, I would still put my money on prioritized replay due to its simplicity. Thirdly, improving sample efficiency for deep RL is a laudable goal, but really this goal should be pursued in a problem setting where sample efficiency is important. Unfortunately, the paper only evaluates sample efficiency in the Atari and continuous control tasks domain; two domains where sample efficiency is not important. Thus, it is not clear that the proposed method ACER will generalize to problems where we really care about sample efficiency.\n\nSome technical aspects which need clarifications:\n- For Retrace, I assume that you compute recursively $Q^{ret}$ starting from the end of each trajectory? Please comment on this.\n- It's not clear to me how to derive eq. (7). Is an approximation (double tilde) sign missing?\n- In section 3.1 the paper argued that $Q^{ret}$ gives a lower-variance estimate of the action-value function. Then why not use it in eq. (8) for the bias correction term?\n- The paper states that it uses a replay memory of 50000 frames, so that across threads it is comparable in size to previous work. However, for each thread this is much smaller compared to earlier experiments on Atari games. For example, one million experience replay transitions were used in the paper \"Prioritized Experience Replay\" by Schaul et al. This may have a huge impact on performance of the models (both for ACER and for the competing models). In order to properly assess the improvements of ACER over previous work, the authors need to also experiment with larger experience replay memories.\n\n\nOther comments:\n- Please move Section 7 to the appendix.\n- \"Moreover, when using small values of lambda to reduce variance, occasional large importance weights can still cause instability\": I think what is meant is using *large* values of lambda.\n- Above eq. (6) mention that the squared error is used.\n- Missing a \"t\" subscript at the beginning of eq. (9)?\n- It was hard to understand the stochastic duelling networks. Please rephrase this part.\n- Please clarify this sentence \"To compare different agents, we adopt as our metric the median of the human normalized score over all 57 games.\"\n- Figure 2 (Bottom): Please add label to vertical axes.",
        "  The dominant background for observations of gamma-rays in the energy region\nabove 50 GeV with Imaging Atmospheric Cherenkov telescopes are cosmic-ray\nevents. The images of most of the cosmic ray showers look significantly\ndifferent from those of gamma-rays and are therefore easily discriminated.\nHowever, a small fraction of events seems to be indistinguishable from\ngamma-rays. This constitutes an irreducible background to the observation of\nhigh-energy gamma-ray sources, and limits the sensitivity achievable with a\ngiven instrument. Here, a Monte Carlo study of gamma-like cosmic-ray events is\npresented. The nature of gamma-like cosmic-ray events, the shower particles\nthat are responsible for the gamma-like appearance, and the dependence of these\nresults on the choice of the hadronic interaction model are investigated. Most\nof the gamma-like cosmic ray events are characterised by the production of\nhigh-energy pi0's early in the shower development which dump most of the shower\nenergy into electromagnetic sub-showers. Also Cherenkov light from single muons\ncan mimic gamma-rays in close-by pairs of telescopes. Differences of up to 25%\nin the collection area for gamma-like proton showers between QGSJet/FLUKA and\nSibyll/FLUKA simulations have been found.\n",
        ";\n,,,,,,,,, Slightly pull away from the edge towards the center.\n\n,, Take your time carefully making sure it is wrapped tightly and without folding creases.\n\n,,, When you're done wrapping and securing the liner, set it aside.\n\n,, Carefully wrap it.\n\n,,,, Make sure that the inside pot is the same diameter size as the wearer's head\n\n,, Slowly push it down to shape the felt like the top of the hat.\n\n,,,, Indiana is typically seen in cargo/khaki pants, at least when he's out adventuring. Add a brown belt to complete the lower half.\n\n, In the movies, Indiana mostly wears a pale shirt, but it gets covered in dust and grime pretty quickly. Pick something beige if you can, and consider roughing it up with some dust outside (keeping in mind it might damage the shirt for good!). Keep the collar open and roll up the sleeves, unless you're wearing a jacket.\n\n, Indiana doesn't always wear it, but his most famous look is with a slightly beaten up brown leather jacket. If you happen to have one, wear it!\n\n, Your look is complete!\n\n",
        "  A coarse-grained computational procedure based on the Finite Element Method\nis proposed to calculate the normal modes and mechanical response of proteins\nand their supramolecular assemblies. Motivated by the elastic network model,\nproteins are modeled as homogeneous isotropic elastic solids with volume\ndefined by their solvent-excluded surface. The discretized Finite Element\nrepresentation is obtained using a surface simplification algorithm that\nfacilitates the generation of models of arbitrary prescribed spatial\nresolution. The procedure is applied to compute the normal modes of a mutant of\nT4 phage lysozyme and of filamentous actin, as well as the critical Euler\nbuckling load of the latter when subject to axial compression. Results compare\nfavorably with all-atom normal mode analysis, the Rotation Translation Blocks\nprocedure, and experiment. The proposed methodology establishes a computational\nframework for the calculation of protein mechanical response that facilitates\nthe incorporation of specific atomic-level interactions into the model,\nincluding aqueous-electrolyte-mediated electrostatic effects. The procedure is\nequally applicable to proteins with known atomic coordinates as it is to\nelectron density maps of proteins, protein complexes, and supramolecular\nassemblies of unknown atomic structure.\n",
        ";\n, At first, it was thought one might save a lot of time if one had read this article - \"Create a Pink Love Note of Spheres in Form of a Heart\", because there is a section that was introduced with that article that gets used again here. But, as it turns out, there have been changes in the interim.\n\n, The formulas for A1 and A2 are the same. Adjuster = 1.\nEnter -61 for TURNS into cell B2.\nThe result in cell B5 now shows -81,152 and the formula is \"=IF(TURNS>0,VLOOKUP(TURNS,TURNS_LOOKUP,2),VLOOKUP(TURNS, TURNS_LOOKUP_NEG,2))\" which is the same I believe.\nEnter 26 for S's Count into cell C1. (although 28 will appear.)\nThe result in cell C2 = 2230 for Designer and the formula has not changed from \"=VLOOKUP(S_COUNT,SPHEROIDS_COUNT_LOOKER,2)\"\nThe formula for Var, cell C3, has been overwritten with the value 25.\nThe formula for Cc is unchanged and the result is -.03141593\ndb is unchanged at 4.5\nGMSL and GMLL are unchanged.\nThe formula for top is unchanged at \"=ROUND((-B4*PI())+(Adj),0)\" and the result is 886771.\nAAA is unchanged at 1.00244189810508\nEnter 27.5 for Divisor into cell E3.\nEnter Y into YN in cell E4.\nThe formula for Factor is unchanged at \"=IF(E4=\"Y\",IF(ODD(S_COUNT)=S_COUNT,-S_COUNT*0.01,S_COUNT*0.01),-0.25)\" and the result  is +.26\nEnter 0 for Power into cell F2.\nCells H1 and J1 are unchanged at .92 and .96 respectively and the formulas for Sync1 and Sync2 are the same.\n\n,\nB7'S formula has not changed from \"=IF(EVEN(S_COUNT)=S_COUNT,ROUND((-B4*PI())+(Adj),0),TOP)\"\nB8:B1447's formulas have not changed from \"=((B7+(-TOP*2)/(AdjRows)))*$B$1\"\nC7's formula has not changed from \"=ROUND(-EXP((PI()^2)+(Cc*-(db))),0)+Designer\"\nEdit Go To cell range D7:D1447 and with D7 active, enter the formula w/o quotes, \"=X7/Divisor+COS((ROW()-7)*PI()/180*Factor)\" and Edit Fill Down.\nEdit Go To cell range E7:E1447 and with E7 active, enter the formula w/o quotes, \"=Y7/Divisor+SIN((ROW()-7)*PI()/180*Factor)\" and Edit Fill Down.\nCell range F7:F1447's formulae remain the same as \"=IF(A7=0,F6,((PI())*((SIN(B7/(C7*2))*GMLL*COS(B7)*GMLL*(COS(B7/(C7*2)))*GMLL)+D7)))\"\nCell range G7:G1447's formulae remain the same as \"=IF(A7=0,G6,((PI())*((SIN(B7/(C7*2))*GMLL*SIN(B7)*GMLL*(COS(B7/(C7*2)))*GMLL)+E7)))\"\nCell range H7:H1447's formulae remain the same as \"=F7*GMLL*Sync1\"\nCell range I7:I1447's formulae remain the same as \"=G7*GMLL*Sync1\"\nCell range J7:J1447's formulae remain the same as \"=F7*GMSL*Sync2\"\nCell range K7:K1447's formulae remain the same as \"=G7*GMSL*Sync2\"\n\n, The worksheet does not contain a Defined Variable Name of TT either. No need for you to enter it. Probably part of some approach to a problem that was solved in another way., TURNS_LOOKUP is now in cells $M$2:$N$1441. TURNS_LOOKUP_NEG is in cells $O$2:$P$1442 and runs in column O from -1440 to 0 and in column P from -104192 to 412 (an increment of 72). SPHEROIDS_COUNT_LOOKER is in cells $R$2:$S$1441 and increments by 251175.5 from a base value of -6,363,636 in column S, and from 0 to 1 in cells R2:R25 and then increments by 1 to  a value of 1416 in cell R1446 and then 10,000 in cell R1447., Select cell AA7 and input the value 28.6470945405378. In cell W1, enter \"Pool Ball Process:\" . In cell V2, enter the note \"1) Figure out H, K's per Grid Analysis or previous values;\", In cell V3, enter the note \"2) Figure out if FFF variable changes -- it adjusts rows up-Y a little -- best ball fit;\". In cell V4, enter the note \"3) Fill down formulas for X, Y and Radius -- R must = .50000!!\" and in cell X5 enter the title \"POOL BALL X,Y Determinators:\" and Format Cell Font Bold. Edit Go To cell range X7:X1446 and with X7 active enter the formula \"=(COS(ROW()-7)*PI()/180*FFF)+V7\". Edit Go To cell range Y7:Y1446 and with Y7 active enter the formula \"=(COS(ROW()-7)*PI()/180*FFF)+W7\". Then read the rest of this section and realize there may be changes in the interim which need updating, but that basically you have now the same info., Enter into cell AA7 28.6470945405378 and Insert Name Define name FFF to cell $AA$7. Enter Per Goal Seeking in cell AA8 and select cell range AA7:AA8 and Format Cells Border Black bold Outline. Enter the notes \"Be careful of formulas below\" in cell range AA10:AA12. Enter H=x center of each into cell AA14 and K=y center of each into cell AA15.\n\nEnter .115 into cell range AA52:AA141 via Edit Fill Down.\nEnter .230 into cell range AA142:AA276 via Edit Fill Down.\nEnter .345 into cell range AA277:AA456 via Edit Fill Down.\nEnter .460 into cell range AA457:AA681 via Edit Fill Down.\nEnter .575 into cell range AA682:AA951 via Edit Fill Down.\nEnter .690 into cell range AA952:AA1266 via Edit Fill Down.\nEnter .805 into cell range AA1267:AA1446 via Edit Fill Down.\nEnter .5 into cell range Z7:Z1446 via Edit Fill Down.\nEnter \"=(SIN(ROW()-7)*PI()/180*FFF)+W7\" into cell range Y7:Y1447 via Edit Fill Down.\nEnter \"=(COS(ROW()-7)*PI()/180*FFF)+V7\" into cell range X7:X1447 via Edit Fill Down.\nIt will help speed things up to build upon larger and larger sequences already done above by copying them below as one goes. Enter 1 into the following cells, each 45 cells apart, in column AB: AB52, 97, 142, 187, 232, 277, 322, 267, 412, 457, 502, 547, 592, 637, 682, 727, 772, 817, 862, 907, 952, 997, 1042, 1087, 1132, 1177, 1222, 1267, 1312, 1357, 1402.\nEnter 1 into cell U7 and Edit Go To cell range U8:U1446 and with U8 active enter the formula without quotes, \"=IF(AB8=1,1+U7,U7)\". This will build the balls to 32.\nEnter 1.5 into cell V7 and 3 into cell W7 and Edit Fill Down to cell range V7:W51.\nEnter 1 into cell V52 and 2.12 into cell W52 and Edit Fill Down to cell range V52:W96.\nEnter 2 into cell V97 and 2.12 into cell W97 and Edit Fill Down to cell range V97:W141.\nEnter 2.5 into cell V142 and  1.23 into cell W142 and Edit Fill Down to cell range V142:W186.\nEnter 1.5 into cell V187 and  1.23 into cell W187 and Edit Fill Down to cell range V187:W231.\nEnter 0.5 into cell V232 and  1.23 into cell W232 and Edit Fill Down to cell range V232:W276.\nEnter 0.0 into cell V277 and  .345 into cell W277 and Edit Fill Down to cell range V277:W321.\nEnter 1.0 into cell V322 and  .345 into cell W322 and Edit Fill Down to cell range V322:W366.\nEnter 2.0 into cell V367 and  .345 into cell W367 and Edit Fill Down to cell range V367:W411.\nEnter 3.0 into cell V412 and  .345 into cell W412 and Edit Fill Down to cell range V412:W456.\nEnter 3.5 into cell V457 and   -.54 into cell W457 and Edit Fill Down to cell range V457:W501.\nEnter 2.5 into cell V502 and   -.54 into cell W502 and Edit Fill Down to cell range V502:W546.\nEnter 1.5 into cell V547 and   -.54 into cell W547 and Edit Fill Down to cell range V547:W591.\nEnter 0.5 into cell V592 and   -.54 into cell W592 and Edit Fill Down to cell range V592:W636.\nEnter -.5 into cell V637 and   -.54 into cell W637 and Edit Fill Down to cell range V637:W681.\nEnter -1 into cell V682 and -1.425 into cell W682 and Edit Fill Down to cell range V682:W726.\nEnter 0.0 into cell V727 and -1.425 into cell W727 and Edit Fill Down to cell range V727:W771.\nEnter 1.0 into cell V772 and -1.425 into cell W772 and Edit Fill Down to cell range V772:W816.\nEnter 2.0 into cell V817 and -1.425 into cell W817 and Edit Fill Down to cell range V817:W861.\nEnter 3 to cell V862 and -1.425 to cell W862 and Edit Fill Down to range V862:W906.\nEnter 4 to cell V907 and -1.425 to cell W907 and Edit Fill Down to range V907:W951.\nEnter 4.5 to cell V952 and -2.31 to cell W952 and Edit Fill Down to range V952:W996.\nEnter 3.5 to cell V997 and -2.31 to cell W997 and Edit Fill Down to range V997:W1041.\nEnter 2.5 to cell V1042 and -2.31 to cell W1042 and Edit Fill Down to range V1042:W1086.\nEnter 1.5 to cell V1087 and -2.31 to cell W1087 and Edit Fill Down to range V1087:W1131.\nEnter .5 to cell V1132 and -2.31 to cell W1132 and Edit Fill Down to range V1132:W1176.\nEnter -.5 to cell V1177 and -2.31 to cell W1177 and Edit Fill Down to range V1177:W1221.\nEnter -1.5 to cell V1222 and -2.31 to cell W1222 and Edit Fill Down to range V1222:W1266.\nEnter 0 to cell V1267 and -3.195 to cell W1267 and Edit Fill Down to range V1267:W1311.\nEnter 1 to cell V1312 and -3.195 to cell W1312 and Edit Fill Down to range V1312:W1356.\nEnter 2 to cell V1357 and -3.195 to cell W1357 and Edit Fill Down to range V1357:W1401.\nEnter 3 to cell V1402 and -3.195 to cell W1402 and Edit Fill Down to range V1402:W1446.\n\n, It's a project I'm working on. 15 balls and a cue ball times 2 makes 2 games simultaneously. Adequate for a pool shark on a holiday break.,,,\nFor more art charts and graphs, you might also want to click on Category:Microsoft Excel Imagery, Category:Mathematics, Category:Spreadsheets or Category:Graphics to view many Excel worksheets and charts where Trigonometry, Geometry and Calculus have been turned into Art, or simply click on the category as appears in the upper right white portion of this page, or at the bottom left of the page.\n\n",
        "  In a data warehousing process, the data preparation phase is crucial.\nMastering this phase allows substantial gains in terms of time and performance\nwhen performing a multidimensional analysis or using data mining algorithms.\nFurthermore, a data warehouse can require external data. The web is a prevalent\ndata source in this context, but the data broadcasted on this medium are very\nheterogeneous. We propose in this paper a UML conceptual model for a complex\nobject representing a superclass of any useful data source (databases, plain\ntexts, HTML and XML documents, images, sounds, video clips...). The translation\ninto a logical model is achieved with XML, which helps integrating all these\ndiverse, heterogeneous data into a unified format, and whose schema definition\nprovides first-rate metadata in our data warehousing context. Moreover, we\nbenefit from XML's flexibility, extensibility and from the richness of the\nsemi-structured data model, but we are still able to later map XML documents\ninto a database if more structuring is needed.\n",
        "  We consider 5D braneworld models of quasi-localized gravity in which 4D\ngravity is reproduced at intermediate scales while the extra dimension opens up\nat both the very short and the very long distances, where the geometry is flat.\nOur main interest is the interplay between the zero mode of these models,\nwhenever a normalizable zero mode exists, and the effects of zero energy\ngraviton resonant modes coming from the contributions of massive KK modes. We\nfirst consider a compactified version of the GRS model and find that\nquasi-localized gravity is characterized by a scale for which both the\nresonance and the zero mode have significant contribution to 4D gravity. Above\nthis scale, gravity is primarily mediated by the zero mode, while the resonance\ngives only minor corrections. Next, we consider an asymmetric version of the\nstandard non-compact GRS model, characterized by different cosmological\nconstants on each AdS side. We show that a resonance is present but the\nasymmetry, through the form of the localizing potential, can weaken it,\nresulting in a shorter lifetime and, thus, in a shorter distance scale for 4D\ngravity. As a third model exhibiting quasi-localization, we consider a version\nof the GRS model in which the central positive tension brane has been replaced\nby a configuration of a scalar field propagating in the bulk.\n",
        "There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track.",
        " If your employer, college, or university provides you with a search engine or directory, begin there. If you have access to a library database of research articles, such as EBSCOhost, start there.Library databases provide you with access to peer-reviewed research, which is the gold standard for academic study. “Peer-reviewed” means that top experts in the field have reviewed the research to make sure it is accurate, trustworthy, and informed before publishing it. Even if you’re just trying to learn something for your own personal benefit, academic research will provide you with the most up-to-date, reliable information.\n\n\nYou can usually access these databases through your home library’s website. Some academic and universities libraries may require a password if you are accessing them remotely (from somewhere other than in the library itself).\nIf you don’t have access to a library, try using Google Scholar for your searches. You can find academic research through this search engine, and Google Scholar will show you where you can find free copies of the articles online.;\n, Depending on the area of your research, you have several options for online databases specific to your field. For example, if you are looking for research on education, the ERIC (Education Resources Information Center) is sponsored by the United States Department of Education and provides peer-reviewed research and informational materials on education topics.If you’re looking for medical or scientific research, PubMed, sponsored by the United States National Library of Medicine, is a great place to start., If you have access to a library, make an appointment to speak with your reference librarian. These people are specially trained in helping you access the best research and knowledge available.They can help you find sources and also help you determine whether sources are credible.\n\n, Search engines crawl the web indexing pages by reading the words and phrases that appear on those pages. From there, the process is automated. Each search engine has an algorithm that’s used to rank results for specific searches. This means that no human is vetting the accuracy of the results. The “top” result is simply the result of an algorithm. It’s not an endorsement of the content or quality of the result.Most search engines can be “gamed” by savvy websites in order to ensure their content comes up first. Moreover, each search engine has its own algorithm, and some tailor their results based on your browsing history. So the “top” result on Google will not necessarily be the “top” result on Yahoo, even with the exact same search phrasing.Be aware that simply because you find information online doesn’t make it credible or authoritative. Anyone can make a webpage, and the amount of poor, unverified, and just plain wrong information often outweighs the good stuff online.To help you sift through the useless stuff, talk to your teacher or librarian, and use library or academic search engines when possible.\n\n, For any given inquiry, there are an almost limitless number of potential word and phrase choices you could enter into a search engine. Therefore, it’s important to think carefully about what you hope your search will find, as well as try multiple different search combinations.\n\n\nIf you’re using an academic search engine, such as your library’s search feature, try using a combination of keywords and Boolean Operators, or words you can use to narrow down your search: AND, OR, and NOT.For example, if you are doing research on feminism in China, you might run a search for “feminism AND China.” This will return results that include both of those topic keywords.\nYou can use OR to run searches for related keywords. For example, you could search for “feminism OR feminist OR social justice.” This would return results that contain one or more of those terms.\nYou can use NOT to exclude keywords from your search. For example, you could search for “feminism AND China NOT Japan.” You would not get any results that included Japan.\n\n\nYou can use quote marks to search for full phrases. For example, if you want to search for academic performance, you would search for the whole phrase inside quotation marks: “academic performance.” Be aware, though, that using quotation marks will kick out any result that isn’t an exact match. For example, you would not get results about “school performance” or “academic functioning” because they are not worded exactly the way you searched.\nUse specific keyword phrases to locate the most relevant information. For example, if you are looking for information social welfare expenditures in the U.S., you’re more likely to get the results you want by searching for “total yearly amount spent on welfare programs in U.S.” than searching for “welfare,” which would bring up definitions of welfare, types of welfare in other countries, and thousands more results you don’t want. Be aware, though, that you can’t always find information like this -- the more words you enter, the fewer results you’re likely to get.\nUse alternate words or keyword phrases to locate additional research sources. For example, if you are researching “welfare,” consider using “safety net” or “social programs” or “public assistance” in place of “welfare” to find different results. In many cases, your word choice might unintentionally bias your results, since terms like “welfare” are often politically loaded. Using a wider variety of terms ensures that you’ll be exposed to a broader — and therefore potentially less biased — set of sources.\n\n, If you’re researching a topic about which you’re relatively uninformed, begin your search with broad terms, then use the information culled from that first search to begin narrowing your search.\n\n\nFor example, in your search for “total yearly amount spent on welfare programs in U.S.,” you’ll quickly discover that there are several different public assistance programs, such as Temporary Assistance for Needy Families (TANF) and Supplemental Nutrition Assistance Program (SNAP). Use that information to decide which program(s) you’re interested in, and then perform a new (more specific) search, such as “total yearly SNAP expenditures in U.S.”\n\n, Perhaps the most difficult — and important — task in internet research is ensuring the sources you select are credible. Generally, you want to prioritize information from government sources, academics, and nationally recognized news organizations.Government sources will often have “.gov” somewhere in the webpage. For example, the United States Department of State’s website is www.state.gov. The official website for Australia’s Department of Defence is www.defence.gov.au.\nWebsites that end in .edu belong to colleges and universities. However, you do need to be careful with .edu sites, because often faculty and students can run personal webpages that will have the .edu extension, but the information there may not be vetted by the university.It’s better to find academic sources through an academic database or search engine, like EBSCOhost or Google Scholar.\nWebsites that end in .org belong to non-profit organizations. While some of these are highly credible, some are not. Anyone can purchase a website with a .org extension. Check these sites carefully, and don’t rely on them as your sole source of information if you can avoid them.Major news sources such as The Guardian, CNN, and Al Jazeera tend to be credible, but you also need to make sure you’re reading a factually based article and not an opinion piece. Many news sites also have blogs and editorial sites where people can state their opinions, which aren’t necessarily backed up by facts.\n\n, Don’t limit yourself to the first few results in the search engine. Look beyond the first page of search results to find information for your research.While it’s impossible to view all of the results for most searches, it’s important to view at least several pages of results in order to ensure you’re not missing important information. Because of search engine optimization, if you’re using a regular search engine like Google or Yahoo, the first several pages might contain the links that were most effectively promoted, not the ones with the best information., Go to the original source whenever possible.\n\n\nFor example, if you are writing a report on penguins, you could start with the Wikipedia page on Penguins. Scrolling to the References section would show you several peer-reviewed academic journal articles on penguins, along with references to book chapters by academic publishers. Look at those sources for more authoritative information.\n\n, During your research, you will find many statements online, but not all of them are true or useful. Some sources will not cite any references, or they may twist the reference to say something other than what it originally stated. Don’t take anything at face value. Particularly when the website reporting a fact or statistic is questionable, you should attempt to find the original source.\n\n\nFor example, if you’re doing research on changes in welfare expenditures during the past 20 years, there’s no reason to trust Yahoo answers, a blog, or any secondary source. Most credible sources will note that they’re using data from federal agencies. Therefore, it’s usually better to search for the original government data sources and cite them directly, rather than citing a page that is itself just reporting (possibly incorrectly) the data.\nCiting the original source will also make your own research more authoritative and credible. For example, it is much more impressive to your teacher if you cite an article from the National Institutes of Health (a US government source) than if you cite an article from webMD -- even if they have the same information. If you can cite the original scholarly research that produced the information you're discussing, that's even better.\n\n, If you can’t find the original source for a fact, your best bet is to verify the fact on multiple, credible sites.\n\n\nNo matter what information you’re seeking, if you can’t find a single official source, it’s advisable not to trust a piece of information until you find identical information on several independent sites. So, for example, if you can’t find an original source for SNAP expenditures in 1980, enter the data you found into a search engine to ensure that the same number is reported on multiple sites and that those sites are not all citing the same (potentially erroneous) source.\n\n, For example, the Mayo Clinic website is owned by the Mayo Clinic, one of the most prestigious hospitals in the world. It is a not-for-profit organization, so it is not out to make money from its content. Its articles are written by medical professionals. These are good clues that information you find on this site will be credible. On the other hand, a “health” website that has a storefront or lots of ads, and doesn’t have any institutional or professional affiliations, won’t be as credible.\n\n\nIf you’re using an academic database, check out who published the article or book. Texts from prestigious journals, such as the New England Journal of Medicine, and books from academic publishers like the Oxford University Press, carry more weight than sources from less-known publishers.\nIf you’ve never heard of a source, the first place to look is the “About Us” (or similar) portion of the website. If that doesn’t provide you with a good idea of who’s producing the web page, try conducting an internet search for the site itself. Often news articles, Wikipedia entries, and the like that reference a source will include information about its affiliation(s), ideology, and funding. When all else fails, consider using a web domain search engine to discover who owns the website. However, if you’ve had to go to that length, chances are good that the site is too obscure to be trusted.\n\n, Unfortunately, many internet sources will not list an author. If you are searching online for peer-reviewed research, however, you will usually find sources with named authors. Look at their credentials.For example, does this person have education in his/her field? Neil deGrasse Tyson has a Ph.D. in Astrophysics from the prestigious Columbia University, so it’s likely that what he says about astrophysics is credible and authoritative (meaning trustworthy and up-to-date).On the other hand, an amateur star-watcher’s blog will not be authoritative, even if the information is accurate.\nHas the author written anything else on the topic? Many authors, including journalists and academic scholars, have areas of specialty and have spent years studying and writing about these topics. If the author has written many other articles on the same area, this makes him/her more credible (especially if those articles are peer-reviewed).\nIf there is no author, is the source credible? Some sources, particularly government sources, will not list an author. However, if the source you are getting the information from is authoritative -- such as an article on chickenpox from the Centers for Disease Control and Prevention -- the absence of an author isn’t cause for concern on its own.\n\n, It’s important to make sure that your information is as up-to-date as possible, especially if you’re research a medical or scientific topic. Scientific consensus changes with the presence of new studies and information. Check when the article or website was published. Being more than five years old isn’t necessarily bad, but look for the most recent articles you can find for the best shot at updated information.For example, if you were writing a research paper on treatments for cancer, you wouldn’t want to use only articles from the 1970s, even if they were published in prestigious academic journals.\n\n, There are many sources out there that claim to be fact-based but aren’t. Websites that appear to have a clear agenda are usually not good sources, because they may ignore or misrepresent evidence that disagrees with their position.\n\n\nLook for the site’s sources. A credible internet site will cite its sources. A really great site may even link out to the original research articles so you can track them down. If you can’t find any references for the information provided, or if the references are out of date or poor quality, it’s a good sign that your site isn’t reliable.\nWatch for bias. Highly emotional language, inflammatory rhetoric, and informal writing are all signs of potential bias in your source. Most academic writing tries to steer clear of these and aim for impartiality and objectivity as much as possible. If your website uses emotional language like “Manipulative big pharma companies are out to keep you broken and unhealthy to line their own pockets!” it’s a good sign that there is bias present.\nReview each website for grammatical errors and broken links. If the website is credible and reliable, grammar and spelling should be accurate and all links should take you to the appropriate landing page. Websites with numerous grammatical errors and broken links may be copying their information from another source or may not be legitimate.\n\n, In order to avoid the same errors made by inaccurate sites, you should always document your sources. This will allow you to return to them later, if necessary, and will allow others (when applicable) to verify your sources themselves.\n\n\nBibliography entries for webpages traditionally consist of the author of the web article or webpage (if available), the title of the article or page, the name of the site, the site’s web address, and the date on which you accessed the article or page.\n\n, Just because a source is there today doesn't mean it's going to be there tomorrow. In order to guard against making your research irrelevant, consider your options for preserving web pages.\n\n\nThe simplest way to save a webpage as you see it today is to print a hard copy or save it as a PDF.This will allow you to refer back to the page, even if it's moved or deleted.\nSince a hard copy or PDF version will only be available to you, you should periodically check the links in your research if it is published on the web. If you discover a web page has been deleted or moved, you can keyword search for its new location in a search engine or check to see if it was archived by Archive.org's Wayback Machine, which preserves web pages as they previously displayed., There are numerous free web browser features, apps, and services that can help you save your sources quickly and organize them easily.\n\n\nUsing the bookmarks feature of your web browser is the simplest way to save sources. Rather than saving every source in the parent “Bookmarks” folder, consider creating subfolders for specific topics. For example, if you’re researching welfare, you might want to create a folder for “Welfare” in “Bookmarks” and then maybe even create more folders within “TANF,” “SNAP,” etc.\n\n, Beyond simple bookmarking features and apps, more advanced research software and services can help you create your own personal repository of sources.\n\n\nNumerous services and apps have made it possible to sync sources to the cloud, capture images of web pages as they appear on the day you accessed them, add keywords to sources, etc.\nMany of these services, such as Zotero, are freeware created by academics and other open-source advocates. Others, such as Pocket, offer some services for free and charge for others. If you need functions beyond your web browser’s standard bookmarking features, consider using one of these sources to make organizing your sources easier.\n\n,",
        "Dorothea (\"Thea\") Burns is an independent art researcher and former chief conservator of works on paper at the Weissman Preservation Center of Harvard University. She is an expert on pastel art and metalpoint drawing.\n\nEducation\nBurns earned a BA in fine arts from McGill University in 1966 and an MA in art conservation from Queen's University in 1978. She earned her PhD at the Courtauld Institute of Art, University of London. She earned a certificate in the conservation of works of art on paper from the Center for Conservation and Technical Studies at the Fogg Art Museum, Harvard.\n\nCareer\nBurns joined Queen's University in 1989 where she directed the paper objects component of the master's degree program in art conservation and was a tenured associate professor of paper objects conservation.\n\nIn January 2002, Burns was appointed the first Helen H. Glaser Conservator at the Weissman Preservation Center of Harvard University in which capacity she was the senior paper conservator at the Harvard College Library.\n\nHer first book, The invention of pastel painting (2007) was described by reviewer Rosie Freemantle in Journal of the Institute of Conservation as an exceptional work that \"covers the topic with an amount of detail unseen in previous works on the subject\".\n\nWith Philippe Saunier, she is the author of L'art du pastel (2014) which was translated into English and published by Abbeville Press in 2015 as The art of the pastel. The book was described by the publishers as \"The only comprehensive history of pastel art\".\n\nSelected publications\n\nEnglish\nThe invention of pastel painting. Archetype Publications, London, 2007. \nThe luminous trace: Drawing and writing in metalpoint. Archetype Publications, London, 2012. \nThe art of the pastel. Abbeville Press, 2015. (With Philippe Saunier) (Translated by Elizabeth Heard)\n\nFrench\nL'art du pastel. Citadelles et Mazenod, 2014. (With Thea Burns)\n\nReferences \n\nLiving people\nConservator-restorers\nYear of birth missing (living people)\nAlumni of the Courtauld Institute of Art\nMcGill University alumni\nQueen's University at Kingston alumni\nQueen's University at Kingston faculty\nHarvard University staff",
        "  Kumar et al. (2006) obtained a fifth order polynomial in $\\omega$ for the\ndispersion relation and pointed out that the calculations preformed by Porter\net al. (1994) and by Dwivedi & Pandey (2003) seem to be in error, as they\nobtained a sixth order polynomial. The energy equation of Dwivedi & Pandey\n(2003) was dimensionally wrong. Dwivedi & Pandey (2006) corrected the energy\nequation and still claimed that the dispersion relation must be a sixth order\npolynomial. The equations (11) $-$ (19) of Dwivedi & Pandey (2006) and the\nequations (24) $-$ (32) Kumar et al. (2006) are the same. This fact has been\nexpressed by Kumar et al. (2006) themselves. Even then they tried to show this\nset of equations on one side gives the sixth order polynomial as they got; on\nthe other side, the same set of equations gives the fifth order polynomial as\nKumar et al. (2006) obtained. The situation appears to be non-scientific, as\nthe system of equations is a linear one. These are simple algebraic equations\nwhere the variables are to be eliminated. However, it is a matter of surprise\nthat by solving these equations, two scientific groups are getting polynomials\nof different degrees. In the present discussion, we have attempted to short out\nthis discrepancy.\n",
        "Stephen Falk Krantz (May 20, 1923 – January 4, 2007) was a film producer and writer, most active from 1966 to 1996.\n\nCareer\nBorn in Brooklyn, New York City, Krantz graduated from Columbia University and went on to serve in the U.S. Army Air Forces in the Pacific during World War II as a second lieutenant.\n\nHe worked as a comedy writer for Milton Berle and Steve Allen. His later years were devoted to the production of animated cartoons in Canada. After firing Shamus Culhane from the animator's supervising director job on Rocket Robin Hood, director Ralph Bakshi and background artist Johnnie Vita were brought to Toronto, not knowing that Krantz and producer Al Guest were in the middle of a lawsuit.\n\nFailing to reach a settlement with Guest, Krantz told Bakshi to grab the series' model sheets and return to the United States. When the studio found out, a warrant for Bakshi's arrest was issued by the Toronto police. Bakshi's animation studio, Bakshi Productions, took over Rocket Robin Hood and another Krantz-produced series, Spider-Man, beginning Krantz' working relationship with Bakshi.\n\nBy 1968, Krantz was producing live-action shows (such as the Canadian supernatural series Strange Paradise). Krantz agreed to produce Bakshi's animated film Heavy Traffic, but told Bakshi that Hollywood studio executives would be unwilling to fund the film because of its content and Bakshi's lack of film experience. Bakshi later pitched a film adaptation of Robert Crumb's comic strip Fritz the Cat, and Krantz sent Bakshi to San Francisco in an attempt to persuade Crumb to sign the contract. Krantz later acquired the film rights through Crumb's then-wife, Dana, who had Crumb's power of attorney and signed the contract. Fritz the Cat was released on April 12, 1972, opening in Hollywood and Washington, D.C. A major hit, it became the most successful independent animated feature of all time.\n\nTowards the end of the year, Krantz began coproducing Heavy Traffic with Samuel Z. Arkoff, but Krantz had not compensated Bakshi for his work on Fritz the Cat, and halfway through the production of Heavy Traffic, Bakshi asked when he would be paid. Krantz responded, \"The picture didn't make any money, Ralph. It's just a lot of noise.\" Bakshi found Krantz's claims dubious, as the producer had recently purchased a new BMW and a mansion in Beverly Hills. Bakshi soon accused Krantz of ripping him off, which the producer denied. When Bakshi attempted to work with Albert S. Ruddy on another film, Krantz locked Bakshi out of the studio and called several directors, including Chuck Jones, in search of a replacement. Arkoff threatened to withdraw his financial backing unless Krantz rehired Bakshi, which Krantz did a week later.\n\nAfter 1974, live-action motion pictures dominated Krantz' filmography. He wrote two novels, including Laurel Canyon (Pocket Books, 1979, paperback original), which was a best-seller.\n\nPersonal life\nKrantz married magazine writer Judith Tarcher on February 19, 1954. In the mid-1970s, as Judith Krantz, she began her career as a best-selling novelist. Judith's first book, Scruples, was published in 1978, and reached number one on the New York Times bestseller list.\nThe couple had two sons, Tony Krantz and Nicholas. Tony is a film and television writer, director and producer.\n\nHis sister-in-law is puppeteer and ventriloquist Shari Lewis, who is famous for performing Lamb Chop. He is of Jewish faith.\n\nDeath\nHe died in Los Angeles, California, on January 4, 2007 from complications of pneumonia, aged 83.\n\nReferences\n\nExternal links \n \n\n1923 births\n2007 deaths\nFilm producers from New York (state)\nAmerican male screenwriters\nDeaths from pneumonia in California\nColumbia College (New York) alumni\nWriters from Brooklyn\nMilitary personnel from New York City\nScreenwriters from New York (state)\nJewish American screenwriters\n20th-century American male writers\n20th-century American screenwriters\nUnited States Army Air Forces personnel of World War II\nUnited States Army Air Forces officers\n20th-century American Jews\n21st-century American Jews",
        "Armenian needlelace (also known as Bebilla, Nazareth Lace and Knotted Lace) is a pure form of needle lace made using only a needle, thread and pair of scissors.\n\nHistory\nLike lacis, or filet lace, Armenian needlelace seems to be an obvious descendant of net making. Where lacis adds decorative stitches to a net ground, Armenian needlelace involves making the net itself decorative.\n\nThere is some archeological evidence suggesting the use of lace in prehistoric Armenia and the prevalence of pre-Christian symbology in traditional designs would certainly suggest a pre-Christian root for this art form.\n\nIn contrast to Europe where lace was the preserve of the nobility, in Armenia it decorated everything from traditional headscarves to lingerie and lacemaking was part of many or most women's lives.\n\nTechnique\nThe lace is made by tying knots, usually tied onto the previous round of the piece creating small loops of thread onto which the next round of knots can be tied. Patterns are created by varying the length of the loops, missing loops from the previous round, adding extra loops and similar.\n\nWhen used as an edging the lace can be made directly onto the hem of the fabric being edged. When a doily or freeform object (such as the birds and flowers decorating traditional headscarves) is being started a series of loops is tied onto a slip knot which is pulled tight to complete the first round.\n\nSee also\nList of fabric names\n\nReferences\n\nSpecific\n\nNeedle lace\nArmenian art\nArmenian culture\nTextile arts of Armenia",
        " This project is perfect for using up a bit of leftover acrylic in a bright color. Also choose a suitable hook.\n, Chain about two and a half to three inches' worth of stitches (7-7.5cm) for whatever yarn and hook you are using, perhaps 20-25 stitches. The length is more important than the number of stitches. The objective is to end up with a tube about an inch in diameter, or maybe a little bit more.\n\n, Do avoid twisting the chain.\n\n, That is, if the loop was about 1 inch (2.5cm) around, you should make it about 1 inch (2.5cm) tall. You're aiming for a rough proportion here, not any specific number of rows. It's all right to join rows, so that this part is a spiral.\n\n, However, do not crochet a turning chain. Each consecutive row will therefore decrease. If you know how to do a single crochet decrease, you can decrease even more. Tie off the colorful yarn, but leave the tails for now. Here again, the exact proportions are not critical.\n\n, Pinch the face together as you go to see how it will look., They'll take a little bit of extra time, but they have a bit more style and a better shape than anything you'll buy in the store. Leave the tails attached on the side of the eyeball and neatly tie off and trim the tails extending from the back.\n\n, Use a large tapestry needle. You will want the face open at this stage so that you can reach inside to stitch the eyes to the inside of the fabric and tie a knot, but you can pinch it closed with your fingers to see where the eyes should go.\n\n\nThe eyes go very close to the top of the face. They are basically symmetrical, but this is a goofy looking alien, so it's all right if they're a little lopsided.\nTie the loose ends tightly. You can even tie them to one another. Then trim them close so they don't show through the mouth.\n\n, Make sure all the tails inside are covered up, and tie in and trim any remaining loose ends of the tail.\n\n, You may wish to cut one or two pieces first to get the length right. Four feet (1.2m) is a good length to start with., On the end opposite the face, which is to say the bottom end, put a hook through any loop from the starting chain. Fold one of the cut lengths of yarn in half and grab the middle with your hook. Pull it through.\n\n, This creates a Larks Head knot, as shown in the first four photos under Steps in this article. You should now have two long tails with loose ends.\n\n, Leave your hook in this loop.\n\n, Make them fairly tight stitches; you want them to curl and you won't be crocheting into them. You may even want to use a hook that is a size or two smaller than the one you used for the rest of the body.\n\n,, This is one tentacle. Chain a tentacle from the second loose end. Continue adding tentacles all the way around the base of the body. You can also chain the bottom tail from making the initial tube, rather than tying it in. It will camouflage with the rest.\n\n\nIt helps to make the tentacles all about the same length. You can eyeball it or count the chain stitches in a tentacle you like and do about that many stitches in the other tentacles.\nThe tentacles should be longer than the rest of the body is tall, by two to three times.\nDon't do too few tentacles\n\n, Done.\n\n",
        "  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n",
        " They can be found at:\n\n\nParty Shops\nNovelty shops and stands (like at carnivals)\nThe toy aisle in some grocery and department stores;\n, There will probably be a label on it and sometimes a band to protect the string to stop it from firing when not wanted (in packet).\n\n,,, (There are some fun things to do with these)\n\n,,\n\n\nYou may need to use a toothpick or alike to thread some of the string through the small hole.\nYou may also need to widen the small hole slightly.\nAlthough removing it should be safe as long as the string isn't pulled with reasonable force, they are explosives so still express caution while doing removing.\n\n, Once you have removed the explosive there are a few ways to detonate it:\n\nBy simply pulling the string from the rest of the explosive.\nBy igniting the explosive directly with fire (instant ignition).\nBy using a fuse (candle wick works) to lead fire down to the explosive.\n\n, Although it is quieter than most fire crackers, it is still loud and could cause hearing damage., Firmly hold the body of the explosive and the string in the other., The small piece of flint inside should light the gunpowder immediately and there will be a satisfying \"bang.\",\nRemove the wax part from the tin.\nRemove the wick and the wick base (you can use later this for the 'Fuse' method).\nOptionally, cut back the tin so only a small 'tray' is left.\n\n, Alternatively you can hold it with hand protection on (gloves) but it is more dangerous.\n\n, Don't expect it to work first time, but prepare for it to go off as soon as the flame touches it., Alternatively, for a faster burning fuse but a more dangerous method, dip the cotton string in oil, petroleum jelly, or other flammable substance.\n\n, Let the flame get down to the base part of the explosive and hope for the bang.\n\n",
        "This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.\n\nThe background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.\n\nThe experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \"likely a language implementation\", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.\n\nSome minor problems are listed below.\n1) In Section 2, the notation \"p_i:p_i+1\" is not clearly defined.\n2) In Section 3.1, typo: \"efter\" - \"after\"\n3) All the algorithms in this paper are not titled. The input and output is not clearly listed.\n4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.\n",
        "Ewa Kuls-Kusyk (née Kuls, born 3 September 1991) is a Polish luger, born in Gorzów Wielkopolski. She competed at the FIL World Luge Championships 2012 in Altenberg, at the FIL World Luge Championships 2013 in Whistler, British Columbia, at the 2014 Winter Olympics in Sochi in women's singles, and at the 2018 Winter Olympics in Pyeongchang in women's singles and team relay.\n\nReferences\n\nExternal links \n \n  (2018)\n  (2014)\n \n\n1991 births\nLiving people\nPolish female lugers\nOlympic lugers of Poland\nLugers at the 2014 Winter Olympics\nLugers at the 2018 Winter Olympics\nSportspeople from Gorzów Wielkopolski",
        "The simple answer is that teeth-baring isn't always a hostile gesture in animals either. There's a good response to the same question [here](_URL_0_): \n\n > \"Baring one's teeth is not always a threat. In primates, showing the teeth, especially teeth held together, is almost always a sign of submission. The human smile probably has evolved from that.\n\n > \"In the primate threat, the lips are curled back and the teeth are apart--you are ready to bite. But if the teeth are pressed together and the lips are relaxed, then clearly you are not prepared to do any damage.\n\nAnd on the topic of teeth baring supposedly being 'friendly' in humans:\n\n > \"The evolution of smiles is opaque and, as with many evolutionary accounts of social behavior, fraught with just-soism. Among human babies, however, the 'tooth-baring' smile is associated less with friendship than with fright--which, one might argue, is related to the tooth-baring threats of baboons. On the other hand, a non-toothy, not-so-broad-but-open-lipped smile is associated with pleasure in human infants. Somehow we seem to have taken the fright-threat sort of smile and extended it to strangers as a presumably friendly smile. Maybe it is not as innocent as it seems.\n\nThe link above gives some book suggestions to read more about the topic, but there's a good example of it in macaques in this paper: [The Phylogenetic Status of Siberut Macaques: Hints from the Bared-Teeth Display](_URL_1_)",
        " Do what you can.\n\n, Lord, I expect Your blessing as Your will is done in my life.\" This approach holds true in business, at home or in school. Ask the guidance of our Almighty Father. Always ask his permission and blessing in everything you do; don't forget to pray about your dreams in life.\n\n, Then trust God to strengthen you.\n\n, Know your privileges of asking for assistance in the situation and overcome your doubt. God blesses those who believe Him and act on that faith with confidence in Him. The second you need to do is hard work to reach your dreams. Always think positive about your dreams. Always remember this saying \"If others can do it, why can't I?\"\n\n, God promises to show you His will in John 16:13, \"However, when He, the Spirit of truth, has come, He will guide you into all truth; for He will not speak on His own authority, but whatever He hears He will speak; and He will tell you things to come.\" Jeremiah 33:3, ‘Call to Me, and I will answer you, and show you great and mighty things, which you do not know.’\n\n, Help me to show your Love to others. Pray with thankfulness: \"Continue earnestly in prayer, being vigilant in it with thanksgiving.\" Colossians 4:2\n\n, Forgive so that you can be forgiven! Be a peacemaker and peace keeper...\n\n, Words of faith in God are the most powerful things in the Universe. God said, \"Let there be light\" and there was light. It was His will. Yes, it's like the law of gravity will tell you that you can't last very long walking in the air. So you must believe God's promises (His will.) are laws that are as sure as gravity. Believe to receive.\n\n, That is what it is: it's yours. You must continue praising God for the answer whatever it is--even if you do not agree with the timing or the way it comes.\n\n,,, Go forward believing; look up and expect the answers to happen, and leave them to the power of the will of God. If your dreams don't come true don’t get depressed and most of all don't loose hope. Always remember that God may be preparing a better dream for you to fulfill. Remember also that in every failure and success there is always a reason. When God closes one door, He opens another.\n\n,;\n, He sees it all.\n\n, It will never work!\n\n,\n\n\n\nJustice is coming and \"you shall receive the desires of your heart,\" so watch the bad attitudes or your \"rewards\" can be opposite of your desires whether in the short or long run.\n\n,\n\n\nOne is never perfect, but \"God is always just.\" He knows how one accepts His good graces, gifts, shares his abilities, and knowledge and how one, then forgives those who seek forgiveness and does not despise the weak or hopeless.\n\n, And, believe that He is already answering your prayer... it's that little thing called \"faith\"!\"Do not fret... or be envious of those who do wrong... 'Trust' in the LORD, and 'do good'... 'Delight' yourself 'in the LORD' and he will give you the desires of your heart. 'Commit' your way to the LORD...\" Psalms 37:1 - 5 ",
        "Actually very interesting topic, whilst a good number are just insults, the majority of English language swear words are Germanic origin words as opposed to Latin origin words; when the french-speaking Normans conquered England, they would include more french words when they spoke, whereas the Anglo-Saxons, which made up the majority of English population, had all been including German origin words. Wealthy people (both Norman and otherwise) wished to seem more Norman in order to get further ahead in life, so shunned German origin words; eventually this evolves into \"only a poor person says that\" meaning a proper person looking to forward there standing in society would never sound so common, then just flat out rude as people continued to emulate the \"proper people\". Example: cunt is a germanic origin word for vagina, and it's predecessor can be found in 13th century poetry, yet became considered slang, then by the nineteenth century it had the same vulgarity as it does now.\n\n & #x200B;\n\nTLDR: Swear words tend to become swear words because poor people used to say them.\n\nEdit: I love that my most upvoted comment is explaining the origin of the word cunt. I'm a true fucking Scotsman",
        "Armstrong & Getty are the hosts of The Armstrong & Getty Show, a morning drive radio show airing in California, Nevada, Utah, New Mexico, Oregon, Texas, Florida, Alaska, Colorado, Illinois, Iowa, Ohio, Montana, South Carolina, South Dakota, Washington state, and Washington, D.C. on several radio stations owned by iHeartMedia and other broadcasting companies.  The show is hosted by Jack Armstrong and Joe Getty. The talk show format is a mixture of libertarian political commentary, observations on local, national, and international news as well as reflections on social issues presented with humor.\n\nThe show\nThe Armstrong & Getty Show airs live from the studios of 650 KSTE in Sacramento, weekdays 6 to 10 a.m. Pacific Time.  It is also heard in Los Angeles; Las Vegas; San Francisco; San Diego; Portland, Oregon; Reno; Seattle; Tacoma; Palm Springs; Monterey; Salinas; Santa Cruz; Eugene; Salt Lake City; Ventura; Fresno; Fairbanks; Redding; Santa Maria, California; Aspen; Washington, DC; Daytona Beach; Tallahassee; Chicago; Ramsey, Illinois; Ames, Iowa; Cleveland; Great Falls, Montana; Missoula, Montana; Kansas City, Kansas; Greenwood, South Carolina; Myrtle Beach, South Carolina; Florence, South Carolina; Houston; and Sioux Falls, South Dakota. The show won the Best of Sacramento award numerous years in a row.\n\nDoug Stephan incident\nOn July 28, 2010, Armstrong and Getty were tipped off by a listener that a fellow radio talk show host, Doug Stephan, had been stealing audio material from the Armstrong and Getty show, editing the audio, and using it in his show in an attempt to portray it as if he were speaking to their caller.\n\nDuring that same broadcast, the hosts were able to speak to Douglas Stephan, himself, regarding the alleged plagiarism that appeared evident upon comparison of the duo's show content and Douglas Stephan's \"callers\" some days later. Stephen never admitted to the allegations, but apologized, saying that the calls were misplaced and accidentally played on his Good Day show.\n\nReferences\n\nExternal links\n \n\nAmerican talk radio programs",
        "  It has long been suggested that helium nuclei in the intracluster plasma can\nsediment in the cluster gravitational potential well. Some theoretical\nestimates for the cores of relaxed clusters predict an excess of helium\nabundance by up to a factor of a few over its primordial value. The\nintracluster helium abundance cannot be measured directly. This presents a\nsignificant source of uncertainty for cosmological tests based on the X-ray\nderived cluster quantities, such as the gas mass, total mass, and gas mass\nfraction, all of which depend on the assumed helium abundance. We point out\nthat cluster distances derived by combining the Sunyaev-Zeldovich (SZ) and\nX-ray data also depend on the helium abundance. This dependence can be used to\nmeasure the abundance, provided the distance is known independently. For\nexample, if one adopts the WMAP H_0 value, then the recent H_0 measurement by\nBonamente and collaborators, derived from SZ data on 38 clusters assuming a\nprimordial helium abundance, corresponds to an abundance excess by a factor of\n1.9+-0.8 within r~1 Mpc (using only their statistical errors). This shows that\ninteresting accuracy is within reach. We also briefly discuss how the SZ and\nX-ray cluster data can be combined to resolve the helium abundance dependence\nfor the d_a(z) cosmological test.\n",
        ",\n\n\nThe Project Library icon resembles a film reel and is located in the bottom left section of your Final Cut Pro window.\n\n,,\n\n\nThe Timeline is located in the bottom portion of your Final Cut Pro session and is where you will perform all your editing.\n\n,\n\n\nThe Viewer window is an area in the top middle section of your project session that allows you to preview your edits.\n\n,\n\n\nChoose \"Show Audio Animation\" or use the keyboard shortcut of Control-A to edit your audio effects.\nTo edit your video effects, choose \"Show Video Animation\" or use the keystrokes of Control-V.\n\n,,,\n\n\nIf you just want to disable, or turn off, your effect, click within the box containing the check mark to remove the check mark.\nThis will turn off your effect until you return to this section to add the check mark back into the box and enable your effect.\n\n,\n\n\nYou can also use the shortcut keyboard strokes of Command-4 to open the Inspector tool.;\n,,\n\n\nChoose \"Audio\" if you are removing an audio effect and \"Video\" to remove a visual effect.\n\n,,\n\n\nIf you want to turn off the effect instead of deleting it, click within the blue box to remove the check mark.\n\n",
        "  We present a Spitzer based census of the IC 348 nebula and embedded star\ncluster. Our Spitzer census supplemented by ground based spectra has added 42\nclass II T-Tauri sources to the cluster membership and identified ~20 class 0/I\nprotostars. The population of IC 348 likely exceeds 400 sources after\naccounting statistically for unidentified diskless members. Our Spitzer census\nof IC 348 reveals a population of protostars that is anti-correlated spatially\nwith the T-Tauri members, which comprise the centrally condensed cluster around\na B star. The protostars are instead found mostly at the cluster periphery\nabout 1 pc from the B star and spread out along a filamentary ridge. We find\nthat the star formation rate in this protostellar ridge is consistent with that\nrate which built the exposed cluster while the presence of fifteen cold,\nstarless, millimeter cores intermingled with this protostellar population\nindicates that the IC 348 nebula has yet to finish forming stars. We show that\nthe IC 348 cluster is of order 3-5 crossing times old, and, as evidenced by its\nsmooth radial profile and confirmed mass segregation, is likely relaxed. While\nit seems apparent that the current cluster configuration is the result of\ndynamical evolution and its primordial structure has been erased, our findings\nsupport a model where embedded clusters are built up from numerous smaller\nsub-clusters. Finally, the results of our Spitzer census indicate that the\nsupposition that star formation must progress rapidly in a dark cloud should\nnot preclude these observations that show it can be relatively long lived.\n",
        "I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks.",
        "Jackie Akello is a Uganda singer/songwriter and entrepreneur with a coffee brand Village Belle which she launched in 2017. She sings in Acholi, Luganda, Swahili, and English. She is known for her hit love ballad \"Amari\", the gospel hit \"Samanya\" with Levixone, and the war-themed pop hit \"Apwoyo\". Akello is an Acholi  from northern Uganda and most of her songs like \"Apwoyo\" talk about the suffering of the Acholi both during and after the Lord's Resistance Army war that left Acholis out of their homes for a long time.\n\nCareer\n\nMusic\nJackie joined a number of live performing bands before going solo. She was a member of Janzi, The Sundowners and then her own Amari Band. She worked with Levixone on their hit gospel single \"Samanya\". She also worked with other musicians such as Kaweesa, Suzan Kerunen, Myko Ouma, Tshila and Kinobe Herbert.\n\nShe worked as a backup singer for Lilian Mbabazi on songs including \"Vitamin\". Jackie took on an acting role in Maurice Kirya's \"Busaballa\" video as Proscovia.\n\nShe has performed on live stages including World Music Day 2014 and 2017 in Kampala, Blankets and Wine festival, Bayimba festival, and many other private and public events. She also toured France.\n\nJackie worked on song titled \"Black Yellow Red\", an all-star project with Cindy, Irene Ntale, Michael Ross, Viboyo, Nick Nola, and many others. She also collaborated with Mun*G and T-Bro on the song \"Ffena awamu.\" She also went international where she participated in the Singing wells projects/Abubilla Music based in the U.K in which she got to work with different artists from East Africa.\n\nThe singer entertained guests Forest Whitaker during his Ugandan visit to champion peace and development initiatives through young people.\n\nCoffee Business\nIn 2017, Jackie launched her own coffee brand Village Belle which she said she had had on her mind for a long time.\n\nConfusion with look-alike\nAkello has a striking resemblance to Kenyan and Oscar award winner Lupita Nyong'o. During shooting of the film Queen of Katwe, most people and journalists that met or saw a photo of Akello thought it was Nyong'o since everyone knew she was the country shooting the movie. The circus started when Akello's photo was published on Maurice Kirya's Facebook page to promote a music video for the song \"Busabala\" in which Jackie acted, which raised a lot of attention to the singer.\n\nDiscography\n\nAlbums\n Akello Music\n\nSongs\n Amari\n Apwoyo\n Samanya\n Wan Wilobo\n Hallelujah\n\nNominations & Awards\n\nReferences\n\nUgandan women musicians\nUgandan singer-songwriters\n21st-century Ugandan women singers\nSoul musicians\nLiving people\nUgandan jazz musicians\nJanzi Band members\n1986 births",
        "Don Roy Zephrin Rajapakse (born 26 August 1905) was a Ceylonese politician.\n\nDon Roy Zephrin Rajapakse was born 26 August 1905 and received his education at St. Aloysius' College, Galle and Saint Joseph's College, Colombo. After leaving school he became a Sanitary Inspector and worked in that capacity for eight years. In 1941 he was elected to the Beralapanatara Village Committee and in 1957 became Chairman of the Committee, retaining that position until he was defeated at the village committee elections in 1961.\n\nAt the 1st parliamentary elections, held in 1947, he ran as an independent for the seat of Deniyaya. He was unsuccessful coming third out of five candidates.\n\nHe contested the 4th parliamentary election, held on 19 March 1960, in Hakmana electorate, as the Mahajana Eksath Peramuna nominee. He won the seat defeating the United National Party candidate, Victor Ratnayake, by 2,199 votes. Before the July parliamentary elections he resigned from the Mahajana Eksath Peramuna and joined the Sri Lanka Freedom Party. He ran as that party's nominee and successfully retained his seat, defeating Sirisena Hettige from the United National Party by 2,609 votes. At the 6th parliamentary election, held on 22 March 1965, he lost the seat to Hettige by 2,023 votes.\n\nHe ran again at the 7th parliamentary election, held on 27 May 1970, regaining the seat defeating the United National Party candidate, Harshanath Wanigasekera, by 9,473 votes.\n\nAt the 1977 parliamentary elections Rajapaske was one of 21 sitting members of parliament who decided not to contest the elections.\n\nReferences \n\n1905 births\nYear of death missing\nAlumni of St. Aloysius' College, Galle\nAlumni of Saint Joseph's College, Colombo\nMahajana Eksath Peramuna politicians\nSri Lanka Freedom Party politicians\nMembers of the 4th Parliament of Ceylon\nMembers of the 5th Parliament of Ceylon\nMembers of the 7th Parliament of Ceylon",
        " Also buy the strip of wood if necessary., Show them your sketch of the double frame: You'll need 3 long sides and 5 short sides( 1 long side ( air side) is reinforced so it adds up to 3 long sides; 1 short side ( air side ) is also reinforced, plus the central reinforcement so it adds up to 5 short sides ).You can also buy the materials needed to make the 2 clamps if you had decided to follow this option . Show the people at the hardware store the clamp sketch as well., Choose a corner, clear it up and start work.,, To do this, take the ladder and the wood strip and place them next to the corner.Take the level and check the verticality of the wood strip at the chosen corner.Take your pencil and draw a short line flush at the top of the wood strip., Make a pencil mark thru the \"x\" hole next to the corner. Take the profile away and drill an 8mm hole at the centre of the mark. Hammer in a plastic plug and with screw and washer fix the profile to the wall. Next prop the wood strip under the free end of the profile, put the level on the flat lower part and gently, push the profile up into the horizontal position and hold it there by propping it up with the wood strip. Once done, mark the other \"x\" hole. Gyrate the profile a bit out of the way, drill and fix, and the first profile is attached to the corner wall., Make sure that the wood strip doesn't budge. Take the marked long profile, a bolt, washer and nut and raise the profile making it sit on the corner end of the already fixed profile and on the top of the wood strip.Place the bolt etc at the corner to unite the two profiles and tighten by hand. Check horizontally, tighten fully and mark thru the 4th hole from the wood strip end end. Drill a hole on the wall through the \"x\" marked hole and fix. Next do the other \"x\" hole in the same way. Space evenly the rest of the fixation points on both profiles and half of the frame ( wall side ) is done., from the middle point. Leave ends free. Do the same with the 4 short profiles, no middle point needed with them. Leave ends free as well. Check end holes and tips of the 3 double profiles are well aligned and flush, then tighten up all bolts.\n, A mark at 120 centimeter (47.2 in) on the strip of wood will help you find the point on the beam.\n\nOpen the clamp and fix it not too tightly to the beam, add a D shackle and a length of chain stopping a bit short from the frame. Now add the carabiner and hang the long double profile from its middle. Fix with a bolt,etc, to the free end of the short profile on the wall.Check level again and correct right angle ( chain & double profile ) by using some well-cut ample surface. Adjust and fix clamp and everything tightly. Now you can add the last short double profile that closes the frame. Place bolts etc and tighten.\n\nThe clamp that holds the main chain will be located on the vertical of the protruding corner of the frame. Proceed as with the other clamp but now adding two carabiners. One at the end of the chain and the other a bit further up using a D shackle. The two carabiners hold and further unite the two double profiles meeting at the corner.\n\n,,, File and wood saw might be needed or some narrow strips of wood to get a tight fit of the boards.\n\n",
        "Section 4, experiments: This section contains so much misleading information to misguide the audience.\n The issues with 4.1 has been discussed in Section2(1).\nThe experiments in 4.4 are very deceiving.The minor misleading part is the title of Figure 3(b), ( c)  saying “before/after ReLU”, note the ReLU they refer to is at the input level not the *activation* level. In other words, they did not consider ReLU after the linear responses,  and only compare whether the input to this conv layer goes through ReLU or not, which does not add any difficulty to their theoretical analysis, because they assumed the input comes from the span of the conv weights, which is also [1] and [5]’s assumption. However, the most serious issue is how they collect the results for Figure 3. I tried to replicate their result for conv(1,2) because the authors claim that “we choose conv(5, 2), while other layers show similar results”. Precisely, I pass randomly sampled ImageNet Validation images through VGG-16, take the activation of conv(1,2) after max-pooling to obtain c, and then reconstruct conv(1,1) activation by multiplying c with the transpose of the conv weights W^T after subtracting the bias to obtain the reconstruction (W^T*c), and calculate the term, ||W^T c||/||c||.  However, the norm ratio I obtained is 3.16 +- 0.10, very far from their 1 +- 0.1.  My experiments give a distortion constant \\delta strictly bigger than 2, which is out of the range of 0 and 1 hence a meaningless value. This gap between our result and their result can potentially originate from the following causes: the authors using fancier sparse approximation method for c instead of following the procedures of feedforward CNNs, the authors discarding ReLU on the linear response therefore their c has negative components, and the authors have bugs in their code.  \n",
        "This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks. \n \n The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons). \n \n The AC thus recommends accepting this work as a poster.",
        "Chmieleń Wielki () is a village in the administrative district of Gmina Krzynowłoga Mała, within Przasnysz County, Masovian Voivodeship, in east-central Poland. It lies approximately  north-west of Przasnysz and  north of Warsaw.\n\nThe village has a population of 130.\n\nDuring Nazi Occupation it was part of New Berlin military training area\n\nReferences\n\nVillages in Przasnysz County",
        "The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn’t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n",
        " When pickling peppers, it is up to you to decide which kind of peppers you would like to pickle. Many picklers mix hot peppers and red or green sweet peppers for a balanced flavor, but the the peppers you choose are up to you. However, there are some characteristics you should keep in mind, regardless of what type of peppers you use:Look for peppers that are firm and have a smooth skin.\nAvoid old peppers that are soft and wrinkled or have brown spots, as old peppers can taste bad and become chewy when pickled.;\n, This is the standard amount. The step-by-step process listed below will yield 9 pint jars.A bushel of peppers generally weighs 25 pounds and can yield 20 to 30 pints.\n\n, You can use cold or lukewarm water with the same results.\n\n, Remove any bad spots on the peppers. Quarter the now seedless peppers.\n\n\nSmall peppers can be left whole. If you choose to leave them whole, cut several slits along their sides., If you have already cut your peppers, make sure to place them skin side down during whatever heating method you choose to use.\n\n\nPreheat your oven or broiler to 400º or 450ºF (205º to 232ºC). Place the peppers on a cookie sheet and stick them in the oven or broiler for 6 to 8 minutes. Using tongs, turn the peppers often so that they blister evenly on all sides.Place peppers on a wire mesh for the stovetop method. Hold the wire mesh over a heated electric or gas burner. Turn the peppers frequently with a pair of tongs. Make sure each side is heated equally.Heat an outdoor grill. Place the peppers 5 to 6 inches above glowing coals. Rotate the peppers using tongs., Place a damp cloth over them. Doing this allows the peppers to cool quickly and makes the skins easier to peel., Occasionally rinse the peppers with water.Use a knife to scrape off any skin that does not peel easily.\n\n, Place 5 cups (1.2l) vinegar, 1 cups (240ml) water, 4 tsp. (20g) pickling salt, 2 tbsp. (28g) sugar and 2 cloves of garlic in a pot.The garlic is optional. It adds to the flavor but is not necessary.\n\n, Once boiling, reduce the heat to a simmer for 10 minutes.\n\n, Discard used garlic cloves.\n\n, You don't want any bacteria festering in your jar of pickled peppers.\n\n, Leave them in the pot for 10 minutes.\n\n,, Leave 1 inch of space at the top of the jar. Flatten whole peppers.Add ½ a teaspoon of salt if you would like your peppers to be saltier.\n\n, Leave a 1/2 in. (1.3cm) of space at the top of the jars.\n\n, Bubbles can cause mold to form inside the jar once it has been sealed.\n\n,,, Once all of the jars are placed inside, lower the rack down into the canner.\n\n\nIf you do not have a canner, you can make your own. Find a pot or saucepan that is large enough to hold the jars. There should also be room for an inch of water above the jars. Place a washcloth or towel at the bottom of the pot before you put the jars in. This will keep the jars from directly touching the metal of the pot.\nIf you do not have a jar lifter, place rubber bands at the ends of tongs. The tongs will act exactly like a jar lifter.\n\n,, Be sure that the water is at a constant boil for 10 minutes., After 2 minutes, remove the jars from the canner and place them in a safe place to cool.\n\n",
        "- Strengths: well written, solid experimental setup and intriguing qualitative\nanalysis\n\n- Weaknesses: except for the qualitative analysis, the paper may belong better\nto the applications area, since the models are not particularly new but the\napplication itself is most of its novelty\n\n- General Discussion: This paper presents a \"sequence-to-sequence\" model with\nattention mechanisms and an auxiliary phonetic prediction task to tackle\nhistorical text normalization. None of the used models or techniques are new by\nthemselves, but they seem to have never been used in this problem before,\nshowing and improvement over the state-of-the-art. \n\nMost of the paper seem like a better fit for the applications track, except for\nthe final analysis where the authors link attention with multi-task learning,\nclaiming that the two produce similar effects. The hypothesis is intriguing,\nand it's supported with a wealth of evidence, at least for the presented task. \nI do have some questions on this analysis though:\n\n1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two\nmodels are aligned? Is it safe to do so?\n\n2) Section 5.2, I don't get what you mean by the errors that each of the models\nresolve independently of each other. This is like symmetric-difference? That\nis, if we combine the two models these errors are not resolved anymore?\n\nOn a different vein, 3) Why is there no comparison with Azawi's model?\n\n========\n\nAfter reading the author's response.\n\nI'm feeling more concerned than I was before about your claims of alignment in\nthe hidden space of the two models. If accepted, I would strongly encourage the\nauthors to make clear\nin the paper the discussion you have shared with us for why you think that\nalignment holds in practice.",
        "  We calculate growth rates and corresponding gains for RX and LO mode\nradiation associated with the cyclotron maser instability for parameterized\nhorseshoe electron velocity distributions. The velocity distribution function\nwas modeled to closely fit the electron distribution functions observed in the\nauroral cavity. We systematically varied the model parameters as well as the\npropagation direction to study the dependence of growth rates on model\nparameters. The growth rate depends strongly on loss cone opening angle, which\nmust be less than $90^{o}$ for significant CMI growth. The growth rate is\nsharply peaked for perpendicular radiation ($k_{\\parallel} = 0$), with a\nfull-width at half-maximum $1.7^{o}$, in good agreement with observed k-vector\norientations and numerical simulations. The fractional bandwidth varied between\n10$^{-4}$ and 10$^{-2}$, depending most strongly on propagation direction. This\nrange encompasses nearly all observed fractional AKR burst bandwidths. We find\nexcellent agreement between the computed RX mode emergent intensities and\nobserved AKR intensities assuming convective growth length $L_c\\approx$20-40 km\nand group speed 0.15$c$. The only computed LO mode growth rates compatible\nobserved LO mode radiation levels occurred for number densities more than 100\ntimes the average energetic electron densities measured in auroral cavities.\nThis implies that LO mode radiation is not produced directly by the CMI\nmechanism but more likely results from mode conversion of RX mode radiation. We\nfind that perturbation of the model velocity distribution by large ion solitary\nwaves (ion holes) can enhance the growth rate by a factor of 2-4. This will\nresult in a gain enhancement more than 40 dB depending on the convective growth\nlength within the structure. Similar enhancements may be caused by EMIC waves.\n",
        "  We present radio and X-ray observations of an impulsive solar flare that was\nmoderately intense in microwaves, yet showed very meager EUV and X-ray\nemission. The flare occurred on 2001 Oct 24 and was well-observed at radio\nwavelengths by the Nobeyama Radioheliograph (NoRH), the Nobeyama Radio\nPolarimeters (NoRP), and by the Owens Valley Solar Array (OVSA). It was also\nobserved in EUV and X-ray wavelength bands by the TRACE, GOES, and Yohkoh\nsatellites. We find that the impulsive onset of the radio emission is\nprogressively delayed with increasing frequency relative to the onset of hard\nX-ray emission. In contrast, the time of flux density maximum is progressively\ndelayed with decreasing frequency. The decay phase is independent of radio\nfrequency. The simple source morphology and the excellent spectral coverage at\nradio wavelengths allowed us to employ a nonlinear chi-squared minimization\nscheme to fit the time series of radio spectra to a source model that accounts\nfor the observed radio emission in terms of gyrosynchrotron radiation from\nMeV-energy electrons in a relatively dense thermal plasma. We discuss plasma\nheating and electron acceleration in view of the parametric trends implied by\nthe model fitting. We suggest that stochastic acceleration likely plays a role\nin accelerating the radio-emitting electrons.\n",
        "A summary of strengths and weaknesses brought up in the reviews:\n \n Strengths\n -Paper presents a novel way to evaluate representations on generalizability to out-of-domain data (R2)\n -Experimental results are encouraging (R2)\n -Writing is clear (R1, R2)\n \n Weaknesses\n -More careful controls are needed to ascertain generalization (R2)\n -Experimental analysis is preliminary and lack of detailed analysis (R1, R2, R3)\n -Novelty and discussion of past related work (R3)\n \n The reviewers are in consensus that the idea is exciting and at least of moderate novelty, however the paper is just too preliminary for acceptance as-is. The authors did not provide a response. This is surprising because specific feedback was given to improve the paper and it seems that the paper was just under the bar. Therefore I have decided to align with the 3 reviewers in consensus and encourage the authors to revise the paper to respond to the fairly consistent suggestions for improvement and re-submit. Mentime, I'd like to invite the authors to present this work at the workshop track.",
        "NOTE: A calorie is not a Calorie (aka, kilocalorie, food calorie). One Calorie (the measurement used for food) is equal to 1,000 calories. So, one calorie would heat one milliliter of water one degree, but one Calorie would heat one liter of water one degree. Having said that...\n\nIt depends on what you mean by boil. You could certainly increase the temperature of the water by 100 degrees. But to cause phase change (liquid - >  gas) uses a great deal more energy than to increase the temperature of a substance by one degree.\n\nIF:\nheat of vaporization of water = 2257 J/g = 540 cal/g (which is true)\n\nTHEN:\nIt would take 100 Calories to heat a liter of water by 100 degrees (from 0 to 100 degrees celsius), but it would take 540 Calories to vaporize a liter of 100 degree water.\n\nThis is all assumed to take place in a vacuum, and so, ultimately, the answer to your question is no. 100 Calories are the amount that would be utilized to raise the temperature 100 degrees, but you'd have to use more than that much fuel, due to radiation and convection and conduction drawing the heat out of the water.",
        "  We consider a $SO(d)$ gauge theory in an Euclidean $d$-dimensional\nspace-time, which is known to be renormalizable to all orders in perturbation\ntheory for $2\\le{d}\\le4$. Then, with the help of a space-time representation of\nthe gauge group, the gauge theory is mapped into a curved space-time with\nlinear connection. Further, in that mapping the gauge field plays the role of\nthe linear connection of the curved space-time and an effective metric tensor\narises naturally from the mapping. The obtained action, being quadratic in the\nRiemann-Christoffel tensor, at a first sight, spoils a gravity interpretation\nof the model. Thus, we provide a sketch of a mechanism that breaks the $SO(d)$\ncolor invariance and generates the Einstein-Hilbert term, as well as a\ncosmological constant term, allowing an interpretation of the model as a\nmodified gravity in the Palatini formalism. In that sense, gravity can be\nvisualized as an effective classical theory, originated from a well defined\nquantum gauge theory. We also show that, in the four dimensional case, two\npossibilities for particular solutions of the field equations are the de Sitter\nand Anti de Sitter space-times.\n",
        "The work presented in this paper proposes a method to get an ensemble of neural networks at no extra training cost (i.e., at the cost of training a single network), by saving snapshots of the network during training. Network is trained using a cyclic (cosine) learning rate schedule; the snapshots are obtained when the learning rate is at the lowest points of the cycles. Using these snapshot ensembles, they show gains in performance over a single network on the image classification task on a variety of datasets.\n\n\nPositives:\n\n1. The work should be easy to adopt and re-produce, given the simple techinque and the experimental details in the paper.\n2. Well written paper, with clear description of the method and thorough experiments.\n\n\nSuggestions for improvement / other comments:\n\n1. While it is fair to compare against other techniques assuming a fixed computational budget, for a clear perspective, thorough comaprisons with \"true ensembles\" (i.e., ensembles of networks trained independently) should be provided.\nSpecificially, Table 4 should be augmented with results from \"true ensembles\".\n\n2. Comparison with true ensembles is only provided for DenseNet-40 on CIFAR100 in Figure 4. The proposed snapshot ensemble achieves approximately 66% of the improvement of \"true ensemble\" over the single baseline model. This is not reflected accurately in the authors' claim in the abstract: \"[snapshot ensembles] **almost match[es]** the results of far more expensive independently trained [true ensembles].\"\n\n3. As mentioned before: to understand the diversity of snapshot ensembles, it would help to the diversity against different ensembling technique, e.g. (1) \"true ensembles\", (2) ensembles from dropout as described by Gal et. al, 2016 (Dropout as a Bayesian Approximation).",
        "  In a non supervised Bayesian estimation approach for inverse problems in\nimaging systems, one tries to estimate jointly the unknown image pixels $\\fb$\nand the hyperparameters $\\thetab$. This is, in general, done through the joint\nposterior law $p(\\fb,\\thetab|\\gb)$. The expression of this joint law is often\nvery complex and its exploration through sampling and computation of the point\nestimators such as MAP and posterior means need either optimization of non\nconvex criteria or int\\'egration of non Gaussian and multi variate probability\nlaws. In any of these cases, we need to do approximations. We had explored\nbefore the possibilities of Laplace approximation and sampling by MCMC. In this\npaper, we explore the possibility of approximating this joint law by a\nseparable one in $\\fb$ and in $\\thetab$. This gives the possibility of\ndeveloping iterative algorithms with more reasonable computational cost, in\nparticular, if the approximating laws are choosed in the exponential conjugate\nfamilies. The main objective of this paper is to give details of different\nalgorithms we obtain with different choices of these families.\n",
        "\n\n\nIt will have skin stretched tightly over its body.\nIt will also have a very bony, concave face.\nIts neck will most likely be extremely thin. Its back will be bony and its shoulders should show clearly through the pelt.\nThe ribs should be easily noticeable (note that healthy horses as well as recently foaled mares will often times have their ribs slightly showing through their hides). Just behind their ribs around their gut, it should be stretched and concave. The skin stretching from belly to thigh or over the stifle, should be stretched.\nThe hips should be extremely noticeable and pull out from beneath the skin. They should point out around the croup of the horse.\nThe butt should be bony and concave. There are always some variations to starvation in horses of course as sometimes they do not appear to be.;\n, If your horse has not had water in a while, give water to it slowly, about a 1/2 cup of water every 30 minutes until your horse is hydrated and doesn't drink immediately when you water it. Do this for at least 3 days after adoption to ensure the horses safety. Then you may fill up a water bucket, pan, or stall mate for your horse.\n\n, Begin by giving it light foods such as flakes or grass hay. Grass pickings are also good to start with as they are light and lower in value.\n\n, Then, give a simple handful of grass, of maybe grass hay. Give 2-4 handfuls of grass or grass hay every 30-40 minutes. It may seem excruciatingly slow, but small, persistent feedings are best. Do this for about five hours. This will ease food into the system and allow the horse to start its digestive process. Do this for just one day.\n\n, In the morning and at nights seem to work best. If your horse is severely starved, you may want to feed this quarter pound by handfuls again. You may also want to only give it a quarter of a pound twice. Repeat again the next day.\n\n, You may also began feeding your horse a carrot or an apple for a snack to ensure some extra energy from its diet.\n\n, Add some alfalfa to the hay to give more nutrients and energy to the horse's diet—just a handful or so per feeding (once again thrice a day) to insure the horse's health.\n\n,, This is to help out the horse by allowing it to eat freely as well as eat its hay. Three feedings each day must be continued.\n\n, A short, few minute walk before feeding, or allowing free range for 30 minutes in a clean yard. This is to help if the horse is not doing OK with its diet. You may also give a couple handfuls of wheat to your horse a day to give it energy and fat gain. Only do this 4-5 days a week.\n\n, Continue allowing your horse out for 30 minutes daily. If there is no grass available simply add an extra 1/2 pound of grass hay to your horses diet. Go for about 1.5 week.\n\n, of alfalfa twice a day. Or, you can do 3 pounds of alfalfa and 2 pounds of grass hay for 3 times. Let the horse out for 45-60 minutes and continue walking and/or lunging your horse. 0.6 ounces of oats can be added to the overall daily feed.\n\n\nIf you have a pony, you may continue with the above diet if not continue on. Over the course of 2-3 weeks change your horse main diet from grass hay to alfalfa.\nSwitch it around slowly so your horse is eating 5-6 pounds of alfalfa twice a day and 2-3 pounds of grass hay once a day.\nEncourage exercise and begin doing trotting exercises on the lunge. You may begin giving short, light rides to your horse for about 20-30 minutes and only twice a week. Also, allow your horse out to graze for an hour and a half.\nTry to make these grazing sessions with other horses. If you have a stallion, he will now be able to cover a mare although it is not desired. This is for 2 weeks.\n\n, Grazing should be somewhere between 2-2½ hours a day. You can ride twice a week for about 1 hour a day. 1-2½ ounces of oats may be give on days when you ride, and only once a day. Do this for a week or just over.\n\n,\n\n\nNow the riding sessions can go up to 2 to 2 1/2 hours a day and only twice a week. 1-2½ ounces of oats can be given before each ride. A mare may now be covered although it is not desired.\nMinor training in dressage, reining, and/or trail riding may began.\nGive carrots and/or apples before and after rides or training.\nDo this for 3-4 weeks.\nGraze for only 2-2½ hours.\nAdd one-half cup of molasses to the feed on riding days for energy, but only twice a week.\n\n, 2-4 pounds of grass hay. Ride for anywhere from 1-4 hours a day and 2-3 times a week.\n\n\nBegin cantering and more serious training. Ranch work can also begin although for only 1-4 hours and 2-3 times a week as with the riding (training counts as riding).\nGrazing can be anywhere from 2-4 hours a day. A couple ounces of oats can be added to the daily feed on riding/training days. Mountain riding can begin. Do this for 2-3 weeks.\n\n, Do this forever. 3-4 pounds of grass hay will be good once a day. Oats can be given (1-5 ounces a day) on working days. The horse should be fit enough for jumping and galloping.\n\n\nBegin training for rodeos, cross-country, and racing. Grazing should go for about 1-5 hours a day and only 3-4 days a week. Riding can be done 2-4 days a week.\nThe horse may also begin entering competitions and will be ready for almost anything.\nRides can be 1-5 hours long. The horse should be almost fully back in shape.\nCarriage work or hard farm work can also begin and be increased after 2-3 weeks.\n\n",
        "  A large symmetry group is perhaps experimentally observed in excited hadrons\nwhich includes the chiral group U(2)_L x U(2)_R as a subgroup. To possess this\nlarge symmetry a dynamical model for excited hadrons, presumably a string\nmodel, should explain formation of chiral multiplets and, at the same time,\npredict coinciding slopes of the angular and radial Regge trajectories. This is\npossible only if both the dynamics of the string and the chirality of the\nquarks at the ends of the string are considered together. We construct a\nmodel-independent unitary transformation from the relativistic chiral basis to\nthe ^{2S+1}L_J basis, commonly used in hadronic phenomenology as well as in the\nstring models, and demonstrate that a hadron belonging to the given chiral\nrepresentation is a fixed superposition of the basis vectors with different L's\nand S's. Thus the description of highly excited hadron in terms of a fixed L is\nnot compatible with chiral symmetry and has to be disregarded in favour of the\ndescription in terms of the total hadron spin J. Therefore, dynamics of the\nstring must deliver the principal quantum number ~n+J, in order chiral\nmultiplets with different spins to become degenerate, as required by the large\nsymmetry group.\n",
        "I very much like the underlying idea for this paper. I wasn't convinced by the execution in its current state. My primary concern is the one I expressed in my pre-review question below, which I don't think the authors addressed. Specifically, I think the choice of q(s | s') = p(s | s') will make the forward and reverse trajectories almost pathologically mismatched to each other, and will thus make the variational bound extremely loose and high variance. \n\nThe claim about the tightness of the bound in Appendix D relies on the assumption that the transition distribution obeys detailed balance. The learned transition distribution in the paper does not obey detailed balance, and therefore the tightness claim in Appendix D does not hold. (In Section 2.1 you briefly discuss the idea of learning an energy function, rather than directly learning a transition distribution. I think this would be excellent, and in that case you could choose an MCMC transition operator that does obey detailed balance for that energy function.) I did not go through Appendix D beyond this step.\n\nThe experimental results were not visually impressive. I suspect this is primarily driven by the mismatch between generative and inference trajectories. See my concern above and in the pre-review question below.\n\nAlso, see note below for sec. 5. I suspect some terms are being dropped from the training gradient.\n\nThe paper is optimizing a variational bound on log likelihood. You should really, really, really report and compare log likelihoods against competing methods!\n\nDetailed comments below. Some of these were written based on a previous version of the paper.\nsec 1.2 - first paragraph is very difficult to follow\n\"these modes these spurious modes\" -> \"these spurious modes\"\nsec 2.1 - \"s = (v,h)\" -> \"s = {v,h}\"\nsec 2.2 - \"with an MCMC\" -> \"with an MCMC chain\"\n\"(ideally an MCMC)\" -> \"(e.g. via MCMC)\" MCMC is not ideal ... it's just often the best we can do.\nsec 3, last bullet - could make the temperature infinite for the last step, in which case the last step will sample directly from the prior, and the posterior and the prior will be exactly the same.\nsec. 4 -- Using an energy function would be great!! Especially, because many MCMC transition operators obey detailed balance, you would be far less prone to suffer from the forward/backward transition mismatch that is my primary concern about this technique.\neq. 12,13 -- What is alpha? How does it depend on the temperature. It's never specified.\nsec. 5, last paragraph in GSN section -- Note that q also depends on theta, so by not backpropagating through the full q chain you are dropping terms from the gradient.\nsec. 5, non-equilibrium thermodynamics -- Note that the noneq. paper also increases the noise variance as the distance from the data increases.\nFig. 1 -- right/left mislabeled\nFig. 2 -- label panes\nFig. 3 -- After how many walkback steps?",
        "  We explore observational constraints on possible deviations from Newtonian\ngravity by means of large-scale clustering of galaxies. We measure the power\nspectrum and the bispectrum of Sloan Digital Sky Survey galaxies and compare\nthe result with predictions in an empirical model of modified gravity. Our\nmodel assumes an additional Yukawa-like term with two parameters that\ncharacterize the amplitude and the length scale of the modified gravity. The\nmodel predictions are calculated using two methods; the second-order\nperturbation theory and direct N-body simulations. These methods allow us to\nstudy non-linear evolution of large-scale structure. Using the simulation\nresults, we find that perturbation theory provides reliable estimates for the\npower spectrum and the bispectrum in the modified Newtonian model. We also\nconstruct mock galaxy catalogues from the simulations, and derive constraints\non the amplitude and the length scale of deviations from Newtonian gravity. The\nresulting constraints from power spectrum are consistent with those obtained in\nour earlier work, indicating the validity of the previous empirical modeling of\ngravitational nonlinearity in the modified Newtonian model. If linear biasing\nis adopted, the bispectrum of the SDSS galaxies yields constraints very similar\nto those from the power spectrum. If we allow for the nonlinear biasing\ninstead, we find that the ratio of the quadratic to linear biasing\ncoefficients, b_2/b_1, should satisfy -0.4 < b_2/b_1<0.3 in the modified\nNewtonian model.\n",
        "This paper proposed to use the BPA criterion for classifier ensembles.\n\nMy major concern with the paper is that it attempts to mix quite a few concepts together, and as a result, some of the simple notions becomes a bit hard to understand. For example:\n\n(1) \"Distributed\" in this paper basically means classifier ensembles, and has nothing to do with the distributed training or distributed computation mechanism. Granted, one can train these individual classifiers in a distributed fashion but this is not the point of the paper.\n\n(2) The paper uses \"Transfer learning\" in its narrow sense: it basically means fine-tuning the last layer of a pre-trained classifier.\n\nAside from the concept mixture of the paper, other comments I have about the paper are:\n\n(1) I am not sure how BPA address class inbalance better than simple re-weighting. Essentially, the BPA criteria is putting equal weights on different classes, regardless of the number of training data points each class has. This is a very easy thing to address in conventional training: adding a class-specific weight term to each data point with the value being the inverse of the number of data points will do.\n\n(2) Algorithm 2 is not presented correctly as it implies that test data is used during training, which is not correct: only training and validation dataset should be used. I find the paper's use of \"train/validation\" and \"test\" quite confusing: why \"train/validation\" is always presented together? How to properly distinguish between them?\n\n(3) If I understand correctly, the paper is proposing to compute the BPA in a batch fashion, i.e. BPA can only be computed when running the model over the full train/validation dataset. This contradicts with the stochastic gradient descent that are usually used in deep net training - how does BPA deal with that? I believe that an experimental report on the computation cost and timing is missing.\n\nIn general, I find the paper not presented in its clearest form and a number of key definitions ambiguous.",
        "- Strengths:\ni. Motivation is well described.\nii. Provides detailed comparisons with various models across diverse languages\n\n- Weaknesses:\ni.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.\n\n- General Discussion:\nThis paper issues a simple but fundamental question about word representation:\nwhat subunit of a word is suitable to represent morphologies and how to compose\nthe units. To answer this question, this paper applied word representations\nwith various subunits (characters, character-trigram, and morphs) and\ncomposition functions (LSTM, CNN, and a simple addition) to the language\nmodeling task to find the best combination. In addition, this paper evaluated\nthe task for more than 10 languages. This is because languages are\ntypologically diverse and the results can be different according to the word\nrepresentation and composition function. From their experimental results, this\npaper concluded that character-level representations are more effective, but\nthey are still imperfective in comparing them with a model with explicit\nknowledge of morphology. Another conclusion is that character-trigrams show\nreliable perplexity in the majority of the languages. \n\nHowever, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental\nlanguages. This paper chose ten languages in four categories (up to three\nlanguages per a category). But, one basic question with the languages is “how\ncan it be claimed that the languages are representatives of each category?”\nAll the languages in the same category have the same tendency of word\nrepresentation and composition function? How can it be proved? For instance,\neven in this paper, two languages belonging to the same typology\n(agglutinative) show different results. Therefore, at least to me, it seems to\nbe better to focus on the languages tested in this paper instead of drawing a\ngeneral conclusions about all languages. \n-         There is some gap between the claim and the experiments. Is the\nlanguage modeling the best task to prove the claim of this paper? Isn’t there\nany chance that the claim of this paper breaks in other tasks? Further\nexplanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for\nArabic. Is there any reason why the experiment is performed only for Arabic?\nThere are plenty of languages with automatic morphological analyzers such as\nJapanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is\nthere any good reason to choose only character-trigram? Is it always better\nthan character-bigram or character-fourgram? In general, language modeling with\nn-grams is affected by corpus size and some other factors. \n\nMinor typos: \n- There is a missing reference in Introduction. (88 line in Page 1)\n- root-and-patter -> root-and-pattern (524 line in Page 6)",
        "Dear Reviewers and Readers,\n\nFor the codes of the models and the tasks which we have explored/experimented in our paper, please see our repo:\n",
        "Modifications to the last revision:\n- Section 2, SampleRNN model. Better model description, changed order of sub-sections, explained the upsampling method more clearly.\n- Added a paragraph (before Section 3.1) detailing the training procedure and hyper-parameters.\n- Added subsection 3.3 Quantifying information retention. (See authors' response to AnonReviewer3 on Dec. 2 titled \"time horizon\")\n- Minor modifications (typo, citation, rephrasing, etc.)",
        "This paper is a parallel work to Improving Generative Adversarial Networks with Denoising Feature Matching. The main solution of both papers is introducing autoencoder into discriminator to improve the stability and quality of GAN. Different to Denoising Feature Matching, EBGAN uses encoder-decoder instead of denoising only, and use hingle loss to replace original loss function.\n\nThe theoretical results are good, and empirical result of high resolution image is unique among all recent GAN advantages.\n\nI suggest to introduce Improving Generative Adversarial Networks with Denoising Feature Matching as related work.",
        ";\n,, Instead of that, use a simple bookmark. You don't need to buy a fancy bookmark, an old piece of carton or anything thin clean and solid helps.\n\n, Or take a break eat and drink then wash your hands. This doesn't mean you can't eat before reading, just remember to wash your hands before even touching your book.\n\n, Only allow someone you trust VERY well to read the book next to you. Even if you trust someone you can't just give your book to him/her to read it in his/her place. There are other people in his place too, Do you know or trust them?\n\n, Standing position seems more practical and logical to you. In fact by time if your book was in standing position, the glue on your book will be tiered and its papers will come off. Only if your book is squeezed in the library your glue can last for a longer time. Remember sleeping position can keep the book the way it was forever.\n\n,,,, You're not Mr. muscle or Windex and you can't use Tip-ex or any kind of cleaning product or pen from your school pencil-bag.\n\n, Or else your book will be wavy.\n\n, You'll end up crashing a car a street light, an old lady or anyone.\n\n, Reading on the toilet can cause serious bottom problems and the bathrooms are usually humid.\n\n, Otherwise you'll forget it was there, your sibling, little cousin or anyone can harm it.\n\n,",
        "  We study the effects of dissipation on electron transport in a semiconductor\nsuperlattice with an applied bias voltage and a magnetic field that is tilted\nrelative to the superlattice axis.In previous work, we showed that although the\napplied fields are stationary,they act like a THz plane wave, which strongly\ncouples the Bloch and cyclotron motion of electrons within the lowest miniband.\nAs a consequence,the electrons exhibit a unique type of Hamiltonian chaos,\nwhich creates an intricate mesh of conduction channels (a stochastic web) in\nphase space, leading to a large resonant increase in the current flow at\ncritical values of the applied voltage. This phase-space patterning provides a\nsensitive mechanism for controlling electrical resistance. In this paper, we\ninvestigate the effects of dissipation on the electron dynamics by modifying\nthe semiclassical equations of motion to include a linear damping term. We\ndemonstrate that even in the presence of dissipation,deterministic chaos plays\nan important role in the electron transport process. We identify mechanisms for\nthe onset of chaos and explore the associated sequence of bifurcations in the\nelectron trajectories. When the Bloch and cyclotron frequencies are\ncommensurate, complex multistability phenomena occur in the system. In\nparticular, for fixed values of the control parameters several distinct stable\nregimes can coexist, each corresponding to different initial conditions. We\nshow that this multistability has clear, experimentally-observable, signatures\nin the electron transport characteristics.\n",
        "Classic Maya hieroglyphic inscriptions don't frequently mention people outside of the Maya area unless they're directly interacting with Maya political life. This is really a sampling bias, as the only texts carved on stones are those commissioned by kings to honor royal dynasties. However, we can piece together a general picture of their relationships with their neighbors based on archaeological data and scraps of text. We do know that the later Aztecs had a pejorative term for the nomadic peoples of North Mexico. (They called them \"Chichimecs\" which means \"Dog People.\" Probably.)\n\nHowever, the Maya did **not** have nomadic neighbors. To the west were other urban, pyramid-building, city dwellers like the Zapotecs, the Totonacs, the Otomí, and others. On the Eastern end of the Maya region was an agrarian culture called the Lenca, who lived in moderately sized towns with only occasional use of stone architecture. The relationship between the Maya and the Lenca is complicated. In the Early Classic Period (200s - 400s AD), the Maya displaced the local Lenca people living in the Copan Valley, Honduras and created a Maya city-state there. The earliest recorded king of Copan, Yax Kuk' Mo, is described in the inscriptions as a foreigner in Copan (that is, not Lenca, and probably from Tikal.) He was given a fairly multicultural burial. Part of this was due to the fact that he was associated with a Teotihucano (non-Maya) dynasty, although it also suggests that the early Maya king was trying to incorporate the Lenca into the Maya political system. For their part, it doesn't look like the Lenca did much to assimilate into the Maya culture. Although they adopted some Mesoamerican traits early on (such as the Ballcourt and many aspects of Mesoamerican religion), the archaeology seems to suggest a much more egalitarian, communal society than the highly stratified, aristocratic Maya.\n\nEven though we can see from the archaeology that the Maya of Copan were interacting very directly with these neighboring groups, almost no mention of them is made in the Hieroglyphic inscriptions. This seems to suggest that, as far as Maya dynastic politics were concerned, such groups were beneath their notice. This is actually pretty ironic, as when Copan collapsed at the end of the Late Classic, the Lenca slowly moved back in and recolonized the region.",
        "This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.\nThe group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts. \nI also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.",
        "I actually wrote a paper on a similar topic.  It comes down to something called present bias, and tends to match up very well with how individual people actually make decisions.\n\nIn making decisions, economic models typically assume some sort of discounting.  That's just a fancy way of saying that things nearer to the present matter more than things far in the future.  I'm willing to pay $10 ten years from now in order to have $5 tomorrow, because I will presumably have a higher income, and because decisions really far out in the future have minimal impact on my utility today, but having an extra $5 tomorrow means a lot more, because that decision is very near the present.\n\nThe problem is that we tend to value the present (the immediate now) even more than would be assumed by a traditional discounting model.  When faced with decisions to give something up tomorrow in order to get something next week, we might decide right now that it's a good idea to do so.  But tomorrow, that's no longer the case.  The benefit from exercise is gradual, and happens later in the future, but the pain of going to the gym is borne right now.  It's immediate, it's certain, and so we tend to consider that pain more strongly. Even though I might come to the conclusion that it's in my benefit to go to the gym tomorrow, when tomorrow comes, I'm going to decide that the long-run benefit doesn't match up with the immediate, short-term pain of dealing with getting up and getting exercise - all of the sudden, that snooze button seems far more attractive.\n\nEDIT: To those people who are asking to read my paper, I don't have a published version of my paper, but I would recommend the following two seminal papers on the topic:\n\n\"Doing it Now or Later\" (1999)  &  \"Choice and Procrastination\" (2001), both by Ted O'Donoghue  &  Matthew Rabin.  You should be able to find both of these papers pretty easily with a Google Scholar search.",
        "  In the last decade evidence has accumulated that small domains of 50-700 nm\nin diameter are located in the exoplasmic leaflet of the plasma membrane. Most\nof these domains supposedly consist of specific sets of lipids and proteins,\nand are believed to coordinate signal transduction cascades. Whether similar\ndomains are also present in the cytoplasmic leaflet of the plasma membrane is\nunclear so far. To investigate the presence of cytoplasmic leaflet domains, the\nH-Ras membrane-targeting sequence was fused to the C-terminus of the enhanced\nyellow fluorescent protein. Using single-molecule fluorescence microscopy,\ntrajectories of individual molecules diffusing in the cytoplasmic leaflet of\nthe plasma membrane were recorded. From these trajectories, the diffusion of\nindividual membrane-anchored enhanced yellow fluorescent protein molecules was\nstudied in live cells on timescales from 5 to 200 ms. The results show that the\ndiffusion of 30-40% of the molecules is constrained in domains with a typical\nsize of 200 nm. Neither breakdown of actin nor cholesterol extraction changed\nthe domain characteristics significantly, indicating that the observed domains\nmay not be related to the membrane domains identified so far.\n",
        " from the Joomla! download page to your computer. It is suggested to save this file to your Desktop so it can be easily found.;\n,,, If you saved the file to your Desktop, the default destination will be a new folder on your Desktop which is ideal. Click the Extract button.\n\n\nIt may take a few minutes to extract all the files.\nYou should see that a new Joomla folder exists now that you have uncompressed the ZIP file. It is best to rename the folder similar to the website that will be using Wordpress.\n\n,\n\n\nIn this example, rename the folder to 1and1help.\n\n,\n\n\nNow that the folder has been uploaded, you will have to ensure that the domain points to this folder. You must log into the 1&1 Control Panel and change the destination folder for the domain to the folder you just uploaded. To edit destination, please reference How do I set up or change a destination for the domain?\n\n,, In this example, the domain is http://1and1help.com/\n\n, installation page. If you receive a message that you need PHP5 to run Joomla!:\n\n\nClick the Start Menu and click Run.\nType notepad and press ENTER\nWrite !AddType x-mapp-php5 .php in the text file and save it as htaccess.txt\nUpload the file to the folder that holds your Joomla! files\nUsing your FTP program, rename the file to .htaccess. You must include the period before the file-name.\nReload the web-page.\n\n,, will perform a Pre-Installation Check to make sure that requirements are met and display whether the recommended settings are already in place. Display Errors may be Enabled which is not the recommended setting but it is best to keep this enabled for now. This will display any PHP errors, which may help you resolve any complications during the install and initial setup of your Joomla! site.\n\n,\n\n\nYou will need to accept the GNU General Public License for Joomla! in order to install it. Click the Next button if you agree with the terms.\n\n, knows which database to store the site data in. Please reference How do I create a new MySQL database? to create a new database or to find out where the database info is kept in the 1&1 Control Panel.\n\n, The Table Prefix is used to differentiate one Joomla! install from another in cases where you are using one database for multiple Joomla! sites (which is not recommended). You can leave the auto-generated Table Prefix or write in your own. Click the Next button when you have finished entering the database details.\n\n,,, you may want to click the Install Sample Data button so that after Joomla! installs you can view a sample site.\n\n,, installs, click the Remove installation folder\n\n,, site or log in to the Admin section.\n\n",
        "Süderbrarup is an Amt (\"collective municipality\") in the district of Schleswig-Flensburg, in Schleswig-Holstein, Germany. The seat of the Amt is in Süderbrarup.\n\nThe Amt Süderbrarup consists of the following municipalities:\n\nReferences\n\nÄmter in Schleswig-Holstein",
        " Sending a cute text can be very sweet. Sending a nice e-card (which is sweet sense you can add a little message). Call them every now and then. Just be sure you don't call her too little and or too much.;\n, Think of things that can be special to you and her. If you go to the same school go to their locker and slide in a love letter (with your name on it or they'll think it's from somebody else), make sure it isn't too small and unnoticeable, or too big. About a 3X3 card with a cute ribbon on it should be nice, but only if you think she'll like it.\n\n, Anything from dance classes, creating a band, and ice skating classes can be fun. If one of you doesn't like the activity, then try finding something else to do.\n\n, Dating is very important, but don't date too much unless you both like it. Go somewhere she or he likes, somewhere new, not always the same place. Water parks, amusement parks, zoos, movies, restaurants, and concerts are good. Share drinks, food, and have fun! Anything should do as long as you're together.\n\n,,, It's romantic and sweet.\n\n, When proposing make it romantic. She won't think you’re serious if you try to do it though the phone, email, or a text.\n\n,, Don't always give them advice (as this can be annoying) but sometimes when nobody else listens, you can be a good shoulder to cry on, and always comfort her.\n\n, Girls like guys that smell nice and keep themselves clean and tidy. Comb your hair and always take a shower or bath to get rid of body odors.\n\n, As a goodbye or a hello, sometimes a peck on the cheek, a 5-second kiss, or a passionate kiss can be sweet. Don’t always make out or she'll think that's all you want, or talk about it too much! Occasionally a surprise kiss can be cute! However, don’t keep it a peck; make it a long passionate kiss.\n\n,, This you will have to figure out and really talk through to your parents. Don’t just whine, actually sit them down and have a serious talk. Racial differences can have a huge impact on your partner so that is something you really need to prove to your parents that their wrong and your right. Maybe you parents don't want you kissing or having anything sexual at a young age. You will have to make sure your parents understand you are smarter than this and never lie to them about these things. You'll lose a lot of your trust and they'll most likely end up saying I told you so. Nevertheless, if you truly love your partner, then you can’t give up.\n\n, This can be your rooftop, a tree, a hill, a restaurant, or a certain bench! Somewhere worthy of being called special, like where you’re first kiss was, or where you proposed, your first date, or where you asked them out!\n\n",
        "# Summary\nThis paper proposes an algorithm to learn the structure of continuous SPNs in a single pass through the data,\nbasically by \"growing\" the SPN when two variables are correlated.\n\n## NOTE\nI am not an expert on SPNs, and can not really judge how impressive the presented results are due to lack of familiarity with the datsets.\n\n# Pro\n- This looks like possibly impactful work, proposing a simple and elegant algorithm for learning SPN structure single-pass, rather than just using random structure which has been done in other work in the online settings.\n\n# Con\n- The paper is heavily updated between submission deadline and submission of reviews.\n- The paper reads like a rush job, sloppily written - at least the first version.\n- Comparison to literature is severely lacking; eg \"several automated structure learning techniques have been proposed\" followed by 6 citations but no discussion of any of them, which one is most related, which ideas carry over from the offline setting to this online setting, etc. Also since this work presents both joint structure & *parameter* learning, comparison to the online parameter learning papers (3 cited) would be appreciated, specifically since these prior approaches seem to be more principled with Bayesian Moment Matching in Jaini 2016 for example.\n- I do not know enough about SPNs and the datasets to properly judge how strong the results are, but they seem to be a bit underwhelming on the large datasets wrt Random\n\n# Remaining questions after the paper updates\n- Table 3: Random structure as baseline ok, but how were the parameters here learned? Your simple running average or with more advanced methods?\n- Table 1: you are presenting *positive* average log-likelihood values? This should be an average of log(p<=1) < 0 values? What am I missing here?\n\nI recommend reject mostly because this paper should have been finished and polished at submission time, not at review deadline time.",
        ";\n, If your Wii or Wii U has been modified from its original state using Homebrew, select the Homebrew option; if your Wii or Wii U has not been modified, select the Hackless option.\n\n,, The Project M file requires 2.0 GB free space. If necessary, use an SD card adapter to insert the SD card into your computer via USB.\n\n, The SD card must be in FAT32 format to be compatible with your Nintendo Wii system.\n\n\nWindows: Click on “Start,” select “Computer,” right-click the SD card, then select “Format.”\nMac OS X: Open the Applications folder, select “Utilities,” click on “Disk Utility,” select your SD card, then select “Erase.”\n\n, The root folder is also known as the root directory, and is the highest folder in a folder-based hierarchy system.\n\n,,, This will automatically launch Project M.\n\n, Super Smash Bros. Brawl will launch successfully with Project M., A locked SD card will prevent your system from reading and copying files., Dirt, dust, and other debris can prevent systems from reading SD cards., This file is required for Project M to integrate successfully with Super Smash Bros. Brawl. If this file is not present, repeat steps #1 through #6 from Part One to copy Project M to the SD card., Sometimes, the Hackless version of Project M can cause your system to freeze.",
        " Everyone has at least one physical apart that sets him apart from the rest of the crowd. A particularly distinctive facial feature, tattoo, birthmark or scar will all attract attention for the fact that they make you unique. Recognize these traits in yourself, and find ways of covering them up from view.\n\n\nFor instance, if you have a striking set of eyes, donning sunglasses will remove this feature from view.\nIf you have tattoos on your arms, wearing a long sleeved shirt will cover them up.\nIf you have a distinctive facial structure, you can even out your features by growing facial hair.;\n, For most casual situations, it's a good idea to dress as you think a completely average person would. Avoid clothing that has distinctive logos or graphic designs on it.\n\n\nYour dress should ultimately be dictated by the setting. If you are at a formal event, for example, dressing yourself down will only bring negative attention your way.\nIt helps to know what other people will interpret as being \"normal\".\nAlthough bargain bin clothing is often a good choice for hiding in plain sight, there's sometimes the risk of those old clothes becoming fashionable again., While the use of camouflage is only helpful when you're trying to hide from a distance, it's the best-known way of truly hiding in plain sight.\n\n\nWearing camouflage gear in a public setting will attract a lot of unwanted attention to you. Special camouflage clothes are only recommended in instances where you have a chance at being completely hidden, and not just inconspicuous.\n\n, If you want to avoid attention, it's a good idea to avoid these little add-ons completely. If you're in close proximity with people, wearing a watch or necklace will make you more interesting to look at, however watches are not that conspicuous.\n\n, If you want to hide in plain sight, keep your hairstyle plain and vague. Avoid anything aesthetic like hair gels or dye. Just make sure your hair is relatively clean.\n\n\nIf you usually gel your hair, go without it when you don't want to attract notice. If your hair is completely natural, there won't be as much that stands out.\nIf you have distinctive hair and don't want to lose it for the sake of hiding, wearing a hood or hat will cover it up for you.\nAn uncommon hair color(usually light-colored) or texture(i.e. big hair) relative to the crowd can be a dead giveaway, so if you have either, cover it up under a hat or something\n\n, This is especially true if you are breaking laws. On top of risking criminal charges, you'll draw all sorts of eyes in your direction. While this applies to obvious felonies such as assault, it also encompasses things you may do innocently, such as jaywalking, or treading upon park grass. Try to keep in mind what the lawful, orderly thing to do would be in every situation you're trying to hide in.\n\n, Even if you're trying to be inconspicuous to everyone, it helps a lot to know where people are looking, and what they're occupied with. By keeping a healthy sense of situational awareness about you, you'll be able to avoid areas where people are more aware.\n\n, If there's ever a point where you're not sure about what to do, look at the people around you for an example. Is there a common mood around? Are the people around you walking fast or slow? All of these details can be used to help you blend in.\n\n, Try to avoid the fringes and the very center. Being halfway in radius to the center is least conspicuous., Although silence isn't always possible in some situations, it is a major part of keeping a low profile. For however long you're trying to hide, avoid unnecessary conversations and interactions with others. Make as little sound as you can, even if you're in a relatively busy environment. Even something as innocuous as a cough or sneeze can draw attention to you if left unchecked.\n\n\nWhen you have to use your voice, make an effort to be only half as loud as normal. This quietness should still make it capable to communicate when you need to, but you won't attract attention of anyone you don't need to., If this happens to you, you should make an effort to be pleasant and agreeable, without necessarily trying to advance the conversation anywhere. Smile, nod, and agree with what's being said. If you're asked questions about yourself, keep your answers short and to the point. It is not recommended that you try to ignore or stay silent, as this will only bring unwanted attention your way.\n\n\nThe word \"Yes\" will be your best friend if you're trying to go unnoticed in a conversation.It can be applied as a response to questions and answers alike.\n\n, If this is the case, it's important to keep your movements casual. Someone who is walking faster than usual will give off anxious signals to anyone watching. Be conscious of the pace and style you're moving in. Your movements should remain fluid, without coming across as hurried or nervous.\n\n\nIf you have time to prepare in advance, it often helps to practice in front of a mirror. That way, you'll have a better idea of the ways in which others will perceive you.\n\n, Avert your gaze from others around you. A good trick is to keep your eyes fixed on the ground ten metres ahead of you. Although you should keep your head down, you don't want to make it look obvious that you're intentionally avoiding eye contact.\n\n\nKeep in mind that there is a difference between casually avoiding eye contact, and looking like you're deliberately averting someone's gaze.You'll inadvertently attract more attention to yourself if someone gets the impression that you're trying to go undetected.\n\n, If you're hiding in a populated environment, don't be afraid to use other people as a tool. Placing yourself in the middle of a crowd will make it more difficult to spot you. Avoid going to the front, side or back of a crowd, as these are usually the places people check first when they're looking for someone.\n\n\nAvoid walking in less crowded places. People to tend to notice each other more when there aren't many people to look at.\n\n, If you want to disguise yourself and blend in, it's a good idea to choose a hair colour that seems natural and common. Shades of black and brown are good choices in this regard.\n\n, Accessories like hats and sunglasses work well to obscure someone's regular appearance. Sunglasses and a hat by themselves are a great casual disguise.Other accessories like jewellery and bowties won't necessarily obscure your appearance, but they'll add a different tone to your look that others may not be used to seeing from you.\n\n\nIf you are known to dress conservatively, putting on a bunch of jewellery or flashy colours will disguise your typical identity, even if the get-up draws attention to you otherwise.\n\n, You can add years to your appearance by using makeup. Squint and bit, and lightly pencil in the creases caused by your squint.It's important to keep these cosmetic changes subtle. Otherwise, it will be easy to pick up on the fact that it's a disguise.\n\n\nIf you are elderly, smooth out your facial creases with skin foundation. Dying your hair to mask the grey or white will also help disguise your identity.\n\n, However, you can tell a fair amount about a person's character and mood by their gait. Switch your walking style to suit the disguise. If you're trying to look older, for example, you should walk slower than you normally would.\n\n\nWalk for a bit in your typical manner, and identify your walking personality. If you're typically a fast walker, for instance, you can disguise your identity by walking slowly. Likewise, you should speed up your gait if you're a more leisurely walker.\n\n, If you are in an area with video surveillance, you may want to take measures to change the appearance of your facial structure. Puff out your cheeks and stuff your nostrils with toilet paper to change the shape of your face.This can help you escape any facial recognition technology that may be used to track you down.\n\n\nDoing things like puffing your cheeks and filling your nostrils will help disguise your identity, but you may end up drawing more attention to yourself.\n\n, Using a fake moustache is arguably the most common and famous type of disguise around. You can buy some facial hair from a costume store. Apply some spirit gum (a substance used to affix fake facial hair to actors, not SPENT gum, like chewing gum) to the area you want to have bearded, then set the disguise on your face and allow it some time to settle.\n\n\nIf you need to disguise yourself quickly and are known for your facial hair, quickly shaving it off will transform your appearance.\nThis can possibly work even if you're a female.\n\n",
        "  Recently, the collisionless expansion of spherical nanoplasmas has been\nanalyzed with a new ergodic model, clarifying the transition from\nhydrodynamic-like to Coulomb-explosion regimes, and providing accurate laws for\nthe relevant features of the phenomenon. A complete derivation of the model is\nhere presented. The important issue of the self-consistent initial conditions\nis addressed by analyzing the initial charging transient due to the electron\nexpansion, in the approximation of immobile ions. A comparison among different\nkinetic models for the expansion is presented, showing that the ergodic model\nprovides a simplified description, which retains the essential information on\nthe electron distribution, in particular, the energy spectrum. Results are\npresented for a wide range of initial conditions (determined from a single\ndimensionless parameter), in excellent agreement with calculations from the\nexact Vlasov-Poisson theory, thus providing a complete and detailed\ncharacterization of all the stages of the expansion.\n",
        "Pacheco Creek is a  west by southwest flowing stream which heads in the Diablo Range in southeastern Santa Clara County and flows to San Felipe Lake, the beginning of the Pajaro River mainstem, in San Benito County, California.\n\nHistory\nThe creek is named for Francisco Pacheco and Juan P. Pacheco who were granted the Rancho Ausaymas y San Felipe land grants in 1833 and 1836, and 1843 respectively. An early name for the creek was Arroyo de San Felipe. Francisco Pacheco came to California in 1819. \n\nJust north of the earthen dam on North Fork Pacheco Creek was one of the last refuges of the Amah-Mutsun band of the Ohlone people, and is rich archeologically with multiple burial sites and artifacts, including projective points so large that they would have been used for bear or elk.  In 1993, Mark Hylkema documented eight different Native American sites in this area, dating from 1000 B.C. to 500 A.D.\n\nFlooding\nOn 11 January 2017, a levee break at Pacheco Creek affected fifty local homes; some homes had mudlines about five feet high. On 12 January, health officials advised some local residents not to drink local tapwater pending contamination testing.\n\nWatershed\nThe mainstem Pacheco Creek is formed by the confluence of the North Fork Pacheco Creek and South Fork Pacheco Creek about  west of Pacheco Pass. Significant flows are contributed to the Pacheco Creek mainstem by the North and South Forks of Pacheco Creek, and Cedar Creek. \n\nThe North Fork Pacheco Creek tributary is a  stream beginning in Henry W. Coe State Park at  and receives the  East Fork Pacheco Creek, at Chimney Rock before reaching Pacheco Reservoir, the latter just north of Highway 152 and  above the confluence of North and South Forks Pacheco Creek. The Mississippi Creek tributary of North Fork Pacheco Creek is  has an impoundment (Mississippi Lake) above  elevation, and sources on Bear Mountain on the northern side of Henry W. Coe State Park.\n\nThe South Fork Pacheco Creek tributary receives flows from the shorter Middle Fork Pacheco Creek just below Highway 152. From here the South Fork Pacheco Creek flows  to it confluence with North Fork Pacheco Creek, forming the source of the Pacheco Creek mainstem. From here Pacheco Creek generally follows Highway 152, passing from Santa Clara County to San Benito County, and continuing until it empties into San Felipe (Soap) Lake, the source of the Pajaro River.\n\nTequisquita Slough joins Pacheco Creek just above San Felipe Lake. The latter has 3 main tributaries, Santa Ana Creek, Arroyo de Los Viboras, and Arroyo Dos Pichachos. Santa Ana Creek is apparently named for the Rancho Santa Ana y Quien Sabe land grant.\n\nStream flow in Pacheco Creek is influenced by releases from the North Fork Pacheco Reservoir, which is operated by the Pacheco Pass Water District.\n\nEcology\nSignificant remnants of the historic riparian California sycamore (Platanus racemosa) habitat still exists on Pacheco Creek and are a good example of the Central Coast Sycamore Alluvial Woodland habitat type.\n\nPacheco Creek historically hosted steelhead trout (Oncorhynchus mykiss) as evidenced by a physical specimen collected in 1945 by D.H. Simpson in the California Academy of Sciences, \"19.5 miles east of Gilroy on Hwy. 152\". Cedar Creek and South Fork Pacheco Creek hosted steelhead trout runs in wet years and juveniles found on stream sampling indicated the presence of perennial pools suitable for oversummering in headwater reaches. Pacheco Reservoir (North Fork Dam) is an impassable barrier to in-migrating steelhead trout, preventing access to the nearly  of stream consisting of North Fork Pacheco Creek, Mississippi Creek and East Fork Pacheco Creek. In 1973 Fish and Game Warden W. I. Donahue reported that \"high quality spawning and rearing habitat with perennial flow occurred upstream from Pacheco Dam on the North Fork, but was unavailable to steelhead because of the dam.\" In addition, resident rainbow (the landlocked form of steelhead trout) successfully rear in fast-water habitats grow rapidly and reach smolt size by the end of their first summer. In many years in late spring, prior to reservoir releases for agriculture, low stream flows and high water temperatures severely impact steelhead fry and small juveniles. \n\nOther native fish in Pacheco Creek include Monterey sucker (Catostomus occidentalis mniotiltus) and Sacramento pikeminnow (Ptychocheilus grandis).\n\nSee also\n Rivers of California\nPacheco Creek (San Benito County), a tributary of the Pajaro River, in San Benito County, California.\nSouth Fork Pacheco Creek, a tributary stream of Pacheco Creek (San Benito County)\nEast Fork Pacheco Creek, a tributary stream of Pacheco Creek (San Benito County)\nNorth Fork Pacheco Creek, a tributary stream of Pacheco Creek (San Benito County)\nPacheco Reservoir, California, a reservoir formed by a dam on the north fork of Pacheco Creek (San Benito County) a.k.a. \"North Fork Dam\"\n\nReferences\n\nExternal links\n Pajaro River Watershed Council\n\nTributaries of the Pajaro River\nRivers of Santa Clara County, California\nRivers of Santa Cruz County, California\nRivers of Monterey County, California\nRivers of San Benito County, California\nRivers of Northern California",
        " Landing the aircraft begins with a good landing pattern. If we are talking about an aircraft, such as a Super Decathlon, J3 Cub etc... An aircraft with an approach speed around the 50 mph (80 km/h) (Please note it is a round number). Always perform a pattern as similar as the last one. (3/4 of a mile from the airport is always a safe distance)\n\n, After performing a good downwind - base - and turn to final approach, a great idea is to \"Warm up the Rudders\" which means deflect the rudders left and right, keep inputting small left and right inputs and prepare yourself to use them, because YOU WILL!. Let's remember that a tail dragger has the tendency to bring the tail forward, which will happen if you do not use those rudders to prevent it.\n\n,,,\n\n\nIMPORTANT!! Do not look just immediately in front of the airplane, look all the way to the end of the runway, this will give you a much more clear view of what kind of input you have to put into those rudders.\n\n, So we have to compensate so we are not either blown away or into the airfield!;\n, Your nose will be deflected towards the 030 heading (maybe 020) and the right wing into the wind.\n\n,,, maintain the centerline, you will eventually land with the tail wheel and the right wheel, with the left wheel still in the air.\n\n,, If you are going downwind, Forward input will be necessary.\n\n",
        "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.",
        "Colin Stewart may refer to:\n Colin Stewart (alpine skier) (1927–2015), American Olympic skier\n Colin Stewart (footballer) (born 1980), English-born Scottish goalkeeper\n Colin Stewart (record producer) (born 1974), record producer and audio engineer\n Colin Stewart (rugby union) (born 1980), Scottish rugby union player\n R. Colin Stewart (1926–1994), Canadian politician\n\nSee also\n Colin Stuart (disambiguation)",
        "We have thought more deeply about the fact that the results in Bengio et al. appear to be in contradiction with ours, in the sense that the Always Sampling training scheme (corresponding to our 100% Pred. Frames training scheme) seems to perform worse than mixed schemes. We also had a chat with one of the authors of the Bengio et al. paper to get a better understanding of the methods and experiments. \nWe have reached the conclusion that the difference might be due to the fact that Bengio et al. focus on discrete problems and to the fact that we were mostly reasoning about long-term prediction, whilst Bengio et al. focus on shorter-term prediction (e.g., in Image Captioning Section, the average prediction length is 11 (this is not mentioned in the paper, but has been pointed out to us by one of the authors)) -- if we look at our results for short-term prediction only, they do not look so much different anymore.\n\nIn Figures 12-16 of the latest version of our paper, in addition to the prediction error at time-step 100, we also plot the prediction error at time-steps 5 and 10 for most games. We can notice that the prediction error with the 100% and the 0%-100% Pred. Frames training schemes (called here Schemes I and II) (red and dark green lines) is almost never lower than the prediction error with the other mixed schemes for time-steps 5 and 10, and often higher (see for example Fishing Derby in Fig. 13). The situation is different at time-step 100, where Schemes I and II are always almost preferable to the other mixed schemes. Therefore, by looking only at the error up to time-step 10, we would also prefer other schemes to Schemes I and II. The error is higher at lower time-steps with Schemes I and II as the objects are less sharply represented in the frames. In other words, these two schemes capture the global dynamics (as this enables better long-term prediction) at the expense of not representing the details very accurately, as lack of the details at earlier predictions do not harm subsequent predictions. In Bengio et al., where the problem is discrete, one error at the beginning of the sequence might lead to drastic errors at subsequent time-steps.\nWe thank the reviewer for this question, as he made us thinking more carefully about this point. We need to modify the paper to include as summary of this discussion.\n\nWe also thank the reviewer for pointing out that in “Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016”, the unconditional model can perform jumpy prediction. We briefly mention this paper in the introduction at the moment, but we need to add a discussion. Notice that, in this paper, prediction is shorter-term than in our case, namely 10-13 time-steps ahead. We are not sure whether, at training time, the real or the generated frames are fed in the conditioned model. From the sentence “At test time we feed in the generated frame from the previous step without adding any noise. At training time we feed in the ground truth.” in Section 2.3, it would look like the model is trained with feeding in the real frames, i.e. with the 0% Pred. Frames training scheme. This approach seems to work better than the unconditional model -- this is surprising to us, as it is in contradiction with our results.\n",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.",
        " Humor can lighten the mood, but it is often taken at another’s expense. Many jokes degrade women, ethnicities and disabled people.;\n, Promoting language education, rather than fighting it, can decrease discrimination based on ethnicity.\n\n, Thinking of things as “women’s work” or “man’s work” may lead to gender discrimination in the next generation. As more and more families become dual-income households, make sure both sexes are treated as partners.\n\n, Join a club, group or team where you have contact with people of different gender, sexuality or race. People often get into routines where they aren’t exposed to diversity in the community.\n\n, Although you might be devout, a little research can help you find the common ground between another faith and your own. The next time someone makes a religious slur, talk about the commonalities, rather than the differences.\n\n, Starting a conversation about creating an open group may avoid issues based on these topics in the future.\n\n, Judge the candidates based on these professional parameters. If you fear a colleague is discriminating in their choices, ask for a third party to judge the candidates without seeing their names or faces.\n\n\nSet non-negotiable equal pay for new hires (based on their degrees). This stops discrimination regarding salary offers, and accounts for the fact that women are on average more hesitant to negotiate their salaries.\n\n,, If you exist in a diverse population, but your employees are all from the same race, you may be inadvertently promoting hiring practices based on discrimination.\n\n, Post it in a common location and add it to your employee handbook. Place a human resources manager in charge of complaints.\n\n\nSome smaller companies can’t afford to hire human resource managers. However, someone at the business should be a “contact officer,” who is responsible for handling discrimination issues before the employee contacts a state agency or a lawyer.\nTake complaints very seriously. No one should be told to \"stop being so sensitive\" if they are being mistreated at the office.\n\n, Make it clear that there is a zero tolerance policy when it comes to discrimination. Announce how complaints and disciplinary action will be handled.\n\n\nTraining should include topics of gender, race, LGBTQ status, size, disability, religion, and age.\nExplain that the company does not tolerate microaggressions, such as sexist jokes, the r-word, or derogatory racial terms.\n\n, You can protect yourself and your employees by noting each interaction.\n\n, Installing a ramp at a retail location, installing a sit/stand desk for an employee with a chronic back problem, or protecting an autistic person's ability to fidget at board meetings helps you avoid disability discrimination.Be flexible about medical needs, such as doctor visits or needing to work from home sometimes.\nAllow people with mood disorders and intellectual/developmental disabilities to act a little unusual. This means accepting behavior that looks a little odd to non-disabled people, such as stimming (rocking, fidgeting), nervousness, or pacing.\n\n, Include a statement at the end that says, “We are an equal opportunity/affirmative action employer.” The following are common ways that people discriminate when they advertise jobs:\n\n\nStressing that a job is or is not for a student or a “youthful,” “mature” or “retired” person is age discrimination.\nSaying that a job is or is not for a woman, man, dad or “stay at home mom” is gender discrimination.\nRequiring that a job is only for US citizens or green card holders is discrimination based on nationality. Anyone who is qualified to work in the country should be allowed to apply. Work permits or visas are reviewed during the last step of the hiring process.\nStating that the person must be “clean-shaven” can lead to religious discrimination.\nReferencing any race or ethnicity can be racial discrimination.\nWriting that the person is “able-bodied,” “able to stand” or “slender” can be disability or weight discrimination.",
        "Eiffelospongia is a genus of sponge known from the Mount Stephen Trilobite Beds.\n\nReferences\n\nExternal links \n \n\nHexactinellida genera\nPrehistoric sponge genera\nBurgess Shale sponges",
        "Because the authors did not respond to reviewer feedback, I am maintaining my original review score.\n\n-----\n\nThis paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.\n\nStrengths:\n- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.\n- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.\n- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.\n\nWeaknesses:\n- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional \"baselines\" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.\n- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).\n- It's unclear what is the purpose of introducing the inequality in Eq. 9.\n- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).\n- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).\n\nThis is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.",
        " Patapons will be celebrating a dancing around the fire. Others giving you advice. Keep pressing button to carry on. The PSP keypad will now pop up. It may ask you for either for your name or like naming the tribe but it's all in Japanese, so type anything and press start to continue. It will say if you want to save, so do so. After that talk to the one with the cloak., selects things. Press to save and click the top option to do save or the bottom option to not save. The button is if you want to quit. Bottom option is no, top one is yes to quit. For both screens.,, , scroll left and right. goes back and selects a level. You will only be able to do the first square at the moment. Press to enter it.,\nButton goes to the last screen.\nSelects stuff.\nHighlights stuff.\nHides things.\nButton brings up units status.\nThe squad status screen shows your total squad status all together.\nHighlight your Patapons with the D - pad press triangle and you will been on the unit status section. This screen shows stats for each Patapon. with D - pad you can select a Patapon and then by pressing up or down select either the weapon icon or helmet icon and press to select it. This will open a box where you can select weapons and items etc. You won’t have any to select at the moment so press to go back. Play around with the options until you get comfortable with it and press the button when you’re ready to start the mission.\n\n,\n\nNew Sequence: = Attack.\nVoice: Pon Pon Pata Pon.\nEnemies: Three birds and four pigs.\nYou now have a new button sequence, Pon Pon Pata Pon. This makes your Patapon attack in this case throw spears at the wall. You can practice your Pata Pata Pata Pon instead but they won’t move but you can build a fever up by getting 10 combo points. Anyway attack the wall by doing keep on doing it until you destroyed the way you can build up combos with this one too. Remember to do the next one only after the Patapons have finished singing. After you have destroyed the wall do your victory dance by keep on doing Pon Pon Pata Pon four more times in a row after that move on forward with Pata Pata Pata Pon. when you get near the totem pole you will notice that it has symbols but you can't do anything there at the moment so keep on going. if you see enemies like birds and other things don’t stop to close because less advantage for you. attack and try to kill them if you can, sometimes helps if you're in a fever (10 combo points). you do more damage in a fever. the birds you see do run away easily.\nYou can get combos by doing Pata Pata Pata Pon, and then doing Pon Pon Pata Pon, it doesn’t have to be the same one each time. The more combo points you have the more damage you do. You can get coins from the sunflowers and enemies if they get destroyed while you’re attacking. Move over the coins to collect them. All the enemies on this level run away and don’t attack so don’t be afraid to attack them. Keep moving forward and attacking to get to the flag pillar and the end of the mission. Mission Complete!\nOn the mission complete screen you will see the Patapons carrying on carts the beasts you killed back to their home.\nAnother information screen will pop up.\n\n, Talk to the Patapon with the cloak that is standing next to what looks like a shrine. Click on the shrine and this will take you to a screen with the items you have collected like how many spears you have and how much food you have from the beasts you have killed and lots more. Also how many coins you have is on the top right. Press to go back. Click on the big black pillar when you want to go to the world map and the next mission., You might want to build up a fever to do more damage. The 4 Zigotron behind the wall will move right behind the next wall what is next to another wall. A Patapon with a sword and shield will come out of the bush and join your group. Destroy the next wall will your group (your new Patapon is very strong). While destroying the walls watch out for enemy Zigotron. Just keep on moving right destroying the walls and the enemy and collecting the items as you go. Destroy the small black pillar then the small house. After that your new Patapon will die and turn into a butterfly. Pick the items it drops and go through the end of level flag. Mission Complete!\n\n, When you select it, a menu opens up showing the Patapons you can make, how much you have all together, how much it cost to make a specific Patapon, and what items are need e.g. to make a spear Patapon you need meat, wood, and 80 coins. The circles show how many of each Patapon you already have., You need to press circle in time to make the tree shake and drop an item. If you fail you can try again but you need to give one piece of meat again. when you hear the person go for example Pon Pon Pon when he finishes you press circle, circle, circle and so on. Listen to what the person says, if it says four words you have to press the circle button four times but you have to do it in the same tempo as the singer. Once you done that you will get the items you need (wood).\nThe sequence is: O/O/O O/O/O O/O/OO O/O/OO O/OO/OO O/OO/OO O/O/O O/O/OO\nIf you get stuck just talk to the Patapon with a cape.\nYou have to pass the tree mini-game so if you fail it and don’t have any meat you have to play a previous level to collect some meat. You will know that the level contains meat because it will have a meat symbol on the world map. So play the level with the meat symbol and get some meat by killing beasts and completing the level. In this game it’s good to play over the old levels and a lot of the time is required.\nOnce you have passed and got a branch, make a spear Patapon, you will notice that it is all orange. Then make sure it’s on your team then play the second square on the world map.\n\n, You will have to fight lots of Zigotron now, there not that hard if you have an axe Patapon because it will go up front and take all the damage while you’re attacking with your spear throwers behind. this is probably be the first time you get a proper fever going and maybe the first time a Patapon dies none did for me though. Keep on moving forward and attacking the enemy and trying to keep the combos going till you get to the end. Destroy the building and go through the flag. Mission complete!\nRemember to upgrade your Patapons with better weapons when you can.\n\n, Destroy the first wall, then take out the tower, watch out for the archers on the tower and the other Zigotron patrolling below it once they're all dead and building destroyed a Patapon archer will appear. for the next bit do the same tactic as before and destroy the wall, tower, house and the next tower if you see a green potion pick it up, it restores your Patapons life. If you get fever going it won’t take long to kill them all. The archer you got fires fire arrows which can set the Zigotron a light this makes them run around and not fight back. After you they're all dead and buildings destroyed, move forward. Destroy the black pillar. When you get near the house the bow Patapon will die and turn into a butterfly pick up the items it drops, destroy the building and go through the flag. Mission complete!\nYou can now make archers.\n\n, This level might be hard if you’re not prepared because of the tough grim reaper type Zigotron you face, it keeps on attacking you with tornadoes which make your Patapons jump in the air and get hurt. In the middle is a Patapon trapped in a cage; you need to save him too so make sure you don’t kill him by accident. Keep on moving forward and attacking the enemy, having fever is very helpful and picking up green potions. Your probably won't are able to kill the boss here because it will probably run away from you. Mission complete!\nTo defeat the boss coming up you will need a lot of Patapons so do the second square, the one with the meat icon a few times to build up a bigger squad and use the best equipment you have available. Also do the tree mini game if you need more wood.\n\n,\nGo forward and destroy the pillar. You will get a new drum Chaka and a new sequence Chaka Chaka Pata Pon. Does the new sequence a few times, when ready keep moving forward. When the fire breathing dragon comes use Chaka Chaka Pata Pon each time, this makes you defend and attack. Only the Patapons with shields defend. If Patapons die make sure you pick up there helmets and just keep on attacking with Chaka Chaka Pata Pon. It isn’t that hard if you got the most Patapons you can get at the moment. I had 11 Patapons, 3 axes, 6 spears and 1 archer.\nIf any Patapons died, make sure you reborn them when you get back to your village. You do this by going to the heart shaped tree.\n\n, If you go past it, you will start burning and dying very quickly because of the extreme heat of the desert. You will need something to cool it down., I don’t know if it needs to be raining for it to work., Basically the same as the last boss fight but a bit harder. Keep on attacking with Chaka Chaka Pata Pon. Your Patapons might get eaten so watch out for that. Keep attacking if your entire shields Patapons die then attack using Pon Pon Pata Pon. Keep at it and it should die eventually. The boss dragon drops a rain item.\n\n,\nNew Sequence: = miracle\nVoice: Don DoDon DoDon\nDo the rain miracle at the start. You have to be in fever to do it. If you can’t do it, practice a lot. After that move forward to the tower. Destroy the tower then keep moving forward. You will see beasts that you can attack also one will have a blue symbol on it, if you attack that one it will turn into a snail. When you get tot he talking sign destroy the rock in front and get fever. Once the rock in front is destroyed use rain miracle then move forward and destroy the rocks and keep on moving forward. you need to try to keep fever going so you can use the rain miracle straight away or if you move forward really quickly and destroying the rocks as you go you can get out the desert with only doing the rain miracle once but you will lose a lot of Patapons this way so make sure you pick up the helmets they drop. Keep going forward destroy the building at the end and go through the flagpole.\n\n,, They will attack in groups and each group has two zigotrons on horses. This levels isn’t that hard if you use all the things you have learned so far like knowing when to attack and charge or when to hold your ground. When they are near most of the time its best to use the defend command. When they are not near use the attack command. Make sure you try to stay in fever for most of the time because this will help you do more damage. Make sure you pick up the green potions they drop. When you get near the end you will see zigotrons hiding in the bushes. Also near the end lots of them will attack if you are in fever and you use the attack command they will run away and retreat. So you don’t have to kill all of them near the end. Destroy the building then go through the flagpole.,, Do the rain miracle at the start. Move forward and you will see a blue beast, hit it and it will turn into a snail now all you have to do is kill the snail. Make sure you are in fever mode most of the time and keep on attacking. If it gets to far make sure you move forward with it and keep on attacking. When it dies it will drop a hat pick it up and go through the flagpole at the end., You will now have a new mini game. You can select the new mini game by pressing to get to it.,, When the game starts keep on pressing in time with the flashing sides or at a steady pace. While you do this the farmer will plant flowers, water flowers and do stuff with the big flower when this is done the big flower will talk to you. This is to allow you to get ready for the next bit. When your farmer is holding a leaf over its head, keep on pressing fast and if done right the farmer will catch a big cabbage. Well done you have completed the flower mini game., Make sure you have lots of Patapons with good equipment. This boss fight can be hard but if you have lots of Patapons it isn’t as hard, the boss does three attacks. A fire attack when it breaths in air. A head attack when it raises its head and a ground attack when it lies on the ground. When the boss is about to do an attack do the defend command then after that do the attack command. If you keep in fever most of the time and defend when it attacks and attack when it isn’t then you can beat it quite easy. When its eyes start to flash that means its going to die soon.,,, horse squad for the archer squad etc., Also you need to watch out for the zigotron army who are out in force to stop you. They are quite easy to defeat if you make sure your Patapons are up to the job and you have quite a lot of them. Make sure to remember all the tactics you have learned and use them in battle.,, You will see a zigotron that looks like a grim reaper. You don’t have to kill it. All you need to do is kill all the zigotron that are hiding in the bushes. The easiest way is to get fever going and keep on attacking. A big zigotron will come and attack in the middle of the battle but is easy to kill. After there all dead the grim reaper zigotron will retreat so all is left to do is go through the flagpole. If you fail the first time try again and you’re more likely to complete it.",
        "We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.\n",
        "  The properties of certain networks are determined by hidden variables that\nare not explicitly measured. The conditional probability (propagator) that a\nvertex with a given value of the hidden variable is connected to k of other\nvertices determines all measurable properties. We study hidden variable models\nand find an averaging approximation that enables us to obtain a general\nanalytical result for the propagator. Analytic results showing the validity of\nthe approximation are obtained. We apply hidden variable models to\nprotein-protein interaction networks (PINs) in which the hidden variable is the\nassociation free-energy, determined by distributions that depend on\nbiochemistry and evolution. We compute degree distributions as well as\nclustering coefficients of several PINs of different species; good agreement\nwith measured data is obtained. For the human interactome two different\nparameter sets give the same degree distributions, but the computed clustering\ncoefficients differ by a factor of about two. This shows that degree\ndistributions are not sufficient to determine the properties of PINs.\n",
        "  We report on the transport and magnetic properties of hybrid trilayers and\nbilayers that consist of low spin-polarized Ni80Fe20 exhibiting in-plane but no\nuniaxial anisotropy and low-Tc Nb. We reveal a magnetoresistance effect that is\npronounced. In our trilayers the magnetoresistance exhibits an increase of two\norders of magnitude when the superconducting state is reached: from the\nconventional normal-state values 0.6 % it goes up to 1000 % for temperatures\nbelow Tc. In contrast, in the bilayers the effect is only minor since from 3%\nin the normal state increases only to 70 % for temperatures below Tc.\nMagnetization data of both the longitudinal and transverse magnetic components\nare presented. Most importantly, we present data not only for the normal state\nof Nb but also in its superconducting state. Strikingly, these data show that\nbelow its Tc SC the Nb interlayer under the influence of the outer Ni80Fe20\nlayers attains a magnetization component transverse to the external field. By\ncomparing the transport and magnetization data we propose a candidate mechanism\nthat could motivate the pronounced magnetoresistance effect observed in the\ntrilayers. Adequate magnetostatic coupling of the outer Ni80Fe20 layers is\nmotivated by stray fields that emerge naturally in their whole surface due to\nthe multidomain magnetic structure that they attain near coercivity. Atomic\nforce microscopy is employed in order to examine the possibility that such\nmagnetostatic coupling could be promoted by interface roughness. Referring to\nthe bilayers, although out-of-plane rotation of the magnetization of the single\nNi80Fe20 layer is still observed, in these structures magnetostatic coupling\ndoes not occur due to the absence of a second Ni80Fe20 one so that the observed\nmagnetoresistance peaks are only modest.\n",
        "  I show that several observable properties of bursting neutron stars in metric\ntheories of gravity can be calculated using only conservation laws, Killing\nsymmetries, and the Einstein equivalence principle, without requiring the\nvalidity of the general relativistic field equations. I calculate, in\nparticular, the gravitational redshift of a surface atomic line, the touchdown\nluminosity of a radius-expansion burst, which is believed to be equal to the\nEddington critical luminosity, and the apparent surface area of a neutron star\nas measured during the cooling tails of bursts. I show that, for a general\nmetric theory of gravity, the apparent surface area of a neutron star depends\non the coordinate radius of the stellar surface and on its gravitational\nredshift in the exact same way as in general relativity. On the other hand, the\nEddington critical luminosity depends also on an additional parameter that\nmeasures the degree to which the general relativistic field equations are\nsatisfied. These results can be used in conjunction with current and future\nhigh-energy observations of bursting neutron stars to test general relativity\nin the strong-field regime.\n",
        "Avo or AVO may refer to:\n\nCompanies and organizations\n ÁVO, Hungarian secret police (1950–1956)\n Avo Photonics, optical electronics firm\n AVO Cigars, tobacco company founded by Avo Uvezian\n\nScience\n Alaska Volcano Observatory, a hazard monitoring program\n Amplitude versus offset, a concept used in reflection seismology\n Astrophysical Virtual Observatory, a European research project\n Avometer, a brand of multimeter that measures amps–volts–ohms\n\nOther\n Agent–verb–object, a sentence structure in linguistics\n Apprehended Violence Order, an injunction in Australia\n Avon Park Executive Airport (IATA code), in Florida, U.S.\n  of a Macanese pataca\n  of a Portuguese Timorese pataca\n \"AVO\", a song on Amor Vincit Omnia by Pure Reason Revolution\n A character from Fable\n Nom-de-guerre of Armenian-American revolutionary Monte Melkonian (1957–1993)\n Short for avocado, the fruit\n\nSee also \n Avos (disambiguation)\n Avo (name)\n\nde:AVO",
        " Meetings never run as efficiently and quickly as you would think. What you expect should take twenty minutes will invariably turn into forty. Don't schedule a parent meeting to occur fifteen minutes before the students come back from lunch.;\n, If other teachers are coming as well, make sure that they remember the time and location through email reminder or note. Have a plan to quickly locate and retrieve those staff members if they have forgotten.\n\n, That includes papers to grade, your water bill, cell phone, or laptop (no surfing the net at this time).\n\n, This includes work samples, behavior log, current report card or class average, and details of missing assignments if you plan to allow the child to make up missing work.\n\n, Many times there is something discussed that requires one of the people at the meeting to do some follow-up activity. Don't assume you'll remember what you discussed or promised to do later. Write it down. You may want to provide a small pad of paper and pen for parents too, as many of them don't bring anything to write with.\n\n, Try to seat yourself next to them rather than across from them. If there are multiple staff members attending, make sure that you don't physically seat yourselves together apart from the parent. Try for more of a circular seating pattern than a panel style arrangement. Parents are already nervous, and many feel intimidated when they are alone in a chair facing a line of teachers.\n\n, Hopefully you haven't gotten this far into the year without that knowledge, but be sure of it before you start. You'll also want to make sure you know the relationship of the visiting adult to the child in question. Don't assume it is a mom or dad. The visitor might actually be the child's aunt, grandfather, or even a foster parent. Find out who you are dealing with.\n\n, If there are multiple teachers present, introduce the parent to them. You can simply say, \"Okay, let's get started. This is Ms. Smith, Jimmy Smith's mom. She's here today to talk with us about Jimmy's work in our classes.\" As each teacher gets a turn to talk to the parent, he or she can introduce herself at that time.\n\n, Parents like to hear what their children are doing well. It should be specific and genuine, however, as parents are already wary and expectant of bad news. Even if the child is a holy terror, there is something good that can be said of them. \"Jimmy really has a lot of friends\" or \"Jimmy is always surprising us with his unique ideas.\"\n\n, You don't need to be mean, but you shouldn't sugar coat the truth either. If the student is failing in science, tell the parents why. Be prepared to show a print out or record of missing assignments, failed quizzes, or whatever the problem is.\n\n, For example, if the child isn't keeping up with workbook pages, bring a workbook to show the parents what the child should be doing. If the child isn't passing tests, bring the study guide you use to show parents what you are giving the students to prepare them for the tests. Tell the parents briefly about how your class works in respect to the area in which the child is struggling: how often you give homework, what you do to review for a test, how students are required to keep a notebook, or how tests always fall on Fridays.\n\n, Write any important information on the top of it, such as due date, or credit possible (50% for late work). Make any notes on the work about what resource the student should use (pages 3-9 of the social studies book), because if the work is already overdue, chances are the student won't remember how it was supposed to be done.\n\n, Encourage them to express their concerns. During the meeting, pause to ask them questions such as: Does this make sense? Do you find that this happens at home? What is the best way for me to get in touch with you if I need to let you know more about my concerns? What does your child say about my class? Does he seem to enjoy it? How has he performed in the past with math/etc.?\n\n, Education, like any other field, is riddled with acronyms that only make sense to other educators. Be plain with your communication. If you must use an acronym, be sure and explain it to the parent (IEP, SST, EIP, MO, ELL, LAB, etc.)\n\n, Refer to the notes you took during the meeting, for example: \"Okay Ms. Smith, so what I have understood today is that Ms. James and I are going to make sure that Jimmy is moved closer to the front of them room in our classes. Mr. Ellis is going to be sending some makeup work home in math, and Ms. Lewis has given you some makeup work today for social studies that you will make sure Jimmy has back by next Monday. Does this all sound correct?\" Be sure and ask at that point if the parent has any questions for anyone.\n\n, Express your knowledge that they have taken time out of their busy schedule, and how you wish that all other parents were as conscientious. Make sure that the parent has knowledge of how to contact you, and that know they are welcome to do so at any time. Shake hands with them, and lead them to the door. (Make sure they took anything with them that you gave them, such as the makeup work!)\n\n, If the student has improved, say so. If the problems are continuing, the parent needs to know.\n\n",
        " It's good to have both a general regional guide, as well as a more specific guide for your area. You can never have enough field guides - the more images you have the easier it will be to identify a particular tree. Field guides can be obtained for free at your local library. Used bookstores can be another good source, where guides can be bought at about half the price. Another great resource is a plant identification terminology book. This will help you understand the language in the guide. If you're just starting out, focus on using mostly photos of leaves and bark and flowers and fruit to identify trees. Then move on to using keys and drawings, which are more technical and precise. Various guides can also provide fascinating anecdotal information about different trees.;\n, Anything planted in a garden, lawn, roadside, park, is often an ornamental. Ornamentals are garden varieties of native species that are dissimilar enough to throw you off, and make pinpointing an exact species very difficult. If you want to learn your area ornamentals, there are many guides for the horticulturist about planting and growing trees that will have enough images that you might just find your tree. There are not that many different native trees in a specific area, and they are easy to learn. But if you include the city ornamentals, you're talking up to a thousand trees planted from all over the world.\n\n, If you have the time to sit at the base of a tree you want to learn, and pore over field guides - great. Identification takes a lot of patience and concentration. It may take at least 30 minutes of studying various guides before you've found it. If you cannot sit with the tree, take some photos and samples home with you.\n\n, Pick a tree that has at least one, if not two easily identifiable characteristics - such as a leaf or flower or fruit. Bark is not so easy to go by, nor buds, or scars, or growth habit. Leaves are the easiest to use. Start with an evergreen. There are very few native evergreen broad-leaved trees in the U.S. This magnolia with its large glossy evergreen leaves in the South is a dead giveaway:\n\n, The tulip tree with its huge vertical trunk and papery upright flowers, that remain all through the winter, is very common and easy to identify (the 'flowers' are the remains of the fruit axis, not actually blooms). It gives the tree a candelabra effect.\n\n,, A peach, a cherry, a plum, a hawthorn, a juneberry, a pear, are all in the same family, with edible fruit - the rose. Trees with pods are in the legume, (or bean) family, such as silk tree, mesquite and locust. There are not that many different tree families in any temperate region of the world - if you can group your trees in their respective families you'll be better able to understand them, and know their characteristics. Beech and oak and chestnut are all in the same family (the beech), and produce edible nuts. Cottonwood is really a gigantic member of the willow family, growing beside water and having deeply fissured bark, just like other willows.\n\n\nAnd the tulip tree is in the magnolia family - it has large showy flowers, just like magnolia, and look at a comparison of the fruit:\n\n, Hackberries are in the elm family, and both the seed and flesh of the prolific berries is edible - they're much like candy, and can persist on the tree deep into winter in some varieties and locales:\n\n\nOr the flaky peeling bark of this river birch:\n\n, The scarlet flowers stand out beautifully against the smooth gray bark. Red maple's twigs are also red. Its red keys typically don't last that far into winter:\n\n,, . . there are also old woody capsules from the previous year's fruit:\n\n,, . . and often holding on to their cones, such as the tiny 1/2\" cones on this hemlock:\n\n, There's an incredible amount of information about various tree species online, as well as lots of images - you should arrive at a dead certainty about the identification of your tree at this point. You will also find a wealth of anecdotal information, as well as what possible uses the tree has and any toxic or edible parts. Always cross-reference and verify from multiple sources the edibility of a certain tree - don't go by one opinion alone. That having been said, there are very few trees in the U.S. that are actually poisonous, such as buckeye and yew.\n\n, Ultimately you'll create your own database of area trees, and become an expert. It will become an excellent reference source for you year after year, and you will come to know your trees from all angles, in all habitats, in all seasons.\n\n, This is a great project for kids, and it spurs their interest in trees.",
        "We fixed minor typographical error in author's name and Section. 4. etc.\nOur policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry. ",
        "I have problems understanding the motivation of this paper. The authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time, but didn't demonstrate convincingly that images help (not to mention the setup is a bit strange when there are no images at test time). What I see are only speculative comments: \"we observed some gains, so these should come from our image models\". The qualitative analysis doesn't convince me that the models have learned latent representations; I am guessing the gains are due to less overfitting because of the participation of images during training. \n\nThe dataset is too small to experiment with NMT. I'm not sure if it's fair to compare their models with NMT and VNMT given the following description in Section 4.1 \"VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT\". There should be more explanation on this.\n\nBesides, I have problems with the presentation of this paper.\n(a) There are many symbols being used unnecessary. For example: f & g are used for x (source) and y (target) in Section 3.1. \n(b) The ' symbol is not being used in a consistent manner, making it sometimes hard to follow the paper. For example, in section 3.1.2, there are references about h'_\\pi obtained from Eq. (3) which is about h_\\pi (yes, I understand what the authors mean, but there can be better ways to present that).\n(c) I'm not sure if it's correct in Section 3.2.2 h'_z is computed from \\mu and \\sigma. So how \\mu' and \\sigma' are being used ?\n(d) G+O-AVG should be something like G+O_{AVG}. The minus sign makes it looks like there's an ablation test there. Similarly for other symbols.\n\nOther things: no explanations for Figure 2 & 3. There's a missing \\pi symbol in Appendix A before the KL derivation.\n",
        "He pretty much invented cubism  (along with Georges  Braque) and brought it to the forefront of the art world. He was also hugely influential in the worlds of surrealism and symbolism, and continued to innovate throughout his life, in various styles and media.  The argument could be made that he was the most influential artist of the past hundred years. \n\nedit - ELI5 definitions pulled from around the web to break it down further;\n\nCubism - Cubism is an early-20th-century art movement that revolutionized European painting and sculpture, and inspired related movements in music, literature and architecture. In Cubist artwork, objects are analyzed, broken up and reassembled in an abstracted form—instead of depicting objects from one viewpoint, the artist depicts the subject from a multitude of viewpoints to represent the subject in a greater context. Cubism has been considered the most influential art movement of the 20th century. \n\nSurrealism - Surrealism is a cultural movement dedicated to expressing the imagination as revealed in dreams, free of the conscious control of reason and convention. From the 1920s onward, the movement spread around the globe, eventually affecting the visual arts, literature, film, and music of many countries and languages, as well as political thought and practice, philosophy, and social theory. \n\nSymbolism - Symbolism was a late nineteenth-century art movement of French, Russian and Belgian origin in poetry and other arts. Symbolist painters believed that art should reflect an emotion or idea rather than represent the natural world in the objective, quasi-scientific manner. \n\nedit 2  - I was too simple in the my original response. Tried to clean it up a bit.",
        "The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns about the computational speed of the approach as well as its advantage over existing methods for some textures, reviewers are excited by the ability of this work to produce structured texture that requires long-range interactions. Overall, the work has contributions that are worth presenting at ICLR.",
        "Paper Summary\nThis paper proposes a variant of dropout, applicable to RNNs, in which the state\nof a unit is randomly retained, as opposed to being set to zero. This provides\nnoise which gives the regularization effect, but also prevents loss of\ninformation over time, in fact making it easier to send gradients back because\nthey can flow right through the identity connections without attenuation.\nExperiments show that this model works quite well. It is still worse that\nvariational dropout on Penn Tree bank language modeling task, but given the\nsimplicity of the idea it is likely to become widely useful.\n\nStrengths\n- Simple idea that works well.\n- Detailed experiments help understand the effects of the zoneout probabilities\n  and validate its applicability to different tasks/domains.\n\nWeaknesses\n- Does not beat variational dropout (but maybe better hyper-parameter tuning\n  will help).\n\nQuality\nThe experimental design and writeup is high quality.\n\nClarity\nThe paper clear and well written, experimental details seem adequate.\n\nOriginality\nThe proposed idea is novel.\n\nSignificance\nThis paper will be of interest to anyone working with RNNs (which is a large\ngroup of people!).\n\nMinor suggestion-\n- As the authors mention - Zoneout has two things working for it - the noise and\n  the ability to pass gradients back without decay. It might help to tease apart\nthe contribution from these two factors. For example, if we use a fixed\nmask over the unrolled network (different at each time step) instead of resampling\nit again for every training case, it would tell us how much help comes from the\nidentity connections alone.",
        "  Redundancy of experimental data is the basic statistic from which the\ncomplexity of a natural phenomenon and the proper number of experiments needed\nfor its exploration can be estimated. The redundancy is expressed by the\nentropy of information pertaining to the probability density function of\nexperimental variables. Since the calculation of entropy is inconvenient due to\nintegration over a range of variables, an approximate expression for redundancy\nis derived that includes only a sum over the set of experimental data about\nthese variables. The approximation makes feasible an efficient estimation of\nthe redundancy of data along with the related experimental information and\ninformation cost function. From the experimental information the complexity of\nthe phenomenon can be simply estimated, while the proper number of experiments\nneeded for its exploration can be determined from the minimum of the cost\nfunction. The performance of the approximate estimation of these statistics is\ndemonstrated on two-dimensional normally distributed random data.\n",
        "This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.\n\nEffective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.\n\nOne minor red flag: \n- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.\n\nTwo writing comments:\n- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.\n- Saying that \"LSTM auto-encoders are more effective at encoding word order than word content\" doesn't really make sense. These two quantities aren't comparable. ",
        "This paper proposed to use GAN for encrypted communications.\n\nIn section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.\n\nIn section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.",
        "  Following the early Swift X-ray observations of the latest outburst of the\nrecurrent nova RS Ophiuchi in February 2006 (Paper I), we present new 1D\nhydrodynamical models of the system which take into account all three phases of\nthe remnant evolution. The models suggest a novel way of modelling the system\nby treating the outburst as a sudden increase then decrease in wind mass-loss\nrate and velocity. The differences between this wind model and previous\nPrimakoff-type simulations are described. A more complex structure, even in 1D,\nis revealed through the presence of both forward and reverse shocks, with a\nseparating contact discontinuity. The effects of radiative cooling are\ninvestigated and key outburst parameters such as mass-loss rate, ejecta\nvelocity and mass are varied. The shock velocities as a function of time are\ncompared to the ones derived in Paper I. We show how the manner in which the\nmatter is ejected controls the evolution of the shock and that for a\nwell-cooled remnant, the shock deceleration rate depends on the amount of\nenergy that is radiated away.\n",
        "This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\". The paper is well written including some rather poetic language [*].\n\nThe heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).\n\n[*] this reviewer's favourite quotes:\n\"Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it’s undesirable.\"\n\"The child can learn to adjust its behaviour without actually having to stab someone.\"\n\"... the catastrophe lurking just past the optimal shave.\"",
        "Cyanopepla chelidon is a moth of the subfamily Arctiinae. It was described by Herbert Druce in 1893. It is found in Colombia and the Upper Amazon region.\n\nReferences\n\nCyanopepla\nMoths described in 1893",
        "Description:\n\nThis paper presents a reinforcement learning architecture where, based on \"natural-language\" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks.\n\nThe subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an \"analogy-making\" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs).\n\nThe meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not.\n\nTraining involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen.\n\nThe system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types.\nIt is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization.\n\n\nEvaluation:\n\nThe proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the \"right\" way to do it.\n\nI do not feel the grid world here really represents a \"large-scale task\": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work.\n\nMoreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works.\n",
        " To set up a server, you'll need some files from the Minecraft website:\n\n\nVisit https://minecraft.net/en/download/server in Safari.\nDownload the JAR file for the server program.;\n, This will be the folder for your server program. You can place it somewhere easy to access, such as on your desktop. The server can be labeled anything, such as \"Minecraft Server.\"\n\n, When you run the file, the folder will fill with the various configuration files for the new server. For now, just drag the downloaded server JAR file to the new folder.\n\n, You'll want to remove the version numbers from the end of the file. This will make it easier to run commands for the server later.\n\n, You can find this in your Applications folder. From the desktop, click the Go menu and select \"Applications.\"\n\n, This will switch the new document to a plain text document.\n\n, This set of commands will start the server. You can replace -Xms1G -Xmx1G with -Xms2G -Xmx2G to increase the RAM for the server from 1 GB to 2 GB:#!/bin/bash\ncd \"$(dirname \"$0\")\"\nexec java -Xms1G -Xmx1G -jar minecraft_server.jar\n\n\n\n\n, Select \"Save\" from the TextEdit menu and save the file in the same folder as the server file you downloaded from the Minecraft website.\n\n, You can find this in the Utilities folder. From the desktop, click the Go menu and select \"Utilities.\"\n\n, Make sure to include a single space after a+x.\n\n, This will add the path to that file at the end of the chmod a+x command.\n\n, This will change the permissions of the start.command file, allowing it to start your server.\n\n, This will start your server. You'll see some error messages, but this is only for the first time you run the sever. The server will generate several files in the folder its in.\n\n\nThe server will stop automatically after running the first time.\n\n, You'll need to make a small change to this file to proceed.\n\n, Save the change to the file and close it.\n\n, This will start the server back up and show the server command line. Additional files will be downloaded and the server's world will be generated, which may take a few moments.\n\n, Replace <username> with your Minecraft username. This will give you admin privileges when you connect to the server from your Minecraft account.\n\n, Double-click this file and select TextEdit when prompted for a program to open it. You can change the values of these entries to change how the server works, but be aware that incorrect entries may cause the server to malfunction. You'll need to restart the server after making any changes.The gamemode entry lets you choose from 0 - Survival, 1 - Creative, 2 - Adventure, 3 - Spectator.\nYou can change the level-seed entry to enter any seed you'd like to use.\n\n, When you're connecting to the server from other computers on the same network, you'll need to know the server's IP address.\n\n\nOn the Mac running the server, click the Apple menu, select \"System Preferences,\" then select \"Network.\"\nSelect your network connection and look for the \"IP Address\" entry. Make note of this address.\n\n, You don't need to forward any ports or change other advanced settings if the other computer is on the same local network as the server computer. If your server computer is powerful enough, you can run Minecraft on it at the same time, but this isn't recommended for most computers.\n\n\nIf you want to have your friends join your server over the internet, see the next section.\n\n, This will begin scanning for available games. There's a good chance that you won't see your server available, even though it is.\n\n, This will open a window allowing you to enter an address.\n\n, After entering the address, you will connect directly to it and the game will load. If you cannot connect, make sure both computers are on the same network.\n\n\nMultiple computers can connect to the same server using this address, as long as they are all on the same local network.\nIf you are playing on the same computer as the server, enter localhost instead of the server IP address.\n\n, You'll need this address in order to properly forward ports so that others can connect to your server.\n\n\nClick the Apple menu on the server Mac and select \"System Preferences.\"\nClick the \"Network\" option and then select your active connection.\nNote the \"IP Address\" line.\n\n, In order for others to connect to your server over the internet, you'll need to configure your router to allow incoming connections. To do this, you'll have to access your router's settings. If you are using a router like Netgear or Belkin, you can access your router's configuration page from your web browser. If you're using an Apple AirPort router, you can use the AirPort Utility from your Utilities folder.\n\n\nSee Access a Router for detailed information on opening your router's configuration page.\n\n, The location of these settings will vary from router to router. Generally, you'll find them in the WAN or the Advanced section. It may be labeled \"Applications and Gaming\" or \"Virtual Servers.\"\n\n, Enter your server's IP address into the IP address field, then enter 25565 into the Port field. Make sure to select \"TCP\" as the protocol. Save your changes when complete.\n\n, Your friends will need to enter your public IP address in order to connect to your Minecraft server. The quickest way to determine your public IP address is to open Google on the server computer's web browser and type \"my IP.\" Your public IP will be displayed at the top of the search results.\n\n, Now that the server is accessible from the internet, your friends can connect by opening the Multiplayer menu in Minecraft, clicking \"Direct Connect,\" and then entering the server address.\n\n, Whenever your server computer restarts, it will get a new local IP address from your router. When this happens, you'll need to change the port forwarding rules to reflect the new address, or no one will be able to connect from the internet. Also, your internet service provider may occasionally change your public IP address, which will need to be entered whenever your friends connect.\n\n",
        " Visit http://www.ubuntu.com and download Ubuntu 10.04 LTS server. Burn the downloaded ISO to CD and install on your spare PC. During the install select LAMP and OpenSSH.\n,\n\nsudo apt-get install subversion libapache2-svn\n\n,\n\nsudo svnadmin create /svn\n\nAlternately, if you want to store the subversion repository on another drive, you could enter something like /media/disk2/svn instead of /svn\n,\n\nsudo vim /etc/apache2/mods-enabled/dav_svn.conf\n\nThen paste the following and carefully replace all the elements starting YOUR_ with appropriate properties for your active directory.\n\n\n\n\n<location svn>\n  DAV svn\n  SVNPath /svn\n \n  # Use LDAP auth against an active directory\n  AuthName \"Enter your domain username and password\"\n  AuthType Basic\n  AuthBasicProvider ldap\n  AuthLDAPBindDN \"CN=YOUR_USER,OU=-- System Accounts,OU=YOUR_OU,DC=YOUR_DC,DC=YOUR_DC\"\n  AuthLDAPBindPassword \"YOUR_PASSWORD\"\n  AuthLDAPURL \"ldap://YOUR_DOMAIN_CONTROLLER:389/ou=YOUR_OU,dc=YOUR_DC,dc=YOUR_DC?samAccountName?\"\n  #require valid-user\n  AuthLDAPGroupAttributeIsDN on\n  require ldap-group CN=Software Development,OU=YOUR_OU,DC=YOUR_DC,DC=YOUR_DC\n</location>\n\n\n\n\n,\n\nsudo a2enmod authnz_ldap\n\n,\n\nsudo addgroup svnusers\nsudo adduser administrator svnusers\nsudo adduser www-data svnusers\nsudo chgrp -R svnusers /svn\nsudo chmod -R g+w /svn\n\n,\n\nsudo /etc/init.d/apache2 restart\n\n, Browse to http://YOUR_SERVER/svn A successful install should prompt for your domain credentials and then show:- svn - Revision 0: / Powered by Subversion version 1.6.6 (r40053).,\n\nsudo apt-get install trac libapache2-mod-python libapache2-mod-python-doc\n\n\ncd /\nsudo trac-admin trac initenv\n\nAnswer the following questions:\n\nProject Name > YOUR_PROJECT\n\nDatabase connection string > <return></return>\n\nRepository type > <return></return>\n\nPath to repository > /svn\n,\n  #require valid-user\n  AuthLDAPGroupAttributeIsDN on\n  require ldap-group CN=Software Development,OU=YOUR_OU,DC=YOUR_DC,DC=YOUR_DC\n</location>\n\n\n\n\n,\n\nsudo chgrp -R svnusers /trac/\nsudo chmod -R g+w /trac/\n\n,\n\ntrac-admin /trac permission add administrator TRAC_ADMIN\n\n,, Browse to http://YOUR_SERVER/trac, enter your your domain username and password., See the related article at the bottom for how to mount a windows share on an Ubuntu server., This script will dump the entire repository to the backup location as a single dump file.\n\n\nsudo su (switch to root)\n\n\nmkdir /root/scripts && cd /root/scripts (make a directory for scripts and change to it)\n\n\nvi svn-full-backup.sh (edit the backup script and append the following)\n\n\n\n\n\n\n\n#! /bin/sh\n# Dump the entire svn repository to /tmp\nsvnadmin dump /svn > /tmp/svn-full-backup.dump\n# Remove the previous backup\nrm -f /mnt/backup/svn-full-backup.dump\n# Copy the new backup file to the backup location\ncp /tmp/svn-full-backup.dump /mnt/backup/svn-full-backup.dump\n\n\n\n\n\n\"Write and quit\" (use :wq as described earlier)\n\nchmod +x svn-full-backup.sh (Make the script executable)\n\n\n./svn-full-backup.sh (Initiate the backup operation)\n\n\n, This script will dump all revisions since the last incremental backup to another file. All files created specify the range of revisions that have been saved.\n\n\nvi svn-full-backup.sh (edit the incremental backup script and append the following)\n\n\n\n\n\n\n\n#!/usr/bin/perl\n \nuse strict;\nuse warnings;\n \nmy $repo = '/svn';\nmy $local_dir = '/tmp';\nmy $savedir = '/mnt/backup/Incremental';\n \nmy $last_saved_file = $savedir.'/last_saved.txt';\n \n# read the last saved revision from the file\nopen(LAST_SAVED, '<', $last_saved_file);\nmy $last_saved = <LAST_SAVED>;\nchomp $last_saved;\nclose(LAST_SAVED);\n \n# get informed of the current last revision (head)\nmy $head = `svnlook youngest $repo`;\nchomp $head;\n \n# of course, if the head is not younger than the last saved revision\n# it's useless to go on backuping\nif ($last_saved == $head) {\n    exit();\n}\n \n# if the last saved is 1000 and the head is 1023, we want the backup\n# from 1001 to 1023\nmy $from = $last_saved + 1;\nmy $to = $head;\n \n# the backup filename looks like svn-01001_01023.svndump\nmy $dumpfile = sprintf(\n    '/svn-%05u_%05u.svndump',\n    $from,\n    $to\n);\n \nmy $local_dump_file = $local_dir.$dumpfile;\n \nmy $command = sprintf(\n    'svnadmin dump -q -r%u:%u --incremental %s > %s',\n    $from,\n    $to,\n    $repo,\n    $local_dump_file\n);\n \nsystem($command);\n \nif (grep /^Revision-number: $to/, `grep --text ^Revision-number: $local_dump_file`) {\n    open(LAST_SAVED, '>', $last_saved_file);\n    print LAST_SAVED $to, \"\\n\";\n    close(LAST_SAVED);\n \n    # here we compress the dump file\n    system('gzip '.$local_dump_file);\n \n    # let's add the md5sum of the file to MD5SUMS file storing md5sums of\n    # all Subversion backups\n    chdir($local_dir);\n    use File::Basename;\n    system('md5sum '.basename($local_dump_file).'.gz >> '.$savedir.'/MD5SUMS');\n}\n \nmy $mv_command = sprintf(\n    'mv %s %s',\n    $local_dump_file.'.gz',\n    $savedir.$dumpfile.'.gz'\n);\n \nsystem($mv_command);\n\n\n\n\n\n\"Write and quit\" (:wq as above)\n\nchmod +x svn-inc-backup.sh (Make the script executable)\n\n\n./svn-inc-backup.sh (Initiate the incremental backup operation)\n\n\n, Edit the cron table to schedule the two scripts to run. The incremental backup is run every night at 1 AM. The full backup is run on the 1st day of the month at 2 AM.\n\ncrontab -e\n\n\n\n\n\n\n0 1 * * * /root/scripts/svn-inc-backup.sh\n0 2 1 * * /root/scripts/svn-full-backup.sh\n\n\n\n\n\nExit the editor using CTRL-X\n\n, On another machine, perform the same subversion setup. Run the following command to import the dump file back into your empty repository.\n\nsvnadmin load /svn < /mnt/backup/svn-full-backup.dump\n\n",
        "  We present spectroscopic observations of a sample of 72 emission-line\nobjects, including mainly HII regions, in the spiral galaxy M 33. Spectra were\nobtained with the multi-object, wide field spectrograph AF2/WYFFOS at the 4.2m\nWHT telescope. Line intensities, extinction, and electron density were\ndetermined for the whole sample of objects. The aim of the present work was to\nderive chemical and physical parameters of a set of HII regions, and from them\nthe metallicity gradient. Electron temperatures and chemical abundances were\nderived for the 14 HII regions where both [OII] and [OIII] emission line fluxes\nwere measured, including the electron temperature sensitive emission line\n[OIII] 436.3 nm and in a few cases [NII] 575.5 nm. The ionization correction\nfactor (ICF) method was used to derive the total chemical abundances. The\npresence of abundance gradients was inferred from the radial behaviour of\nseveral emission-line ratios, and accurately measured from chemical abundances\ndirectly derived in 14 HII regions. The oxygen abundances of our HII regions,\nlocated in the radial region from ~2 to ~7.2 kpc, gave an oxygen gradient\n-0.054+/-0.011 dex/kpc The overall oxygen gradient for M 33 obtained using ours\nand previous oxygen determinations in a large number of HII regions with direct\nelectron temperature determination as well as abundance in young stars\npresented a two slope shape: -0.19 dex/kpc for the central regions (R<3kpc),\nand -0.038dex/kpc for the outer regions (R>=3kpc).\n",
        "Yes. There certainly is. I'm sure you have heard about black holes. They form when there is such a huge amount of mass enclosed in a certain volume of space that it causes a rip in the fabric of space-time.\n\nNow, mass is really just a \"form\" of energy. Remember the formula E=mc^{2}, which tells us exactly this. every photon has a certain amount of energy, related to its frequency. So hypothetically, if you squeeze enough photons into a box, the energy density could become so high in that region that a black hole would form.\n\nFor anyone who knows of the Schwartzchild radius formula, you can literally interchange M with E/c^{2} and replace the E with nhv, where v is the frequency of a photon, and n is the number of photons. This simple formula assumes that all the photons have the same frequency. But it tells you how many photons you need to squeeze into a certain volume of space for a black hole to form.\n\nTo respond to turtleintegral's answer: while it is true that the Pauli Principle forbids electrons from sharing states - your answer doesn't really make sense. Even in the most simplistic quantum mechanical model, the particle-in-a-box, there are still an infinite number of energy levels! So true, the electrons would just stack higher and higher up, so as to avoid sharing the same state, but by this logic, there is no limit to the number of electrons that can fit into the box. They would just all occupy different and higher and higher possible states. The distinction between fermions and bosons plays no role here.",
        " Every school's ManageBac website is different. Ask your teacher or principal for the website.\n\n\n\n ;\n, Enter your email address and password. You should have created your password through the email that was sent to you. If you want your account to be remembered for 12 hours, there is a check-box for that. Checking the check-box is not recommended for public computers.\n\n\n\n\n\n\nForgot your password? No problem! You could press the Forgot your Password? An email would be sent to your email address. You can then change your password.\n\n, It will be on the top which gives you quick access to five pages:\n\n\nDashboard - This is your homepage. You can also view your upcoming events and deadlines.\nProfile - This profile that others can view. You can also view your scores and marks, portfolio, and other information.\nIB Manager - This is where you organise your IB activities.\nClasses - Join classes and view assessments.\nGroups - Join groups to do more activities.\n\n, There is a help button to the right of the navigation bar under the logout button. Clicking it will open a new tab in your browser where you can look for help.\n\n, It displays your full name, a message box, and a logout button.\n\n\nFull Name - You can change your full name. Clicking this will link you to changing your email address and password for ManageBac. You could also set some notifications.\nMessage Box - Teachers can notify students of new tasks or messages by sending messages.\nLogout - A simple click on this button would log you out of your account on your current device.\n\n, Your dashboard is your homepage where you can view your next 2 weeks of activities, upcoming deadlines and events, activities, and classes and groups.\n\n, To do this, place your mouse over the i icon and wait for the information to load. You can view the date, time, files, class, and description of the deadline or event.\n\n, To the right of the screen there will be a student guide, my classes, and my groups.\n\n\n\n\n\n\nStudent Guide - This is the same thing as the Help button.\nMy Classes - You can expand this to reveal all the classes that you participate.\nMy Groups - You can expand this to reveal all the groups that you participate.\n\n, Scroll down the page to see your top Community&Service or Service as Action activities. You can read the description of your activity.\n\n, In your profile, you can view your academic progress, portfolio, reflections, and reports. You can also view your parent, homeroom, and MYP adviser information.\n\n\nAcademic Progress - Here you can view your personal information, scores for assessments, MYP progress, and subject units. You can also see if you have submitted your work or not.\nPortfolio - Your portfolio is divided into multiple sections that cover all your subjects ranging from Community Project homework to Physical and Health Education homework. You can also upload files, photos and link videos and websites. You can even write a journal!\nReflections - You can use this page to set your term/semester goal.\nReports - You can view your academic reports here. Note!: If it is blank throughout your academic year, it means that your school didn't purchase the ManageBac Report Card system.\n\n, You can change your profile by clicking the pencil button at the top right. When editing your profile, you can edit your gender, date of birth, email address, contact information (e.g. city and house address). If you prefer to be called a name, you could put that in the Preferred Name box or if you have another name, you can put that in the Other Name box. If you are more comfortable to view webpages in Spanish or French, you can change the UI (user interface) language as well. Unfortunately, there are only 3 UI languages available. You also have the choice to edit your profile picture which anybody can view, so be sure not to upload any inappropriate pictures. You can also always delete your uploaded picture to restore the default picture. There are family details at the bottom of the page but it is not for you to change. Once you're done editing your profile, click Save Changes at the bottom.\n\n, Go to the academics profile tab. Expand the subject you want to view. Be sure that you are on the tasks tab, not the units tab. Each task and assessment is given its own colour (but not always) in the column graphs. Click the task label next to the column graph to hide specific columns. Enlarge a specific task tab to get more details such as date, information, and scores. You can also see the teacher's comment for you. Place your mouse over the i button next to your score to view the rubric for the score.\n\n, A drop-down box showing the current term or semester of the scores will be above the subject tabs. You could select what year and semester that you want to view your scores. After selecting a term or semester, the page will reload and you will see past results when expanding the lesson tabs. You can only see the scores for the classes that aren't archived and the classes that you join.\n\n, Scroll down to the bottom of the page and there you will see your progress of your community project, service as action, and personal project.\n\n, Go to the top of the page and click the Portfolio tab. As said before, the portfolio is divided into categories. So if you submitted a task that is in English Language and Literature and another for Second-Language Language and Literature, then the work that you submitted for those two subjects will be grouped together into the Language and Literature category. Don't forget that you could add files and photos and link videos and websites. Also don't forget that you could write your own journal to keep track of your work and to keep your thoughts and ideas.\n\n, Click the reflections tab and type in your goals and reflections. Make your goals that are smart: specific, measurable, attainable, relevant, time-bound.\n\n, Your reports can be viewed in the reports tab. Your reports will talk about your overall grades for each subject. There will also be comments from teachers. Don't forget that you will also be judged on your behaviour., It is divided into 9 parts:\n\n\nOverview - This is mainly the 6 latest activities on the ManageBac IB Manager. You post a new message or view the full calendar.\nPlan - You can view your personal information and subject levels for the diploma programme here although it is not needed since you are only using the MYP. Here you can change your full name as well as date of birth and gender.\nSA - In this page, you can view your SA worksheet, documents, and even post notes or interview your supervisor.\nPersonal Project - This is the page to do your personal project stuff and whatnot.\nCommunity Project - Here is where you work on your community project stuff.\nMessages - To view all messages from your teachers and classmates, go here.\nCalendar - You can view all events and activities that are related to the school or your year level.\nFiles - View all files or upload them here.\nMembers - View all members of your year level grouped into their homeroom advisers.\n\n, In the SA and Community Project worksheets, you are able to view quick start guides to tell you more on certain areas of the subject. They are to the right of the worksheets.\n\n\n\n\n\n, This will help you organise your bibliographies. The IB system wants students to cite their sources using the Harvard method. Links to these generators could be found on Personal and Community Project worksheets.\n\n\n\n\n\n, Click the button at the top right of the page that says Add SA Activity. After that, you will have to set your activity name, location, start date and end date, your supervisor, and your targeted learning outcomes. If you selected an activity from a group, the supervisor information will fill itself automatically. Then save the SA Activity by going to the bottom of the page and clicking Add SA activity.\n\n\n\n\n\n, At the top of your worksheet, there will be three buttons: timeline, outcomes, and hours. You can click each of them to view statistics and progress., Go to the section below the statistics to upload documents. Click Upload SA Document and then click Browse to look for the document that you want to upload. Once you've found the document to upload, click Upload Document which is in a green background. Accidentally uploaded the wrong document? Chill. There is a trash can button for you to delete your uploaded document., Scroll down to the next section and type in your note. An email will alert your supervisor of your note.\n\n, The page will explain what a personal project is. It will also guide you in your personal project so don't worry. If you can't find help from the page, you can find it from your supervisor.\n\n, There is a button in red titled Edit Personal Project Proposal on the top right of the worksheet. Clicking it will automatically bring you to editing your personal project proposal. You will need to choose your topic, set your goal, select your global context(s), make your inquiry question, set your criteria, and select your project supervisor. Click Save Project after you've finished with editing your personal project proposal., To the right of the worksheet, there will be two tabs: Worksheet and Process Journal. Click Process Journal to add reflections and evidence. You are able to add journals, websites, YouTube videos, photos, and files. A process journal guide will be there at your disposal to read at the bottom of the page., You need to choose a topic for your community project. The community project worksheet is similar to the SA worksheet. You will need to set your topic, your target, supervisor, etcetera., To-do notes help you to keep track and remind you of different activities you do. You can add to-do notes below the community project proposal., Doing this is similar to the one in the SA worksheet.\n\n\n\nNote: If your supervisor has marked your community project complete, then you won't be able to edit your proposal, add to-do notes, upload files, and post messages.\n\n, There are three tabs that you can see to the right. There is the worksheet, process journal, and academic honesty. Click process journal then click Add New Entry at the top. This will bring you to a page similar to the process journal in the SA worksheet. As said before, you can add a journal (like a diary), a website, a YouTube video, photos, and files. Photos and files can give evidence.\n\n\n, Click the Process Journal tab and you will see a button that says Export to PDF. This will export all your process journal entries into a PDF and will automatically upload the PDF into your project files in your worksheet., You can view the academic honesty by clicking the Academic Honesty tab., Click on Classes in the navigation bar. To the right of the page, you will be able to filter classes. Choose the classes that you want. For example, select the MYP program. If you are grade 9, check grade 9 only. Then select the subjects that you want to filter. Click Filter below and the classes will pop up. Find the right classes for you and click Join Class. By default, a maximum of 10 classes will appear in a drop-down box when you hover the mouse on Classes, allowing you quick access to those classes. But of course, you can join more than 10 classes, it actually depends on your grade level and your school.\n\n, After clicking Classes in the navigation bar, the page will be able to show your classes. Just simple click Show my classes. After that, you could click leave class.\n\n, Select one class. This will bring you to the class overview. Each class will be divided into 6 parts:\n\nOverview - This page is the class homepage and shows the top 6 latest activities in the class including messages and tasks.\nTasks - You can view all your completed and upcoming tasks, scores, etcetera here.\nMessages - View all recent messages of the class here.\nCalendar - View deadlines and upcoming events class-related here.\nFiles - You can view all files related to the class here.\nStudents - All students in the class are listed here.\n\n, Select a task. Select the Dropbox tab. Click Browse to search your computer for the document or file that you want to upload. Remember, files should be less than 500 Megabytes. You can upload multiple files by clicking Add another file. If you want to upload a new file or document after the deadline, it will be marked late. Click Upload Files and the files would be uploaded.\n\n\nYou can delete submitted work if it is before the due date. If the due date is passed, you can delete your submitted work.\n\n, Assessment criteria help guide you and tell you what score you will get for your work and effort. You can view these by going to the task in question and scrolling down to see the assessments. Look for the criteria for how to earn the maximum score so that you could try your best with it.\n\n, You and your teacher can annotate the files that you upload. Annotation only works on Word Documents and won't work on Powerpoints, Excel Spreadsheets, etcetera. Your teacher might give you comments in the annotations so be sure to check for comments. You can see if your teacher has annotated the document by hovering on the annotated symbol.\n\n\n\n\n\n, TurnItIn is a program that compares uploaded work to other students' work and the information found on the internet. TurnItIn is a great way for teachers to check on the academic honesty of students of not plagiarizing other people's work. The TurnItIn similarity index will show the percentage of similarity of your work compared to others' work.\n\n, Groups could be afterschool activities, home-groups, year level groups, and etcetera. Click Groups in the navigation bar. Joining groups and leaving groups is similar to doing it with classes. You can also see your own groups and the groups that you don't join. Unlike classes, there is no filter in groups. You can only manually search for the groups or type in the name of the group.\n\n\nIf there is a locked sign next to do group, then it means that students can't join or leave the group by themselves. To join these groups, you have to notify the advisor so that the advisor can add you into the group.\n\n\n\n\n\n\n\n, Each group is divided into 5 sections:\n\nOverview - This page shows the latest 6 activities. You can also add new events, files, and messages.\nMessages - This page shows all messages along with the files that go along with it.\nCalendar - This page shows all events and deadlines in calendar form for the specific group.\nFiles - This page shows all files. You can upload files, and find them through filtering categories, authors, name, file size, and modification date.\nMembers - This page shows all members. You can add members into the group if it is allowed and you could also export the files into excel.\n\n, You can send messages and comments back and forth so that there could be communication and clarification. Scroll down to learn more about posting messages and comments., If you want to view the full calendar for all events and deadlines, go to the Dashboard first. Click Full calendar. This will bring you to the full calendar where you can view every month or every week individually.\n\n\n\n\n\n, All events and deadlines would have a category. There are tests, presentations, tasks, creative writing, projects, and many more. You can click which category or categories that you want to filter on the top right side of the worksheet.\n\n, You can view your calendar by month or by week. To switch between viewing these two aspects, there are two buttons labeled Month and Week. You can view each individual half-hour in week mode but only individual days in month mode., To the top left side of the calendar, there will be two arrows. Click the left arrow to go to the month or week before and the right arrow to go to the month or week after., Next to the two arrows, there will be a button labeled Today. If you are viewing another week or month that does not include today's date, then the button will be available to click. Clicking the button will bring you to the month or week that includes today. Today's day will be highlighted with a pale orange colour., You can post messages and comments in classes and groups. To get started with posting messages and comments, go to the class or group that you want to start with and go to Messages. Here you can view all newer messages.\n\n, A Post New Message button will be on the top right corner of the worksheet. Click this button to post a message. You will need to do these things to proceed to posting a message:\n\n\n\n\n\n\n\nEnter the Subject - This will like the headline of your message so make it simple such as \"Help with Math HW\" or \"Reason for Late Submission\".\nFill in the Message - Under message subject, you will need to type in your message that explains your problem or whatsoever.\nNotify the Class or Group via Email - A check-box will be to the right of the message subject and checking it would mean that you want to notify the whole class (students and teachers) via email.\nAttach Files - Click Browse button to browse through your computer for the files or documents that you want to attach. Files should be less than 500 Megabytes. You can add multiple files by clicking Add another file.\nPost the Message - Once you're done with typing up your message and attaching files, you can click Post Message to post the message to the rest of the class.\n\n, If you accidentally posted a message, you can delete it by viewing the message board in the class or group and placing your mouse over the message that you posted. To the left of the message, there will an Edit button and a rubbish bin. Click the rubbish bin to delete your message. Click Edit to edit your message.\n\n\n\n\n\n, Comments are made to messages. They are much simpler to handle but you can only add text or links with comments. You cannot attach files. To post comments, you first have to go to the message you want to comment on. Scroll down the message and you will be able to add a comment. Once you're finished with typing the comment, you should click Add comment only once. If you click it 5 times, chances are that 5 comments will appear. Also, ManageBac will notify the class or group of the new comment. So posting 5 comments will send 5 messages.\n\n, You can edit and delete comments in the same way as you can edit and delete messages except that you have to go to the message itself to delete the comments itself., Click your name at the top of the page. You will be able to change your email address and also your password. To change your password, type in your new password then confirm it by typing it again.\n\n, To the right of the worksheet, you will be able to see two tabs: General and Notifications. Click Notifications. Now in this page, you can select whether or not you want to receive message notifications for specific things such as class files, class messages, and etcetera. You can also select whether or not you want to receive an email concerning the specific thing. If you are confused as to what a specific thing is, you can place your mouse over the question mark that is next to the specific thing to find out what it is. Click Save Changes after you're done.\n\n, For example, you don't need email notifications if you constantly check ManageBac. Some notifications might just annoy you in email and then you would consider it spam, so to turn it off, turn off the notification settings., The message box is at the top right corner of the ManageBac page. If you get a new message, a little number will show on the top right corner of the message box. After clicking it, a dialogue box will appear which will show you all your message notifications.\n\n, Check-boxes will be to the left of each message. You can easily select all messages by clicking All at the top of the message box. Just as you can easily select all messages, you are also able to select no messages by just clicking None.\n\n\n\n\n\n\nNote: The selection only applies to the current message box page you are viewing.\n\n, Deleting messages lets you to clear some space in your message box. You have to first select the message(s) that you want to delete and then click Delete Selected.\n\n\n\n\n\n\nNote: Even if you don't delete any messages, all messages will be automatically deleted after 2 weeks.\n\n, Marking messages as read means that it will reduce the number of new messages. It will mean that they aren't new. Select the message(s) that you want to mark as read then click Mark as Read.\n\n\n\n\n\n, Click a message to view its content. The message will tell you who sent it and when it was sent. In messages, you can only see the text and links posted by the sender. To view the files, you have to go to the message itself by clicking Click here at the bottom of the message. This will open a new tab in your internet browser and redirect you to the whole message. You can go back to your message inbox by clicking Back to Inbox or you can delete the message by clicking Delete which will also let you go back to your inbox.\n\n, After reading the messages, you can close the message box by clicking Close on the top right corner of the message box.\n\n\n\n\n\n, Remember that your dashboard is your homepage and you can view and do lots of stuff here. One of those things is adding an event. By adding events, you can view personal deadlines and events. Since these are personal, no one else can view these events that you add.\n\n, Click Add event. This button is next to the Full calendar button. To continue adding an event, type in the name of the event. For example, \"Cross Country\". You can choose whether or not to add a location and notes but the name of the event is a must.\n\n, Pick the date of the event from the calendar. Then select the time of the event and whether or not it is am or pm. You can also choose whether or not the event is an all-day event. This means that the time doesn't matter. For example, a school carnival would be an all-day event but a cross country race wouldn't be an all-day event because it only lasts for 2 or 3 hours. Once you're done, click Add Event.\n\n\n\n\n\n, Click or go to the event that you added. At the top right corner of the event page, you can see two buttons: Edit Event and Delete. You know what to do. Click Edit Event to edit the event and similarly, click Delete to delete the event. Have fun with ManageBac!\n\n",
        "  Using mappings to computer-science problems and by applying sophisticated\nalgorithms, one can study numerically many problems much better compared to\napplying standard approaches like Monte Carlo simulations. Here, using\ncalculations of ground states of suitable perturbed systems, droplets are\nobtained in two-dimensional +-J spin glasses, which are in the focus of a\ncurrently very lifely debate. Since a sophisticated matching algorithm is\napplied here, exact ground states of large systems up to L^2=256^2 spins can be\ngenerated. Furthermore, no equilibration or extrapolation to T=0 is necessary.\nThree different +-J models are studied here: a) with open boundary conditions,\nb) with fixed boundary conditions and c) a diluted system where a fraction\np=0.125 of all bonds is zero. For large systems, the droplet energy shows for\nall three models a power-law behavior E_D L^\\theta'_D with \\theta'_D<0. This is\ndifferent from previous studies of domain walls, where a convergence to a\nconstant non-zero value (\\theta_dw=0) has been found for such models. After\ncorrecting for the non-compactness of the droplets, the results are likely to\nbe compatible with \\theta_D= -0.29 for all three models.\n  This is in accordance with the Gaussian system where \\theta_D=-0.287(4)\n(\\nu=3.5 via \\nu=-1/\\theta_D). Nevertheless, the disorder-averaged spin-spin\ncorrelation exponent \\eta is determined here via the probability to have a\nnon-zero-energy droplet, and \\eta~0.22$ is found for all three models, this\nbeing in contrast to the behavior of the model with Gaussian interactions,\nwhere exactly \\eta=0.\n",
        "Dato' Mohammad Ridzuan bin Mohamad Puzi  (born 27 September 1987) is a Paralympic athlete from Malaysia who competes in T36 classification sprint (running) and long jump events. Mohammad Ridzuan represented Malaysia at the 2016 Summer Paralympics in Rio de Janeiro, where he won the gold medal in the 100 metres event.\n\nPersonal life\nMohamad Ridzuan had been diagnosed with cerebral palsy at the age of one. In his early years, he took part in athletics during school meets. His talent was spotted by coach Affizam Amdan in 2004. Since then, coach Affizam has been guiding, grooming and training him.\n\nAthletics career\nHe debut as a professional para-athlete during 2011 ASEAN Para Games at Surakarta, Central Java, Indonesia. He managed to get gold medal in his debut competition. His success continues year by year with more achievements recorded.\n\n2014 Asian Para Games\nHe bagged three medals in this multi-sport events. One gold medal was from long jump event, and two others was silver for 100 metres and 200 metres.\n\n2015 IPC Athletics World Championships\nAt the event held in the evening 29 October, at the Suheim Bin Hamad Stadium, Mohamad Ridzuan make a time record of 12.08 seconds to win gold medal in his favourite event, 100 metres.\n\n2016 Rio Summer Paralympics\nIn 10 September, he won the first gold medal for Malaysia in the 2016 Paralympic Games in the 100m event.\n\n2017 World Para Athletics Championship\nHe competed in two events; 100 meters and 200 metres. Earlier in 200m events, he managed to qualified to the final event and finished fourth. Meanwhile, in the 100m events, he did not manage to retain his gold medal in the previous edition after he only managed a silver medal. He lost to Yang Yifei of China by 0.27 seconds.\n\n2018 Asian Para Games\nHe competed in three events; 100 meters, 200 metres and long jump. He achieved gold medal hat-trick in this multisport event, added by some record-breaking. He broke the 100 metres world record of Evgenii Shvetsov, and Asian record made of Yang Yifei in long jump event.\n\nHonours\n\nHonours of Malaysia \n  :\n  Officer of the Order of the Defender of the Realm (K.M.N.) (2017)\n  :\n  Commander of the Order of the Territorial Crown (JMW) (2022)\n\nReferences\n\nLiving people\n1987 births\nPeople from Perlis\nMalaysian people of Malay descent\nMalaysian Muslims\nParalympic athletes of Malaysia\nAthletes (track and field) at the 2016 Summer Paralympics\nParalympic gold medalists for Malaysia\nMedalists at the 2016 Summer Paralympics\nOfficers of the Order of the Defender of the Realm\nParalympic medalists in athletics (track and field)\nAthletes (track and field) at the 2020 Summer Paralympics\nMalaysian male sprinters",
        "As far as I'm aware, we still don't *quite* know.\n\nCompared to humans, we've known for some time that insects are generally more resistant to ionizing radiation, and multiple hypotheses have been proposed to explain this radioresistance.\n\nFor a long time it was thought that because actively dividing cells are those most sensitive to radiation, insects would succumb less as, unlike humans with our leagues of constantly dividing cells, insects undergo discontinuous periods of growth (only with every moult). But this whole organism approach to radioresistance was tricky to interpret, as the physiology between us and, say, invertebrates is very different.\n\nAt a cellular level however, experiments on cells controlling for proliferative rate have revealed that insect cells are *de facto* more radioresistant than human cells, leading us to believe division rate actually might only have a little to do with it. When you blast human and insect cells with ionising radiation, the DNA within the insect cells itself undergoes much less damage, and what damage is present is more effectively repaired. Likewise, those same insect cells experience lower oxidative stress as a consequence of radiation exposure (radiation triggers the production of rather harmful reactive oxygen species that, amongst other things, trigger cells to commit [apoptotic](_URL_1_) suicide). \n\nSo yup, it appears the suite of repair enzymes insects utilise are simply better at dealing with DNA damage, explaining why insects have greater radioresistance. As for the evolutionary reason why they're more efficient, we're still not quite sure.\n\n___\n\n^**Sources:**\n\n[^(Cheng, I.C, Lee, H.J.  &  Wang, T.C. (2009)^) ^(Multiple factors conferring high radioresistance in insect Sf9 cells. *Mutagenesis* 24 (3)^), ^259-369](_URL_0_)\n\n\n[^(Bianchi, N.O., Lopez-Larraza, D.M.  &  Dellarco, V.L. (1991)^) ^(DNA damage and repair induced by bleomycin in mammalian and insect cells. *Environ Mol Mutagen*. 17, 63-68)](_URL_3_) ^((research gate) [^here](_URL_2_)^)",
        "  We use new and published near-IR spectra, with synthetic spectra, to derive\nphysical properties of three of the latest-type T dwarfs. A new R~1700 spectrum\nof the T7.5 dwarf HD 3651B, with existing data, allows a detailed comparison to\nthe well-studied and very similar dwarf, Gl 570D. We find that HD 3651B has\nboth higher gravity and metallicity than Gl 570D, with Teff=820-830K, log g=\n5.4-5.5, [m/H]= +0.2 and Kzz=10^4cm^2/s. Its age is 8-12 Gyr and its implied\nmass is 60-70 M_Jup. We perform a similar analyis of the T8 and T7.5 dwarfs\n2MASS J09393548-2448279 and 2MASS J11145133-2618235 using published data,\ncomparing them to the well-studied T8, 2MASS J04151954-0935066. We find that\nthe two dwarfs have the same Teff as the reference dwarf, and similar\ngravities, but lower metallicities. The parameters are Teff=725-775K and [m/H]=\n-0.3; log g=5.3-5.45 for 2MASS J09393548-2448279 and log g=5.0-5.3 for 2MASS\nJ11145133- 261823. The age and mass are ~10Gyr and 60M_Jup for 2MASS\nJ09393548-2448279, and ~5 Gyr and 40M_Jup for 2MASS J11145133-261823. A serious\nlimitation is the incompleteness of the line lists of CH4 and NH3 at lambda\n<1.7um. Spectra of Saturn and Jupiter, and of laboratory CH4 and NH3 gas,\nsuggest that NH3 features in the Y- and J-bands may be useful as indicators of\nthe next cooler spectral type, and not features in the H- and K-bands as\npreviously thought. However large uncertainties remain, as the abundance of NH3\nis likely to be significantly below the chemical equilibrium value, and\ninclusion of laboratory NH3 opacities predicts band shapes that are discrepant\nwith existing data. It is possible that the T spectral class will have to be\nextended to low temperatures around 400K, when water clouds condense in the\natmosphere [abridged].\n",
        "The 2008 City of Ipswich 400 is the seventh round of the 2008 V8 Supercar season. It was held on the weekend of July 18 to 20 at Queensland Raceway in Ipswich, Queensland.\n\nPractice\nPractice featured another opportunity for teams to test endurance co-drivers with Steve Owen ending up the fastest of those drivers in the Jim Beam Racing Falcon, second only in the session to Russell Ingall. Warren Luff was fifth fastest in the second Jim Beam Falcon just ahead of Paul Radisich in the HSV Dealer Team Commodore. Next were Craig Baird (Holden Racing Team) and David Besnard (Stone Brothers Racing). Other co-drivers in the session were Dean Canto (Ford Performance Racing), Glenn Seton (Holden Racing Team), Jason Bargwanna (Rod Nash Racing), Grant Denyer (Ford Rising Stars Racing), Luke Youlden (Ford Performance Racing), Mark Noske (Tasman Motorsport), Jack Perkins (Jack Daniel's Racing), Adam Macrow driving the Team Kiwi Racing Falcon instead of for his enduro team Britek Motorsport, Brad Jones (Brad Jones Racing) and David Reynolds (HSV Dealer Team).\n\nQualifying\nQualifying was held on Saturday July 19.\n\nRace 1\nRace 1 was held on Saturday July 19.\n\nRace 2\nRace 2 was held on Sunday July 20.\n\nRace 3\nRace 3 was held on Sunday July 20.\n\nResults\nResults as follows:\n\nQualifying\n\nRace 1 results\n\nRace 2 results\n\nRace 3 results\n\nStandings\nAfter round 7 of 14.\n\nSupport categories\nThe 2008 City of Ipswich 400 had five support categories.\n\nReferences\n\nExternal links\nJuly 2008.QR1 Official timing and results\n\nCity of Ipswich 400",
        "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically).",
        "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments.\nThat said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.",
        "This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. \n\nFor the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.\n\nI am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). \nThe experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). \n\nI also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? \n\nFinally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.\n\nPreliminary Rating:\nI think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). \n\nClarification:\nIn the paper you say \"While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study.\" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?\n\nMinor notes:\n\tPlease provide years for Prabhat et al. references rather than a and b.\n\tFootnote in 4.2 could be inline text with similar space.\n\t4.3 second paragraph the word table is not capitalized like elsewhere.\n\t4.3 4th paragraph the word section is not capitalized like elsewhere.\n\nEdit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.",
        "I just wanted to comment that I found this to be a really great paper. There's a lot of new ideas, and the writing and experiments are extremely well executed.\n\nThe analysis of section 3-3.1 is novel (to the best of my knowledge) and very valuable, as it's interesting to see in play the DAE ideas in 3.3. Figure 1 (sec 2.1) is extremely clarifying. The criticism of section 5.6 is very interesting, and might lead to a new direction of research if handled with care, the question about the precise mathematical meaning of \"producing plausible samples\" is extremely important and far from solved.\n\nMinor comments / questions:\n- Have you tried comparing with an architecture such as the ones used in segmentation or structured prediction? The use of the mean field CRF approach will lead you to pick a mode and get sharp predictions as has been done for a long time, since you are training a conditional unimodal distribution with the inverse KL. The issue of continuous variables can be ameliorated by discretizing as in pixel CNN for example. As another bonus, these architectures are very stable and can be trained with much much bigger models.\n- Page 3. Item 2. Typo on employs -> employ",
        "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the firstÊend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.Ê(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper.",
        "This paper proposes the RIMs that unrolls variational inference procedure. \n\nThe author claims that the novelty lies in the separation of the model and inference procedure, making the MAP inference as an end-to-end approach. The effectiveness is shown in image restoration experiments.\n\nWhile unrolling the inference is not new, the author does raise an interesting perspective towards the `model-free' configuration, where model and inference are not separable and can be learnt jointly. \n\nHowever I do not quite agree the authors' argument regarding [1] and [2]. Although both [1] and [2] have pre-defined MAP inference problem. It is not necessarily that a separate step is required. In fact, both do not have either a pre-defined prior model or an explicit prior evaluation step as shown in Fig. 1(a). I believe that the implementation of both follows the same procedure as the proposed, that could be explained through Fig. 1(c). That is to say, the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters.  \n\nMoreover, the RNN block architecture (GRU) and non-linearity (tanh) restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm. This is also similar with [1] and [2].\n\nBased on that fact, I have the similar feeling with R1 that the novelty is somewhat limited. Also some discussions should be added in terms of the architecture and nonlinearity that you have chosen. ",
        "This paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets.  An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble.  Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models.  Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained.  Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss.  The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit.  That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style.  Quoting from the template:  \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set → remaining 10,000 images as the validation set\n\nevaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization → more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models → the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical →  clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view\n\n",
        " Most furnaces have a service switch on or near the boiler itself designed specifically for service personnel to use that ensures the power remains off while they are working on the equipment. If unable to locate the service switch, shut off power one way or another to prevent furnace from starting. Place a note on the electrical panel explaining the the circuit is off because the furnace is being serviced.\n,,, You risk injury from burns if you do not allow enough time for the furnace cool down before attempting this work., Most modern furnaces have the oil burner supported through an opening in a hinged door. Take a photo with your cell phone or camera before beginning to help you remember how to reconnect the lines and cables later. If the fuel line must be removed, be sure also shut off the oil supply valve at or just before the point of disconnect. Many times, the easiest point and method to disconnect is by unbolting the fuel pump cover from the furnace motor itself. Gently move it out of the way and try not to move the fuel lines any more than necessary to help minimize the chances of kinking lines or disrupting the mating surfaces between flared ends of the lines and compression fittings. Remove and (clean or) have a replacement filter ready to install when reassembling later., The door has an important gas-tight seal that is maintained usually by a hex nut or bolt that squeezes the door seal against the furnace opening. If it is undamaged and you are not interested in replacing it, try not to disturb it., Wear personal protective equipment consisting of safety glasses, long sleeve shirt, protective gloves and mask to prevent inhaling fibers and getting them in your skin. Wetting the inside of the chamber with water will help prevent fibers from becoming airborne but also reduces the ease with which a vacuum will be able to pick up the debris in the next step., Remove the soot, rust and other debris that has collected or formed on the floor and walls of the chamber. A soot vacuum or HEPA vacuum is recommended if the inside hasn't been wet down in the previous step. If it has been wet down, a common shop vacuum will do., Reach in and pull the preformed refractory lining from rear wall and blanket material on the floor out of the chamber. Use a screw driver to gently break and scrape off the old lining material completely and then use a wisk broom to sweep all pieces into the garbage bag. Gently clean steel surfaces with a wire brush. Do not wedge screwdrivers and similar objects between boiler sections. Wisk broom or vacuum all surfaces once again., There may be small liner blanket behind the preformed refractory liner that should be removed as well. Clean the area the same as the inside of the chamber and vacuum., If the seal is undamaged and you do not want to replace it - you can skip this step - and those appearing later that describe replacement. Run a screwdriver around the recessed channel to scrape it clean and then wire brush and wisk broom this area clean.,, You may need to tip it to get it into proper position. Make sure it is all the way back against the rear wall by pressing with palms firmly all the way around., Orient the the blanket as per instructions and lay it inside the chamber. Butt the blanket up against the bottom edge of the rear wall refractory liner and extend it out of the chamber. Mark the blanket to indicate where it meets the outside edge of the opening to the chamber. Arrange the blanket so the sides extend equally up the sides of the boiler wall. Make a mark on the blanket and chamber floor so you can place it in the same position later., Use a straight edge and razor knife so it fits the chamber as per the instructions., The water glass is wet and sticky and will harden like glue when heated.  Pour about 3/4 of the water glass onto the chamber floor and then spread it with gloved fingers. Save the rest of the water glass to help secure the rope seal into the recess of the door in a later step. If you are not replacing the rope seal, using more than needed to cover the area needed by the blanket will only drip and leak out the front of the chamber., Hold the blanket inside and above the chamber floor (above the water glass covered surfaces). Align the marks on the blanket and the chamber floor. When satisfied with the position, lower and press it into place .,, Some liners come with paper strips to help it slide over rough edges of the cast iron door. Use them as needed to prevent cracking or breaking of the liner while pressing to fit. The liner is considered in position when the burner opening is flush with the liner to as much as 1/4\" recessed behind the liner - or as the instructions indicate with your liner kit., This step should be skipped if you did not remove the undamaged rope seal in a previous step. Remove the paper backing from the rope seal and press the adhesive side into the recess - starting with the middle of the rope seal at the mid point at the top of the door. Press the rope seal into the recess all the way around both sides of the door from top to bottom. Cut the rope seal to allow them to overlap slightly at the bottom middle of the door.,, Try not to move the fuel lines any more than necessary to help minimize the chances of disturbing the mating surfaces between flared ends of the lines and compression fittings. Wipe away any excess oil from lines, fittings and floor., Visually check for oil leaks at all fittings and correct as needed.,  If the burner fails to start and locks out, the oil pump may need to primed.  The priming procedure can be found here: Restart-a-Furnace-After-Running-out-of-Oil\n, Tighten fittings or make the necessary repairs to stop any and all leaks.",
        "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in “This approximation to the Dirichlet prior p(θ|α) is results in the distribution”, it should be “This approximation to the Dirichlet prior p(θ|α) results in the distribution”\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML’14\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML’14",
        "what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture.",
        "  Performance of object-oriented database systems (OODBs) is still an issue to\nboth designers and users nowadays. The aim of this paper is to propose a\ngeneric discrete-event random simulation model, called VOODB, in order to\nevaluate the performances of OODBs in general, and the performances of\noptimization methods like clustering in particular. Such optimization methods\nundoubtedly improve the performances of OODBs. Yet, they also always induce\nsome kind of overhead for the system. Therefore, it is important to evaluate\ntheir exact impact on the overall performances. VOODB has been designed as a\ngeneric discrete-event random simulation model by putting to use a modelling\napproach, and has been validated by simulating the behavior of the O2 OODB and\nthe Texas persistent object store. Since our final objective is to compare\nobject clustering algorithms, some experiments have also been conducted on the\nDSTC clustering technique, which is implemented in Texas. To validate VOODB,\nperformance results obtained by simulation for a given experiment have been\ncompared to the results obtained by benchmarking the real systems in the same\nconditions. Benchmarking and simulation performance evaluations have been\nobserved to be consistent, so it appears that simulation can be a reliable\napproach to evaluate the performances of OODBs.\n",
        "The 1988 Hall of Fame Tennis Championships and the 1988 Virginia Slims of Newport were tennis tournaments played on grass courts at the International Tennis Hall of Fame in Newport, Rhode Island, in the United States that were part of the 1988 Nabisco Grand Prix and of the Category 3 tier of the 1988 WTA Tour. The men's tournament was held from July 4 through July 10, 1988, while the women's tournament was held from July 11 through July 17, 1988.\n\nFinals\n\nMen's Singles\n Wally Masur defeated  Brad Drewett 6–2, 6–1\n It was Masur's 1st title of the year and the 9th of his career.\n\nWomen's Singles\n\n Lori McNeil defeated  Barbara Potter 6–4, 4–6, 6–3\n It was McNeil's 6th title of the year and the 16th of her career.\n\nMen's Doubles\n Kelly Jones /  Peter Lundgren defeated  Scott Davis /  Dan Goldie 6–3, 7–6\n It was Jones' only title of the year and the 2nd of his career. It was Lundgren's only title of the year and the 5th of his career.\n\nWomen's Doubles\n\n Rosalyn Fairbank /  Barbara Potter defeated  Gigi Fernández /  Lori McNeil 6–4, 6–3\n It was Fairbank's 1st title of the year and the 16th of her career. It was Potter's 1st title of the year and the 22nd of her career.\n\nExternal links\n Official Website\n ATP Tournament Profile\n\nHall of Fame Tennis Championships\nVirginia Slims of Newport\nHall of Fame Open\nVirginia Slims of Newport\n \nHall of Fame Tennis Championships\nTennis tournaments in Rhode Island\nHall of Fame Tennis Championships\nHall of Fame Tennis Championships",
        "  Indirect information about the possible scale of supersymmetry (SUSY)\nbreaking is provided by B-physics observables (BPO) as well as electroweak\nprecision observables (EWPO). We combine the constraints imposed by recent\nmeasurements of the BPO BR(b -> s gamma), BR(B_s -> mu^+ mu^-), BR(B_u -> tau\nnu_tau) and Delta M_{B_s} with those obtained from the experimental\nmeasurements of the EWPO M_W, sin^2 theta_eff, Gamma_Z, (g-2)_mu and M_h,\nincorporating the latest theoretical calculations of these observables within\nthe Standard Model and supersymmetric extensions. We perform a chi^2 fit to the\nparameters of the constrained minimal supersymmetric extension of the Standard\nModel (CMSSM), in which the SUSY-breaking parameters are universal at the GUT\nscale, and the non-universal Higgs model (NUHM), in which this constraint is\nrelaxed for the soft SUSY-breaking contributions to the Higgs masses. Assuming\nthat the lightest supersymmetric particle (LSP) provides the cold dark matter\ndensity preferred by WMAP and other cosmological data, we scan over the\nremaining parameter space. Within the CMSSM, we confirm the preference found\npreviously for a relatively low SUSY-breaking scale, though there is some\nslight tension between the EWPO and the BPO. In studies of some specific NUHM\nscenarios compatible with the cold dark matter constraint we investigate\nM_A-tan_beta planes and find preferred regions that have values of chi^2\nsomewhat lower than in the CMSSM.\n",
        "The Smart Money Woman is a Nigerian TVseries based on a 2016 novel of the same name by Arese Ugwu. The series premiered on Africa Magic Showcase in September 2020. The series was filmed to exclusively act and bring to life what was in the novel. The series which initially aired as a single season of 13 episodes on Africa magic was released on Netflix as a single season with 7 episodes on Netflix on 16 September 2021. It was executively produced by the writer of the novel, Arese Ugwu and produced by Kemi Lala Akindoju. It starred Osas Ighodaro, Timini Egbuson, Ini Dima-Okojie, Kemi Lala Akindoju and many others.\n\nPlot summary \nThe Smart Money Woman revolves around five young women and how they take control of their finances and assets, the series focuses on spending culture of women and how it ultimately affects their finances on the long run, the series also talks about how friendship, peer pressure and societal influence can affect how we spend money, It also features and teaches how women should learn to invest in their themselves amidst romantic and financial losses. it also has some addendum quotes on how to become a smart money woman and also discusses challenges women face with societal pressure and desire to meet up with standards.\n\nEpisodes\n\nSelected cast \nOsas Ighodaro as Zuri\nKemi Lala Akindoju as Adesuwa\nToni Tones as Lara\nEbenezer Eno as Ladun\nEku Edewor as Banke\nTimini Egbuson as Bobby\nIni Dima-Okojie as Tami\nSeun Ajayi as Soji\nNonso Bassey as Olumide Sanni\n\nReferences\n\n2020s Nigerian television series\n2020 Nigerian television series debuts",
        "Hi,\n\nI have few questions about the paper.\n\n1- What attacker's goal did you consider in your paper? Is it recovering the training data, or checking whether a specific sample has been in the training data? \n\n2- If the attacker's goal is to recover the training data, does the attacker want to recover the exact data or an approximation would be OK?\n\n3- Talking about neural networks:\n- Do you think there is any attack method to recover an exact training data from the learning model?\n- Do you think there is any defense method to prevent an attacker from recovering even an approximate training data?\n\n4- How can we quantify the strength of a learning model (specifically neural networks) without any defensive mechanism?\n\n5- How can we quantify the strength of a learning model which has not been trained on exact training data? For example, some forms of adversarial training methods never train the model on the clean data; instead, at each epoch, the model is trained on different adversarial data derived from the real data. \n- How can the model \"memorize\" the training data, when 1) it has never seen the real data, 2) it has been trained on different data in different epochs?\n\n6- How do you compare the performance of your method with adversarial training?\n\nThanks.",
        " The strategy in the case of Doctor Mario is DEFENSE then counter. You cannot go into battle with Doc with an offensive strategy (well you can and you will but not in the same manner as say Marth). You need to focus on what I call Counter Attack Strategy.\n\n, What is the counter attack strategy? Well basically you want to bait your opponent in to a compromised position. This may be self-explanatory but I write it out to emphasize its importance (I will repeat myself often in this guide, take it as something particularly important if I do). Rarely ever do you want to rush into an attack with Doc, you will get punished if you try.\n\n, This will force him to attack you. This is the position you want to be in initially. If and when your opponent gets by the pills you then either attack his compromised position (he will likely have jumped over the pills or rolled by them) or continue to defend by utilizing your sheet (cape). The idea here is to be as aggravating as humanly possible to force your opponent into making critical mistakes.\n\n, Once the opponent is in a compromised position you lay on the hurt. This is IMPORTANT for your success as Doc. You must COMBO to be effective as Doc. Use your tilts and up-throw to get them in the air and SHFFL to keep them there (after giving them about 20-30 damage with pills ect). Remember: you can't really combo that well until they have some damage so get some pills to connect before the offensive counter (this is not quite the case with the floaters: Marth, Peach. You should combo early against them because they fly to far when they have damage)\n\n, At this point you want your opponent off the stage. Try a forward or back throw or a D-Smash to do it. This where you put one of Doc's best assets to the test.\n\n, This is very simple: you do not ever, ever, EVER, let an opponent back on the stage in this game. With Doc this is pretty easy. The following attacks are very good for this purpose:\n\n,,, Don't worry if you take a hit here as you will likely get pushed back towards the stage, getting hit here is generally expected and its worth it.\n\n, you do this by floating kind of low then Up-B to just barely grab the edge thus allowing you to avoid the edge-guard by your opponent. If you get hit really far out you can try the tornado (Down-B) to get more distance, although this takes a LOT of practice to gain from.\n\n, I said WAVEDASH! This move is very good with Doc and I often use it for all of my on ground movement (you can move faster than running in many cases);\n, When the opponent is at about 30-40 damage I usually go for W-Dash to Down-Smash. You can often trick your opponent by facing away and W-Dashing at them D-Smash and hit them with the back end of the attack (they won't realize what you are doing thinking you can't hit them facing backwards :)). When they have about 60-80 and you can't get them off the stage try a W-Dash to Forward-Smash to slide right up next to them and connect (this is particularly effective as Doc's F-Smash does most damage up close).\n\n,",
        "Film editor here. \n\nIn films, you have to establish the facts of a situation very quickly. A classic example is the 'baguette sticking out the top of a large brown paper bag' device used when a character needs to enter a scene.\n\nI'll explain: let's say you have a scene set in a house, and you need a character to enter. If he (or she) just appears, the viewer immediately thinks 'where has he been?' 'How come he's just appeared now?' \n\nBy giving the character some shopping bags, it explains that he has just come back from the shops. A mystery has been explained visually, without any need for excessive dialogue explaining where he has just been. But if you give the character any old bags, the viewer will be thinking 'what's in the bag?' 'Why has he just been out shopping? What was he getting? Is it relevant to the plot?\n\nBy having a baguette sticking out the top of a brown paper bag it immediately establishes that he has just been doing a regular food shop, again without the need for dialogue explaining the situation. \n\nNobody in real life actually gets their supermarket shopping in a large brown paper bag with a baguette sticking out the top, but in film, it instantly transfers a wealth of information, and nobody will be questioning why that character has appeared in that scene at that moment.\n\nIn the same way, glowing green has become a device to say 'radioactive'. If you see something glowing green, you know it is radioactive without it having to be explained in dialogue. \n\nI can't tell you why it was originally green rather than, say, blue. /u/shorvok suggests it originated in comic books which I can imagine is true. But the reason it is still used today is because it is a device to instantly transfer information to the viewer by using visuals.",
        "Former employee of a sulfuric acid plant here.\n\nMost sulfur comes as a byproduct of petroleum refining.  At the refinery, the sulfur stream is already liquid, and the pumps are of mature designs that do a pretty good job of handling it.  The railcars are filled with this hot (300F +/-10) molten sulfur.  The cars are insulated to\n\nA: help retain whatever heat it can and \n\nB: to protect whatever poor thing might bump into it.  \n\nSo this sulfur is usually is frozen solid by the time it gets where it is going.  The railcar has steam coils in it.  ~75psi steam will melt a railcar in about 12-18 hours depending on the weather.  Once there are no solid chunks, a valve on the bottom of the car is opened, and it dumps into a pit.  From there it is pumped to a storage tank, and then gravity fed into another pit.  This pit has high pressure pumps that spray it into a furnace to start the sulfuric acid production process.\n\n\n\nTL;DR- the source produces molten sulfur, and the end user needs it in its molten state.",
        "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!",
        "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina.\n\nOn the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology.\n\nOn the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective.\n\nI think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale.\n\nI suspect followup work building on this proof of concept will be increasingly exciting.\n\nMinor comments:\nSec 3.2:\nI didn't understand the role of the 0.833 ms bins.\nUse \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\".\n\nFig. 4 would be better with the x-axis on a log scale.",
        "  Incoherent noise is manifest in measurements of expectation values when the\nunderlying ensemble evolves under a classical distribution of unitary\nprocesses. While many incoherent processes appear decoherent, there are\nimportant differences. The distribution functions underlying incoherent\nprocesses are either static or slowly varying with respect to control\noperations and so the errors introduced by these distributions are refocusable.\nThe observation and control of incoherence in small Hilbert spaces is well\nknown. Here we explore incoherence during an entangling operation, such as is\nrelevant in quantum information processing. As expected, it is more difficult\nto separate incoherence and decoherence over such processes. However, by\nstudying the fidelity decay under a cyclic entangling map we are able to\nidentify distinctive experimental signatures of incoherence. This result is\ndemonstrated both through numerical simulations and experimentally in a three\nqubit nuclear magnetic resonance implementation.\n",
        "The human mind has two systems for representing numbers: a [subitizing system for numbers up to four](_URL_4_), and an approximate ratio estimation system for larger numbers. Your choice of the number \"five\" is interesting because it is right on the edge of the subitizing system's capabilities, but you are probably able to see that there are five without actually having to count them. Let's spell out the difference here to be clear.\n\nFor numbers less than four, you can immediately tell precisely how many there are without having to count them (this is what the subitizing system does). For numbers larger than four you can only get an approximate estimate unless you count them (this is what the approximate number system does). The approximate number system [works like Weber's law, in terms of ratios](_URL_5_). This means that you can discriminate say 90 from 100 and 900 from 1000 about equally easily because they are both a ratio of 9:10.\n\nNow to counting, which is actually a cool little invented trick that expands the capacity of the subitizing system by using language to precisely enumerate more than 4 objects (keep in mind you can't get a precise count of more than 4 objects without counting them). The way this trick works is as follows. We all memorize a verbal list of numbers that we store in long term memory (1, 2, 3, 4, 5, 6, ...). You may remember this being a big part of learning when you were 4 or 5 years old, and you can see that it isn't all that natural because it takes kids some time and effort to memorize this list. Now, once you have this list memorized, you can use the following counting algorithm to precisely enumerate more than 4 objects. You can then count the number of objects you're looking at by giving each a label from the memorized list of numbers, and continue this process until each object has a label, and has only one label. The label that you end at is the number of objects there are. \n\nSo, let's say you had seven objects on a table, there are two ways you could precisely enumerate them. The first would be to create two groups of objects that are subitizable (say, identify one group of three objects, and one of four objects), process them immediately and then add them together. The other way would be to start labeling them from your list (the first gets the label \"one\", the second \"two\", and so on). Then you will run out of objects to label precisely at the label \"seven\" and you will know you have seven objects. If you wanted to count 90 objects though, you would be forced to run the counting algorithm because there is no way to break that up into a manageable number of subitizable sets (sets of four or less objects).\n\nWhen I first learned this it blew my mind, but if you think about it a little bit, you realize that is precisely what you are doing when enumerating some group of objects. You should notice that you can immediately recognize up to about four objects without counting (and can increase this with the little grouping trick I mentioned above, that I often use for numbers less than ten or so). However, notice that if you have to enumerate, say 17 objects, you probably won't be able to do so without the \"little voice in your head\", which you are using to recite your memorized list of numbers. \n\nThis also explains how some cultures don't have number systems that go above two or three. All cultures have words that distinguish one object from multiple objects, but some stop there, or have counting systems that are something like \"one\", \"two\", \"many\". These cultures simply have not invented this linguistic counting trick because the need has not arisen, and this is not uncommon among hunter-gatherers and hunter-horticulturalists: they don't need to enumerate identical objects because most objects in the natural world can be identified individually because they are all unique. While counting seems incredibly natural to us, it is only because it is so well learned that we overlook how we got there in the first place, and so the idea that some people can get by without the counting trick can seem really odd to educated people. Interestingly, number systems seem to arise when the need arises, and specifically when people need to keep track of large numbers of roughly identical objects, or keep some record of the number for the future. When does this happen? Often with the invention of agriculture, since this often leads people to be trading, tracking, and exchanging larger numbers of nearly identical objects (e.g., bushels of wheat). This is why the counting trick has been independently invented many times over across many different cultures, yet has not been invented by all of them. For some cultures the need simply never arose.\n\nIt is a little tricky to give sources for all of this because it is a broad summary of a ton of research, but here are some good places to start:\n\nWhere Mathematics Comes From by Lakoff and Nunez\n\nHuman Universals by Donald Brown\n\nDevelopmental psychology work on numerical cognition by [Elizabeth Spelke](_URL_6_), and [Karen Wynn](_URL_7_). \n\n\nEdit: Thanks for the comments and gold. I'm glad you all found this interesting. I would love to keep fielding questions here, but I should probably get back to doing real work. However, I did want to add a call out for anyone who knows more about this topic to post something on variation in subitizing ability. It seems like over half of the comments are asking about whether this can be greater than 4, and I don't know for sure or have a source off hand. My memory is that subitizing capacity does vary, but only around about 3-5, so you can't subitize much higher than that. If anyone can find a source for this please post it. Thanks.\n\nEdit 2: Looks like /u/svof posted a source on individual differences in subitization below. He points out that 4 is the modal subitization ability, which is a helpful elaboration. The general points hold, but there is more nuance in subitizing abilities than my answer implied.\n\nEdit 3: Wow, thanks everyone. I just wanted to add that there are other ways to assess the number of objects without counting them or subitizing them, for example by using a heuristic based on shape. Many comments/questions keep stating that people don't need to count higher numbers on dice or dominoes, and that is because you have memorized the shapes that the marks make, and how each shape relates to a specific number. So, there are other ways to figure out the number of objects, such as spatial heuristics, and I bet there are probably a lot of other work arounds one could come up with. The key to these work arounds would be figuring out visual stimuli that are immediately perceptible and map onto the number of objects somehow (e.g., like if every time there were 33 objects, they would be red, and only when there were 33 objects would they be red--then you could just instantly see the red and know there were 33 objects).\n\nEdit 4: Man did this blow up. Thanks for all the gold, and for the interest. I just wanted to add this edit to say that I probably won't be answering any more questions. If a unique one comes in, I'll try to respond, but almost every new comment/question is about one of the things I addressed in the post or the edits above (variation in subitizing ability, counting by subitizing in multiple groups, or counting by pattern recognition). Since I addressed those here, I'm not going to go through and answer each one over and over. One other common question is why four specifically, and I think /u/99trumpets [gave the best answer for this below](_URL_2_). The last thing people keep asking about is subitizing savants (e.g., people that can instantly count 100 objects), and I just want to say I know nothing about that. I haven't seen a single credible source on it though, as everyone just references some vague thing they heard or Rain Man, so it's hard to tell if it is a real documented phenomenon or not. If someone does post a source on it, I'll add it in up here, otherwise I'm not really sure how to address that specific topic. Thanks again for reading, and I'm glad you all found this so interesting.\n\nEdit 5: /u/SirSoliloquy [built a cool little web app to demonstrate subitization.](_URL_0_) [Check it out!](_URL_3_)\n\nEdit 6: Radiolab did a segment on exactly this topic. You can listen to it [here](_URL_1_).",
        "  We analyze several possibilities for precisely measuring electronic\ntransitions in atomic helium by the direct use of phase-stabilized femtosecond\nfrequency combs. Because the comb is self-calibrating and can be shifted into\nthe ultraviolet spectral region via harmonic generation, it offers the prospect\nof greatly improved accuracy for UV and far-UV transitions. To take advantage\nof this accuracy an ultracold helium sample is needed. For measurements of the\ntriplet spectrum a magneto-optical trap (MOT) can be used to cool and trap\nmetastable 2^3S state atoms. We analyze schemes for measuring the two-photon\n$2^3S \\to 4^3S$ interval, and for resonant two-photon excitation to high\nRydberg states, $2^3S \\to 3^3P \\to n^3S,D$. We also analyze experiments on the\nsinglet-state spectrum. To accomplish this we propose schemes for producing and\ntrapping ultracold helium in the 1^1S or 2^1S state via intercombination\ntransitions. A particularly intriguing scenario is the possibility of measuring\nthe $1^1S \\to 2^1S$ transition with extremely high accuracy by use of\ntwo-photon excitation in a magic wavelength trap that operates identically for\nboth states. We predict a ``triple magic wavelength'' at 412 nm that could\nfacilitate numerous experiments on trapped helium atoms, because here the\npolarizabilities of the 1^1S, 2^1S and 2^3S states are all similar, small, and\npositive.\n",
        "The current version of the paper is improved w.r.t. the original arXiv version from June. While the results are exactly the same, the text does not oversell them as much as before. You may also consider to avoid words like \"mantra\", etc. \nI believe that my criticism given in my comment from 3 Dec 2016 about \"randomly generated task\" is valid and you answer is not.",
        "(Figure I.2)\nAt this time, the “Start Application in Compatibility Mode” window appears with the “Program Compatibility Wizard” inside;\n,\nClick “Cancel” button to quit.\nIn this case, we click “Next” to continue.\n\n, It asks you which way you want to locate your program.\n\n\nThere are 3 options:\nI want to choose from a list of programs.\nSearch your primary partition (Usually C: drive) and list all programs that are stored in it or are installed directly to your computer.\nI want to use the program in the CD-ROM drive.\n\n,\n\n\nI want to locate the program manually.\nBy selecting this option, you will have to browse the application yourself (use this option when you can’t find your target program using option 1)\nIn this case, I choose option 1; then click “Next.”\n\n, Choose your target program; then click “Next.” (Figure I.3)\n\n\nNote: This step will be skipped if you choose option 3 in step 3.\n\n,\nMicrosoft Windows NT 4.0 (Service Package 5).\nMicrosoft Windows 98/ Windows Me.\nMicrosoft Windows 2000.\nMicrosoft Windows XP (Service Package 2).\nMicrosoft Windows Server 2003 (Service Package 1).\nDo not apply a compatibility mode.\nFor the first six options, choose the one that are recommended for your program or that you had previously run your program successfully on.\nChoose the last option only if you know that with some changes, your program will work in Vista as well.\nThen click “Next.”\n\n,\nMost old games only support this much colors.\n640 x 480 screen solution.\nDisable visual themes.\nTry this option if you are experiencing problems with menus or buttons on the title bar of the program.\nDisable desktop composition.\nTry this option if you are experiencing problems with the display when running the program.\n\n,\n\n\nTry this option if you are experiencing problems with the program display on high DPI settings.\nThen click “Next.”\n\n,\nNote: choose this option only if you are administrator and your program requires administrative rights to perform itself (such as, installer and system utilities, and older programs created for Windows 98 or earlier)\nClick “Next.”\n\n, (Figure I.6)\n\n\nClick “Next” to test the program.\nNote: if Windows asks you for permission, allow it.\nAfter running the program, go back to the wizard to perform step 9.\n\n,\nYou satisfy and want to apply the changes, you have made so far, to your program.\nNo, try different compatibility settings.\nYou want to try different settings.\nNote: by taking this option, the program will bring you back to step 5.\nNo, I am finished trying compatibility settings.\nYou want to quit the wizard without applying any changes.\n\n,,\n\n\nClick “Finish” to quit and apply all your settings so far.\nClick “Cancel” to quit without any changes.\nClick “Back” if you want to change some of your settings.\n\n",
        ";\n,\nAfter unwrapping the peanut butter cups place them in the freezer until needed, so they will be inserted easier later and stay firm.\nMake sure to unwrap the peanut butter cups before the mini muffin tin comes out of the oven, will be needed immediately.\n\n,\nIf you are not using a mini muffin tin, place the foil mini muffin tin liners on a sheet pan and make sure they are not touching.\n\n,\nIf you are not using pre-portioned chocolate chip cookie dough, fill the mini muffin tin liners until they are about 3/4 full.\nThere is no need to shape the cookie dough in the mini muffin tin liners; they will bake to be the right shape.\n\n,,\nDo not bake the cookies until the center is firm or the peanut butter cups will not insert as easily.\n\n,,\nIf you do not do it immediately the cookie may get too firm and the peanut butter cup will not insert easily. The cookie may crack if not done immediately.\nFreezing the peanut butter cups is useful in this step so that they do not melt as quickly as you are inserting the rest of the peanut butter cups. This also makes it so the peanut butter cups stay in their original form.\n\n,\nBe sure they are completely cool so they do not lose their shape when being taken out of the pan.\n\n,\nTake a knife and place it in between the mini muffin tin and the mini muffin tin liner to prop the Peanut Butter Cup Cookie Bites out of the pan.\n\n,,",
        " If you don’t already have a pet, you should think carefully about the specific animal and/or breed before you get one.\n\n\nAdopt a dog that is not high energy, like an English Bulldog. For a list of high energy or low energy dogs, look at: http://www.canismajor.com/dog/apart.html.\nAdopt a smaller dog that takes up less space, like a Chihuahua or a Shi Zhu.\nAdopt a cat who has already lived in an apartment or a cat that you know will be better off not roaming.;\n, Many apartment communities will charge pet owners a number of fees depending on the size, breed, and type of their pet. Some of these charges can be very high, so consider them before moving in or getting a new pet.\n\n\nPet rent. This could vary from $5-$10 a month to much higher.\nPet fee. Often these are one time fees of several hundred dollars.\nAn additional security deposit. These may or may not be refundable.\n\n, Often times, even the most well-behaved dog or cat will cause some damage in a home. While your security deposit or pet deposit might cover some of these costs, be prepared to cough up the amount of money needed to make repairs to the apartment before you vacate.\n\n, Most dogs need lots of space to run and play. Cats, too, need space to roam as well. Apartments, by their very nature, offer limited space. Consider the following:\n\n\nYour cat probably won’t be able to roam freely outdoors.\nYou’ll have to walk your dog before and after work, rather than just letting them out into your fenced yard.\nYour future apartment community might not have a lot of green space. Many dogs are picky about where they pee and poop. If your apartment community does not have much green space, chances are it’ll be saturated with the pee and poop of other dogs, and your dog might have a tough time finding a spot.\nLook for apartment communities that have one or more dog parks., When you live in a house, if you want to walk your dog, you can simply go out the front door. For many people, apartment living is much more of a challenge. Consider the following:\n\n\nIf you live on a second or third floor, dog walks might turn into a huge pain.\nIf you live on a higher level, bringing your sick or injured pet to the car will become very difficult.\nBringing dog poop bags, pee pads, or litter to the garbage could also become a large chore., Neutering and spaying will take care of several problems associated with pet ownership. This is highly recommended for apartment living, and for pet owners in general.\n\n\nFor the owners of male cats, your cat might be less prone to spraying after being neutered.\nSpaying will also eliminate problems associated with cats and dogs in heat.\nNeutering male cats and dogs might make them less aggressive and could reduce destructive behavior.\n\n, Much like with smells, sounds travel easily in apartment buildings. You’ll have to consider your neighbors below, above, and those on the other side of shared walls. Think about the following:\n\n\nYour dog might bark when you’re not home, disturbing your neighbors.\nYou and your dog might cause a lot of noise if you play inside. While throwing a ball in your living room in a single family home might seem like no problem at all, the associated noise in an apartment building will certainly disturb your neighbors.\nAny other noise that might be related to your pet that could potentially disturb your neighbors., Most apartment communities require that you inform them of the type and breed of pet before you move in. If you don’t clear it with your landlord, you could face fines, fees, or even eviction. Consider the following when looking to acquire a new pet or picking an apartment community:\n\n\nWhether the community allows dogs or cats.\nThe size or weight limit of pets allowed in the community.\nBreed restrictions for the community or even the county or municipality.\nThe number of pets your community will allow.\n\n, Most apartment communities require a vet reference and documents detailing vaccinations. If your pet is not up to date on his/her vaccinations, you need to consider this, and be ready to get him/her vaccinated before you move in.\n\n\nMake sure to be signed up with a local vet who has your complete file on hand\nMake sure to have vaccination documentation on hand, as most apartment communities require these before they will approve your pet.\nMake sure to have references from vets, neighbors, or former landlords who will vouch that your pet is well-kept and well-behaved., If you have a dog, you should purchase baggies and be prepared to pick up your dog’s waste every time it poops. There are a number of reasons for this:\n\n\nRespect your neighbors. You want to keep your apartment community looking and smelling nice.\nRespect your neighbors and maintenance workers who don’t want to step in dog poop.\nYou could face fines from the apartment community if you don’t pick up your dog poop.\n\n, Apartment communities are unique in that you’ll be living in relatively close proximity to other people. Smells can often travel through hallways and from one apartment to another. As a result, you want to make sure you clean your litter box regularly in order to prevent your neighbors from also living with the smell of your cat.\n\n, The central feature of apartment communities is shared space. As a result, you don’t want to be letting your dog or cat roam freely in your community. Keep your dog on a leash, and keep your cat inside (unless you want to walk him/her on a leash, too). There are a number of reasons for this:\n\n\nSome pet owners, although required to by the community, might not vaccinate their pets. Letting your pet roam would make him/her more vulnerable to parasites and other illnesses.\nOne of your neighbors might accidentally hit your pet with their car.\nYour cat or dog might get into a fight with another cat or dog.\n\n, Some dogs and/or cats are very nervous when they meet new people. You want to make sure that your dog or cat won’t attack or be overly aggressive to neighbors or unsuspecting maintenance workers.\n\n\nConsider enrolling your dog in obedient school.\nTrain your dog to not lunge, growl, or bark at people on the street.\nAlthough you shouldn’t let your cat roam in the first place, if your cat tends to escape frequently, make sure he or she is not aggressive. This will help minimize potentially costly or harmful cat fights., One of the biggest challenges of apartment living with dogs is the limited ability to go to the bathroom. As a result, you need to be diligent about scheduling times for your dog to go to the bathroom. Consider the following:\n\n\nYour dog should be allowed to have potty time at a minimum of 3 times a day.\nConsider having someone visit your home while you are at work to take your dog to the potty around lunchtime.\nIt is mean and unhealthy to make your dog hold his/her pee and poop for long periods of time., Section off an area of your apartment for your pet's toys, food and water bowls, and beds. Since they’ll be confined to a small area, you should do your best to create an environment that is comfortable and stimulating for them.\n\n, All pets, even low energy ones, need exercise. While you might be able to exercise your cat inside by playing with them, your dog will likely need to spend a substantial time outside several times a week. Consider doing the following:\n\n\nPlan long walks for your dog at least three times a week. Devote 30 minutes to an hour walking around the community or the surrounding area.\nLocate a dog park near you. This way, you’ll be able to let your dog off leash and he/she will be able to run around and play with other dogs. Be careful, though, and supervise your pet. You don’t want a dog fight to break out.\nHire a dog walker or find a relative or a friend to walk your dog if you don’t have enough time. Many communities have dog walking services that will, for a relatively low fee, come to your house and walk your dog., Some first-time cat owners discover to their dismay that Garfield leaves little \"presents\" for them, if their litter is not kept clean. If you don't clean your litter box, you'll feel like your living in a litter box. Consider:\n\n\nMaking sure you have the right sized litter box for your cat's breed.\nMaking sure you purchase litter that is agreeable to your cat.\nSetting a schedule to change your litter box.\n\n, Get a proper wire kennel or crate, and work with your dog until he/she is completely trustworthy in the house. Diligence is a must; so until your dog is completely housebroken, do not let them have the run of the house unless you can be there every minute to watch them.\n\n\nKeep your dog in the room with you with the other doors shut until he is 100% potty trained.\nProvide pee pads for puppies or older dogs who have a hard time holding their pee.\nDon’t yell at your pet if he/she makes a mess. Positive reinforcement is the best route to housetraining., This loosens dead hair that's about to be shed, removes dander, and keeps your dog or cat's coat healthy and looking fine. This will also help keep your home cleaner and smelling better.\n\n, Providing your pet with toys is an important part of keeping a happy pet in any environment. This will not only benefit your pet directly, but it will benefit you. Toys will help keep your pet entertained during times you might not be able to devote much attention to your dog or cat.\n\n\nChewing for a dog will relieve nervous energy and help promote good dental hygiene.\nCats like to scratch things and sharpen their claws, scratching posts will indirectly protect your furniture.\nProviding a nylabone or another safe chew toy is far superior to losing the knob of a kitchen cabinet.\n\n, Feeding your pet people food encourages begging and bad behavior, and it might also contribute to them making a mess in the house. Feed your pet a good quality food on a set schedule.\n\n, Keeping up to date with immunizations is especially important since your pet will probably be exposed to other pets that could carry parasites or other contagions. Keeping your pet healthy also insures that any messes they make are just mischief or lack of training and not illness.\n\n, Cats and many dogs continually shed hair. They also bring dirt in from the outside and produce other forms of grime. In the small space of an apartment, this hair and dirt can quickly turn your home into a unwelcoming environment. Vacuuming often, at least twice a week, will help keep your apartment clean and create a better environment for you and your pet, not to mention visitors.\n\n, Cats tend to like to pull their food out of the bowl then chew it off to the side, leaving bits of kibble or canned food all over. Some dogs do the same thing. Using a mat underneath will help keep your home cleaner.\n\n, Pet dander and hair accumulate very quickly in small homes with pets. Dust and dander may aggravate allergies and contribute to decreased air quality in your home. Make sure to dust your home at least once a week.\n\n, Rugs and doormats will help decrease the muck and grime that your pet brings in after you’ve gone on your daily walks. Rugs will also help collect dander and hair, making your home easier to clean. Rugs, especially small ones, can easily be thrown in the wash.\n\n",
        "Heeyyyyy I just spent two months in Central America researching this exact question and consulting with the experts of banana pathology and the top figures of the banana industry.  First, I must say that your hypothetical scenario is already happening.  Panama disease (Fusarium oxysporum cubense) is what wiped out the Gros Michel.  Since new strains of Panama disease have popped up, that first one was named \"Race 1.\"  Race 4 is now spreading around Asia.  As soon as it gets to the Americas, it's basically game over for Cavendish exportation.\n\nSo which will be the replacement?  What I found was that the top dogs of industry basically refuse to think about this possibility.  First, they say that Race 4 has been contained in Asia long enough that it looks like it won't reach the Americas.  While I was there, it was officially confirmed in Jordan.  There are even whisperings in Brazil and Bolivia.  It's going to happen.  The pathologists all know it.  The most reasonable industry execs and the pathologists have been concerned with making contingency plans for containing Panama disease once it appears - quarantines, burning plantations, etc.  But that will only slow the spread.\n\nThe truth is that none of the top figures in the banana world are really planning to replace Cavendish.  They don't want to.  But they're dreaming.  There are breeding facilities around the world, but the best one - the most prominent, oldest, and that which has produced the most results - is FHIA (Fundación Hondureña de Investigación Agricola, or the Honduran Foundation of Agricultural Research).  FHIA has bred a number of bananas that are resistant or tolerant to Panama disease race 4, as well as other banana maladies like black sigatoka.  Some of the other breeding institutions have made some resistant bananas, but nothing like FHIA quality.  There is also some GMO banana research, particularly in Belgium, but again, nothing resultant quite like the FHIA hybrids.\n\nSo I do honestly believe that when the crisis hits, perhaps with the urging of some of the pathologists/breeders, worried exporters will start to incorporate FHIA hybrids and those will eventually take over.\n\nSo which are the top contenders?  Basically, as of now, I see four FHIA hybrid bananas that could potentially become the staple of banana exportation.\n\n1. FHIA-01 (AKA Goldfinger).  FHIA's first success.  A very strong banana, but its flavor is probably not what the American and European market would want.  It was bred from a Brazilian prata (apple-flavored banana).  People in Brazil like apple-flavored bananas, which are tart more than sweet.  But Americans who have tried them don't like them.  Honestly, neither do I.  I don't think this one is it.\n\n2. FHIA-02 (AKA Mona Lisa).  It's a little sweeter than Goldfinger, but still more tart, and it has lots of post-harvest problems.  For example, the fingers (individual fruits) tend to fall off the bunches, and they ripen somewhat inconsistently.\n\n3. FHIA-17.  This was bred from Gros Michel, so it tastes more like Gros Michel.  It's sweet and strong.  However, people in industry don't really think of it as a viable contender for the export market because it isn't like a Cavendish, since it was bred from Gros Michel..  True, but Gros Michel is better than a Cavendish!  They're worried about familiarity, but when Gros Michel disappeared, Americans caught on to Cavendish (which was inferior).  So why wouldn't we eventually catch on to a banana that's somewhat unfamiliar but better?  This would be a great replacement, in my opinion.\n\n4. FHIA-23.  Another Gros Michel hybrid.  Same deal as FHIA-17.  \n\nThe FHIA hybrids are not perfect.  They don't have as good yield as Cavendish, and they take a long time to harvest and ripen.  The big companies like Chiquita and Dole are pretty secretive about their research, but I believe that they are working, either with FHIA or on their own, to breed a resistant Cavendish cultivar.  This is a very difficult task.  Cavendish is nearly impossible to breed - it basically never gets seeds, which are needed for breeding, whereas Gros Michel gets them once in a while.  The top banana pathologist in Central America (who used to work at FHIA, a huge figure in the banana world) basically told me it wasn't going to happen.\n\nBut it might be some time before Panama disease Race 4 forces substitution.  Maybe the GMO people will make some progress, but that would require not only success on their part, but also changes in public perception and laws against GMOs (come on people!).  But I'd say that if this happened tomorrow, it would be FHIA-17 or FHIA-23.\n\nI know a LOT about bananas and would be happy to answer other questions.",
        "The Robert L. Spotswood House, also known as the J. Clyde Glenn House, is a historic residence in Mobile, Alabama, United States.  It was built in 1926 in the Spanish Colonial Revival style.  The house was placed on the National Register of Historic Places on July 12, 1991, as a part of the Spanish Revival Residences in Mobile Multiple Property Submission.\n\nReferences\n\nNational Register of Historic Places in Mobile, Alabama\nHouses on the National Register of Historic Places in Alabama\nHouses in Mobile, Alabama\nHouses completed in 1926",
        "The paper studies the impact of using customized number representations on accuracy, speed, and energy consumption of neural network inference. Several standard computer vision architectures including VGG and GoogleNet are considered for the experiments, and it is concluded that floating point representations are preferred over fixed point representations, and floating point numbers with about 14 bits are sufficient for the considered architectures resulting in a small loss in accuracy.\n\nThe paper provides a nice overview of floating and fixed point representations and focuses on an important aspect of deep learning that is not well studied. There are several aspects of the paper that could be improved, but overall, I am leaned toward weak accept assuming that the authors address the issues below.\n\n1- The paper is not clear that it is only focusing on neural network inference. Please include the word \"inference\" in the title / abstract to clarify this point and mention that the findings of the paper do not necessarily apply to neural network training as training dynamics could be different.\n\n2- The paper does not discuss the possibility of adopting quantization tricks during training, which may result in the use of fewer bits at inference.\n\n3- The paper is not clear whether in computing the running time and power consumption, it includes all of the modules or only multiply-accumulate units? Also, how accurate are these numbers given different possible designs and the potential difference between simulation and production? Please elaborate on the details of simulation in the paper.\n\n4- The whole discussion about \"efficient customized precision search\" seem unimportant to me. When such important hardware considerations are concerned, even spending 20x simulation time is not that important. The exhaustive search process could be easily parallelized and one may rather spend more time at simulation at the cost of finding the exact best configuration rather than an approximation. That said, weak configurations could be easily filtered after evaluating just a few examples.\n\n5- Nvidia's Pascal GP100 GPU supports FP16. This should be discussed in the paper and relevant Nvidia papers / documents should be cited.\n\nMore comments:\n\n- Parts of the paper discussing \"efficient customized precision search\" are not clear to me.\n\n- As future work, the impact of number representations on batch normalization and recurrent neural networks could be studied.\n",
        "This paper incorporates the PL/ML sides really nicely, and I think it's plausible that highly hybridized approaches such as this might become more and more popular. Great work!\n\nA few questions/comments/things I was wondering:\n1. Were you able to experiment with longer programs or with larger integer ranges, or were there fundamental limitations that prevented this? If not, how did this degrade/improve results?\n2. C_hat is only computed once per input/output pair and not recomputed as the search produces intermediate programs; was there a reason to not include any partial conditioning, or would this likely not help much?\n3. It's true that the augmented searches reach 20% accuracy on the test set much faster than baseline methods, but all methods seem to converge to 100% solving of the test set at the same time -- do you have any conjectures as to why this might be true? It seems like learning slows down enough for the baseline to catchup. ",
        "1) Summary\n\nThis paper investigates the usefulness of decoupling appearance and motion information for the problem of future frame prediction in natural videos. The method introduces a novel two-stream encoder-decoder architecture, MCNet, consisting of two separate encoders -- a convnet on single frames and a convnet+LSTM on sequences of temporal differences -- followed by combination layers (stacking + convolutions) and a deconvolutional network decoder leveraging also residual connections from the two encoders. The architecture is trained end-to-end using the objective and adversarial training strategy of Mathieu et al.\n\n2) Contributions\n\n+ The architecture seems novel and is well motivated. It is also somewhat related to the two-stream networks of Simonyan & Zisserman, which are very effective for real-world action recognition.\n+ The qualitative results are numerous, insightful, and very convincing (including quantitatively) on KTH & Weizmann, showing the benefits of decoupling content and motion for simple scenes with periodic motions, as well as the need for residual connections.\n\n3) Suggestions for improvement\n\nStatic dataset bias:\nIn response to the pre-review concerns about the observed static nature of the qualitative results, the authors added a simple baseline consisting in copying the pixels of the last observed frame. On the one hand, the updated experiments on KTH confirm the good results of the method in these conditions. On the other hand, the fact that this baseline is better than all other methods (not just the authors's) on UCF101 casts some doubts on whether reporting average statistics on UCF101 is insightful enough. Although the authors provide some qualitative analysis pertaining to the quantity of motion, further quantitative analysis seems necessary to validate the performance of this and other methods on future frame prediction. At least, the results on UCF101 should be disambiguated with respect to the type of scene, for instance by measuring the overall quantity of motion (e.g., l2 norm of time differences) and reporting PSNR and SSIM per quartile / decile. Ideally, other realistic datasets than UCF101 should be considered in complement. For instance, the Hollywood 2 dataset of Marszalek et al would be a good candidate, as it focuses on movies and often contains complex actor, camera, and background motions that would make the \"pixel-copying\" baseline very poor. Experiments on video datasets beyond actions, like the KITTI tracking benchmark, would also greatly improve the paper.\n\nAdditional recognition experiments:\nAs mentioned in pre-review questions, further UCF-101 experiments on action recognition tasks by fine-tuning would also greatly improve the paper. Classifying videos indeed requires learning both appearance and motion features, and the two-stream encoder + combination layers of the MCNet+Res architecture seem particularly adapted, if they indeed allowed for unsupervised pre-trainining of content and motion representations, as postulated by the authors. These experiments would also contribute to dispelling the aforementioned concerns about the static nature of the learned representations.\n\n4) Conclusion\n\nOverall, this paper proposes an interesting architecture for an important problem, but requires additional experiments to substantiate the claims made by the authors. If the authors make the aforementioned additional experiments and the results are convincing, then this paper would be clearly relevant for ICLR.\n\n5) Post-rebuttal final decision\n\nThe authors did a significant amount of additional work, following the suggestions made by the reviewers, and providing additional compelling experimental evidence. This makes this one of the most experimentally thorough ones for this problem. I, therefore, increase my rating, and suggest to accept this paper. Good job!",
        ",,,,).\n\n, Again, this is done by pulling gently up on the connector., Remove the 4 fat-head Phillips screws (marked in blue, two of which are under that black sticker that looks like it shouldn't actually peel up, but does, again gently using a flathead screwdriver).\n\n, Along with the four screws, they hold the lower half of the phone onto the upper half. Once you've removed the four screws, you can gently slide the bottom half of the phone forward off of the upper half.,, Pop the side rails off of the upper housing.\n\n, CAREFULLY. The little brown tab flips up (again, flathead screwdriver, but take great care not to scratch the harness with the edges of the screwdriver) and the harness should slide out with a gentle pull on the harness itself.\n\n, The keypad should fall out after you've disconnected it.\n\n, The screen is adhered to a protective plastic layer on the front, so it'll sound like it's peeling off of something. Once the screen is hanging free, peel off the green piece of tape, and disconnect the camera and speaker harnesses. Again, the white tabs flip up, and the wires slide out. You can use the needlenose pliers to gently work them out if you need to.\n\n, Make sure there are no fingerprints/dust on the back of the protective plastic layer that goes in front of the screen. Wipe it off with a paper towel and some mild cleaner if there is. Avoid using cleaners, as they may make the plastic cloudy., This is where those needlenose pliers really help. Flip the white tabs up, maneuver the harness ends into the connectors with the pliers, and snap the white tabs down. Press the screen down into it's spot. There are tabs all along the edge of the screen that line up with corresponding holes in the housing. Replace the green tape.,,, They go in starting at the top, the round nub at the end slides into the housing, and the rest of them is reinstalled by pressing down gently, you'll hear some little pops as the tabs pop back in., Reinstall 6 Phillips screws. But wait, there's more! There's two different lengths of screws. The longer screws go in the top next to the camera and speaker (marked in red) and the shorter four go in the remaining holes (marked in blue).\n\n, The two metal tabs slide into the holes in the battery compartment (marked in red). You can tell if the tabs are in when the four mounting holes line up (marked in blue). Reinstall those four, fat-head Phillips screws. Replace the skinny black sticker and the white Samsung sticker.\n\n,, It takes some work, the harness folds in a funny way once it's connected.\n\n,, The circuit board and number pad should all fit nicely into the lower housing now. Make sure the main harness isn't smashed or crimped in any way underneath everything. Also be sure that the microphone is tucked down into the housing, in front of the hole.\n\n, Reinstall the six Phillips screws. Again, two lengths of screws. The shorter screws go at the bottom of the phone. Replace the rubber caps over the bottom screws.\n\n, Turn the phone on. It should display the AT&T logo, but if after that, if it just goes to white, don't panic! Turn the phone back off, wait a few seconds, and turn it back on. This time, it should display the logo and then immediately the progress bar at the bottom should appear. You're golden., Test all the buttons (side ones, too), have someone call you from another room to make sure the speaker works, check to see if vibration mode still works, and check the camera.\n\n",
        "  The common wisdom that the phonon mechanism of electron pairing in the\nweak-coupling Bardeen-Cooper-Schrieffer (BCS) superconductors leads to\nconventional s-wave Cooper pairs is revised. An inevitable anisotropy of sound\nvelocity in crystals makes the phonon-mediated attraction of electrons\nnon-local in space providing unconventional Cooper pairs with a nonzero orbital\nmomentum in a wide range of electron densities. As a result of this anisotropy\nquasi-two dimensional charge carriers undergo a quantum phase transition from\nan unconventional d-wave superconducting state to a conventional s-wave\nsuperconductor with more carriers per unit cell. In the opposite\nstrong-coupling regime rotational symmetry breaking appears as a result of a\nreduced Coulomb repulsion between unconventional bipolarons dismissing thereby\nsome constraints on unconventional pairing in the Bose-Einstein condensation\n(BEC) limit. The conventional phonons, and not superexchange, are shown to be\nresponsible for the d-wave symmetry of cuprate superconductors, where the\non-site Coulomb repulsion is large.\n",
        "Learning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.",
        "It has a nuclear reactor on board.  It's not the same kind of nuclear reactor that you see in power plants or large naval vessels; it just has a piece of radioactive material which gets hot thanks to its radioactivity.  You can harvest a little bit of electricity whenever there's a difference in temperature between two places.\n\nThe nuclear reactor is able to produce a *ton* of energy over the course of its life.  I'm fond of [this](_URL_0_) comic showing just how massive the energy density of nuclear fuels is compared to chemical fuels like coal and gasoline.  Nuclear fuels get their energy from Einstein's famous E = mc^(2), where the energy you get is equal to the mass the fuel destroys when it decays, multiplied by the speed of light squared.  The speed of light is a huge number, so you get a *ton* of energy this way.\n\nOver time the fuel runs out, though.  The rate at which fuel is radioactively decaying is proportional to the amount of fuel that's left, and the energy it produces is proportional to the amount of decay.  This means that when you first build the reactor it produces the most power, but over time it produces less and less.  Voyager uses Plutonium 238 as its fuel; Pu-238 has a half life of about 88 years, so every 88 years the power production will have dropped by half.  The other components of the reactor also degrade, so the power loss is somewhat faster than this.\n\nThe Voyager probes have been in space since 1977, so the reactors are now producing about 1/4 less power than when they were launched.  To compensate for this the probes have been shutting down systems as necessary to conserve power.",
        "Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:\n\nI don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \"as large as possible\" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).\n\nThe setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. \n\nFurthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be \"What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?\" - I don't think this is sufficiently novel nor interesting for the community.\n",
        "My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.",
        "Tracy is an unincorporated community located within Monroe Township in Middlesex County, New Jersey, United States. The settlement is located at the site of a former station on the Freehold and Jamesburg Agricultural Railroad in the southeastern edge of the township. Most of the area is forestland with some homes and light commercial businesses located along Federal Road and Tracy Station Road.\n\nReferences\n\nMonroe Township, Middlesex County, New Jersey\nUnincorporated communities in Middlesex County, New Jersey\nUnincorporated communities in New Jersey",
        ", This is located near the headlight, towards the middle front of the car. Start the pulling motion at the center of the plate and pry the plate away from its secure position just enough to get underneath it, not to completely remove it., Lift up on the lip and remove the white plastic clip.,, This could prove difficult if it is the first time removing the Impala headlight, as the plastic of the headlight and the metal of the car frame sometimes stick together. Do not completely detach, as the wiring is still connected to the headlight., This allows the Impala headlight to be completely detached from the car.,,,,,,, Be sure to secure the white plastic clips in a directly straight down motion. Because there are crevices and indents within the clips, it is easy to hear the clips snap into as if it were in place when it truly is not. Be certain the clip has gone all the way down so the lip of where the headlight attaches to the clip is at the top of the clips longest straight lined indentation in order to verify the headlight is securely in place.,,",
        "  Potassium intercalation in graphite is investigated by first-principles\ntheory. The bonding in the potassium-graphite compound is reasonably well\naccounted for by traditional semilocal density functional theory (DFT)\ncalculations. However, to investigate the intercalate formation energy from\npure potassium atoms and graphite requires use of a description of the graphite\ninterlayer binding and thus a consistent account of the nonlocal dispersive\ninteractions. This is included seamlessly with ordinary DFT by a van der Waals\ndensity functional (vdW-DF) approach [Phys. Rev. Lett. 92, 246401 (2004)]. The\nuse of the vdW-DF is found to stabilize the graphite crystal, with crystal\nparameters in fair agreement with experiments. For graphite and\npotassium-intercalated graphite structural parameters such as binding\nseparation, layer binding energy, formation energy, and bulk modulus are\nreported. Also the adsorption and sub-surface potassium absorption energies are\nreported. The vdW-DF description, compared with the traditional semilocal\napproach, is found to weakly soften the elastic response.\n",
        "Emmett Reuben Hicks (March 7, 1854 – October 27, 1925) was an American lawyer.\n\nBorn in Waukau, Wisconsin, Hicks received his bachelor's degree and law degrees from University of Wisconsin–Madison and practiced law in Oshkosh, Wisconsin. He served as Wisconsin Attorney General 1899–1903 as a Republican.\n\nHicks died in Oshkosh when he was run over twice by the same car while crossing the street.\n\nReferences\n\n1854 births\n1925 deaths\nPoliticians from Oshkosh, Wisconsin\nPeople from Waukau, Wisconsin\nUniversity of Wisconsin–Madison alumni\nUniversity of Wisconsin Law School alumni\nWisconsin Republicans\nWisconsin Attorneys General",
        "This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.\n \n Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.\n \n It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.",
        "Americans generally spend 30% of their total income on housing.  That's 30% of EVERYONE's paycheck going to a single industry. That's why the housing market is such a big deal.  \n\nSo, you want to buy a house.  In the 70's/80's, this was a big deal, because you had to have excellent credit to score a sweet 15% interest rate.  If you default, you lose your house, which means you generally become a social outcast so you lose your friends and family too. \n\nThat made the loans very stable.  Folks would default on everything else in life before missing a house payment.  Eventually, banks start lowering the interest rate because it's such a safe investment for them. \n\nThe only problem is that when a bank writes a mortgage, they don't have that money back to write more mortgages for another 15-30 years.  To fix that, they will sell your mortgage to bigger bank.  \n\nSay they loan you $100k today.  In 30 years they'll make $200k after interest and payments.  But they won't have their original $100k back for 15 years.  Same goes for the other 10 people that took out mortgages this week.  So, they bundle it all together, and sell that debt package (worth 10x $200k) to a bigger bank for $1.05m, netting them a quick $50k profit.  \n\nThe big bank that bought it will eventually collect the $2m in debt they just bought over time.  \n\nOoooor they could bundle it with a bunch of other packages and sell THAT package for cash today, netting themselves a quick profit, and transferring your debt to an even bigger bank.  \n\nEventually the piles of loans get so big, it's tough to determine how credit worthy the debtors are still.  Every once in awhile, someone defaults, and that debt package becomes a little less valuable.  So, you buy insurance.  If a single loan defaults, the insurance company pays the difference in exchange for a flat rate.  \n\nBasically, the bank is gambling that more people will default this year, and the insurance company bets that their premiums will make more money than they pay out in defaulted mortgages.  \n\nAnd know what? The insurance company has insurance too.  Bets on bets on bets.  \n\nHonestly, this is all great.  Keeps the economy well funded, interest rates stay low, and everyone can have what they need to grow TODAY instead of having to wait and save.  \n\nJust one thing, it all depends on EVERYONE paying their mortgage on time.  \n\nProblem is that the low level bankers are conditioned to think in short term exclusively.  Their profit incentive is to write as many loans as possible, because whatever they write will be packaged up and sold within the week.   They'll make their quick $5k, and if you can't pay after that it's not their problem.  \n\nSo now you see strippers qualifying for 3 $500k homes because their income looks fucking fantastic TODAY, and nobody's asking how they're going to make their payments in 30 years.  McDonalds workers are finally able to buy houses for their whole families because interest rates are so low.  \n\nWhen they default, the big banks that bought the loans don't care because they just kick the folks out and file the loan insurance claim.  \n\nBut then 2008 happened.  The insurance companies ran out of cash to pay the claims and file for bankruptcy.  Suddenly the banks have to deal with the steaming pile of shit that are these loan packages.\n\nThey start to run out of cash, because folks aren't paying their mortgages.  The street level lenders can't write more loans because they're out of cash to give, and can't offload last week's take.  \n\nNow the banks own all these abandoned and foreclosed houses.  Normally they'd sell at half price, but since the entire neighborhood looks like foreclosed houses, they're lucky to get 20% of the purchase value.  \n\nAlso, nobody's buying because they can't get loans, because street level lenders are tapped.  \n\nThis means construction stops.  Those that sell parts of houses shut down, the entire lending industry has a lobotomy.  The whole economy just stops because nobody can spend money.  \n\nThat's where Obama was forced to do bailouts.  A cash pump to stimulate the economy.  Anything to put some cash back into the economy to get the machine running again.  \n\nDid we learn anything? Fuck no.  Fun fact: They're still playing the same games with Mortgages.  Funner fact: They're doing it with Student Loans too.  Funnest fact: Total student loans now exceed total mortgage AND Credit card debt combined.  And there's not even a house you can sell for quick cash when the loan defaults.  \n\nI figure within 4 years we'll have another collapse, but worse.\n\nEdit: For those of you asking if you should buy a house now or later, it depends. Houses are cheaper during a crash, but loans are more expensive and harder to qualify for. When things are good, loans are cheaper but houses are more expensive. My bet? If you're paying cash, buy later. If you're hoping for a good rate? Buy now, but responsibly. Like, 50% of what you qualify for.\n\nAlso, remember that YOUR debt situation doesn't matter. If the housing or student loan market crashes, that means the banks don't have cash. That means they don't have the cash to finance Amazon's new HQ, and all those construction workers are laid off. Those workers in turn don't make their loan payments, and the bank has less cash to lend, which means more layoffs to those depending on easy debt.\n\nEven if you don't owe anyone anything, you are affected if your employer needs loans to cover large capital projects, which is pretty much everyone working somewhere with more than a dozen employees.",
        "Usually both; although the army as a whole might carry a banner representing their employer, individual companies would want to use their own banners, since those would be the ones their men were most familiar with. However, there was no all-encompassing rule; different combinations of personal and political banners could be used, and different captains might even change their standards over the course of campaigns as alliances and allegiances shifted. An example is Giovanni de' Medici: although he was active during the early modern era, he stands out for having painted his banners black after the death of his uncle and employer, Pope Leo X. Giovanni had been in Lombardy as part of a Papal Army commanded by Prospero Colonna: the Papacy had sided with Charles V in a plot to expel the French occupiers from the Duchy of Milan and prop Francesco II Sforza (incidentally Giovanni's maternal cousin). Although Francesco was successfully propped up in Milan, Giovanni refused to return to Rome, and instead his company stayed in Lombardy to help his half-sister, the widowed Countess of San Secondo, assert her right to her late husband's fief against the claims of an ambitious relative. \n\nEven though Giovanni would later return to serve under the Papacy, he would forever thereafter be known as Giovanni *Dalle Bande Nere*, meaning Giovanni of the Black Banners. \n\nSome employers, notably the Republic of Venice, made a big ceremony of empowering commanders with banners, which in Italy were called \"*Gonfalone*\" (or, more accurately, in the plural form: *Gonfaloni*). This was particularly marked even at the individual company level, because the Venetian army was slightly less dependent on individual captains to raise fighting men: in times of war Venetian cities could be tasked with raising companies who would usually fight under their city banner (sometimes also conducting similar ceremonies to that in Venice) in addition to whatever units were raised by individual *condottieri*. However, here too there was no all-encompassing rule; although a Captain-General presented with a city's banner might be appointed to oversee that individual city's contribution to the Venetian war effort, that city might also delegate raising companies to individual captains who have their own banners. \n\nAt the end of the day, a banner or pennant needs to serve as a rallying point; a reference point for fighting men so they can keep good order. How that reference point was constructed could depend on the needs of the time, the dynamic of the particular army in question, and the whims of individual commanders. \n\n**Edit**: All this is, of course, with definitional problems regarding mercenaries set aside. I actually wrote a similar answer that goes into more depth [here](_URL_0_) that also provides more examples.",
        "Very Interesting work for extending VAE towards Bayesian non-parametric! \n\nI wonder what is the motivation of constraining \\pi to be positive and summed up to one (via taking the stick-breaking process)? \n\nI would suggest to have a comparison with our work \"Variational Auto-encoded Deep Gaussian Processes\". It is another Bayesian non-parametric VAE via using Gaussian process as the decoder, ",
        "There are a lot of different theories on this, actually.\n\n'Neoconservative' theory holds that internal conflicts in a nation come into prominence when external conflicts are minimal. The Bush Wars caused America's intolerance and outrage to be pointed outwards, and our new era of relative isolation has caused it to point back inwards again.\n\nOther theories would state that the rise in racial conflict is a result of economic depression. We don't like to see empty plates, and in hard economic times we tend to become paranoid and xenophobic, talking about spending less on the poor and disliking people we view as different from us.\n\nStill other theories would state that we don't have actual data on how good 'race relations' are and that expanded communications or simply fads have brought it to our attention right now. These fads might also cause variations.\n\nFinally, a fourth group would say that we see the past through rose-tinted glasses, and that race relations now are actually better than ever, it's just that we've finally reached a point where society in general expects police to treat African Americans with dignity. That this issue comes to the forefront is a mark on how much better things are than they used to be, that we notice every problem. Would a shooting of a black person in a white neighborhood have even made the news in 2003? Probably not, they would argue.\n\nOne or more of these is probably true at the same time, and probably some other factors I haven't outlined here.",
        "The paper presents a method to reduce the memory footprint of a neural network at some increase in the computation cost. This paper is a generalization of HashedNets by Chen et al. (ICML'15) where parameters of a neural network are mapped into smaller memory arrays using some hash functions with possible collisions. Instead of training the original parameters, given a hash function, the elements of the compressed memory arrays are trained using back-propagation. In this paper, some new tricks are proposed including: (1) the compression space is shared among the layers of the neural network (2) multiple hash functions are used to reduce the effects of collisions (3) a small network is used to combine the elements retrieved from multiple hash tables into a single parameter. Fig 1 of the paper describes the gist of the approach vs. HashedNets.\n\nOn the positive side,\n+ The proposed ideas are novel and seem useful.\n+ Some theoretical justification is presented to describe why using multiple hash functions is a good idea.\n+ All of the experiments suggest that the proposed MFH approach outperforms HashedNets.\nOn the negative side,\n- The computation cost seems worse than HashedNets and is not discussed.\n- Immediate practical implication of the paper is not clear given that alternative pruning strategies perform better and should be faster at inference.\n\nThat said, I believe this paper benefits the deep learning community as it sheds light into ways to share parameters across layers of a neural network potentially leading to more interesting follow-ups. I recommend accept, while asking the authors to address the comments below.\n\nMore comments:\n- Please discuss the computation cost for both HashedNets and MFH for both fully connected and convolutional layers.\n- Are the experiments only run once for each configuration? Please run multiple times and report average / standard error.\n- For completeness, please add U1 results to Table 1.\n- In Table 1, U4-G3 is listed twice with two different numbers.\n- Some sentences are not grammatically correct. Please improve the writing.\n",
        "The paper introduces a lightweight network for semantic segmentation that combines several acceleration ideas.\nAs indicated in my preliminary question, the authors do not make the case about why any of the techniques they propose is beyond what we know already: factorizing filters into alternating 1-D convolutions, using low-rank kernels, or any of the newer inception network architectures.\n\nI have had a hard time figuring out what is the take-home message of this paper. All of these ideas are known, and have proven their worth for detection. If a paper is going to be accepted for applying them to semantic segmentation, then in the next conference another paper should be accepted for applying them to normal estimation, another to saliency estimation and so on. \n\nAs the authors mention in their preliminary review:\n\"I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper.\"\n\nI do not see however any in-depth discussion of certain choices - e.g. an analysis of how certain choices influence performance or speed. Instead all one gets are some statements \"these gave a significant accuracy boost\" \"this helped a lot\", \"that did not help\", \"this turned out to work much better than that\" . This is not informative - and is more like an informal chat rather than an in-depth discussion. \n\nIf novelty is not that important, and it is only performance or speed that matter, I am still not convinced.\nThe authors only compare to [1,2] (SegNet) in terms of both accuracy and speed. I cannot see the reason why they do so, and they do not really justify it. According to the authors' evaluation, [1] requires ~1 sec. per frame,  while Deeplab v2, without the DenseCRF, runs at 5-8fps. \n(",
        "This paper addresses the problem of allowing networks to change the number of units that are used during training.  This is done in a simple but elegant and well-motivated way.  Units with zero input or output weights are added or removed during training, while a group sparsity norm for regularization is used to encourage unit weights to go to zero.  The main theoretical contribution is to show that with proper regularization, the loss is minimized by a network with a finite number of units.  In practice, this result does not guarantee that the resulting network will not over- or under-fit the training data, but some initial experiments show that this does not seem to be the case.\n\nOne potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters.   One disadvantage of this approach (and maybe any such approach) is that it does not really solve this problem.  The network still has several hyperparameters that implicitly control the number of units that will emerge, including parameters that control how often new units are added and how rapidly weights may decay to zero.  It is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches.  In fairness, the authors do not claim that they have made training easier, but it is a little disappointing that this does not seem to be the case.\n\nThe authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically.  This is potentially important, because smaller networks can reduce run-time at testing, and power consumption and memory footprint, which is important on mobile devices in particular.  However, the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks (eg., by pruning trained networks) so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks.\n\nAnother potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time.  Therefore, training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters. In practice, many parametric approaches require methods like grid search to choose hyperparameters, which can be very slow, but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy.  This means that the cost of grid search is not always paid, but the slowness of the authors’ approach may be endemic.  The authors do not discuss how this issue will scale as much larger networks are trained.  It is a concern that this approach may not be practical for large-scale networks, because training will be very slow.\n\nIn general, the experiments are helpful and encouraging, but not comprehensive or totally convincing.  I would want to see experiments on much larger problems before I was convinced that this approach can really be practical or widely useful.  \n\nOverall, I found this to be an interesting and clearly written paper that makes a potentially useful point.  The overall vision of building networks that can grow and adapt through life-long learning is inspiring, and this type of work might be needed to realize such a vision.  But the current results remain pretty speculative.\n",
        "  We study gravitational waves from a particle moving around a system of a\npoint mass with a disk in Newtonian gravitational theory. A particle motion in\nthis system can be chaotic when the gravitational contribution from a surface\ndensity of a disk is comparable with that from a point mass. In such an orbit,\nwe sometimes find that there appears a phase of the orbit in which particle\nmotion becomes to be nearly regular (the so-called ``stagnant motion'') for a\nfinite time interval between more strongly chaotic phases. To study how these\ndifferent chaotic behaviours affect on observation of gravitational waves, we\ninvestigate a correlation of the particle motion and the waves. We find that\nsuch a difference in chaotic motions reflects on the wave forms and energy\nspectra. The character of the waves in the stagnant motion is quite different\nfrom that either in a regular motion or in a more strongly chaotic motion. This\nsuggests that we may make a distinction between different chaotic behaviours of\nthe orbit via the gravitational waves.\n",
        " Do you know all of the people in the photos? Do you know where or when the photos were taken? Is this information clearly shown in the album? Sit down with your relatives and have an \"album party\" where you go through each photo and record who is who and any other information they know. You will learn more about your family than you ever knew before. You'll want to record those family stories as well. Record the entire session using a tape recorder if you have one. I recommend using small post-it notes to record information about each photo. You can then place the note near the photo in the album. DO NOT write on the actual photos to record the information, it will damage them permanently. And be sure that the stickiness of the post-it note will not damage the photos as well. If you don't have post-it notes or don't want to chance damage to the photos, then record the information on index cards or a legal pad, carefully recording the information for each photo from each page. Make sure you record it in such a way that it will make sense to you later after all of your relatives have gone home.;\n, Do the pictures come off or out of the pages easily? Can you replace them when you are done? Does the album come apart so that you can handle each page separately? Once it comes apart, can you put it back together again? Are you going to replace the old photo album with a new one? All of these are important factors in how you handle the processing of the album. If the photos cannot be removed, then you will probably be scanning the photo from the album page itself. If the photos can be removed and replaced, you will probably be scanning them off of the page directly.\n\n, Accept it now and move on with the scanning. Yes, you want to get the best quality scans from your efforts, but there will always be some degradation from the original. The scans will never be as crisp or exactly the same colors. There are ways to deal with that later. But just having a decent digital version of the photo, especially older photos, is a big step in the right direction, so don't let desired perfection get in the way of your actual work.\n\n, Scanning each photo individually would be ideal, but it will take much longer. Put as many photos as you can reasonably fit into each scan and call this a \"raw\" scan. You can go back later and pull out each photo individually.\n\n, Doing this will ensure that the information you recorded will be associated with the photo permanently in the \"raw\" scan. This will help later when you go to process the raw scan into individual images.\n\n, You could just use consecutive numbers for each scan, like \"000-raw\", \"001-raw\", \"002-raw\", etc. Or you could use a description for each page, like \"page-01-raw\", \"page-02-raw\", \"page-03-raw\", etc. Think ahead about this. If you cannot remove the photos from the album pages and the pages are 10\" x 12\", chances are that you will need to do more than just one scan per page to get all of the photos on the page. How will you name your files then? \"page01-1-raw\", \"page01-2-raw\", etc?\n\n, Going higher in the resolution or scanning in more colors will take proportionately longer and is probably not worth the effort. 300 dpi and millions of colors is typically sufficient and should match images that you would take with a modern digital camera.\n\n, If you are using Photoshop store them in Photoshop format (.psd). Otherwise, store them in TIFF (.tif) format with the \"no image compression\" option. DO NOT store your original scans in JPEG format (.jpg or .jpeg). JPEG is a \"lossy\" format which means that image information will be thrown away in order to compress the image to a smaller file size. You may not see a big difference when you look at the scanned images immediately, but later when you start to process and modify the images, more and more image information will be lost. So, start with a format that does not remove any image information to start with. Any modern image processing software should at least support TIFF format. The file sizes will be larger, but it will be worth it for long term storage.\n\n, All of this will mean better scan quality.\n\n,, Using your image software crop and copy each photo into its own file. Rotate and adjust the photos so that they are oriented correctly (sometimes when scanning it may be easier to scan a photo sideways or even upside down).\n\n,, For example, \"001-1\", \"001-2\" or \"page01-01-1\", \"page01-01-2\". When looking at the files in the file system, the files will be \"grouped\" together by their file names and you will know which raw file the image came from originally.\n\n, This means that your images will get expanded to fit if you decide to make your own photos. You may not want this as it may cause the image to be blurry or cause portions to be cropped out. In order to avoid this, use your image software to change the dimensions of your image canvas to 4\" x 6\" or 5\" x 7\" (whichever is appropriate for the image) without resizing the existing image. Have the original image centered in the new canvas size. Now when you send the image for photo processing, the image will be the same as the original and it will be surrounded by white space. You can cut off the extra white space and mount the photo into your own photo album.\n\n, Most modern image software, like Photoshop, allows you to add \"text layer\" to the file. This layer is maintained separate from the image itself, so the image is not modified. But now you can put the information around the image (like in the extra white space you added when you resized the canvas) and you will know the information for the photo.\n\n, Different software or services need certain image formats. Image formats come and go. Storing in multiple formats ensures that you have formats you need and formats that will probably be supported in the future. I recommend the following formats: JPEG, TIFF, and Photoshop. Scanned images should not be stored in JPEG, but processed images, when completed, should have a version stored in JPEG format. Most photo processors will only work with JPEG format when you want to print the images, so you will need to have JPEG versions. Just don't use the JPEG version for further image modification, instead use the TIFF or Photoshop versions. TIFF and Photoshop (if using Photoshop) versions are stored to maintain the lossless versions of the image and to allow for different formats to be available. Not all image software can read Photoshop format, but most can read TIFF format. Having them ensures future compatibility.\n\n, Include a \"README\" text file that explains what it is (the photo album history) and what you did (how you organized the images and files).\n\n, Tell them to include it in the photo album for future generations.\n\n,, Use the photo information you recorded as labels for the photos in your album.\n\n, If your original copy is destroyed, you have a backup.\n\n, Remember when floppy disks were 8\", then 5 1/4\", then 3 1/2\" and then hard drives came along, then CD's, and now we have DVD's? What data is stored on changes over time and it can happen quickly. You don't want your data to be \"lost\" or \"trapped\" in a format that you can no longer read or access. So, be sure you keep it in a medium that is current and supported. If you can afford to store it in the \"internet cloud,\" that would be great and it makes it easier to share it with others.\n\n",
        "This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.",
        "To understand this, you have to first understand that light is a wave, and like other waves, sometimes the wave pulls up, and other times it pulls down. The brightness at a given point doesn't matter whether the wave is pulling up or down, just on how hard it's pulling. \n\n A laser produces coherent light, so what that means is if you \"cut\" a laser beam perpendicular to its direction (think of a cross section), it would all be pulling the same way.\n\nThe rough pattern you see is called speckle. What happens is that a rough surface caused the light to bounce, and its scattered everywhere. As the light travels, it's phase (whether its pulling up or down) is a dependent on how far it travels. If the distance between two point happens to be one full wavelength, then the two points of light \"pull\" in the same direction, and end up being twice as bright. If the two points are a half-wavelength, then they pull in opposite directions and there's no light. \n\nThe significance of the coherence is that normal light is so random that all these patterns cancel each other out. With lasers, since the light is all the same to start out with, the depth of the scattering really matters.\n\n _URL_0_\n\nedit: Interestingly enough, people used to think that the speckle pattern was random and useless. Later it was discovered that the pattern is related to the surface of the material, and so laser light can be used to image the structure of surfaces.\n\neditagain: To be clear; speckle is subjective, which means that the observed pattern is dependent on from where it is viewed. Its not necessarily that the surface itself is illuminated in a speckle pattern, but that the light scattered off of the rough surface differs in phase, depending on where its scattered from. The light will then form an image on your retina that has an interference pattern. If you want to figure out whether a given point on your retina will be bright or dark, you have to consider the contribution of the scattered light off of every point illuminated by the laser. So the \"distance between two points\" is a bit of a simplification, but still correct.",
        "There are two somewhat related properties of the plastic that affect its ability to evaporate water - its specific heat capacity and its thermal conductivity.\n\nWith specific heat capacity, that is the amount of energy it takes to heat an object up to a particular temperature.  Plastic has a higher heat capacity (1.67 KJ/Kg K) vs clay (0.92, or 1 for bricks (_URL_1_).  This means that if the dishwasher doesn't heat long enough, it is possible for the plastic to actually have a lower temperature than the ceramic.\n\nSecond, and more importantly, is the thermal conductivity.  This is the ability of an object to conduct heat through itself.  This means that even though one side of an object is 100°C, the other side could be room temperature if conductivity is poor (think of home insulation).  Plastics generally have really bad conductivity.  \n\n_URL_0_ shows HDPE has a thermal conductivity of 0.42-0.51 W/mK .  Ceramics are difficult to pin down...there are a lot of variations on what type of ceramic plate you have, but the value for slate is 2.01, sandstone is 1.7, and even Pyrex is 1.005 - all higher than HDPE.  \n\nThis means that there is more energy moving THROUGH the material and that the time it takes to heat up is less as conductivity is higher.  There is less resistance to getting warm and absorbing all the energy its specific heat capacity wants.  It takes energy to evaporate water, and higher thermal conductivity allows the container/plate to recover that lost energy faster.\n\nA great example of how conductivity can really matter is if you put a stainless steel pot and a vacuum insulated container in the dishwasher.  They are made of the same material (same specific heat), but the container is insulated solely because of its shape, which helps prevent heat getting to the inside and reduces conductivity.  The vacuum insulated container may come out wet on the inside (depending on how you placed it in there).\n\nEdit - okay, I realize I was kind of high level, sorry.\n\nThe point is that some things move heat faster than others, and water evaporating takes heat away from the dish. Ceramic replaces the lost heat faster, so it can evaporate the next drop of water faster. \n\nEdit 2 - thank you /u/unclefishbits for the gold!",
        "The sun isn't combusting anything, it's a fusion reaction driven by the weight of the sun pressing down on the core. Adding more mass to the sun will make the sun burn *even hotter*. \n\nThe water also wouldn't *stay* water (especially putting that much on). The heat from the fusion reaction would quickly dissociate the water molecule into hydrogen and oxygen atoms. The oxygen wouldn't make it into the core during the normal main sequence lifetime since the outer layers of the atmosphere at those masses aren't normally convective. This would be a very strange star, though, since almost all the mass you're adding is oxygen. It's possible the normal stellar atmosphere models don't hold up well when the average particle weight is so high. 11 solar masses *is* (probably - there's a bit of uncertainty) enough to start fusing oxygen in the core eventually, once the hydrogen then helium in the core is used up. \n\nIn short, though, it wouldn't extinguish the sun, it would make the sun burn even hotter by increasing pressure on the core. The sun would then burn through the available material more quickly than it would otherwise.",
        "This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ",
        "  Recent rapid localizations of short, hard gamma-ray bursts (GRBs) by the\nSwift and HETE satellites have led to the observation of the first afterglows\nand the measurement of the first redshifts from this type of burst. Detection\nof >100 GeV counterparts would place powerful constraints on GRB mechanisms.\nSeventeen short duration (< 5 s) GRBs detected by satellites occurred within\nthe field of view of the Milagro gamma-ray observatory between 2000 January and\n2006 December. We have searched the Milagro data for >100 GeV counterparts to\nthese GRBs and find no significant emission correlated with these bursts. Due\nto the absorption of high-energy gamma rays by the extragalactic background\nlight (EBL), detections are only expected for redshifts less than ~0.5. While\nmost long duration GRBs occur at redshifts higher than 0.5, the opposite is\nthought to be true of short GRBs. Lack of a detected VHE signal thus allows\nsetting meaningful fluence limits. One GRB in the sample (050509b) has a likely\nassociation with a galaxy at a redshift of 0.225, while another (051103) has\nbeen tentatively linked to the nearby galaxy M81. Fluence limits are corrected\nfor EBL absorption, either using the known measured redshift, or computing the\ncorresponding absorption for a redshift of 0.1 and 0.5, as well as for the case\nof z=0.\n",
        "_URL_1_\n\nwatch all 3 episodes; it's literally about answering the question you posed.  These two paragraphs from the wikipedia article sum up the biggest factors:\n\n_URL_0_\n\n > The first step towards civilization is the move from nomadic hunter-gatherer to rooted agrarian society. Several conditions are necessary for this transition to occur: 1) access to high protein vegetation that endures storage; 2) a climate dry enough to allow storage; 3) access to animals docile enough for domestication and versatile enough to survive captivity. Control of crops and livestock leads to food surpluses. Surplus frees people up to specialize in activities other than sustenance and supports population growth. The combination of specialization and population growth leads to the accumulation of social and technologic innovations which build on each other. Large societies develop ruling classes and supporting bureaucracies, which in turn lead to the organization of nation-states and empires.[2]\n\n > Although agriculture arose in several parts of the world, Eurasia gained an early advantage due to the greater availability of suitable plant and animal species for domestication. In particular, Eurasia has barley, two varieties of wheat and three protein-rich pulses for food; flax for textiles; goats, sheep and cattle. Eurasian grains were richer in protein, easier to sow and easier to store than American maize or tropical bananas.",
        " The support gained from group prayer can be encouraging, especially if you're all praying aloud together in guided form. Being alone can seem more personal, as though what you are praying for is between you and God only. Which setting you feel closest to Him in may depend on your personality, as well as your mood and the circumstances.\n\n,,,, This could be done in a causal manner at any point in the day you have a minute, but is also good for last thing at night when you may be too tired to pray in a more formal way.\n\n,, Try and understand the meanings if the prayers as well as being able to sound them out.\n\n, (see Catholic).\n\n,,,,,, Different positions may have different meanings, for example kneeling suggests you are humbling yourself to pray before him.\n\n,, It's reading a passage from Scripture, meditating on it, then imagining the scene and placing ourselves in it.\n\n,,, Vernacular prayer: bringing God's name back into everyday language - 'Thank God' or 'Thanks be to God' during conversation.\n\n, If/ when you feel worried or are considering considering then jot down your hopes/ thoughts on a post-it note (or the non-branded version), date it and stick it to the sheet of paper. Put the paper somewhere you come across often, such as sticking it to your wall and watch your prayers build up. After things have been accomplished/ have passed, take the prayer note off the paper and reflect on how far you've come.",
        "Samuel Cockburn may refer to:\n\nSamuel Cockburn of Templehall (died 1614), Scottish landowner and diplomat\nCol. Samuel Cockburn (mercenary) (1574–1621), Scottish soldier who served in the Swedish army\nDr. Samuel Cockburn (physician) (1823–1915), outspoken Scottish advocate for homeopathy",
        "  We examine the relationship between little Higgs and 5d composite models with\nidentical symmetry structures. By performing an \"extreme\" deconstruction, one\ncan reduce any warped composite model to a little Higgs theory on a handful of\nsites. This allows us to use 4d intuition and the powerful constraints of\nnonlinear sigma models to elucidate obscure points in the original setup. We\nfind that the finiteness of the Higgs potential in 5d is due to the same\ncollective symmetry breaking as in the little Higgs. We compare a 4d and a 5d\nmodel with the same symmetry to the data. Reviewing the constraints on models\nrelated to the Minimal Composite Higgs (hep-ph/0412089), we see that it has\ndifficulty in producing acceptable values for S, T, and m_{top} simultaneously.\nBy contrast, in a global analysis, the Minimal Moose with custodial symmetry is\nviable in a large region of its parameter space and suffers from no numeric\ntunings. We conjecture that this result is generic for 4d and 5d models with\nidentical symmetries. The data will less strongly constrain the little theory.\n",
        "The authors propose a time-series model with discrete states for robotics applications. I think the proposed method is too simplistic to be useful in the presented form, eg. 1) the state space (dimensionality & topology) is exactly matched to the experiments 2) displacements in the transition model are linear in the actions 3) observations are one-dimensional. This seems to be quite behind the current state of the art, eg “Embed to Control” by Watter et al 2015, where a state representation is learned directly from pixels.\nFurthermore the authors do not compare to any other method except for an out-of-the-box LSTM model. Also, I feel like there must be a lot of prior work for combining HMMs + NNs out there, I think it would be necessary for the authors to relate their work to this literature.   ",
        ", This can help your gastrointestinal system which may have decreased levels of such bacteria often killed by antibiotics or off balance for various reasons, such as poor nutrition.\n\n,,, Consuming several different kinds from time to time may help maintain their balance.\n\n,,\nUse it rather than milk in cereal.\nAdd fresh fruit.\nTry a little swirl of honey, preserves, jelly, etc. not refined sugars.\nEat it as a dessert.\nEat it as a snack.\nDrink yogurt as a beverage.\nSubstitute unsweetened natural yogurt for sour cream such as for baked potatoes.\n\n, Seasonal allergy sufferers in the study consumed either a probiotic-fortified dairy drink or a placebo drink for 5 months. And at the end of the study, those who had knocked back the L. casei drinks showed lower levels of immune substances that contribute to allergies.\n\"By the age of 2 years, 35% of children in a study have developed allergic eczema, a condition in which the skin becomes irritated, red and itchy. But children who had received probiotics were half as likely to develop the skin condition. This cut in eczema risk is the most spectacular, single result to come out of studies on preventing allergic disease. Exactly why friendly gut bacteria might protect against allergies is unclear, but the effect may be an extension of the hygiene hypothesis. This hypothesis holds that the worldwide growth in allergic disease is in part due to our increasingly sterile surroundings. When babies are exposed to germs early on, some experts suggest, their immune systems are steered toward infection-fighting mode -- and away from the tendency to overreact to normally benign substances. Support for this idea comes from studies showing that infants who have more colds and other infections have lower asthma rates later in life. The results of this study suggest that intestine-dwelling bacteria may also play an important role in pushing the immune system away from allergic reactions.,,",
        "The reviewers unanimously recommend rejecting the paper.",
        "Added more data as shown on the conference poster, per audience request.",
        "In geometry, a facet is a feature of a polyhedron, polytope, or related geometric structure, generally of dimension one less than the structure itself. More specifically:\n In three-dimensional geometry, a facet of a polyhedron is any polygon whose corners are vertices of the polyhedron, and is not a face. To facet a polyhedron is to find and join such facets to form the faces of a new polyhedron; this is the reciprocal process to stellation and may also be applied to higher-dimensional polytopes.\n In polyhedral combinatorics and in the general theory of polytopes, a facet of a polytope of dimension n is a face that has dimension n − 1. Facets may also be called (n − 1)-faces. In three-dimensional geometry, they are often called \"faces\" without qualification.\n A facet of a simplicial complex is a maximal simplex, that is a simplex that is not a face of another simplex of the complex. For (boundary complexes of) simplicial polytopes this coincides with the meaning from polyhedral combinatorics.\n\nReferences\n\nExternal links\n\nPolyhedra\nPolyhedral combinatorics\nPolytopes",
        "This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.\nThey propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.\nSuch agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. \n\nThis work contrasts with traditional \"passive\" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.\n\nTo me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the \"possibility\" of control that an agent has over the environment.\nThe proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.\nThe methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.\nI think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.",
        ";\n,\n\n\nWhat about self-controlled flight of the smallest gnat or mosquito with amazing \"piloting\" skills—showing very interdependent activity: could that instinctual ability be by intelligent design. \"Was transmitting of instinct by chance?\" Creationists say it was intelligent design not chance.\nMosquitoes avoid imminent danger. \"What danger. Why react if by chance; why phobia, no reason, if no design?\"\nPrecision use of senses for coordinated physical action, simple as crawling or walking or as organized as being coordinated with eyesight\"Why is there seemingly quite purposeful actions like motility, travel, feeding, digestion, reproduction, etc. Could that be by purposeful design?\" It is to creationists.\n\n, (These are some expectations of many who propose evolution as fact or theory).\n\n, It is related to the mentioned presuppositions and seems nearly dogmatic. Evolution does not meet science's own standards of what has to be found in order to be highly verified data while considering other viewpoints.\n\n,,,,, Are there no other choices? How can non-designed, intricate inter-dependencies at every level of life be intricately linked through mutual dependency — \"with there being absolutely no driving reason to do so\"?, Ask yourself what, indeed, would be the chance of such a contrivance to survive genetically through the eons? How could it ever be possible that evolution not be immune to a sterile, \"random chaos\" driving mechanism? Evolutionists may argue that all non-directed, undesigned, purposeless features get passed on through succeeding generations (presupposing unplanned origins). Whether or not they are beneficial, as there is no awareness of the process by the genetic system, how would the unrelated parts of a body, leg and/or rudimentary brain now all be correlated, by independent mutations-- billions of \"failures\" of the previously existing genetic processes corrupted with deadly \"messes\" from day one. Is it not just as logical that all mutations would be passed on and precipitate a total failure and extinction due to the extreme harshness of the milieu of life? How would anything still be alive at all?\n\n, But, natural selection includes, originally, reproductive genetics from totally non-living material without a reason or known process to begin to truly live with digestion and energy. Modern science hasn't done that (has not formed a functioning, genetic, living cell from basic materials that were not provided by a preexisting life), but ancient dust, water and random stuff could do so?\n~ Realize that the thousands and perhaps millions of related, (prerequisite) processes had to exist in exquisite detail, simultaneously before that basic life's previously unplanned formation, could bootstrap its own creation. All spontaneously fleshed out (yet not knowing how or why). It then had to sustain itself all by spontaneous, undesigned/non-purposeful means. Yet, it had to exist successfully (having inadvertently been alive and reproductive by no known means or reason). Then its genetic plans changed, spontaneously, called mutation, \"ie: failure\" of a previous genetics, map/plan, that had worked (\"and it was good\").\n\n\nStudy the pressure that brought us into modern life! \"It\" had selected, trillions of times, and \"it\" was good? That it is brainless, thoughtless mother nature: \"invented\" with no ideas; made genetic \"blueprints\" by anarchy, with no form of governance/not even kindergarten level skills, not even finger paint design capability. It just \"used\" sterile lava that had eroded into sterile, lifeless dust plus water, acids, bases, salts, static electricity. Dust, mother earth's chemicals were spilling all over, lifeless stuff, life-free watery, primordial-soup of \"sterile mother nature\" became life, with incredibly, interdependent processes. Life had to \"pre-learn\" self-maintenance, to feed itself, digestion, reproduction, making instincts before there were suggestions. Starting, basically from \"lava-dust soup\", in tidal pond or near warm water vents in oceans, primitive, chaotic stress, pressured to \"form life\". That's the stuff: you see how it worked, chance, from dust to dust; ashes to ashes...\n\n,?, Then joints were mapped by genetics, formed for the upper and lower parts of a leg or wing, while growing longer -- eventually forming an extremity, whether scaled, feathered or furry or not. If that joint were even a little bit wrong or out of place there would be no success, extreme discomfort to the bird/bat/mammal... even if the joint could flex and move.\n\n, In essence, this would require: a nervous system, high symmetry, mobility, balancing capacity and coordination.\n\n, \"Do they clearly link to the next form.\", \"Where are these stages to be as conclusive as many claim evolution to be...\"\n\n",
        "  We study the application of AdS/CFT duality to longitudinal boost invariant\nBjorken expansion of QCD matter produced in ultrarelativistic heavy ion\ncollisions. As the exact (1+4)-dimensional bulk solutions for the\n(1+3)-dimensional boundary theory are not known, we investigate in detail the\n(1+1)-dimensional boundary theory, where the bulk is AdS_3 gravity. We find an\nexact bulk solution, show that this solution describes part of the spinless\nBanados-Teitelboim-Zanelli (BTZ) black hole with the angular dimension\nunwrapped, and use the thermodynamics of the BTZ hole to recover the\ntime-dependent temperature and entropy density on the boundary. After\nseparating from the holographic energy-momentum tensor a vacuum contribution,\ngiven by the extremal black hole limit in the bulk, we find that the boundary\nfluid is an ideal gas in local thermal equilibrium. Including angular momentum\nin the bulk gives a boundary flow that is boost invariant but has a nonzero\nlongitudinal velocity with respect to the Bjorken expansion.\n",
        ";\n, be sure to know what sort of area is up next, and the enemies you will be facing.\n\n,,,,,, On harder difficulties, they can have as much as 8,000 health. Be sure to run while shooting, but be careful of teammates. On harder difficulties, it is usually good to light up a tank with a Molotov in the long run.\n\n\nWitches\n\nYou will know when you are near these by listening for a crying woman, Switch off your flashlight, and don't engage her and just keep going, if you do aggravate the witch, she will usually attack the one who aggravated her, and knock them down, keep firing at her until she hits the deck.\n\n\nBoomers\n\nThese enemies will vomit on you and attract the horde to you and any others who were vomited on. You will also be covered in vomit if you blow up a boomer that is too close to you. Make sure these guys are a bit away (shove them away if necessary) before you kill them.\n\n\nSmokers\n\nThese enemies will constrict you, stopping you from attacking, and you will be slowly dragged towards them, the best way to combat these is to have a teammate kill them while they are constricting you, causing them to release you. You can also shove a teammate to get them untangled without hurting the teammate themselves.\n\n\nHunters\n\nHunters will jump large distances, and pin you to the ground, kill these as quickly as you can, as you really won't want to be pinned down during a swarm.\n\n\n\n, There are two tiers of weapons, nearly always found with those in their tier. The first tier is the weapons you find throughout the first few safe houses, and the second tier weapons is usually found in between the safe houses as well as in the last 1 - 2 safe houses.\n\n\nThe first tier weapons are :\n\nPump-action Shotgun. This is your standard powerful, short range, wide burst weapon. Best used in room clearing, and close quarter combat. This shotgun holds 8 cartridges with 128 cartridges in reserve. This will kill the infected in one shot at close range, or injure many infected at moderate range. It becomes almost useless at long range. A trick involving all shotguns in Left 4 Dead is that while reloading the cartridges you can melee attack, which will not affect the reloading of the shotguns.\nSubmachine gun. The SMG is a fast firing but less-than-moderately powerful weapon. It is useful for things like crowd control and taking out the infected at a distance, which the shotgun cannot handle. 6-8 bullets will take care of an infected. The SMG holds 50 rounds in a clip with 480 rounds in reserve. The bullets spray very fast and will hold off the infected but as it lacks penetration and power, it cannot take on a full mob of infected.\n\n\nThe second tier weapons are:\n\nAssault Rifle. The M4 is a more-than-moderately powerful automatic assault rifle like the M4A1 in Counter-Strike, best used to take out swarms, and special infected at a distance. This assault rifle contains 50 rounds in a clip with 360 rounds in reserve. This weapon is rather like the submachine gun although it will spray slower but makes up for it in power. 3-5 bullets will usually drop an infected. This weapon has also more penetration than the Submachine gun so it is also more versatile than the submachine gun. Replace the Submachine gun with this if you found one.\nHunting Rifle. This gun is incredibly powerful. One shot at an infected will drop it. It is useful to picking off enemies from a range as it is equipped with a scope. This weapon has 15 bullets in a round with 180 rounds in reserve. This is not you average sniper rifle but more like an extra powerful assault rifle with a scope. This also has unlimited penetration for the infected, so use this wisely.\nAutomatic Shotgun. This shotgun is as powerful as the pump-action shotgun and fires just as fast as the hunting rifle. This gun can kill mobs of zombies by yourself. However like all other shotguns, the drawback is that you cannot kill ranged enemies. This shotgun holds 10 cartridges with 128 cartridges in reserve.\n\n\nSecondary weapon\n\nPistol. Pistols can be used akimbo (one gun, two hands on it) or dual wielded. One pistol will hold 15 bullets per magazine. Dual wielding allows a high magazine capacity (15 x 2 = 30)which means less frequent reload but it will also take longer to reload your guns. The higher magazine capacity of the dual wielded pistol will also be very useful if you are on the ground as it allows you to kill much more infected with the extra bullets. The pistol has unlimited ammo which is useful in picking off small amounts of the infected, saving your primary ammo for a more intense situation. Be sure to pick up a second pistol if you can.\n\n\nGrenades, or a more adept name, throwables. You can only have one grenade with you at a time. These can be found at the final safe room or when exploring. If you explore more then use often, but if you explore less then use these sparingly.\n\nMolotov's look like beer bottles with a rag hanging outside. When it hits the ground it will start a fire which spreads in a small area. This is very useful for cutting off areas, although some zombie will still run through the fire, Do not use this assuming you will be safe from enemies coming through a doorway.\nPipe bombs are what they sound like, a section of pipe with some small lights on the top. When thrown they will beep and emit a small red light which attracts the ordinary horde. They will attract the horde from everything, including boomer’s bile but will not attract specialist infected or infected when it is already attacking you. This is especially useful when an incoming mob is out to get you. It is also very useful for the finale when you have to get to the escape vehicle.\n\n\n\n",
        "Nicholas F. Oppenheimer (born 8 June 1945) is a South African billionaire businessman. He was formerly the chairman of De Beers diamond mining company and of its subsidiary, the Diamond Trading Company, and former deputy chairman of Anglo American. He is the third richest man in Africa.\n\nEarly life\nOppenheimer is the son of Bridget (née McCall) and Harry Oppenheimer, and grandson of Anglo American founder Ernest Oppenheimer (the first generation to chair De Beers, from 1929). His father was of German Jewish descent. He was educated at Harrow School and Christ Church, Oxford, where he read Philosophy, Politics and Economics, earning an Oxford MA.\n\nBusiness career\nOppenheimer joined Anglo American in 1968, was appointed a director in 1974, then became deputy chairman in 1983. He resigned in 2001, remaining a non-executive director until 2011.\n\nHe was appointed deputy chairman of the then Central Selling Organisation (now Diamond Trading Company) in 1984, and deputy chairman of De Beers Consolidated Mines in 1985. He was also appointed chairman of the Diamond Trading Company in 1985. Chairman of the De Beers Group from 1998 to 2012, he retired when the family stake was sold to Anglo American.\n\nOppenheimer appeared on the Sunday Times Rich List 2018 as the 23rd richest person in the United Kingdom, with a reported fortune of £5.5 billion. He was ranked as the richest person in South Africa on Forbes list of The World's Billionaires for 2019, with a fortune reported as US$7.3 billion and, again, on its 2020 list, with a reported fortune of US$7.6 billion in August 2020.\n\nPhilanthropy\nThe Oppenheimer family has directed much of its philanthropic efforts towards preserving the heritage and cultural importance of the Southern African region, as well as to broader community upliftment in the areas of education, health, nature conservation and the arts. Nicky Oppenheimer and his son Jonathan Oppenheimer established the Brenthurst Foundation in 2005 as a way to contribute to the debate around strategies and policies for strengthening Africa's economic performance and enabling inclusive and sustainable development.\n\nThe family has also long been involved in environmental and conservation issues. The Oppenheimer family partnered with De Beers to establish the Diamond Route in 2006 to maximise the potential of their properties for conservation, research and environmental awareness purposes. The Diamond Route links 8 sites across northern South Africa, stretching from Namaqualand on the west coast, to Kimberley, north to Tswalu in the Kalahari, and to the Brenthurst Gardens in Johannesburg, eastwards to Ezemvelo Nature Reserve and northwards to the Venetia Limpopo Nature Reserve in Limpopo Province. Since 2015 Oppenheimer is also a Rhodes Trustee.\n\nAwards\nIn 2003, the Technikon Witwatersrand awarded Oppenheimer an honorary doctorate. He received the Presidential Order of Honor (2004) from the former President of Botswana, Festus Mogae, and an honorary fellowship (2009) from the London Business School.\n\nPersonal life\nIn 1968, he married Orcillia \"Strilli\" Lasch, daughter of industrial tycoon Helli Lasch; both are Anglican. His father was born Jewish and converted to Anglicanism.\n\nPublications\n\nReferences\n\nExternal links\nBiography from De Beers\nOppenheimer's Waltham Place Gardens and Farm\nInterview in The Guardian, 2005\nProfile in The Economist, 2003\n\n1945 births\nAlumni of Christ Church, Oxford\nSouth African mining businesspeople\nLiving people\nDiamond dealers\nNicky\nPeople educated at Harrow School\nSouth African Anglicans\nSouth African billionaires\nSouth African businesspeople\nSouth African people of German-Jewish descent\nRhodes Trustees\nConservative Party (UK) donors",
        "Rockland, also known as Verdier Plantation, Schley Farm and Knode House, was built by James Verdier between 1771 and 1785 near Shepherdstown, West Virginia.  Verdier was a Huguenot, the son of a French silk weaver, who married Lady Susanna Monei and came to North America to escape religious persecution.  In America he became a tanner, with tanneries in Martinsburg, West Virginia, Sharpsburg, Maryland and Shepherdstown. His children founded Verdiersville, Virginia after his death.  The older portion of the house is stone masonry.  A brick Victorian style addition was built in 1897.\n\nBuilt largely of limestone, the two-story, five-bay center hall house has sandstone accents. A basement kitchen is accessed by a door in the gable end. The interior was remodeled with Greek Revival detailing in the nineteenth century.\n\nReferences\n\nFrench-American culture in West Virginia\nHistoric districts in Jefferson County, West Virginia\nHouses completed in 1785\nHouses completed in 1897\nHouses in Jefferson County, West Virginia\nHouses on the National Register of Historic Places in West Virginia\nHuguenot history in the United States\nNational Register of Historic Places in Jefferson County, West Virginia\nStone houses in West Virginia\nVernacular architecture in West Virginia\nVictorian architecture in West Virginia\nHistoric districts on the National Register of Historic Places in West Virginia",
        "President Richard Nixon and Secretary of State Henry Kissinger discussed bombing the dike network in a 1972 conversation on Operation Linebacker II, later published by Daniel Ellsberg:\n >  \n >  Nixon: We've got to quit thinking in terms of a three-day strike [in the Hanoi-Haiphong area]. We've got to be thinking in terms of an all-out bombing attack - which will continue until they - Now by all-out bombing attack, I am thinking about things that go far beyond. I'm thinking of the dikes, I'm thinking of the railroad, I'm thinking, of course, the docks.\n\n >  Kissinger: I agree with you.\n\n >  President Nixon: We've got to use massive force.\n\n >  Two hours later at noon, H. R. Haldeman and Ron Ziegler joined Kissinger and Nixon:\n\n >  President: How many did we kill in Laos?\n\n >  Ziegler: Maybe ten thousand - fifteen?\n\n >  Kissinger: In the Laotian thing, we killed about ten, fifteen.\n\n >  President: See, the attack in the North that we have in mind, power plants, whatever's left - POL [petroleum], the docks. And, I still think we ought to take the dikes out now. Will that drown people?\n\n >  Kissinger: About two hundred thousand people.\n\n >  **President: No, no, no, I'd rather use the nuclear bomb. Have you got that, Henry?**\n\n >  Kissinger: That, I think, would just be too much.\n\n >  **President: The nuclear bomb, does that bother you?...I just want you to think big, Henry, for Christsakes.**",
        "  Bose-Einstein-condensed gases in external spatially random potentials are\nconsidered in the frame of a stochastic self-consistent mean-field approach.\nThis method permits the treatment of the system properties for the whole range\nof the interaction strength, from zero to infinity, as well as for arbitrarily\nstrong disorder. Besides a condensate and superfluid density, a glassy number\ndensity due to a spatially inhomogeneous component of the condensate occurs.\nFor very weak interactions and sufficiently strong disorder, the superfluid\nfraction can become smaller than the condensate fraction, while at relatively\nstrong interactions, the superfluid fraction is larger than the condensate\nfraction for any strength of disorder. The condensate and superfluid fractions,\nand the glassy fraction always coexist, being together either nonzero or zero.\nIn the presence of disorder, the condensate fraction becomes a nonmonotonic\nfunction of the interaction strength, displaying an antidepletion effect caused\nby the competition between the stabilizing role of the atomic interaction and\nthe destabilizing role of the disorder. With increasing disorder, the\ncondensate and superfluid fractions jump to zero at a critical value of the\ndisorder parameter by a first-order phase transition.\n",
        "This paper argues that being able to handle recursion is very important for neural programming architectures — that handling recursion allows for strong generalization to out of domain test cases and learning from smaller amounts of training data.  Most of the paper is a riff on the Reed & de Freitas paper on Neural Programmer Interpreters from ICLR 2016 which learns from program traces — this paper trains NPI models on traces that have recursive calls.  The authors show how to verify correctness by evaluating the learned program on only a small set of base cases and reduction rules and impressively, show that the NPI architecture is able to perfectly infer Bubblesort and the Tower of Hanoi problems.  \n\nWhat I like is that the idea is super simple and as the authors even mention, the only change is to the execution traces that the training pipeline gets to see.  I’m actually not sure what the right take-away is — does this mean that we have effectively solved the neural programming problem when the execution traces are available? (and was the problem too easy to begin with?).    For example, a larger input domain (as one of the reviewers also mentions) is MNIST digits and we can imagine a problem where the NPI must infer how to sort MNIST digits from highest to lowest.  In this setting, having execution traces would effectively decouple the problem of recognizing the digits from that of inferring the program logic — and so the problem would be no harder than learning to recognize MNIST digits and learning to bubble sort from symbols.  What is a problem where we have access to execution traces but cannot infer it using the proposed method?\n",
        "[update after reading author response: the alignment of the hidden units does\nnot match with my intuition and experience, but I'm willing to believe I'm\nwrong in this case.  Discussing the alignment in the paper is important (and\nmaybe just sanity-checking that the alignment goes away if you initialize with\na different seed).  If what you're saying about how the new model is very\ndifferent but only a little better performing -- a 10% error reduction -- then\nI wonder about an ensemble of the new model and the old one.  Seems like\nensembling would provide a nice boost if the failures across models are\ndistinct, right?  Anyhow this is a solid paper and I appreciate the author\nresponse, I raise my review score to a 4.]\n\n- Strengths:\n\n  1)  Evidence of the attention-MTL connection is interesting\n\n  2)  Methods are appropriate, models perform well relative to state-of-the-art\n\n- Weaknesses:\n\n  1)  Critical detail is not provided in the paper\n\n  2)  Models are not particularly novel\n\n- General Discussion:\n\nThis paper presents a new method for historical text normalization.  The model\nperforms well, but the primary contribution of the paper ends up being a\nhypothesis that attention mechanisms in the task can be learned via multi-task\nlearning, where the auxiliary task is a pronunciation task.  This connection\nbetween attention and MTL is interesting.\n\nThere are two major areas for improvement in this paper.  The first is that we\nare given almost no explanation as to why the pronunciation task would somehow\nrequire an attention mechanism similar to that used for the normalization task.\n Why the two tasks (normalization and pronunciation) are related is mentioned\nin the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism\n(and in fact, one that is then only hampered by the inclusion of an explicit\nattention mechanism?).                    This remains a mystery.  The paper can\nleave some\nquestions unanswered, but at least a suggestion of an answer to this one would\nstrengthen the paper.\n\nThe other concern is clarity.  While the writing in this paper is clear, a\nnumber of details are omitted.                    The most important one is the\ndescription\nof\nthe attention mechanism itself.  Given the central role that method plays, it\nshould be described in detail in the paper rather than referring to previous\nwork.  I did not understand the paragraph about this in Sec 3.4.\n\nOther questions included why you can compare the output vectors of two models\n(Figure 4), while the output dimensions are the same I don't understand why the\nhidden layer dimensions of two models would ever be comparable.  Usually how\nthe hidden states are \"organized\" is completely different for every model, at\nthe very least it is permuted.                    So I really did not understand\nFigure 4.\n\nThe Kappa statistic for attention vs. MTL needs to be compared to the same\nstatistic for each of those models vs. the base model.\n\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\n\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL\napproaches make large changes to the model (comparing e.g. Fig 5) but the\nexperimental improvements in accuracy for either model are quite small (2%),\nwhich seems like a bit of a contradiction.",
        " Then, try to make the dog put his head into the collar: don't push him if he is scared, but bring the harness very slowly closer and closer to the dog head, till he will have worn it; if he gets scared, freeze where you are, let him calm down, and then continue from where you stopped;\n,, You will do this by praising him when he runs, recalling him and encouraging him to resume running when he stops.\n\n,, Also add 2 more words ( like \"SLOW\" and \"RUN\" ), when you decelerate and when you increase your speed. At the beginning, you will start on straight paths: later go where there are some crossroads too: you will repeat \"LEFT\" or \"RIGHT\" any time you turn one or another direction.\n\n, If you are not sure, ask your vet about any doubt! Then, over weeks, slowly add speed and duration. Again, if in doubt, ask your vet, and anyway, better do less than the dog could do, than doing more and hurting him.\n\n,, Keep on praising and encouraging him when he runs in front if you, pulling you. If you notice he is unsure about what to do, increase your speed to make him understand he does have to run in front of you, faster than you, even if you are slowing down.\n\n, At the beginning, do not ride the bike, but run by its side: once the dog will fully understand how to behave, you can start actually riding\n\n, This step is not much different than passing from jogging to biking, but sled for sled dog are difficult to handle if you are not used to. Also, to pull one, you need at least three dogs, of the same size and weight.\n\n, Also, this is a demanding activity so it is important your dog is fit and well muscled to avoid injuries.\n\n, Also follow his/her warnings on how to proceed.\nwhen you start, start with something that is not more demanding than your dog's usual activity: if he is used to walking for 5 minutes a day, start with a 5 minutes walk.\nadd difficulties gradually and slowly: a general rule is that when the dog, once home, stops being as tired as usual but only appears satisfied and still ready to go, it is time to add duration/speed. For example: you start off with a 20 minutes walk, and once home your dog is not willing to do anything more for the whole day; let's say that after 2 weeks, he comes home tired but after, say, 6 hours he is ready to play and run again: then, you can start adding difficulties, for example from 20 to 25/30 minutes walk\ntrain your dog for many activities: only doing one kind of activity/training and nothing else is unhealthy and can lead to injuries ( it happens with humans too ). Instead, alternate sled dog/running training with at least 2 other completely different sports, like disc dog ( lots of jumping ), agility ( jumping and sprinting ), swimming, etc. Also, teach your dog \"tricks\" ( things like: roll on the floor, give paw, etc. You can find many examples if you search for \"dog tricks\" on youtube ): they develop strength, agility and flexibility in dogs, which lead to less injuries, better performance, longer life and better health for the dog.\ndo not overtrain your dog: learn and respect his limit, never force him to do something he is scared of, uncomfortable with. Always bear in mind that doing a bit less than your dog could, will cause no damage, while doing only a little bit more, might cause serious injuries, that might take very long to heal. Also, as said before, train your dog for many different activities, not only sled dog\n\n",
        "Each station broadcasts a radio signal at a particular frequency.  If you could hear electromagnetic waves, and your hearing extended another 10-15 octaves up toward high pitch, you'd hear the stations as pure tones -- the modulation that carries the actual sonic signal has only a tiny effect on the main frequency of that \"carrier wave\".  \n\nWhat your radio scans when it is \"scanning\" is the central tuning frequency of an adjustable bandpass filter.  The antenna receives all the various transmissions in the area all at once, directing them to a tuning filter and amplifier.  The tuning filter blocks most frequencies except one.  It's adjustable.  In the old days, the tuner was an actual analog circuit made from inductors and capacitors, and adjusting the tuning knob would actually change the geometry of some metal pieces, to adjust the capacitance in the tuning circuit.  Nowadays, it's more a software thing.  Either way, as you tune it there is a wire somewhere in your radio that contains only the tiny piece of the electromagnetic spectrum that can make it through the narrow tuning filter.\n\nAnyhow, when the filter is tuned to a frequency where there is an actual station, the output signal through the filter and initial RF amplifier gets quite strong.  In between stations, there isn't \"static\", there's *nothing to receive*.  [If you hear static, it's because your radio has a special circuit called an \"automatic gain control\" (AGC) that cranks up the volume to compensate for weak signals (in AM radios, anyway -- FM and digital radios work slightly differently).  The AGC *divides* by the strength of the incoming signal, and dividing by something close to zero gives you very, very high gain -- which means your preamplifier just reports the quantum mechanical noise of the electrons rattling around its input stage.]\n\nSo when there is a non-zero signal coming out of the radio amplifier stage, your radio knows it found something.  When there is jack diddly coming out, your radio should know it hasn't found anything, but cheap or old radios don't notice that, and you hear static.\n\nSome late corrections: \n\n* thanks to /u/everyusernamesgone for pointing out that tuning isn't in software in most radios -- it uses on-chip variable components rather than those large air-gap variable capacitors, but there is still an analog variable component.\n\n* There are lots of details I glossed over in how the tuning filter works.  Most radios mix the radiofrequency down to a fixed \"intermediate frequency\" and then demodulate *that*.  If you're a pedant, you might object to calling that scheme a simple variable filter, though it acts the same as one for the purposes of tuning.  If you care, look up [superheterodyne](_URL_1_).  (Superhets are how the U.K.'s [TV detector vans](_URL_0_) work, and why you aren't supposed to use a transistor radio on an airplane -- every radio and TV receiver that uses a superheterodyne is basically a miniature transmitter too!)\n\n* In this main article, I deliberately glossed over the difference between quantum shot noise and quantum thermal noise -- they're slightly different things, and they both contribute.  In normal receivers, both noise sources are much stronger than the cosmic microwave background - many people need to unlearn that meme from some years ago.",
        "It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.\n\nWhile I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.",
        " There are certain standards you need to meet in order to be a police officer. In Ohio, police officers must meet the following minimum criteria:Be a United States citizen.\nBe a high school graduate or have a GED at a minimum.\nBe at least 21 years old when testing for the civil servants exam.\nHave no felony convictions.;\n, Highway patrolmen in Ohio have many of the same basic requirements as a police officer, but some are more specific. In Ohio, highway patrolman must meet the following criteria:\n\n\nBe a United States citizen.\nBe 20-34 years of age.\nHave a high school diploma or GED.\nBe an Ohio resident and hold an Ohio’s driver’s license.\n\n, Sheriff’s deputies must meet most of the same requirements as a police officers. However, some criteria vary by county. In Ohio, sheriff’s deputies must meet the following criteria:\n\n\nBe a United States citizen.\nBe at least 18 years old.\nHave a high school diploma or GED.\nBe able to speak, read, and write English.\n\n, All police officers must meet certain levels of physical and medical fitness. Though these tests can be challenging, they are required upon entry to any police training program.All four of these physical fitness tests break down into a cumulative 1-7 score. From this score, you are ranked based on your desirability as an applicant., In this test, you will be prompted by a proctor to complete a 75-yard run while being careful not to bump into or knock over cones or obstacles.\n\n\nYou can do this test twice and your best time counts as your final. If you do bump into any obstacles or cones, you are given a two-minute break before beginning again.\n\n, Only proper push-ups are counted. A proper push-up is considered as your hands shoulder-width apart and your feet 8-12 inches apart.\n\n\nYou must lower yourself to the floor and push back up again to where you started from. You are given two minutes to complete as many push-ups as you can.\n\n, For the sit-up test, you must complete as many sit ups as possible in two minutes. As with the push-up test, only proper sit-ups are counted. A proper sit-up is counted as you lying on your back with your knees at a 45-60 degree angle.\n\n\nWhen prompted, you must sit up until your elbows touch your thighs. Then, you must bring yourself back to the ground until your back touches.\n\n, In this run, you are attempting to go as fast as possible to complete the run.\n\n\nYour run will be measured alongside your push up, sit up, and 75-yard pursuit run for a total score. Even if you don't run extremely fast, you can get a higher score by doing well in those other four areas.\n\n, You will need to be in good health with no major conditions in order to qualify to be a police officer. Completing a full physical will generally fulfill this requirement., While all departments are different, many require that you fall under a maximum body fat percentage.\n\n\nFor women, the maximum body fat percentage is usually 30%, while for men it is 22%.\n\n, Your hearing and vision need to be excellent, though you can wear hearing aids or corrective lens to make for any hearing and vision problems you may have.\n\n, You will need to take a series of psychological tests to determine whether or not you are fit for duty.\n\n\nIt's also important that your background check shows no major history of psychological problems.\n\n, All police officers must undergo a background check. While some past history may not be relevant, there are a few red flags that may prevent you from becoming a police officer in Ohio.If you were dishonorably discharged from the military, that may be a problem if you want to be a police officer. This does not mean you failed out of basic training or were injured, but instead left under bad terms from your military service.\nHaving a poor driving record can also be a red flag in a background check. A few tickets will likely not be a problem, but any reckless driving or unsafe driving may disqualify you.\nPoor or bad credit can also be a problem for potential police officers. A history of bankruptcy or debt could potentially tempt you for bribery.\nA history of drug use or abuse can also disqualify you. If you have any drug convictions, especially felony convictions, you will not be able to be a police officer in Ohio.\n\n, Oral board interviews are one of the final initial qualifications to becoming a police officer. During oral interviews, make sure you convey a positive and respectful image that shows you are mature enough to be an officer of the law., Being late to an interview is always a red flag for potential employers.\n\n, Convey an image of positivity and confidence that shows you can be respectful and also assertive.\n\n, Do your homework so you know what kinds of questions the interviewer will have for you. This way you can speak confidently about the topics they bring up.\n\n, You should be moderate in how you are communicating to the interviewer. This reflects well on your ability to do as a police officer.\n\n, For police officers who are looking to move up in the ranks, they may need a college degree or military experience to advance. It's becoming increasingly true that police departments want their officer to have at least an associate's degree.College degrees are required for most federal police jobs. While you likely do not need a college degree to becoming a police officer in Ohio, it may be harder to move up in the ranks without one.\nAn associate's degree or at least a few years of college can be beneficial if you want to become a police officer. Any education that shows you are dedicated to improving yourself is likely beneficial.\nIf you want to work a federal policing job, or even in the Ohio Bureau of Investigations, you will likely need a bachelor's degree.\nMilitary experience is also beneficial to becoming a police officer in Ohio. While it may not help you advance your career the way a college degree may, military experience prepares you for the physical, mental, and emotional challenges of being a police officer., These tests measure your general aptitude and preparedness for police work. The Compass and Asset tests are often used by colleges to place incoming students in basic skills like reading, writing, and math. How well you do on this test can determine your placement when opportunities are available to become a police officer.\n\n\nThe Compass test is used to determine student's readiness to learn. Along with the Asset test, it places to students entering college.The Asset test is generally used by two-year colleges to place incoming students. It generally tests knowledge from secondary school and your potential to learn beyond that.Potential police officer can opt out of either test if they have a college degree.\n\n, In order to become a police officer in Ohio, you must go through OPOTA training. At this training, you will learn the basics of policing from trained instructors.The Ohio Peace Officer Training Academy or OPOTA is recognized as a national leader in training law enforcement agents. It is also internationally recognized by the Commission on Accreditation for Law Enforcement Agencies for its achievements in accreditation, compliance, policy development and other issues\nPotential police officers in the state of Ohio have to complete the basic police academy training course in a OPOTA accredited academy.\nThere are around 60 schools that in the state of Ohio that provide this training. In general, there is at least one school per county in Ohio.Schools will have to pass all requirements to be approved and accredited as a school. Before attending OPOTA, make sure your specific location is accredited by the state of Ohio.\n\n, There are two types of enrollees. Take note that some schools will accept non-appointed officers while others don't, so please check with the academy prior to sending your application.\n\n\nFirst type are sworn police officers who are either full time, part time, or auxiliary officers coming either from a city, township, or another agency who are required to complete the peace officer basic training course.\nSecond type are individuals who haven't been appointed to any police officer position, but want to attend the training academy at their own expense. These individuals are considered \"open enrollment\"\nOpen enrollment students who are appointed within a year after completing the academy training will have no additional training required.\nFor open enrollment students who are appointed over a year after but less than two years after completing the academy, training will be required to complete an OPOTC approved refresher course before being appointed.\nOpen enrollment students who are appointed more than two years after completing the academy training will be required to repeat the training again.\n\n, OPOTA training has a lot of components, including classes, physical tests, psychological tests, and written exams. Make sure to complete all required parts of your training in order to be prepared to be a police officer in Ohio.\n\n\nAll peace officer training academy students must complete at least 579 hours and any additional hours as provided by the school.\nTraining courses involve self-defense, police tactics, how to deal with suspects, diffusing a violent situation, evidence gathering, review of existing laws, and much more.\nAcademy courses typically last 6 months and possibly more. Cadets are required to live in the academy premises at least 5 days a week. In most cases, you will be paid in your tenure with the academy.\n\n, Aspiring cops need to pass this in order to be certified as a police officer. This exam will test your knowledge from OPOTA.\n\n\nOut of state appointed police officers who have completed the required training and/or education in another state can apply for a prior equivalent training analysis from the Ohio Peace Officer Training Commission.\n\n, Your previous training will be compared to the OPOTC curriculum. If found equivalent, you will be given credit.\n\n, Now that you are qualified to become a police officer, it's time to find a job. Try to look in locations in Ohio where you want to live. Also pay attention to the crime rates in areas, depending how much crime you want to handle.\n\n, Visit police departments where you may want to work to find out when they are hiring and how often.\n\n, You may want to work in an urban, rural, or suburban environment. Though you may not always get a choice, try to choose the place you're most comfortable with.\n\n, If a job becomes available, make sure to apply for it. You'll have to go through some of these steps again, but it's the best way to become a police officer in Ohio.\n\n",
        "When you put food in your mouth, the 'taste' you experience, besides its physical characteristics, is a combination of the basic flavors identified by your tongue, such as sweet, sour, etc. and the scent, which is actually picked up by your nose. Your sense of smell is such a major component of the smell of \"taste\" that if you block it, you'll have enormous trouble telling foods apart. The scents don't have to be inhaled either: your mouth hole and nostrils are actually internally connected roughly at the point where you swallow, so smells from the food you're eating will travel upwards from there.\n\nSo having explained that, the reason why sometimes you will experience an aftertaste will be more understandable to you: because the food that has sat in or passed through your mouth is considerably different from the one that you first experienced when it went in. You chewed through it, exposing its inner parts, and then mixed it with your saliva which is packed with enzymes that we have *specifically evolved for their ability to chemically break down the foods we stuff in our mouths*. This breaking-down mush then stays on your tongue, in your teeth, and in spots on your mucous membrane, and the smell makes it up to your nose. And it's changed because you changed it.",
        "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP).",
        "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper.",
        "  Determination of the cosmic infrared background (CIB) at far infrared\nwavelengths using COBE/DIRBE data is limited by the accuracy to which\nforeground interplanetary and Galactic dust emission can be modeled and\nsubtracted. Previous determinations of the far infrared CIB (e.g., Hauser et\nal. 1998) were based on the detection of residual isotropic emission in skymaps\nfrom which the emission from interplanetary dust and the neutral interstellar\nmedium were removed. In this paper we use the Wisconsin H-alpha Mapper (WHAM)\nNorthern Sky Survey as a tracer of the ionized medium to examine the effect of\nthis foreground component on determination of the CIB. We decompose the DIRBE\nfar infrared data for five high Galactic latitude regions into H I and H-alpha\ncorrelated components and a residual component. We find the H-alpha correlated\ncomponent to be consistent with zero for each region, and we find that addition\nof an H-alpha correlated component in modeling the foreground emission has\nnegligible effect on derived CIB results. Our CIB detections and 2 sigma upper\nlimits are essentially the same as those derived by Hauser et al. and are given\nby nu I_nu (nW m-2 sr-1) < 75, < 32, 25 +- 8, and 13 +- 3 at 60, 100, 140, and\n240 microns, respectively. Our residuals have not been subjected to a detailed\nanisotropy test, so our CIB results do not supersede those of Hauser et al. We\nderive upper limits on the 100 micron emissivity of the ionized medium that are\ntypically about 40% of the 100 micron emissivity of the neutral atomic medium.\nThis low value may be caused in part by a lower dust-to-gas mass ratio in the\nionized medium than in the neutral medium, and in part by a shortcoming of\nusing H-alpha intensity as a tracer of far infrared emission.\n",
        "Stalin and Hitler never met, nor did they ever actually talk to each other. The same goes for Churchill and FDR who never talked to Hitler either. The only allied leader that I can think of who actually met and talked with Hitler and then later fought him in World War Two was Canada's prime minister, William Lyon Mackenzie King. Also Chamberlain and the French prime minister Édouard Daladier. Hitler did however write Stalin a letter and he had Ribbentrop give it to Stalin personally. The letter states that both sides should unite against the allies and that the Soviets should support the German claims in Poland. Its been speculated that Stalin and Hitler may have talked one the phone, but no evidence for such a meeting actually exists. The closest Hitler got was his meeting with Soviet foreign minister Molotov in Berlin in 1940. \n\nAlso for anyone curious (and because it feels weird to have such a short post) here is a list of countries who's prime ministers or presidents met with Hitler at least once:\n\nBritain (Neville Chamberlain)\n\nCanada (PM William Lyon Mackenzie King)\n\nFrance( PM Édouard Daladier)\n\nItaly (The King- Vittore Emmanuel III and Mussolini)\n\nCzechoslovakia (President- Emil Hácha)\n\nHungary (The Regent- Milkos Horthy and the PMs Béla Imrédy and Paul Teleki)\n\nRomania (Prime ministers Ion Antonescu and Ion Gigurtu King Carol II )\n\nBulgaria (king Boris III) \n\nSlovakia (Prime Minister Jozef Tiso) \n\nCroatia (Prime minister Ante Pavelic \n\nVichy France (Prime Minister Pierre Laval and President Phillipe Petain)\n\nSpain (Fransico Franco)\n\nYugoslavia (PM-Dragisa Cvetkovic)\n\nBelgium (King Leopold III)\n\nFinland (Mannerheim)",
        " The first thing that needs to be considered while shopping for a summer beach wedding dress is the fabric. Go for lightweight fabrics to keep you cool and comfortable on your big day. You can buy a wedding dress from a local show designer but it should be made of lighter fabrics such as chiffon, crepe, cotton eyelet, cotton organdy, organza, Georgette and voile.;\n, There are many designers who have added exclusive beach wedding dresses to their collections.\n\n, This type of wedding dress is comfortable in the heat of summer and quite elegant to be the main attraction of the wedding.\n\n, Choose a dress that is casual but still elegant.\n\n,, A tea length is cooler than a full, flowing skirt, and there is no need for you to worry about the hem getting in your way.\n\n, Even a princess style gown will look great.\n\n, You can try a wedding dress in red or pastel colors like pink or lavender. Even wedding dresses in pale blue or mint green will look great on the beach for a summer wedding.\n\n, But don’t forget to dress your groom in a coordinating shirt so that you two look great together!\n\n,",
        "Colimaite, the naturally occurring analog of synthetic K3VS4, is a sulfide mineral discovered in southwestern Mexico.  The potassium-vanadium sulfide was collected from the crater of the Colima volcano.  The mineral colimaite is named after the locality of this volcano and has been approved in 2007, along with its mineral name, by the Commission on New Minerals, Nomenclature and Classification (CNMNC).  It has been given the International Mineralogical Association number of IMA 2007–045.\n\nComposition\nThe chemical formula of colimaite is K3VS4. The enrichment of vanadium in Colima's volcanic gases make it unique. The study of sulfur content in the fumaroles is also important, in order to know how an eruption could affect global climate due to SO2 emissions.\n\nStructure\nColimaite exhibits the Pnma (P 21/n 21/m 21/a) space group making it orthorhombic. The vanadium and sulfur atoms form tetrahedra.  Potassium ions separate these tetrahedra in two different ways.  In one case, the potassium ion is bound to five sulfur atoms at an average distance of 3.296 Å with an additional two sulfur atoms at a distance of 3.771 Å.  In the other case, the potassium ion is bound to eight sulfur atoms at an average distance of 3.314 Å.  In both cases, the potassium ions are in an irregular coordination polyhedron.\n\nPhysical properties\nThe formations of colimaite have been described as hedgehog–like particles due to the acicular habit of extremely fine needles forming the aggregates. The size of these aggregates range from 10 to 100 µm. The needles themselves have been measured up to 50 µm in length and 20 µm in width.  Although colimaite belongs to the orthorhombic crystal class, their crystallographic forms were not observed.  The particles were regular parallelepipeds and elongated rectangular prisms.  The color of colimaite is dark golden and opaque. The streak is a yellow green with a resinous to greasy luster.  It is non-fluorescent.  It is brittle with no observed cleavage and a splintery fracture.  Because of grain size, the hardness and density could not be measured but the density has been calculated to 2.235 g/cm3.\n\nGeological occurrence\nColimaite occurs as a sublimate from the volcanic fumaroles of the Colima volcano in Mexico. Other minerals including cristobalite, arcanite, thenardite, baryte and native gold have been collected from the fumaroles of this volcano. Although minerals were collected at temperatures from 400 to 800 °C, colimaite was assembled in a more narrow temperature interval of 450 - 600 °C. There are similarities between the volcanic gases of the Colima crater and the gases of other volcanoes, but there are some differences that make Colima unique.  Notable differences are the vanadium, zinc and copper enrichment of the Colima gases. These same gases also lack cadmium and molybdenum.\n\nSpecial characteristics\nNot only is colimaite the first new mineral species discovered in Mexico since 1998, but it is also the first newly recognized mineral species collected from the fumaroles of the Colima volcano crater. It was collected by the use of two silica tubes of one meter in length each, placed in a high temperature vent at the volcano's crater.\n\nReferences\n\nExternal links\nEntry on mindat.org\nEntry on webmineral.com\n\nSulfide minerals\nOrthorhombic minerals\nMinerals in space group 62",
        " The potsticker dough may reach a better consistency if the water is slightly warm.Boil 1/3 cup (80 mL) water, then remove from heat and add 2/3 cup (160 mL) room temperature water.\n\n\nAlternatively, heat 1 cup (240 mL) water on low heat and remove from heat after one to three minutes. If the water begins to simmer or boil, remove from heat and let cool to slightly above room temperature before using.;\n, While none of these ingredients are required to make guo tie, some recipes call for salt, vegetable oil, and/or egg.Stirring roughly 1/4 tsp (1 mL) salt (or a small pinch) into the water may add flavor. The other optional ingredients should be added to the flour instead, before you continue to the next step. Mix the flour with 1 tsp (5 mL) vegetable oil and/or 1 small egg to add flavor and help the dough stick together. Continue as usual, but be aware that the egg will add more liquid to the dough, so you will probably not end up using all your water in the next step.\n\n\nIf this is your first time making potstickers, you may wish to skip this step to keep things simple. If the wrappers fall apart or taste too bland, you may add one or more of these ingredients in your next attempt.\n\n, Put 2 cups (480mL) all-purpose flour into a large bowl. Add the warm water a little at a time, stirring the ingredients together with chopsticks or a wooden mixing spoon. Stop adding water once the dough feels slightly sticky, and there is no more dry flour visible.\n\n\nDepending on the brand of flour and the humidity in your kitchen, you may not need to use all of the water you prepared. Keep the extra water around during the next few steps in case the dough dries out.\n\n, Once the dough becomes too sticky to stir, place the dough on a clean, lightly floured surface and knead the flour and water together. The dough should become smooth within a few minutes of kneading. Stop when the dough has no lumps and can be formed into a ball.\n\n\nAdd a light dusting of flour to the work surface or your hands if the dough sticks to them. Knead in more flour if the dough is too wet to work with.\nIf you see dry flour that isn't mixed into the dough, or if the dough won't stick together into a ball, add a little more warm water and knead it in.\nRemember to wash and dry your hands thoroughly before you begin kneading.\n\n, Wrap the dough in plastic wrap or place it in a small bowl and cover it with plastic wrap or a damp cloth. This will trap the moisture released by the dough and cause it to soften.Let the dough sit for at least 10 minutes, and preferably for half an hour.\n\n\nTo save time, make the filling while you wait. This step does not have to be precisely timed, so return to the dough once you are done making the filling.\n\n, Return to the dough after it's had enough time to \"relax,\" or soften into an easily manipulated ball. Pull it apart into small pieces, each one using about 1/20th of the total dough. You may find it easier if you first divide the dough into four large pieces, then cut each of these large pieces into five smaller ones.\n\n\nAlternatively, you may use your hand to roll out the entire ball of dough into a long log 1 inch (2.5 cm) thick. Cut this log into discs 1/2 inch (1.25 cm) wide., Sprinkle flour over a flat, clean counter or cutting board to prevent the dough from sticking. Use a rolling pin to flatten each piece of dough over this surface, creating circles about 3 inches (7.5 cm) across.Use smaller circles if the dough breaks apart or looks translucent, as the dumplings may fall apart if they are rolled too thin.\n\n\nYou can speed up the rolling process by using the heel of your palm to flatten each piece of dough into a roughly circular shape before you roll it out more thoroughly.\nFilling the potstickers may be easier if you keep the center of the circle thick and the outside edges thin.\n\n, After each circle is finished, sprinkle both sides generously with flour to prevent sticking, and add it to the stack of finished dough circles. Your dumpling wrappers are now complete.\n\n, Cover wrappers with a damp paper towel to keep them moist as you work. Once you have made the wrappers, it is best to fill them immediately before they dry out. Store unused wrappers in the fridge if you will use them within a few days, or freeze them and use any time in the next few months., Finely chop the vegetables until you have 1 cup (240 mL). While you can use any hard, leafy, green vegetables, guo tie are traditionally made using Napa cabbage or bok choy. Both of these vegetables are sometimes sold under the name \"Chinese cabbage.\"\n\n\nIf you are making vegetarian potstickers, chop 2 cups (480 mL) of vegetables instead.\n\n, Toss the chopped cabbage in 1 tsp (5 mL) salt. Let sit five minutes while the salt draws out moisture, then drain the cabbage in a strainer or colander., To add a spicy flavor to the guo tie, peel fresh ginger and fresh garlic, then chop them finely until you have 1 tsp (5 mL) of each. Finally chop one or two stalks of green onion (scallions).\n\n, Mix these vegetables in a large bowl containing ground or finely minced meat. In different areas of Asia, ground pork, beef, or shrimp are all commonly used, or a mixture of these.\n\n\nWash your hands in warm, soapy water after handling raw meat to reduce the risk of exposure to harmful bacteria. Clean any surfaces or utensils that came into contact with the meat in hot, soapy water once you are done using them.\n\n, Mix in 2 tsp (10 mL) soy sauce, 2 tsp (20 mL) sesame oil or Chinese cooking wine, and 1/2 tsp (2.5 mL) pepper. There are many variations on this recipe, and you may decide to replace some seasonings or add your own. Other common options include 1/4 cup (60 mL) chicken stock or chicken broth, a dash of chili powder, or a dash of Chinese five spice powder.If you'd like to adjust the seasonings before you make the potstickers, take a small spoonful of filling and fry it in oil until it is browned through. Taste the filling and add more seasoning if necessary.\n\n, Take one of your circular potsticker wrappings and place it on the palm of the hand you use least.\n\n, Take approximately 1/2–1 tablespoon (7–15 mL) of filling using a spoon or chopsticks and place in the center of the potsticker wrapper. If the dough is thin or the circles of dough are small, use less filling.\n\n, Fold the potsticker in half to make a half-moon shape, but do not press the edges completely together. Only press the center of the edges together, so the corners of the dumpling are still unattached.\n\n\n\nNote: If you are using storebought dumpling wrappers, use wet fingers to dampen the edges until they are soft enough to press together.\n\n, Grasp one layer of dough at the corner with your index finger and thumb, then fold it toward the center of the potsticker edge, where the two sides of the circle are pressed together.The soft dough should stretch into a classic potsticker pleat or wrinkle. Press the two layers of dough together at the fold to keep it in place.\n\n, Using the same technique, grasp one layer of dough at the corner and fold it over toward the center edge. Press together with the opposite layer of the potsticker. Repeat this until there are three or four folds on each potsticker, and the potsticker is completely closed.\n\n, Add cooking oil to a wok, flat skillet, or frying pan, just enough to cover the bottom of the pan in a thin layer. Heat over medium heat until the oil begins to shimmer, or when a small piece of vegetable or filling sizzles when placed in the oil.\n\n\nUse a vegetable oil with a high smoking point, such as canola oil or peanut oil.\n\n, Carefully drop the potstickers into the pan from a short distance above the oil. Arrange them with a heat-safe utensil so the dumplings are close together but do not touch each other.You will likely need to cook your potstickers in several batches. Do not pile potstickers on top of each other in the pan, or they may not cook properly.\n\n, Cover the pan, reduce to low heat, and fry for a few minutes until the bottom of the potstickers are crisp and golden-brown. Depending on the temperature of the pan, this could take anywhere from two to seven minutes.You may lift the lid to check on the potstickers' progress.\n\n\nRemove from heat immediately if you smell burning. Use a heat-safe utensil to unstick the potstickers from the pan, and continue after one or two minutes.\n\n, Once one side of the potstickers are brown, lift the lid and pour 1–3 tablespoons (15–45 mL) water onto the pan, just enough to cover the base of the pan with a shallow layer.\n\n\nPour the water while rapidly moving in a circular motion around the edge of the pan. This distributes the water evenly and prevents one part of the pan from cooling down too quickly. This also reduces splatter from hot oil and water coming into contact.\n\n, Cover the pan again and cook on moderate or low heat for 4-5 minutes. Add more water if it boils away before the potstickers are done cooking. Note that you do not need to flip the potstickers at any point in this process; it is intentional that they are only crisp on one side.\n\n\nRemove a potsticker and cut it open to check that it is done. The filling inside should be browned and fully cooked.\n\n, Remove the potstickers from the pan and cook additional batches if necessary. Once all the potstickers are cooked, serve them with a sauce of your choice:\n\n\nAny dark vinegar can be used alone or mixed with an equal amount of soy sauce and a dash of sesame oil.\nMix vinegar with sherry or dry wine and sweet soy sauce for a sweeter dipping sauce.\nBlack pepper and sliced ginger add a sophisticated flavor, and can be provided with or without a dipping sauce.",
        "*** Paper Summary ***\n\nThe paper proposes to learn a predictive model (aka predict the next video frames given an input image) and uses the prediction from this model to improve a supervised classifier. The effectiveness of the approach is illustrated on a tower stability dataset.\n\n*** Review Summary ***\n\nThis work seems rather preliminary in terms of experimentation and using forward modeling as pretraining has already been proposed and applied to video and text classification tasks. Discussion on related work is insufficient. The end task choice (will there be motion?) might not be the best to advocate for unsupervised training.\n\n*** Detailed Review ***\n\nThis work seems rather preliminary. There is no comparison with alternative semi-supervised strategies. Any approach that consider the next frames as latent variables (or privileged information) can be considered. Also I am not sure if the supervised stability prediction model is actually needed once the next frame is predicted. Basically the task can be reduced to predict whether there will be motion in the video following the current frame or not (for instance comparing the first frame and last prediction or the density of gray in the top part of the video might work just as well). Also training a model to predict the presence of motion from the unsupervised data only would probably do very well. I would suggest to stir away from task where the label can be inferred trivially from the unsupervised data, meaning that unlabeled videos can be considered labeled frames in that case.\n\nThe related work section misses a discussion on previous work on learning unsupervised features from video (through predictive models, dimensionality reduction...) for helping classification of still images or videos [Fathi et al 2008; Mabahi et al 2009; Srivastava et al 2015]. More recently, Wang and Gupta (2015) have obtained excellent ImageNet results from features pre trained on unlabeled videos. Vondrick et al (2016) have shown that generative models of video can help initialize models for video classification tasks. Also in the field of text classification, pre training of classifier with a language model is a form predictive modeling, e.g. Dai & Le 2015.\n\nI would also suggest to report test results on the dataset from Lerrer et al 2016 (I understand that you need your own videos to pre train the predictive model) but stability prediction only require still images.\n\nOverall, I feel the experimental section is too preliminary. It would be better to focus on a task where solving the unsupervised task does not necessarily imply that the supervised task is trivially solved (or conversely that a simple rule can turn the unlabeled data into label data).\n\n*** Reference ***\n\nFathi, Alireza, and Greg Mori. \"Action recognition by learning mid-level motion features.\" Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008.\nMobahi, Hossein, Ronan Collobert, and Jason Weston. \"Deep learning from temporal coherence in video.\" Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009.\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhutdinov. \"Unsupervised learning of video representations using lstms.\" CoRR, abs/1502.04681 2 (2015).\nA. Dai, Q.V. Le, Semi-supervised Sequence Learning, NIPS, 2015\nUnsupervised learning of visual representations using videos, X Wang, A Gupta, ICCV 2015 \nGenerating videos with scene dynamics, C Vondrick, H Pirsiavash, A Torralba, NIPS 16",
        "Hi authors,\n\nCongrats for an interesting submission! I wonder what other selectivity indices you could use beside color and class.\n\nI just noticed that your definition of Neuron Feature (weighted average image) is very close to what we use in this paper (an average image):\n\n",
        "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content.",
        " It's best to have a mechanic do this both for maximum safety and to ensure safe refrigerant disposal. Your mechanic will know the required EPA procedures for removing and getting rid of old refrigerant.\n\n\nHave the mechanic remove any remaining mineral oil from the system. Make sure that the mechanic flushes the system with a solvent that's compatible with R-134a.\n\n\n\n\n\n\nThe mineral oil that is put back into your system should match the old oil. If you had PAG oil, then the mechanic should use PAG oil again.;\n, The desiccant helps to remove moisture that may accumulate in your A/C system.\n\n\nIf your A/C system uses an accumulator, you'll find it at the evaporator outlet.\n\n\n\n\n\n\nYou'll find the receiver-drier in systems that use an expansion valve to control refrigerant flow. It's connected to the high-pressure liquid line, between the condenser and the expansion valve.\n\n\n\n\n\n\nMake sure that your desiccant is compatible with R-134a refrigerant., Do this even if you don't think that you need to replace them so that you don't have to do it later if they don't seal.\n\n\nWhen you remove an old O-ring, tape it to a piece of paper. Write down exactly where that O-ring came from and keep the papers for a while.\n\n\n\n\n\n\nIf you have a leak at a connection, grab the O-ring that you took replaced. Double-check to make sure that you replaced the old O-ring with a new O-ring that was the right size. Most A/C leaks are caused by O-rings that aren't properly installed., The hoses that you used with the R-12 refrigerant should work as long as they aren't cracked or damaged. If they are damaged, replace them.\n\n, When your system pressure gets too high, the cutout switch will shut off the compressor to keep from damaging your A/C parts and to prevent the system from venting refrigerant., You'll find this tube connected to the high-pressure side at or near the evaporator. In some cases, you may find it in the condenser outlet. Don't try to clean an orifice tube. You'll need to replace it instead.\n\n, Make sure to use the viscosity recommended in your car's operating manual.,, Doing this will puncture the top of the can.\n\n, The refrigerant will push air out of the hose so that it doesn't get into your air conditioning system.\n\n, Connect the other end of the service hose to the low service fitting on the air conditioner.\n\n, You only want vapor to be pulled into your A/C.\n\n, The gauge will help you to make sure that your recharge is accurate.\n\n, Turn the air conditioner on its maximum setting.\n\n, The extraction may take up to 10 minutes. The air blowing from your car vents should grow gradually colder.\n\n, When the gauge reads 225 to 250 PSI, close the valve on your refrigerant can. Always close the valve before disconnecting the can so that you don't spray refrigerant into the atmosphere.\n\n\nGenerally, your A/C will take in about 12 ounces (355 ml) of refrigerant.\nIf your system isn't fully charged after your refrigerant can is depleted, then you can add another can of refrigerant until your gauge reads in the right PSI range., Refrigerant doesn't go bad, so you can use what remains in the can another time. Just make sure to store it in a cool place so that the can doesn't overheat and explode. You can also look into selling refrigerant back to a reclaiming facility or to a certified technician.\n\n, Doing this will prevent cross-contamination of refrigerant, and it is required by federal law.\n\n",
        "The Canadian Association of Mining Equipment and Services for Export (CAMESE) is a Mississauga-based trade organization supporting the export of Canadian mining exploration equipment and services to mining companies around the world. The organization introduces foreign mining companies to Canadian suppliers by publicizing the expertise and success of domestic mining technologies and mining operations.\n\nTraditionally assisting with market development in the Americas and Russia, CAMESE is making new efforts to support Canadian mining exporters in India and other Asian countries.\n\nCAMESE also supports foreign direct investment in Canadian mining operations.\n\nServices\nCAMESE organizes and participates in mining trade shows (such as internationally known annual Prospectors & Developers Association of Canada (PDAC) in Toronto, see ), industry networking events, policy roundtables, and publishes an Annual Compendium of Canadian mining equipment suppliers and service companies (Compendium of Canadian Mining Suppliers request form at http://www.camese.org/request.cfm), digital version viewable at http://www.camese.org/current \n\nServices are provided in English, French, and Spanish, covering the languages of the major mining areas in North America, and South America.\n\nMembership\nMore than 250 equipment suppliers, manufacturers and service providers are members of the organization, including Export Development Canada (EDC), Geosoft, Gowling Lafleur Henderson LLP (Gowlings), Phoenix Geophysics, the Prospectors & Developers Association of Canada (PDAC), and Hatch  Together, these companies represent a significant portion of the Canadian mine services and mining exploration industry, in turn representing thousands of technicians, engineers, geologists, and geophysicists and millions of dollars of export revenue each year. Much of this employment is located in Northern Ontario, well known for its large mineral deposits (especially of nickel) and extraction programmes by major mining companies including Vale and Glencore.\n\nIn turn, CAMESE is a member of the Prospectors & Developers Association of Canada (PDAC).\n\nHistory\nFounded in 1981 by Canadian mining exporters operating in Lima, Peru, CAMESE celebrated its 25th year of industry advocacy and support in 2006. Membership has grown since the early 1990s, and CAMESE has become  the leading Canadian mining equipment and services export association.\n\nFunding\nFunding for CAMESE is provided by annual membership dues, grants from the Canadian federal government EDC, and grants from the Ontario provincial government.\n\nSee also\n Prospectors & Developers Association of Canada (PDAC)\n Canadian Institute of Mining, Metallurgy and Petroleum\n Geological Association of Canada\n Reflection seismology\n Geophysics\n Exploration geophysics\n Society of Exploration Geophysicists\n\nReferences\n\nExternal links\n CAMESE Association Website:  http://www.camese.org/ \n CAMESE full membership list, by name:  http://www.camese.org/company.cfm?task=list&firstletter=A \n\nOrganizations based in Toronto\nMining organizations",
        "A list of notable buildings and structures in Eritrea:",
        "  The driving mechanism of jets and outflows in star formation process is\nstudied using resistive MHD nested grid simulations. We calculated cloud\nevolution from the molecular cloud core to the stellar core. In the collapsing\ncloud core, we found two distinct flows: Low-velocity flows (sim 5 km/s) with a\nwide opening angle, driven from the adiabatic core, and high-velocity flows\n(sim 30 km/s) with good collimation, driven from the protostar. High-velocity\nflows are enclosed by low-velocity flows after protostar formation. The\ndifference in the degree of collimation between the two flows is caused by the\nstrength of the magnetic field and configuration of the magnetic field lines.\nThe magnetic field around an adiabatic core is strong and has an hourglass\nconfiguration; therefore, flows from the adiabatic core are driven mainly by\nthe magnetocentrifugal mechanism and guided by the hourglass-like field lines.\nIn contrast, the magnetic field around the protostar is weak and has a straight\nconfiguration owing to Ohmic dissipation in the high-density gas region.\nTherefore, flows from the protostar are driven mainly by the magnetic pressure\ngradient force and guided by straight field lines. Differing depth of the\ngravitational potential between the adiabatic core and the protostar cause the\ndifference of the flow speed. Low-velocity flows correspond to the observed\nmolecular outflows, while high-velocity flows correspond to the observed\noptical jets. We suggest that the outflow and the jet are driven by different\ncores, rather than that the outflow being entrained by the jet.\n",
        "  We present VLA and PdBI subarcsecond images (0.15\"-0.6\") of the\nradiocontinuum emission at 7 mm and of the SO2 J=19_{2,18}-18_{3,15} and\nJ=27_{8,20}-28_{7,21} lines toward the Cepheus A HW2 region. The SO2 images\nreveal the presence of a hot core internally heated by an intermediate mass\nprotostar, and a circumstellar rotating disk around the HW2 radio jet with size\n600AUx100AU and mass of 1M_sun. Keplerian rotation for the disk velocity\ngradient of 5 kms-1 requires a 9 M_sun central star, which cannot explain the\ntotal luminosity observed in the region. This may indicate that the disk does\nnot rotate with a Keplerian law due to the extreme youth of this object. Our\nhigh sensitivity radiocontinuum image at 7 mm shows in addition to the ionized\njet, an extended emission to the west (and marginally to the south) of the HW2\njet, filling the south-west cavity of the HW2 disk. From the morphology and\nlocation of this free-free continuum emission at centimeter and millimeter\nwavelengths (spectral index of 0.4-1.5), we propose that the disk is\nphotoevaporating due to the UV radiation from the central star. All this\nindicates that the Cepheus A HW2 region harbors a cluster of massive stars.\nDisk accretion seems to be the most plausible way to form massive stars in\nmoderate density/luminosity clusters.\n",
        "The problem--or at least a problem--is that what constitutes karate is not necessarily so clear. In the first place, the Okinawan system, or more accurately systems, of martial arts that came to be fairly widely practiced in Japan proper as 空手 by the beginning of the twentieth century is and always has been itself a mixed art. On Okinawa elements of Chinese martial arts like the White Crane Style were combined with native practices. When Funakoshi Gichin brought Okinawan martial arts to Japan he called it 空手 and established, along with many other changes, such things as the 二十訓, Shoutoukan's \"Twenty Precepts,\" which integrated Japanese philosophical and martial arts traditions with the Okinawan style, giving such well-known principles as 一、空手道は礼に始まり礼に終る事を忘るな, \"Do not forget that karate begins with a bow and ends with a bow.\" Things like colored belts to show rank are a development of 空手 after it came to Japan, borrowed from Juudou. Lineage is often stressed by karate schools today, both in America and without, but many of the most common karate styles do not derive from Funakoshi's 空手, which eventually became Shoutoukan. \"Karate\" was \"brought\" to Japan several times from Okinawa, and then again to America and elsewhere. Indeed, many styles, such as my own Shitou-ryuu (which takes its name from Mabuni-sensei's own masters' in his original styles), were already in Japan in the early twentieth century describing themselves as mixed styles. \n\nAt the same time, \"karate\" was never homogeneous. Modern karate schools claim descent ultimately from one (or more) of three Okinawan sub-styles: Shuri-te, Naha-te, and Tomari-te, named after the locations that are claimed to be their birthplaces. Individual masters carried their understandings of these particular sub-styles to Japan, where they developed into many different styles--the lineages of these schools was, from an early date, disputed, and often tradition alone is the only way we can find the \"real\" descent of any particular style. Styles developed into new styles, teaching techniques changed, and schools combined with each other as masters learned from one another or learned additional styles. As a result, \"karate\" as practiced by different styles was from very early on very different, even as most schools laid claim to the same basic ancestry and to the same (or similar) precepts as were set out by the earliest masters to bring the art to Japan. So, for example, Goujuu-ryuu has only 12 kata, while Shoutoukan today has 26 or 27 kata (depending on the school), Shitou-ryuu has around 60 (the exact number depends on the branch), and Okinawan master Motobu Chouki is often, probably erroneously, said in doujou today to have known only a single kata. Likewise, you'll find the curriculum completely different across schools or even doujou, and this is even without getting into actual techniques. \n\nMany styles of karate have  been developed in America, have been developed for American pupils, or have been developed with American influence, whatever that means exactly. Whether any of these constitute \"American\" karate is harder to say. It is somewhat disputed who first brought karate to America (probably not least because it's not always clear what 空手 is exactly), but Robert Trias established a school teaching something he called \"karate\" in 1946 and founded the now-defunct United States Karate Association as early as 1948. Trias is a typical example, and he'll do nicely. Trias claimed (the uncertainty on my part here does not necessarily mean anything--I've never bothered to check his lineage, and already in the late nineteenth century masters were claiming lineages that could not be verified) to have trained with Chinese and Okinawan masters while in the Navy, apparently learning some Chinese martial arts, as well as Shuri-te and Kenpou. Trias' style is now known as Shuri-ryu, but was originally known as Shuri Karate Kenpo, Shorei-Goju ryu, and Goju-Shorei-Ryu. The odd changes in name, however, should not raise eyebrows. Trias could not decide on a name himself, but neither could Shitou-ryuu's Mabuni-sensei, who originally called his style Hanko-ryuu. Like many Japanese styles, Trias' style was derived from many different sources, which were not all Japanese or Okinawan karate, and not necessarily karate at all. Which raises a couple questions. In the first place, while many other karate styles are also mixed styles, typically their founders attained mastery with masters in several different styles, before combining their mastery into a single style. Trias, and other American martial artists like him, could not always claim to have studied formally with masters the way someone like Mabuni-sensei could. Whether that means his style is not \"really\" karate is another thing--masters like Shimabukuro, whose Isshin-ryuu was taught to the Third Marine Division on Okinawa and brought back to the United States in modified form, had similarly uncertain lineages, and certainly borrowed from styles that many organizations maintain they had not necessarily formally attained mastery in. Additionally, in many ways Americans like Trias could be said to be *returning* to Okinawan karate. Actually claiming that would be absurd, but the interest that American masters had from early on in martial arts outside of those descended from the three Okinawan sub-styles (themselves mixed styles) is notable. In some cases that interest turned these masters towards Chinese or other Asian martial arts that had influenced or were otherwise related to karate, while in other cases it turned them towards Japanese and Chinese martial arts that had developed in the meantime, sometimes themselves influenced by karate. So we have things like American Kenpo, which Ed Parker originally called Kenpo Karate but which, in its current form, does not actually currently use the karate label.\n\nBut karate was not brought to the United States all at once or in a single place. Prior to Trias karate was being taught in Hawaii, most notably by James Mitose, who taught a somewhat amorphous martial art that had many names over the years. Mitose is very controversial: he taught something between Kenpou and karate, taught only a single kata (Naihanshi), and in a somewhat bizarre court case was convicted of murder and sentenced to life in prison. But already by Mitose's tenure as a teacher of martial arts the very mixed nature of karate and karate-influenced styles developing in America was apparent. Styles that are thought of as being \"American,\" that claim development in America from American (or Asian immigrants) masters, or that call themselves \"American,\" whether they actually call themselves \"karate\" or not, are typically thought of as being mixed--or, to use the more proper terminology, \"eclectic\"--styles. But I hope it should be clear by now that there's really nothing all that strange about that, except perhaps the very concentrated contact that certain American masters had with particular forms of Chinese, Okinawan, and Japanese martial arts, often over a comparatively short period of time. Likewise, it's not even clear who says what constitutes \"karate.\" Different karate associations have different definitions and lists. The World Karate Federation, the only karate federation recognized by the IOC, recognizes last time I checked 8 karate federations worldwide, including the Panamerican Karate Federation, which these days is strongly associated with UFC. The Japan Karate Federation, meanwhile, only recognizes four styles: Wadou-ryuu, Shoutoukan, Shitou-ryuu, and Goujuu-ryuu. This is because the JKF was founded from a unification of the Japanese karate organizations that existed in 1964. Even more specific organizations can't decide on what constitutes \"karate.\" The Japan Karate Association, now a member of the JKF, is a Shoutoukan karate organization formed by some of Funakoshi's original students. Yet it's had something like six or seven splinter groups who can't agree on who's actually practicing karate and who's actually in charge of various styles--importantly there was from pretty much day one a dispute as to whether competition should be allowed in Japanese 空手.",
        "  In a data warehousing process, mastering the data preparation phase allows\nsubstantial gains in terms of time and performance when performing\nmultidimensional analysis or using data mining algorithms. Furthermore, a data\nwarehouse can require external data. The web is a prevalent data source in this\ncontext. In this paper, we propose a modeling process for integrating diverse\nand heterogeneous (so-called multiform) data into a unified format.\nFurthermore, the very schema definition provides first-rate metadata in our\ndata warehousing context. At the conceptual level, a complex object is\nrepresented in UML. Our logical model is an XML schema that can be described\nwith a DTD or the XML-Schema language. Eventually, we have designed a Java\nprototype that transforms our multiform input data into XML documents\nrepresenting our physical model. Then, the XML documents we obtain are mapped\ninto a relational database we view as an ODS (Operational Data Storage), whose\ncontent will have to be re-modeled in a multidimensional way to allow its\nstorage in a star schema-based warehouse and, later, its analysis.\n",
        " To do so, double-click on the blue app icon that contains the letters \"Ps,\" click on File in the menu bar at the top of the screen, click on Open... and select the image.\n\nOriginal images with higher contrasts allow for a cleaner line drawing effect.;\n,,\n\n, It's in the Layers window in the lower-right part of the screen.\n\nThis makes the Background layer invisible but leaves the original image so you can make another duplicate to try out different effects.\n\n,,,,,, Make the following adjustments:\n\nAdjust the \"Intensity\" slider to 60.\nAdjust the \"Contrast\" slider to 65.\n\n,,,,\nAdjust the \"Midpoint\" slider to +30 in the \"Vignette\" section.\n\n,,,,,\n\nSet Hue: to 40.\nSet Saturation: to 65.\nSet Lightness: to -40.\nCheck Colorize.\n\n,,,, It should be the middle layer, between \"Background\" and \"Hue/Saturation1.\", It's a tab in the window just above the Layers window, next to Adjustments.\n\nIf you don't see the Styles tab, click Windows in the menu bar, then click Styles.\n\n,,,\n\nClick Black and White Photo for a grainy black and white image.\nClick Sepia Tone for an early 20th century feel.\nClick Sun-Faded Photo for a 50s/60s instant camera feel.\n\n, Do so by clicking on File in the menu bar and Save As…. Name your file and click on Save.",
        "  The energy conditions provide a very promising model-independent study of the\ncurrent acceleration of the universe. However, in order to connect these\nconditions with observations, one often needs first to integrate them, and then\nfind the corresponding constraints on some observational variables, such as the\ndistance modulus. Those integral forms can be misleading, and great caution is\nneeded when one interprets them physically. A typical example is that the\ntransition point of the deceleration parameter $q(z)$ is at about $z \\simeq\n0.76$ in the $\\Lambda$CDM model. However, with the same model when we consider\nthe dimensionless Hubble parameter $E(z)$, which involves the integration of\n$q(z)$, we find that $E(z)$ does not cross the line of $q(z) = 0$ before $z =\n2$. Therefore, to get the correct result, we cannot use the latter to determine\nthe transition point. With these in mind, we carefully study the constraints\nfrom the energy conditions, and find that, among other things, the current\nobservational data indeed strongly indicate that our universe has ocne\nexperienced an accelerating expansion phase between the epoch of galaxy\nformation and the present.\n",
        "First, it is worth noting that not all Chinese spears and swords have/had tassels. Some have tassels, and some don't have tassels. Second, it should also be noted that many non-Chinese spears and swords also have tassels. So the simple answer to \"why\" is that they have tassels for the same reasons that other people used tassels on their spears and swords - when they used tassels.\n\nTassels on swords traditionally had various possible functions, which can be combined:\n\n1. Ornamental. It's there to look attractive.\n\n2. Unit insignia. The colour and/or style can indicate that the wearer of the sword belongs to a particular unit.\n\n3. Rank insignia. The colour and/or style can indicate the military or government rank of the wearer.\n\n4. A wrist strap to assist retention of the weapon in combat. Typically, the wrist strap is a loop which can pass through a hole in the hilt or a hole in the guard. Sometimes such a wrist strap is very plain, and at other times it can incorporate an ornamental tassel.\n\n5. Magical or talismanic. For example, a tassel with a protective prayer. This can have important effects on combat, and should not dismissed as uninteresting superstition. In particular, if the wielder of a weapon with a protective talisman feels protected, their performance in combat will be less reduced by stress. See Hall, David Avalon Hall, *Marishiten: Buddhism and the warrior Goddess*, Ph.D. dissertation, University of California, Berkeley, 1990, and Serge Mol, *Invisible armor: An Introduction to the Esoteric Dimension of Japan’s Classical Warrior Arts*, Eibusha, 2008.\n\nSome examples of sword tassels:\n\nBritish, ornamental: _URL_15_\n\nBritish, military, ornamental: _URL_12_ The tassels on these basket-hilt swords have been explained as evolution from practical padding:\n\n >  Late in the seventeenth century a short, thick woollen fringe, fitted inside the guard just below the pommel, was introduced - presumably to prevent chafing of the skin. As a probable consequence, it became the practice to leave the lower half of the pommel rough and unpolished, since it was almost entirely concealed by the fringe. This fringe was enlarged during the eighteenth century to the inordinate length it is now, and moved to outside the hilt.\n\n- J. Wallace, *Scottish Swords and Dirks: An Illustrated Reference Guide to Scottish Edged Weapons*, Arms and Armour Press, London, 1970\n\nIndian: _URL_0_\n\nChinese (Qing, military): insignia and retention strap: _URL_7_\n\nChinese (Qing, military): insignia and retention strap: _URL_1_\n\nBritish: insignia and retention strap: _URL_14_\n\nBritish: insignia and retention strap: _URL_13_\n\nBritish: ornamental, and possibly insignia and retention: _URL_6_\n\nModern martial arts instructors and students will often claim other functional uses, such\n\n* Distracting the opponent\n\n* Helping balance the sword\n\n* Provides visual feedback to the instructor and/or student that the technique is being performed properly during training\n\nI have seen no good evidence that it is useful for distraction. While a tassel, especially a heavy one, does affect the balance of a sword, a tassel can move unpredictably and the effect can be undesirable.\n\nThe modern Chinese martial arts sword tassel appears to be a flamboyant evolution of traditional sword tassels - as seen in the Chinese examples above, these were usually modest in size, and functional as retention straps.\n\nSpear tassels can perform the same tasks as sword tassels (except as retention strap, except in unusual cases). In addition, it is often claimed that they are useful for stopping blood from flowing onto the haft. Given that many fighting spears as used by ordinary soldiers lack tassels, this is not an important task. There are, in addition to the functions of sword tassels, three possible additional functions:\n\n1. The tassel itself can reinforce the top of the haft - a binding around the end of the haft can help prevent splitting.\n\n2. The tassel can be used to bind a crosspiece (wood, antler, metal) to the haft to help prevent over-penetration of a target. Such crosspieces were used on both hunting and military spears.\n\n3. The tassel can help prevent water from entering the socket, by diverting rain - a little raincoat for the bottom of the socket.\n\nSome spear tassels:\n\nIndian: _URL_2_\n\nPhilippine: _URL_8_\n\nEuropean: _URL_16_\n\nEuropean: _URL_17_\n\nEuropean: _URL_3_\n\nEuropean: _URL_9_\n\nAfghan: _URL_10_\n\nCanadian: _URL_11_\n\nSome Chinese military tassels (and cross-pieces to prevent over-penetration) can be seen at _URL_5_ - note that some of the spears have no tassels. One example of a Chinese officer with a non-tassel spear: _URL_4_",
        "The following is a list of games developed by Adventure Soft.\n\nVideo games\n\nAs Horror Soft\n\nAs Adventure Soft\n\nReferences\n\nAdventure Soft",
        "Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n",
        "  We show that observations of high-redshift Ly-alpha emitters (LAEs) have the\npotential to provide definitive evidence for reionization in the near future.\nUsing 200 Mpc radiative transfer simulations, we calculate the effect that\npatchy reionization has on the line profile, on the luminosity function, and,\nmost interestingly, on the clustering of emitters for several realistic models\nof reionization. Reionization increases the measured clustering of emitters,\nand we show that this enhancement would be essentially impossible to attribute\nto anything other than reionization. Our results motivate looking for the\nsignature of reionization in existing LAE data. We find that for stellar\nreionization scenarios the angular correlation function of the 58 LAEs in the\nSubaru Deep Field z = 6.6 photometric sample is more consistent with a fully\nionized universe (mean volume ionized fraction x_i = 1) than a universe with\nx_i < 0.5 at >2-sigma confidence level. Measurements in the next year on Subaru\nwill increase their z = 6.6 LAE sample by a factor of five and tighten these\nlimits. If the clustering signature of reionization is detected in a LAE\nsurvey, a comparison with a Lyman-break or a H-alpha survey in the same field\nwould confirm the reionization hypothesis. We discuss the optimal LAE survey\nspecifications for detecting reionization, with reference to upcoming programs.\n",
        "Yes, several members of the delegation were there only for the political implications of their inclusion. This included a soldier named Nicholas Bieliakov, a sailor named Fedor Olich, and a worker named Pavel Obukhov. None of these men were really intended to do anything beyond being a *statement*. As you note though, a peasant was lacking, this key demographic being overlooked.\n\nAn old man named Roman Stashkov, described exactly the way you probably picture an old Russian peasant, was found walking in Petrograd while the delegation was driving to their train, and quite literally picked up off the streets. He identified himself as a Left Social Revolutionary, so was politically agreeable enough to be included, and they took him into their car. He was somewhat cajoled into going at first, being told they would 'give him a lift' to the train station, as he was intending to go to Moscow, but was convinced by payment.\n\nAt the negotiations, he of course did nothing productive, and had little idea what was going on anyways. He reportedly would sometimes slip up from using 'Comrade' and instead call the other members 'barin', the old honorific for 'Master'. He was quite the hit *socially* though, guzzling down the food and drink, and even amusing the Germans somewhat with his earnest simplicity. Reportedly, seated next to Prince Ernst von Hohenlohe, he asked his neighbor for advice on which wine to drink - red or white - as he wanted the one that would get him drunk faster.\n\nBrest-Litovsk: The Forgotten Treaty - John W. Wheeler-Bennett\n\nA People's Tragedy by Orlando Figes",
        "We have just submitted an updated version of the paper, including additional references as well as new results on those agents that are enhanced with auxiliary tasks.\nSpecifically, we investigated:\n1) a new way of performing depth prediction, by formulating it as a classification task (over the quantized depth image) and compared it to depth regression.\n2) the use of depth prediction as an auxiliary task for the policy LSTM, instead of the convnet.\n3) the effect, during actor critic training, of reward clipping on the performance of the agent as well as on the stability of RL learning.",
        ";\n, With only a few differences, only Goodreads Librarian's are able to make edits to books that they themselves didn't create. Other users can only edit book listings they themselves created, but not others.\n\n, By learning this, you'll know what to expect when it comes to adjusting the information in a Goodreads book record as a Goodreads Librarian as the edit page is a modified edition of the Manually Enter Books page on Goodreads. Also, Goodreads Librarians should know and understand when to create alternate cover editions (Goodreads sometimes refers to this as their ACEs) when they don't exist yet, and knowing the proper format is crucial in this pages' knowledge.\n\nAlthough that's one of the jobs a Goodreads Librarian should know how to perform extraordinarily well, knowing what is a new book and what is just a new edition of an already established book is vital to understanding your Librarian role on Goodreads and to understanding what issues you'll come to need to tackle.\n\n\n\n\n\n\n\n, If you find same titles and authors when you search for a book, there's a good possibility that you'll need to combine or split these books instead, but not all books will have editions to edit or combine.\n\n\nIt isn't mentioned too often and is often overlooked, but there are ways to remove absolute duplicate editions with what Goodreads calls a \"merge\". However, it's much trickier to tackle and should be done in only certain circumstances..\n\n, Some books will need edits made that can only be made under Super Librarians, which is sort-of like a supervisor of all Librarians who has even more privileges than that of a regular Librarian. However, the majority of books are editable by regular Librarians. If you still need help editing a book, you can still post in the Goodreads Librarians Group asking for a Super Librarian explaining the situation., Look for the book you'll need to edit the details too.\n\n, Although this link could either be under the \"publishing date and publisher\" or if you have the details expanded, under the \"Other Editions\" and the three links below that, or just a little to the left of the right siderail labeled \"Genres\".\n\n\nMost Goodreads Librarians who work on the Goodreads database, will want to have the book profile details expanded. However, doing this will be left up to you and how you will read to understand if a book needs edits made.\n\n,, These fields include the \"literary awards\", \"characters\", \"series\" and plot settings of the book which Goodreads calls their \"book settings\". These additional fields are found underneath the four existent lines in Work Settings that all users have, just below \"media type\", which need to be spelled out as is shown on the book itself, characters, series and book settings.\n\n\nFor literary awards, these award names must be spelled out as is shown on or in the book itself, and must be rather notable awards. To list these, you'd write the award name followed by a space followed by the year in parenthesis. For those where the book was a nominee, you'd type the word \"Nominee\" to the right of the award title before the opening parenthesis.For \"characters\" field, only list the most crucial characters that you come across and only one character per line entry. However, if the characters name could pose to become a spoiler, type an alias to the character (with the preceding *spoiler* designation) in the box and type their real characters name in the Alias field. However, if you had to combine editions, any characters written to those other editions will combine into this record too, so double check for duplicate character entries before adding others.. If you are currently reading the book and have gotten through knowing the characters well enough, enter each of these characters in the form by clicking on the \"add characters\" link, type the name into the \"Add a character\" textbox and click \"Add\". If there are characters in this box, verify that all the names are correct and push forward. You can correct the names of these characters or merge with other characters by clicking the \"Merge with Another\" button and complete the tasks to merge the characters together.\nFor \"series\" and \"settings\", follow the series rules and this list should populate by itself. And as for settings, you can add popular settings of the book, but realize that if you are working in a series, this list will autopopulate with other settings from other volumes of the book.Add or adjust the plot-setting(or settings) of the book to ensure all readers understand where this book is set. Fill out the remainder of the form after you click the \"add new setting\" or \"edit setting\". These boxes include text-entries for the place name (to designate what place the information is from) and year, as well as the drop-down for the country.\n\n, Look below the line for \"Save Changes\", \"Cancel changes\" and the \"Delete This Book\" link for the \"Add a comment\" link for librarian comments and leave a comment. Perhaps it might be that other reputable sites list the book as having x pages when the book is in front of you has different and you've verified the ISBN/ASIN to match - but this is just one good reason to explain and each reason to leave a comment here is different. This is a completely-optional link that doesn't need to be added to unless it is worthy of noting for further Librarians to understand. However, if you see something in this area when you make an edit, check to make sure the change hasn't been noted for changing and make the change itself.If the comment you leave here is too pressing and needs to be acted upon faster than just a few days to a few months time (or until another Librarian opens up this page), you may want to leave a note on the Google Groups link for action instead., Although short summaries for some edits are possible (such as \"adding additional details\" when the previous version was lacking these details), others may take more words to properly explain (especially when there was data but was inaccurate for some reason. You may end up explaining that you had to research/verify the data at an approved site and included the corrected details.Remember, you can always verify books with Amazon and WorldCat, but never with Amazon's competitors.\n\n\nYou'll need to explain what you did in your edit in this field as all Goodreads Librarians will see this history and ensure that the details were carried out correctly.\n\n, Recognize that while \"Save Changes\" will publish the edit, there are only certain times when books can be deleted from the database (which is part of the merge process, but has certain rules to follow prior to deleting the book. However, if you are unsure that you are making the correct decision to edit a book, you can always back out or click the \"Cancel changes\" button to reverse your own decisions without publishing any portion of your edit (which can sometimes come in handy).\n\n",
        "All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper.",
        "SUMMARY.\n\nThe paper proposes a gating mechanism to combine word embeddings with character-level word representations.\nThe gating mechanism uses features associated to a word to decided which word representation is the most useful.\nThe fine-grain gating is applied as part of systems which seek to solve the task of cloze-style reading comprehension question answering, and Twitter hashtag prediction.\nFor the question answering task, a fine-grained reformulation of gated attention for combining document words and questions is proposed.\nIn both tasks the fine-grain gating helps to get better accuracy, outperforming state-of-the-art methods on the CBT dataset and performing on-par with state-of-the-art approach on the SQuAD dataset.\n\n\n----------\n\nOVERALL JUDGMENT\n\nThis paper proposes a clever fine-grained extension of a scalar gate for combining word representation.\nIt is clear and well written. It covers all the necessary prior work and compares the proposed method with previous similar models.\n\nI liked the ablation study that shows quite clearly the impact of individual contributions.\nAnd I also liked the fact that some (shallow) linguistic prior knowledge e.g., pos tags ner tags, frequency etc. has been used in a clever way. \nIt would be interesting to see if syntactic features can be helpful.\n\n",
        "This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope.",
        " Explain to the user what you are going to do before each action, and why you are doing it. Let them know the reason for the transfer if they did not request it, and involve them in every step of the process. Besides showing them respect, this will let them aid you in the process when they are able.\n\n, You will need to move from one side of the bed to the other several times if you do not have an assistant, but make sure to raise and lock the guard rail each time before leaving a side. It's fine to temporarily lower the rail if it gives you better access to the user when assisting them into the sling.\n\n\nOnce the sling is attached to the Hoyer lift, raise and lock the guard rails again before lifting. The user may wish to hold onto the guard rails for stability as the lifting begins.\n\n, If you are using a bed that can elevate while remaining flat, elevate it to the maximum height you can comfortably work on. The higher it is, the less strain will be put on your back when you assist the user.\n\n, For single and twin beds, they should lie in the center. If they are on a queen bed or other large bed, they should lie nearer the side you will transfer them from.\n\n\nThe user should not be on the extreme edge of the bed.\n\n, Place any objects that could get in the way of the transfer on another surface or near the base of the bed. Straighten the patient's clothing or gown.\n\n, Raise the knee next to you and place the user's foot flat on the bed. Tell them that you will be rolling them onto one side and the raise knee will make it easier.\n\n, Gently hold the raised knee and the opposite shoulder of the user, then carefully push them onto their side, facing away from you.If the user cannot stay on their side without support, place a rolled up towel or similar soft object behind their back to wedge them in place. Alternatively, have an assistant gently hold them in place.\n\n, The bottom end should be just above the user's knees and the top end should be just above the user's armpits.Make sure that the loops and tabs are on the inside when you fold.\n\n\nMake sure the fold of the sling is next to the user, with the open side facing away from them.\n\n, Using the same rolling techniques, roll the user over until they are on their other side, on top of the folded sling.\n\n\nMove to the other side of the bed if you can't comfortably roll the user from the same side.\nIf using a wedge, remove it before rolling the user onto their back to avoid discomfort.\n\n, Pull the sling out to unfold it, so that it lays flat across the bed.\n\n, Arrange the user's limbs according to the construction of the sling and the user's preferences. The arms should either be straight and flat next to the body, or extended out of the way of the sling if the user wishes to place their arms outside the sling. The legs should be flat, and either together or slightly apart, according to the design of the sling.\n\n, Check under the bed for obstructions if the lift won't fit. If you need to, narrow the legs using the shifter handle or foot pedal, but always extend them as much as you can once they are under the bed.\n\n\nThe sling bar should be above and parallel to the patient's shoulders.\n\nAlways lock the wheels of the lift before continuing.\n\n, Lower it enough that the sling loops will reach the sling hooks, but not so low that it touches the patient.\n\n\nIf you do not know how to lower the boom, return to the section titled Familiarizing Yourself. You should always be familiar with the lift before transferring someone with limited mobility.\n\n, There may be multiple loops behind the user's shoulders so you can choose the loop that fits most comfortably. Ask the user for their input if possible. Using straps, chains, or long sling loops, attach each corner of the sling to the correct hook on the sling bar.\n\n\nFor slings with leg loops, cross the leg loops under the user's legs. Make sure the left loop is reaching across to hook to the right hook, while the right loop is reaching across to hook to the left hook, and that the hooks are set away from the boom of the lift apparatus. This criss-cross helps the user's legs stay together and keeps the user from slipping out of the sling.\nSome slings include a flap that can be hooked up to help support the neck and head. This detachable flap may not comfortable for those that are capable of head control.\nKeep the open end of hooks facing away from the user to avoid injury.\n\n, Make sure the loops are hooked firmly in place, and raise the boom until the patient lifts a short distance above the bed. Make sure everything is secure and comfortable before proceeding.\n\n, Unlock the wheels of the lift and roll them carefully to the destination. You may need to adjust the width of the legs, but do not do so while the boom is being raised or lower. You should not raise or lower the boom while rolling the lift.\n\n\nIf you are moving to another room, slowly adjust the swivel bar so the user is facing you as you move the lift.\nPosition the user carefully, directly over the center of the new destination.\n\n, If transferring to a chair or wheelchair, the user should have their hips as far back as possible.\n\n, Only do so when the user is fully sitting or lying in the new destination. Gently remove the sling out from under the user and put it in a safe space.\n\n\nRoll the patient from side to side, and fold and remove the sling if the patient is on a bed or a stretcher. Use the same rolling techniques described at the start of this section.\nTug gently upwards to slide the sling out from behind the patient if the patient is in a wheelchair or a car.\n\n",
        "In short, accurate and inaccurate.\n\n & #x200B;\n\nBounties of the mid 19th C spanned a huge range of rewards from $20-$100 for a single runaway slave (sourced from [preserved ads from the 18th and 19th centuries available from the Library of Congress](_URL_5_)) to $100,000 total for the capture of [Serrat, Wilkes-Booth, and Herold following the assassination of Lincoln](_URL_2_). The deciding factors were typically availability of funds, and the notoriety of the person with the bounty on their head. \n\n & #x200B;\n\nDjango, though one of my favorite movies, plays it a little fast and loose with the history of the West in order to reconcile the events of the plot. Django has to save his wife, who is currently a slave, and bring down an entire plantation, all of which has to take place before the passing of the 13th Amendment in 1865 at the close of the Civil War. Before all of the plantation burning and wife saving, however, Django travels across the American West. The issue with this? The time we associate with the \"lawless\", bounty-hunting, gun-slinging, \"wild\" west started to pick up around 1865. This isn't to say there wasn't bounty hunting taking place before this time, it has been around since before [John Mullowney was hunting down priests](_URL_0_) in the Irish countryside at the end of the 17th century. \n\n & #x200B;\n\nThe 1860s and ESPECIALLY the 70s saw a shift in vigilante justice and bounties in the way that they relate to the law. At the time almost all western territories were controlled by US marshals, popularly elected sheriffs and constables, local police forces, and \"Indian officers\" (John Ball, *The United States Marshals of New Mexico and Arizona Territories*). Primary accounts of the time collected in Michael Canlis' *The Evolution of Law Enforcement in California* and WC Holden's *Law and Lawlessness in the Texan Frontier* indicate a major public distrust with these officials in the western territories. Compound this distrust with the jails that were both few/far between and inadequately monitored, courts in remote regions, and inadequate financial resources to aid in the chase and capture of fugitives and you end up with a population that is ready to take an active role in maintaining order and protecting their individual interests. The reward system we come to associate with the bounty hunter gunslingers of the west emerges at this time because of the physical and financial capabilities of local law enforcement offices ([Stuart Traub, Rewards, Bounty Hunters, and Criminal Justice in the West 1865-1900](_URL_6_)*).* \n\n & #x200B;\n\nCongressional Appropriation Acts initially allocated money for the offering of rewards for the capture of criminals involved in defrauding the government or mail theft. Other acts in the 60s and 70s such as the Postal Service Act of 1867 and the Indian Affairs Act of 1874 included money to pay \"bounties\" (Traub). The government involvement in the funding of reward monies for criminals allowed the acting commissioner of the Treasury to specifically dictate what constituted \"rewardable\" action or information and also made it difficult and time consuming for law enforcement officials in remote areas to receive permission to offer rewards for criminals. This delay resulted in the offering of rewards becoming a more local affair instead of waiting for government clearance to do so. \n\n & #x200B;\n\nThe offered amount of reward money through the US government varied, but we have documentation of AG Brewster authorizing $100 for a mail robber in 1882 (BH Brewster to JL Tidball October 1882 Justice Department Letters Received), [US Marshall CP Drake offering $500 for a group of stage robbers in 1878](_URL_1_), among many others for those patient enough to scour letters from the Justice Department or newspapers of the time. For further context, the Vernon County Court in Missouri during the 1870s offered up $250 for the capture of any murderer or horse thief (US Work Projects Admin - Historical Records Survey Missouri). These rewards become substantially larger for the capture of more well known criminals and outlaws, and business organizations, understandably upset that their stagecoaches were being robbed and nothing was being done about it, began to offer their own rewards that were much higher than federal or local government rewards (Allan Levett, *The Centralization of City Police in the 19th Century*). The Missouri, Kansas, and Texas railroads offered $5000 for each member of the Dalton gang (Glenn Shirley, *Six Gun and Silver Star*). The analysis of various wanted posters from the 1860s to the 1910s (when the Wild West pretty much ended as we know it), indicates that there were a lot of heavy bounties floating around for better known criminals. [$10,000 for Boss Tweed](_URL_4_), [$5,000 for N Appleton Shute](_URL_3_), $5,000 for Billy the Kid, $5,000 for Bill Doolin, $25,000 for Jesse James, etc etc. \n\n & #x200B;\n\nConsidering the notoriety of some of the outlaws of Django Unchained, i.e. the Brittle brothers, it is reasonable for $1,000+ rewards to be offered, but for 1858 considering all of the bigger rewards weren't government funded and privately funded bounties were only starting to be offered in the 1860s. \n\n & #x200B;\n\nTo answer your second question, \"Do we have records of specific criminals and their bounties?\" - no. And also yes. It's difficult to track down any typical \"record\" of bounties. There isn't a leger with a tidy list of criminals and how much the average joe with a rifle and a set of steel balls could make if he brought him in. What we do have is letters, often between US Marshals and the Attorney General granting permission to use federal funds for criminal rewards, WANTED posters, and newspapers. Lots and lots of newspapers. The average person could put out a newspaper ad offering $100 to anyone who can bring their house burglar to justice or return their latest runaway slave. A bank could put out a newspaper ad offering $10,000 for a group of men that robbed them. Newspapers from the 19th century are PACKED with rewards for anyone with information regarding various criminal wrongdoings. Posters and fliers advertising rewards for outlaws are widely available on the internet. So in a sense, yes, we have records. \n\n & #x200B;\n\nFor further reading I would suggest scouring the Library of Congress Archives, the Nova Scotia archives, papers sourced/linked above, and the Western Historical Quarterly (yes, its a real quarterly). The book *Wives, Slaves, and Servant Girls* is a great resource for more reward ads from the last quarter of the 18th Century. Other books about the West  &  law enforcement: *The Western Peace Officer, American Law Enforcement: A History, Six Gun and Silver Star,* and *The Bad Men of the West.*\n\n & #x200B;\n\nTLDR: $1000+ bounties existed and were offered for notorious criminals, typically by private business organizations, but they weren't common during the time period of Django Unchained. We have records of criminals and how much money was offered for their capture or information leading to their capture. \n\nHappy Hunting.",
        " Take quotes from potential suppliers of your desired equipment.;\n,, *The easiest way to do this is by obtaining quotes or invoices from your vendor(s). If you do not have quotes or invoices available, you will need to break down make(s), model(s) and costs of the equipment you are acquiring; You will also need to pull spec sheets for any major pieces of equipment your anticipate acquiring.\n\n, Most leasing companies will also need some type interim statements, containing reviewed or internal year-to-date Balance Sheet & Income Statement (Profit & Loss Statement). If your company is an LLC or S-Corporation, pending on your company's equity structure (but almost always with S-Corps) you may need to provide the leasing company with personal financial statements. If you do not have a personal financial statement, your Lease Consultant can literally put one together for you by asking some you some basic questions over the phone. Make sure you answer honestly, or closely, as the agent will need to fax the statements together to obtain your signature verifying the statement's accuracy. If your company has what is deemed as a \"harder credit\" or \"has some hurdles\" or whatever flowery term your leasing agent uses to describe a difficult transaction, he may ask you for a Proforma invoice, or cash flow statement, at which time you need to consult with your accountant if you have not already. Do not attempt to provide the Leasing Company with any information you do not have available, or cannot easily be put together. Simply tell your agent you do not put together these types of data.\n\n, Once you have completed steps 1 and 2 you are ready to fend off any leasing company and their many questions, saving you a great of time and sorting out the agents of experience with the Newbies to leasing. Once you compile your Financial Package and Equipment Package into one readable piece the package should suffice any leasing company with adequate information to get them to start talking numbers and options with you. Also, before you start prospectively looking at leasing companies you need to have an idea of what type of term length and of term options you are seeking. If you do not know, consult with your accountant, and also ask, though ask cautiously (trust your gut) your Leasing Guru's suggestions are. If you find a good leasing agent, he will have less interest in how much he is going to make on the deal and more interest in getting you the most ideal structure to satisfy your needs.\n\n, If you know and trust your vendor salesperson see if he can refer you to an associate leasing company, or knows a good Leasing Agent or broker that can help you. If you are left to contact a leasing company on your own, but haven't the slightest where to start, google it!. If you are a drilling company located on the outskirts of Cheyenne, WY and you're calling a company out of Los Angeles, CA That's fine! There are probably not a lot of leasing companies in your area, and the leasing agent out of the Los Angeles based company may be more professional and may have greater means to better service your company, than Bill Bottoms from your local bank anyway. 95% of the business most leasing companies do is completed for companies that are located outside of their same state. Just make sure that whatever leasing entity you go with, that it is listed with the ELA (Equipment Leasing Association), and the UAEL (United Assoc. & Equipment Leasing). This information should be on the splash screen of your prospective leasing company's website. If these associations are not listed and its not GE Capital (the largest industry player by far), you need to look somewhere else. If a leasing company is listed on the ELA, or the UAEL, and in good standing, the leasing company will absolutely reference this on their website and probably letterhead too. You should feel comfortable discussing your project with companies listed with these organizations and they should be deemed as \"creditable\" sources. If a prospective leasing company is not listed with BOTH* of these associations, seriously, run for the hills, they either have either engaged in unethical business practice (getting their company blacklisted)or the company has not been around long enough (5+ Years) to gain acknowledgment in the industry.\n\n,: After finding your new prospective leasing company that is listed with BOTH the ELA and the UAEL, you need to start looking at options. After speaking with several leasing companies, you have narrowed your selection down to about 3 sources that you want to potentially do business with. You will need to send your Equipment & Financial Package to these companies, and they will in turn offer you a \"Formal Proposal\" or \"Letter of Intent\" containing an offer or offers to finance your equipment. You will need to speak with each leasing company to understand completely what they are offering, and what works best for you. To do this, you need a type of weighing tables, so to speak, such as: Is having a low payment the most important thing to me? or is getting this thing paid off and having the lowest possible interest more important? By answering these types of questions to yourself, you will be able to push through the leasing agent's and broker's hype and hysteria, and you will be able to make an educated decision as to which company's proposal best serves you to pay for your equipment. You will need to sign the proposal and send in a deposit check, of which is showing your commitment to the deal. You need to verify that your deposit is 100% refundable if the company does not credit approve the transaction as proposed (get this in writing).\n\n,: Your leasing agent may ask you for additional pieces of information that he did not require of you when he originally proposed on the transaction (thank him, he was being kind to you). He may need to collect final pieces of information about yourself, your vendor or your business to get the FINAL PACKAGE into Credit.\n\n,, The leasing company will not be able to pay your vendors until you do this.\n\n, 9 steps later, your deal has funded. The leasing company has paid for your equipment, and you over a duration of time that you agreed to will pay the money back. Through this process you will have learned a great deal of things for the next time, but hopefully, you first run through you were able to satisfy your cash flow requirements, obtain off-balance sheet financing, save your own cash for more lucrative investments or other forms of reinvesting into your business, and you have reserved your operating line and main borrowing power through your bank. You have accomplished all of this with (though it probably doesn't seem like it after reading my overly involved steps) with relatively little time, at a slightly higher or lower cost of capital than what your lead bank would have charged you. Congrats!\n\n",
        " Larger 4'x12' sheets are available, but are harder to work with and are usually used by professionals with a few extra hands. These larger sheets tend to break easily during transport to the job site, although they typically require less work because the larger sheets mean fewer joints to tape.\n\n\nDrywall is normally installed horizontally but can be installed vertically if desired.;\n, The 1/4\" sheets are often used as overlays to existing drywall and are not intended to be used in new construction. Check your local building code for requirements in your area.\n\n, When selecting drywall, use compositions that fit the environment they will be installed in. For instance, there are various moisture resistant products commonly called \"green rock\" designed for installation in high moisture areas such as garages and bathrooms. Check your local building supply store before committing to purchase.\n\n\nGreen rocking the whole house may be overkill, but could be helpful in high moisture areas, such as bathrooms, as long as it's not used to line the bathtub or shower. Green rock drywall is not great in places where it is likely to get wet. Use glass-reinforced cement board around the shower or bath tiles instead.\n\n, Remove all old drywall, nails, screws and anything else that will prevent the new drywall sheets from laying flat on the studs.\n\n, Check that loose blocking, moisture damage, termites, or other problems will not make installation a problem. Don't be surprised to find steel studs instead of wood. Steel studs are generally a good thing since steel provides added strength, and is termite-proof and fire-retardant. When using steel studs, the only difference is that you'll have to use drywall screws instead of nails when hanging the drywall.\n\n, Use Kraft tape to repair tears in the paper backing to maximize your energy efficiency.\n\n, Look for foams that are permanent, rigid, non-shrinking, and waterproof/water resistant. Do not apply foam in or around doors or windows.\n\n, Never leave an end piece of drywall unsupported. The end piece of drywall should always be screwed down to a strapping piece or joist.\n\n\nIf your drywall does not end on a strapping piece or joist, try this:\n\nMeasure to the center of the farthest support piece the drywall gets to and transfer that measurement to the drywall.\nPlace a T-square along the line in your drywall and run a razor along that straight line created by the T-square.\nBreak the end piece off from the scoring line.\nDouble-check the end of the drywall makes it to the center of the strapping piece or joist.\n\n\n\n, Do this right before you intend to hang the drywall.\n\n, You want the edges to be perpendicular to the strapping or joist and tight against the wall.\n\n, Repeat this process for each strapping or joist underneath the drywall.\n\n\nMake sure the five screws are evenly spaced along the strapping or joist.\nLeave 1⁄2 inch (1.3 cm) buffer zones on edges when driving screws. Do not screw too close to the edge of the drywall.\nDrive the screw heads down past the top of the drywall, but not so deep that they break through the surface.\n\n, Start the next row at the edge of the wall, next to the previous row, but make sure the end joints of the drywall offset the first row by at least 4 feet (1.2 m).\n\n, Don't trust that your studs will all be on 16\" or 24\" centers, as they are supposed to be. Some studs are 1/2\" off in either direction, sometimes due to sloppy carpentry work by the builder. A good idea is to run masking tape along the floor while you have the studs exposed and mark the center line of each stud with a high visibility marker.\n\n, Again, it's likely that you will have to cut some pieces of drywall in order to center the end pieces onto a stud.\n\n\nWhen cutting drywall, use a T-square and razor knife to score a line on one side of the drywall paper. Place your knee on the opposite side of the cut and quickly pull the drywall piece towards you while at the same time pushing your knee outward, snapping the drywall in a clean line. Clean up the remaining paper along the newly formed crease with your razor.\n\n, Do this right before you intend to hang the drywall.\n\n, Start in the center and work outwards. Drive in five screws for each stud.\n\n\nExtra screws may help in some situations, but are usually overkill; they will require extra mudding and sanding that may detract from the overall finish.\nConsider using a spring-loaded drywall screw dimpler. They are designed to automatically countersink each drywall screw to precisely the same depth before ratcheting the screw bit, as a sign to quit and back off the drill.\n\n, Continue installing drywall over window and door openings. You'll be able to trim off excess drywall later. At the same time, be mindful that no seams line up with a door or window corner, and do not fasten panels to framing around openings yet.\n\n\nA good practice when installing drywall over protruding pipes is to place the drywall against the pipe and lightly tap with a flat block of wood to dimple the back. Next, pull the drywall away and use a drywall circle cutter or drywall hole saw to cut a perfect hole along the dimple. This should be much easier to finish than if you punch out a large hole that requires 3-4 coats of mud to finish.\n\n, Start the next row at the edge of the wall, next to the previous row.\n\n, Fasten the drywall down around the window or door, and then cut out the proper section using a rotary drill or drywall saw.\n\n, Having the first coat of mud, which you'll apply directly over the seam, a bit runnier than normal will allow the tape to bond well with the mud.\n\n, You don't have to worry about getting it perfect the first time through; you'll wipe off the excess after you apply the tape. Make sure you cover the seam entirely.\n\n, Use your 6\" or 8\" drywall putty knife to flatten the tape, starting at one end and pulling towards you in one smooth motion.\n\n\nHave your drywall tape pre-cut and lightly dampened with clean water. You don't need to soak it down too much.\nSome contractors avoid the perforated and fiber tapes, as they don't produce a flawless finish and require gobs of extra mud and sanding to get the job done right. Do what works best for you and fits in your budget.\n\n, Wipe off excess mud so the surface of the seam is smooth and flattened.\n\n, Wet your blade and flatten then out with another swipe if needed.\n\n, This will give your job a professional finish.\n\n\nApply mud and tape in a similar manner. Apply liberal amount of compound. If it isn't already, crease your tape perfectly in the center and reinforce the crease a couple of times. Apply the tape so that the center of the crease fits directly into the corner of the wall. Wipe away excess compound with your drywall knife.\n\n, Let the mud dry between each coat. It will bubble if you rush it!\n\n\nMany thin coats of mud will give you better results, but patience is required to let it dry.\nDon't apply any mud over freshly taped joints. Allow them to thoroughly dry for one day between coats unless you are using hot mud that will dry in an hour. A great idea is to use pink mud that dries white, indicating it is ready for another coat.\n\n, You shouldn't notice any edges after screening the mud over a joint line or screw dimple. Make sure to hold the blade flat against the drywall and pull towards you in smooth but firm strokes. Practice on an old piece of drywall to refine your technique.\n\n\nScreed some mud over any small imperfections in the drywall that may occur during installation such as missed nail/screw holes.\n\n,, Don't get carried away and sand until you expose the paper. This step goes quickly because the mud will sand off easily.\n\n, Again, caution is key here. A quick couple of scrapes over the joints is all you need.\n\n, The light will help you spot imperfections. Circle any problem areas with the pencil. Use a sponge sander or hand sander to briefly hit any flawed areas.\n\n, Apply a coat of primer to the walls, then sand the entire area lightly using a pole sander. Although most beginners skip this step, it's critical for getting a nice, even finish and for avoiding fuzzy paper residue and fluff left over from the initial sanding.\n\n, Sanding can be satisfying and fun, but sometimes people sand unnecessarily, sanding through tape. If this happens, apply some more mud and re-sand it when it dries.\n\n",
        "The paper proposes a model for the Stanford Natural Language Inference (SNLI)\ndataset, that builds on top of sentence encoding models and the decomposable\nword level alignment model by Parikh et al. (2016). The proposed improvements\ninclude performing decomposable attention on the output of a BiLSTM and feeding\nthe attention output to another BiLSTM, and augmenting this network with a\nparallel tree variant.\n\n- Strengths:\n\nThis approach outperforms several strong models previously proposed for the\ntask. The authors have tried a large number of experiments, and clearly report\nthe ones that did not work, and the hyperparameter settings of the ones that\ndid. This paper serves as a useful empirical study for a popular problem.\n\n- Weaknesses:\n\nUnfortunately, there are not many new ideas in this work that seem useful\nbeyond the scope the particular dataset used. While the authors claim that the\nproposed network architecture is simpler than many previous models, it is worth\nnoting that the model complexity (in terms of the number of parameters) is\nfairly high. Due to this reason, it would help to see if the empirical gains\nextend to other datasets as well. In terms of ablation studies, it would help\nto see 1) how well the tree-variant of the model does on its own and 2) the\neffect of removing inference composition from the model.\n\nOther minor issues:\n1) The method used to enhance local inference (equations 14 and 15) seem very\nsimilar to the heuristic matching function used by Mou et al., 2015 (Natural\nLanguage Inference by Tree-Based Convolution and Heuristic Matching). You may\nwant to cite them.\n\n2) The first sentence in section 3.2 is an unsupported claim. This either needs\na citation, or needs to be stated as a hypothesis.\n\nWhile the work is not very novel, the the empirical study is rigorous for the\nmost part, and could be useful for researchers working on similar problems.\nGiven these strengths, I am changing my recommendation score to 3. I have read\nthe authors' responses.",
        "  Linear polarization in X- and gamma-rays is an important diagnostic of many\nastrophysical sources, foremost giving information about their geometry,\nmagnetic fields, and radiation mechanisms. However, very few X-ray polarization\nmeasurements have been made, and then only mono-energetic detections, whilst\nseveral objects are assumed to have energy dependent polarization signatures.\nIn this paper we investigate whether detection of energy dependent polarization\nfrom cosmic sources is possible using the Compton technique, in particular with\nthe proposed PoGOLite balloon-experiment, in the 25-100 keV range. We use\nGeant4 simulations of a PoGOLite model and input photon spectra based on Cygnus\nX-1 and accreting magnetic pulsars (100 mCrab). Effective observing times of 6\nand 35 hours were simulated, corresponding to a standard and a long duration\nflight respectively. Both smooth and sharp energy variations of the\npolarization are investigated and compared to constant polarization signals\nusing chi-square statistics. We can reject constant polarization, with energy,\nfor the Cygnus X-1 spectrum (in the hard state), if the reflected component is\nassumed to be completely polarized, whereas the distinction cannot be made for\nweaker polarization. For the accreting pulsar, constant polarization can be\nrejected in the case of polarization in a narrow energy band with at least 50%\npolarization, and similarly for a negative step distribution from 30% to 0%\npolarization.\n",
        "  We extend to the first 300 Riemann zeros, the form of analysis reported by us\nin arXiv:math-ph/0606005, in which the largest study had involved the first 75\nzeros. Again, we model the nonsmooth fluctuating part of the Wu-Sprung\npotential, which reproduces the Riemann zeros, by the alternating-sign sine\nseries fractal of Berry and Lewis A(x,g). Setting the fractal dimension equal\nto 3/2. we estimate the frequency parameter (g), plus an overall scaling\nparameter (s) introduced. We search for that pair of parameters (g,s) which\nminimizes the least-squares fit of the lowest 300 eigenvalues -- obtained by\nsolving the one-dimensional stationary (non-fractal) Schrodinger equation with\nthe trial potential (smooth plus nonsmooth parts) -- to the first 300 Riemann\nzeros. We randomly sample values within the rectangle 0 < s < 3, 0 < g < 25.\nThe fits obtained are compared to those gotten by using simply the smooth part\nof the Wu-Sprung potential without any fractal supplementation. Some limited\nimprovement is again found. There are two (primary and secondary) quite\ndistinct subdomains, in which the values giving improvements in fit are\nconcentrated.\n",
        "The reason for the different styles is cost and torque. The slotted head screws are cheap and easy to make. But they're completely useless for powered screwdrivers and you can't put much torque on the screw without it either slipping out or stripping the head (and maring the surface of whatever you're screwing). Phillips screws are self-centering, making powered screwdrivers possible. They're somewhat more expensive to produce than slotted-head. They tend to 'cam-out' easily under torque, making it hard to apply much torque. I've heard they were designed that way to prevent overtightning. However, it's not good for exposed fasteners to look stripped. Robertson-head and allen-head fasteners can handle more torque than phillips-head fasteners, but are more expensive. Because the bottom of the hole is flat (unlike the pointed end of the phillips), there's more contact area and so it's less likely to cam-out. The robertson-head is cheaper than the allen-head, but the allen-head has six points of contact rather than 4, making it less prone to rounding out the hole. The Torx-head fasteners solve the problem of rounding/stripping by having the flat bottom of the robertson/allen that reduces cam-out, but it has much better contact with the driving bit to prevent stripping the head. The points of the 'star' on the driving bit engage the recesses on the screw at nearly right angles, so it has a very positive contact. Torx is becoming more and more popular because of that, particularly in assembly-line work. Because they're less likely than a phillips to be damaged when tightening, the allen (internal hex) heads are often used for exposed ('decorative') fasteners on 'some assembly required' furniture. It's also very cheap to make the allen keys, so they usually include one with the fasteners.",
        "L-SR1 seems to have O(mn) time complexity. I miss this information in your paper. \nYour experimental results suggest that L-SR1 does not outperform Adadelta (I suppose the same for Adam). \nGiven the time complexity of L-SR1, the x-axis showing time would suggest that L-SR1 is much (say, m times) slower. \n\"The memory size of 2 had the lowest minimum test loss over 90\" suggests that the main driven force of L-SR1 \nwas its momentum, i.e., the second-order information was rather useless.",
        "ALPHA Alternative School is an alternative public school in Toronto, Ontario, Canada. Founded in 1972, it is Toronto's oldest elementary alternative school. It was created by parents, and inspired in good part by the Ontario government's Hall-Dennis Report. ALPHA stands for \"A lot of people hoping for an alternative.\"  It is located downtown on Brant Street, near Adelaide St.\n\nThe school was started with no tests or grades. Behaviour rules were created and enforced by students and staff democratically.\n\nBackground \n\nALPHA is based on a philosophy of \"non-coercive, holistic, learner-centered education\". \n\nThe original School site was located on the 3rd floor of the YMCA (open and in use) close to the intersection of Broadview and Gerrard Streets, a few minutes walk to the community of Riverdale.\n\nReception \n\nALPHA was the focus of the Masters thesis Defining and Defending a Democratic Education Site, by Deb O'Rourke. The Journal of Unschooling and Alternative Learning published an article, Letting the Child Work, based on this thesis.\n\nAlpha II \n\nAlpha II Alternative School is a student-directed senior and secondary school for grades 7 to 12 located in Toronto. Part of the Toronto District School Board, Alpha II was founded in 2007 by a group of ALPHA parents. Students decide for themselves what to learn and how to learn it. School-wide decisions are made by a student-led weekly meeting where each person's voice counts in developing consensus.\n\nReferences\n\nExternal links \n\n \nAlpha II Alternative School website\n\nDemocratic free schools\nMiddle schools in Toronto\nHigh schools in Toronto\nSchools in the TDSB",
        "  The blazar 1ES 1101-232 was observed with the High Energy Stereoscopic System\n(H.E.S.S.) of Atmospheric Cherenkov Telescopes (ACT) in 2004 and 2005, for a\nlive time of 43 hours. VHE (E > 10^11 eV) gamma-rays were detected for the\nfirst time from this object. VHE observations of blazars are used to\ninvestigate the inner parts of the blazar jets, and also to study the\nextragalactic background light (EBL) in the near-infrared band. Observations in\n2005 were conducted in a multiwavelength campaign, together with the RXTE\nsatellite and optical observations. In 2004, simultaneous observations with\nXMM-Newton were obtained. 1ES 1101-232 was detected with H.E.S.S. with an\nexcess of 649 photons, at a significance of 10 sigma. The measured VHE\ngamma-ray flux amounts to dN/dE = (5.63 +- 0.89) x 10^-13 (E/TeV)^-(2.94 +-\n0.20) cm^-2 s^-1 TeV^-1, above a spectral energy threshold of 225 GeV. No\nsignificant variation of the VHE gamma-ray flux on any time scale was found.\n1ES 1101-232 exhibits a very hard spectrum, and at a redshift of z=0.186, is\nthe blazar with the highest confirmed redshift detected in VHE gamma-rays so\nfar. The data allow the construction of truly simultaneous spectral energy\ndistributions of the source, from the optical to the VHE band. Using an EBL\nmodel with \\nu F_\\nu = 14 nW m^-2 sr^-1 at 1.5 \\mu m as presented in Aharonian\net al. (Nature, 2006, 440, 1018) suggests an intrinsic VHE power output peak of\nthe source at above 3 TeV.\n",
        "Olivier Bourdeaut (born 1980 in Nantes, Loire-Atlantique) is a French contemporary writer.\n\nHis first novel , published in January 2016 at , was awarded the 2016 Prix France Culture/Télérama, the 2016 Grand prix RTL-Lire, the 2016 Prix Emmanuel Roblès and the 2016 Prix France Télévisions.\n\nBiography \nOlivier Bourdeaut was a real estate agent in Nantes when the loss of his work led him to devote himself to literature. He worked for two years writing a first novel, dark, which however found no publisher. While residing with his parents in Spain he devoted himself to the writing, in seven weeks, of another light and wacky novel that would become En attendant Bojangles. Published by éditions Finitude, the first to acquire its rights, the book received the enthusiastic support of Jérôme Garcin in Le Nouvel Observateur, and immediately met an enormous success with the public.\n\nWork \n2016: \n2018: \n2021:\n\nReferences\n\nExternal links \n Oliver Bourdeaut on Babelio\n Olivier Bourdeaut, ou la revanche d'un \"loser\" magnifique on L'Express (28 March 216)\n Qui est Olivier Bourdeaut auteur du roman \"En attendant Bojangles\" ? on Marie-Claire (25 May 2016)\n Entretien avec Olivier Bourdeaut, En attendant Bojangles on Lecthot (19 April 2016)\n\n21st-century French non-fiction writers\nPrix Emmanuel Roblès recipients\n1980 births\nPeople from Nantes\nLiving people",
        "The Sisters of the Infant Jesus, also known as the Congregation of the Holy Infant Jesus or the Dames of Saint-Maur, is a Roman Catholic religious institute, dedicated to education and the training of underprivileged schoolchildren. They were founded in Rouen, France, in 1666, as part of the work of Nicolas Barré, a Minim friar and Catholic priest (1621-1686), who had gathered some young women for the free instruction of the poor in 1662.\n\nToday, the Infant Jesus Sisters and their lay volunteers have a presence worldwide through social projects and schools. They are also known as Dames of St.-Maur, from the address of their major house in Paris.\n\nThe foundation of Barré's schools and of the Sisters suggested to St. John Baptist de La Salle the idea of accomplishing a similar work for boys, resulting in his founding of the Institute of the Brothers of the Christian Schools.\n\nHistory\n\nOrigins\n\nIn 1659 Barré, who was a respected scholar within his Order, was sent to the monastery of the Order in Rouen. He became widely known as a preacher and his sermons attracted a large audience. In 1662 Barré saw the need for the education of the poor in France.\n\nFrance in the late 17th century was suffering from the effects of the Franco-Spanish War (1635–1659) and a terrible plague. As a result of his efforts to promote a planned parish mission in the nearby village of Sotteville-lès-Rouen, Barré came to see the suffering of the local population. To enable parents to attend the mission, Barré asked two young women to come and help with the children. One was a local resident, Françoise Duval, 18 years old, the other was Marguerite Lestocq, then aged 20, who, like him, was from Amiens and with whom he had family connections.\n\nHe saw the need to make basic education more accessible to all. There were hardly any schools for girls and very few for boys. Most primary school teachers were poorly educated and religious education was almost non-existent; there was profound ignorance of the Catholic faith. In 1662, half the children in Rouen died of famine. Many were homeless and wandered the streets as beggars and, for some, prostitution became one of the few means of livelihood available.\n\nThey began to give daily classes to young girls in a room which they were allowed to use, spending that year in this work. Soon three other young women joined them, and two separate schools were opened. Barré would visit the classes frequently, guiding the young women in how to teach and deal with both the children and their parents, drawing upon his own rigorous education under the Jesuits and his experience as a professor. He taught them the value of “instruction and education” and from the beginning he trained the young teachers to respect the uniqueness of each child and to develop each one’s potential. The teachers were to speak in a humble, gentle and simple manner so that even the youngest could understand, and they were to teach only what they themselves had adequately grasped.\n\nAs the enrollment increased, more schools were established, and four years later, the ladies in charge of these schools began to live in a community under a Superior. This was the beginning of a religious congregation whose main work was the education of the poor.\n\nThe year 1666 saw the founding of the Congregation of the Sisters of the Infant Jesus. After several years of teaching in the schools, the five young women were invited by Barré to consider becoming part of a committed community. After some reflection, they felt that they were indeed called to this way of life and agreed. These women were not bound by religious vows or confined to a cloister. They were free to serve the local community and provide free education for poor children. They committed themselves to this in a legal document drawn up in 1669, becoming called the Charitable Teachers of the Infant Jesus (). As part of their living in trust in God, it was established that the material needs of the schools were to be handled by women outside the new community.\n\nExpansion and division\nDue to his declining health, in 1675 Barré was sent to the Minim monastery in Paris. Though limited in his activities, he promoted new foundations of his \"charitable schools of Providence\", starting with two, Saint Jean en Grèves and Saint Nicolas des Champs, training teachers, both men and women for them. He urged his teachers not to wait until pupils arrived at the school; they were to seek out especially those who might be at risk. He also set up trade schools so that girls could earn a living. Again, the education offered was to be entirely free and any profit derived from the pupils’ work was to go to them.\n\nIn 1677 Barré began to send teachers to other locations in France, starting with his native Picardy, reaching as far as New France in North America. These women were not part of a religious institute, and so were free to serve their local communities as needed, without the barriers that status would have imposed at that time.\n\nAround that time, he acquired a house located on the Rue Saint Maur in the 6th arrondissement of Paris (now called the Rue de l'Abbé Grégoire), which was to become the motherhouse of the Institute. In 1677 a convent was established in Rue Saint Maur, Paris (ii) and the Sisters were subsequently known as the \"Dames of St. Maur\". In 1678, Barré founded a novitiate for the sisters on the Seine.\n\nThe Daughters of Providence were members of a Catholic religious congregation for women founded in 1643, by a pious widow, Marie Polaillon (née de Lumague) under a Rule of Life drawn up by Vincent de Paul. The Daughters would profess annual vows of obedience, chastity, service and stability. In 1681 several houses of the congregation merged with the Sisters of the Congregation of the Holy Infant Jesus, becoming the Sisters of St. Maur and of Providence. A number of ‘Little Charitable Schools’ were established throughout France. In 1683 Mother Françoise Duval, one of the foundresses, was sent to open a school in Lisieux. At the time of Barré's death in 1686, there were over 100 schools being operated by the Sisters of the Holy Infant Jesus throughout France.\n\nThroughout his life, Barré had refused to allow the schools to accept benefices as a means of support, determined to place his trust in God alone, and was followed in this commitment by the teachers of the Institute. Upon his death, however, the lay trustees in Paris and Rouen, who were in charge of the finances of the schools and the teachers who staffed them, strongly disagreed over whether or not to continue this practice.\n\nThis was eventually referred to the royal court, and, in 1691, King Louis XIV divided the Institute into three independent groups, with motherhouses in Rouen, Paris and Lisieux. \nThe Sisters in the original communities became known as the Sisters of Providence of Rouen and in 1921 became a congregation of diocesan right, under the authority of the local bishops where they served, with a missionary outreach in Madagascar and Central Africa.\nThe group based in Lisieux also became a diocesan congregation.\nThe Sisters of St Maur in Paris became an institute of pontifical right with communities in five continents.\n\nCurrent era\nThe French Revolution closed all the schools of the institute and it was not until 1805 a new community of teachers was formed by seven surviving members. Less than twenty-five years after the opening of the motherhouse in Paris, eighty schools for free education and forty boarding schools had been established in France. With the granting of official approval from Rome, the Sisters extended their work to America, England, Spain, Malaysia, Japan and Thailand. The new growth of the Institute was such that, by the mid-19th century, schools were opened in Spain and Asia. 'Sisters of the Holy Infant Jesus (Dames of Saint Maur) were among the pioneer missionaries in British Malaya, Japan and Thailand. In 1887, after over 200 years of service without the taking of religious vows, this congregation of Sisters became a religious institute of pontifical right, able to operate independently worldwide.\n\nSoutheast Asia\nIn 1849 a Catholic missionary in the Straits Settlements, Reverend Father Jean-Marie Beurel, a native of Saint-Brieuc in France, suggested to the colonial governor, William John Butterworth, that it might be worthwhile to found a charitable organisation for girls next to the Church in Victoria Street. In August 1852, Beurel bought the house at the corner of Victoria Street and Bras Basah Road. Beurel then appealed to the Superior General of the congregation in France for Sisters to run a school.\n\nMalaysia\nFour Sisters were sent to the East. After a long and perilous voyage, one had died at sea, three of them landed at Penang in April 1852. That same year, the three Sisters established a convent that contained an orphanage and school in Penang. In September 1852, the Congregation sent four Sisters to Penang, with Mother Mathilde Raclot in charge, to guide and support the group of sisters who had arrived earlier. The school, Convent Light Street (Malay: SMK Convent Lebuh Light), is Penang's oldest girls' school and has occupied its current site along Light Street near historic George Town for over 150 years. While on the peninsula, the Sisters continued establishing schools with help from the local community such as Kuala Lumpur's oldest girls' school Convent Bukit Nanas and the only Chinese convent girls school Convent Datuk Keramat in Penang. In 1952, St Bernadette's convent school was built in Pusing Road, Batu Gajah Perak with land donated and funds raised from communities. During World War II, the Japanese invaded Malaya and either took over or closed down many such mission schools, notably the iconic Convent Primary School in the hills of Tanah Rata. The Tanah Rata convent is one of the few in the region which still contains an operating school and a church. Today, CHIJ schools can be found in most states and many major cities and they continue to educate local girls of all races and religions.\n\nSingapore\n\nIn February 1854, three Sisters led by Rev. Mother Mathilde Raclot arrived in Singapore from Penang and set up the convent in Singapore at Victoria Street. Soon they also started a Convent Orphanage and a Home for Abandoned Babies as they found day-old babies were being left at their doorstep.\n\nTo raise funds for their work, Mother Mathilde taught needlework to her fellow nuns and their students, and they sold their products to the wives of the local Chinese merchants. The school became well-known and within ten years, the enrollment had increased to 300. Secondary education began in 1905. Under Mother Hombeline, the expansion programme continued.\n\nThe convent occupied a full street block bordered by Bras Basah Road, Stamford Road, Victoria St and North Bridge Road. The iconic church was deconsecrated during the 1980s. Part of the Sisters' quarters has been demolished and converted into SMRT Corporation offices. Most of the original buildings were redeveloped as part of the Heritage Board's preservation scheme. The complex has since been redeveloped into a high-end retail complex called CHIJmes while the church is now a popular attraction for tourists and those interested in history.\n\nThe eleven CHIJ schools in Singapore can trace their history to the Victoria Street convent. Satellite schools were founded before and after World War II. The \"original\" convent school is the present-day CHIJ Secondary and CHIJ Primary schools in Toa Payoh. CHIJ Saint Nicholas Girls' School (CHIJ SNGS) was co-located on the same site and functioned as the Chinese section while CHIJ Secondary and Primary were English-medium. After the abolition of vernacular schools, CHIJ SNGS was granted SAP status. All three schools moved out of the Victoria Street complex during the 1980s into larger and more spacious facilities.\n\nEast Asia\n\nJapan\nIn 1872, Mother Mathilde led the first group of French nuns to Japan and founded the Saint Maur International School in Yokohama, where they teach and cared for the disadvantaged Japanese women and children. Mother Mathilde Raclot died, aged 97, in 1911 whilst still in Yokohama, Japan, and buried there.\n\nEurope\nThe shortage of English teachers forced the Sisters to turn to the British Isles in hopes of recruiting and training potential missionary teachers. In 1909, Mother St Beatrice Foley, who had returned from Singapore, established Drishane Convent in Ireland. It had a \"knitting school\" for younger girls and was also used to train teachers for the Asian mission. Less than half a decade after opening, the convent was churning out teachers and Sisters and sending them to Asia and South America.\n\nSouth America\nThe Sisters first set foot in South America during the 1960s. Some of the Spanish-speaking Sisters arrived in Peru in 1967 and have since expanded to several other countries in the continent.\n\nMotto\nThe motto is Simple dans ma vertu, Forte dans mon devoir, which is often translated into \"Simple in Virtue, Steadfast in Duty\", is featured on the badges of IJ schools worldwide. Depending on the individual school and country, the motto may be in either English or French, or in the native language the school is located in.\n\nSchools\n\nEngland\nSt Maur's Convent School, Weybridge (merged with St George's College in 2000)\n\nJapan\nFutaba Gakuen (ja), Yokohama\nSaint Maur International School, Yokohama\n\nThailand\n Mary Immaculate Convent School, Mueang Chonburi District\n Convent of the Holy Infant Jesus, Bangkok (English medium)\n Infant Jesus School, Banphai (Thai medium)\n Mahatinonsombull School\n\nMalaysia\nIncomplete list\n\nPrimary\nSK Marian Convent, Ipoh Perak\nSK Convent Light Street, Penang\nSK Convent Green Lane, Penang\nSK Convent Butterworth, Penang\nSJK (C) Convent Datuk Keramat, Penang (ms)\nSK Convent of the Holy Infant Jesus 1 & 2, Malacca\nSK Convent Father Barre, Sungai Petani, Kedah\nSK Convent of the Holy Infant Jesus, Johor Bahru\nSJK (C) Ave Maria Convent, Ipoh, Perak\nSJK St Bernadette's convent, Batu Gajah, Perak\nSK Convent of the Holy Infant Jesus, Seremban (1904-1994)\n\nSecondary\nSMJK Ave Maria Convent, Ipoh, Perak\nSMK Convent Bukit Mertajam, Penang\nSMK Convent Bukit Nanas, KL\nSMK Convent Butterworth, Penang\nSMK Convent Jalan Peel, KL\nSMJK Convent Datuk Keramat, Penang\nSMK Convent Ipoh, Perak\nSMK St Bernadette's convent, Batu Gajah, Perak\nSMK Convent Klang, Selangor\nSMK Convent Light Street, Penang\nSMK Convent Green Lane, Penang\nSMK Convent Muar, Johor\nSMK Convent Pulau Tikus, Penang\nSMK Convent Taiping, Perak\nSMK St. Nicholas Convent, Alor Setar, Kedah\nSMK Infant Jesus Convent Johor Bahru\nSMK Infant Jesus Convent, Malacca\nSMK Convent, Kajang (ms)\nSMK Convent of the Holy Infant Jesus, Seremban (1904-1994)\n\nNote that mission schools were nationalised by the government during the 1980s and are no longer directly under their respective religious institutions. The IJ schools are no longer run by nuns but still retain the historic crest and motto, albeit translated in Malay. A rare few may still have a nun working in a chaplaincy or pastoral capacity.\n\nRepublic of Ireland\nDrishane Convent\nScoil Íosa (now part of Malahide Community School)\n\nSpain\nColegio Niño Jesús, Burgos \nColegio Blanca de Castilla, Madrid\nEscola Infant Jesús, Barcelona\n\nSingapore\nPrimary\n CHIJ (Katong) Primary\n CHIJ (Kellock)\n CHIJ Our Lady of Good Counsel\n CHIJ Our Lady of the Nativity – formerly CHIJ Ponggol\n CHIJ Our Lady Queen of Peace – formerly CHIJ Bukit Timah\n CHIJ Primary (Toa Payoh)\n\nSecondary\n CHIJ Katong Convent \n CHIJ Secondary (Toa Payoh) \n CHIJ Saint Joseph's Convent\n CHIJ Saint Theresa's Convent\n\nFull\n CHIJ Saint Nicholas Girls' School\n\nIn the 20th century, the Sisters expanded their service, and currently serve in: Bolivia, Burma, Cameroon, China, Czech Republic, France, Guatemala, Ireland, Italy, Japan, Malaysia, Nigeria, Peru, Philippines, Romania, Senegal, Singapore, Spain, Thailand, United Kingdom, and the United States.\n\nIn 1970 a federation was established between the congregations of Paris and Rouen to facilitate a greater sense of cooperation and common identity. In 2007 a revival of the original form of life was established, one open to both women and men, called the Fraternity of the Infant Jesus, whereby they can live and serve in the spirit of Barré.\n\nSee also\nSaint Maur International School, Japan\n\nReferences\n\nExternal links\n IJ Sisters International\n Nicolas Barre's writing\n Infant Jesus Sisters' Archives\n\nLocal websites\n Ireland & England\n Japan\n Singapore\nCHIJ Alumni Singapore\n\n1675 establishments in France\nCatholic religious institutes established in the 17th century\nCatholic teaching orders\nCatholic female orders and societies\n \nReligious organizations established in the 1660s",
        "\nThis paper proposes to leverage \"surprisal\" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.\n\nThe general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.\n\n- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.\n\n- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.\nAlso, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).\n\n- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. \n Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage \"extra\" supervised information through the prediction errors.\n\n\nIn summary:\nPros: \n- Interesting idea\n- Seems to improve performances\n\nCons:\n- Paper writing\n- Weak evaluation (only one dataset)\n- Compare only with approaches that does not use the last-timestep error signal",
        "Jeannotte River is a river that rises in Lac Édouard in the municipality of Lac-Édouard in Haute-Mauricie, agglomerated with the city of La Tuque since 2003, in the administrative region of Mauricie, in the province of Quebec, Canada. The head of the Jeannotte River is one of the two emissaries of Lake Edward (outfall located in the southern part of Lake Edward, in  of the village while the other envoy, the head of the Batiscan River, is near the village, near the former sanatorium). The Jeannotte River, whose course is , is in the valley of the Batiscanie, Quebec. The upper segment of the path of the Jeannotte River marks the Zec de la Bessonne and Zec Jeannotte, then the river flows through the Zec Jeannotte, incorporated in 1978.\n\nCourse\nFrom \"Lac de la grande baie\" (Lake of the Great Bay) (adjacent to the southern part of Lake Edward) in Township Bickerdike, the Jeannotte River flows in the north-south direction in the wild and mountainous area near several small lakes (listed in order from head river):\n\na. Township Bickerdike:\n Lake Zoe.\n\nb. Township Charest:\n Lake Orleans\n Lake of Peace\n Lake of the Beaten.\n\nc. Township Laurier\n Beaver Lake, is formed very elongated, with a bulge in the river Jeannotte way\n Lake shortcut\n Belle Lake Trout\n Lake Bradley\n Lac au Lard,\n Lake Vermilion. Note: At this point, the Jeannotte river bifurcates of 90 degrees to head east toward the Batiscan River.\n\nThe last  of the river course are in the territory of Portneuf Wildlife Reserve. The upper river is normally navigable, only the last four kilometers include rapid considered more difficult for boating. The mouth of the Jeannotte River is in the Portneuf Wildlife Reserve,  downstream of the Ile aux merisiers (island-the-wild-cherry) and about  upstream from the Ile à la Croix (island to the Cross), on the Batiscan River (or  upstream of Linton, on the other bank, along the railway track).\n\nMajor tributaries \n\nThe main tributaries of the Jeannotte River are (starting from the head of the river):\n\nRight bank:\n Outlet of Lake Shiragoo,\n Outlet of Lake Orleans,\n Vermillion River\n Discharge of the Beautiful Trout Lake,\n Bacon Creek Lake.\n\nLeft Bank:\n Outlet of Lake Zoe\n Discharge of Lake Dorval,\n Discharge of Lake Shortcut\n River of kidneys.\n\nToponymy \n\nThe Jeannotte name originates from an aboriginal hunter of the parish of Batiscan, Edward Jeannotte, who accompanied the surveyor Joseph Bouchette (junior) on an expedition of recognition in the territories of Upper Mauricie in 1828, including their visit to Lake Edward. The name \"Jeannotte River\" was registered at the Commission de toponymie du Québec (Geographical Names Board of Quebec) on December 5, 1968.\n\nNotes and references\n\nSee also \n\nBatiscanie\nBatiscan River\nList of rivers of Quebec\n\nRivers of Mauricie\nLandforms of La Tuque, Quebec",
        "Summary\n===\nThis paper presents tic-tac-toe as toy problem for investigating CNNs.\nA dataset is created containing tic-tac-toe boards where one player is one\nmove away from winning and a CNN is trained to label boards according\nto (1) the player who can win (2 choices) and (2) the position they may move\nto win (9 choices), resulting in 18 labels. The CNN evaluated in this paper\nperforms perfectly at the task and the paper's goal is to inspect how the\nCNN works.\n\nThe fundamental mechanism for this inspection is Class Activation\nMapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention\nin the CNN. These implicit attention maps (localization heat maps) are used to\nderive actions (which square each player should move). The attention maps  \n\n(1) attend to squares in the tic-tac-toe board rather than arbitrary\nblobs, despite the fact that one square in a board has uniform color, and\n\n(2) they can be used to pick correct (winning) actions.\n\nThis experiment are used to support assertions that the network understands\n(1) chess (tic-tac-toe) boards\n(2) a rule for winning tic-tac-toe\n(3) that there are two players.\n\nSome follow up experiments indicate similar results under various renderings\nof the tic-tac-toe boards and an incomplete training regime.\n\n\nMore Clarifying Questions\n===\n\n* I am not quite sure precisely how CAM is implemented here. In the original CAM\none must identify a class of interest to visualize (e.g., cat or dog). I don't\nthink this paper identifies such a choice. How is one of the 18 possible classes\nchosen for creating the CAM visualization and through that visualization\nchoosing an action?\n\n* How was the test set for this dataset for the table 1 results created?\nHow many of the final 1029 states were used for test and was the\ndistribution of labels the same in train and test?\n\n* How is RCO computed? Is rank correlation or Pearson correlation used?\nIf Pearson correlation is used then it may be good to consider rank correlation,\nas argued in \"Human Attention in Visual Question Answering: Do Humans and\nDeep Networks Look at the Same Regions?\" by Das et. al. in EMNLP 2016.\nIn table 1, what does the 10^3 next to RCO mean?\n\n\nPros\n===\n\n* The proposed method, deriving an action to take from the result of a\nvisualization technique, is very novel.\n\n* This paper provides an experiment that clearly shows a CNN relying on context\nto make accurate predictions.\n\n* The use of a toy tic-tac-toe domain to study attention in CNNs\n(implicit or otherwise) is a potentially fruitful setting that may\nlead to better understanding of implicit and maybe explicit attention mechanisms.\n\n\nCons\n===\n\n* This work distinguishes between predictions about \"what will happen\"\n(will the white player win?) and \"what to do\" (where should the white\nplayer move to win?). The central idea is generalization from \"what will happen\"\nto \"what to do\" indicates concept learning (sec. 2.1). Why should an ability to\nact be any more indicative of a learned concept than an ability to predict\nfuture states. I see a further issue with the presentation of this approach and\na potential correctness problem:\n\n1. (correctness)\nIn the specific setting proposed I see no difference between \"what to do\"\nand \"what will happen.\"\n\nSuppose one created labels dictating \"what to do\" for each example in the\nproposed dataset. How would these differ from the labels of \"what will happen\"\nin the proposed dataset? In this case \"what will happen\" labels include\nboth player identity (who wins) and board position (which position they move\nto win). Wouldn't the \"what to do\" labels need to indicate board position?\nThey could also chosen to indicate player identity, which would make them\nidentical to the \"what will happen\" labels (both 18-way softmaxes).\n\n2. (presentation)\nI think this distinction would usually be handled by the Reinforcement Learning\nframework, but the proposed method is not presented in that framework or\nrelated to an RL based approach. In RL \"what will happen\" is the reward an\nagent will receive for making a particular action and \"what to do\" is the\naction an agent should take. From this point of view, generalization from\n\"what will happen\" to \"what to do\" is not a novel thing to study.\n\nAlternate models include:\n    * A deep Q network (Mnih. et. al. 2015) could predict the value of\n      every possible action where an action is a (player, board position) tuple.\n    * The argmax of the current model's softmax could be used as an action\n      prediction.\nThe deep Q network approach need not be implemented, but differences between\nmethods should be explained because of the uniqueness of the proposed approach.\n\n\n* Comparison to work that uses visualization to investigate deep RL networks\nis missing. In particular, other work in RL has used Simonyan et. al.\n(arXiv 2013) style saliency maps to investigate network behavior. For example, \n\"Dueling Network Architectures for Deep Reinforcement Learning\" by Wang et. al.\nin (ICML 2016) uses saliency maps to identify differences between their\nstate-value and advantage networks. In \"Graying the black box:\nUnderstanding DQNs\" by Zahavy et. al. (ICML 2016) these saliency maps are\nalso used to analyze network behavior.\n\n\n* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to\nactivate on grid squares because they have constant intensity, yet no empirical\nor theoretical evidence is provided for this claim.\n\nOn a related note, what precisely is the notion of information referenced in\nsection 2.3 and why is it relevant? Is it entropy of the distribution of pixel\nintensities in a patch? To me it seems that any measure which depends only\non one patch is irrelevant because the methods discussed (e.g., saliency maps)\ndepend on context as well as the intensities within a patch.\n\n\n* The presentation in the paper would be improved if the results in section 7\nwere presented along with relevant discussion in preceding sections.\n\n\nOverall Evaluation\n===\nThe experiments presented here are novel, but I am not sure they are very\nsignificant or offer clear conclusions. The methods and goals are not presented\nclearly and lack the broader relevant context mentioned above. Furthermore, I\nfind the lines of thought mentioned in the Cons section possibly incorrect\nor incomplete. As detailed with further clarifying questions, upon closer\ninspection I do not see how some aspects of the proposed approach were\nimplemented, so my opinion may change with further details.",
        "Latha Rajinikanth (born Latha Rangachari; 2 March 1958) is an Indian film producer and playback singer. She is the wife of actor Rajinikanth.\n\nEarly life\nLatha was born in Chennai, India in a Tamil Brahmin Iyengar family. She graduated with degree in English literature from Ethiraj College for Women, Chennai.\n\nCareer\nDuring the 1980s, Latha worked as a playback singer in Tamil cinema. She sung a few songs in films such as Tik Tik Tik (1981) Anbulla Rajinikanth (1984). She also contributed to Rajini 25 (1999), a musical album that commemorated Rajinikanth's 25 years of his career.\n\nIn 1991, Latha founded The Ashram, a school in Velachery, Chennai, which she currently heads.\n\nPersonal life\nLatha is the sister-in-law of Tamil playwright and film actor Y Gee Mahendran. She is also related to former film actress Vyjayanthimala. Latha's brother Ravi Raghavendra is also an actor who is the father of music director Anirudh Ravichander. She married Rajinikanth, whom she met during an exclusive interview, on 26 February 1981 at Tirupati. The couple has two daughters, Aishwarya and Soundarya. Her elder daughter, Aishwarya married actor Dhanush in 2004 and they have two sons, Yathra and Linga. Her younger daughter Soundarya married Chennai based civil engineer Ashwin with whom she has a son named Ved Krishna; the couple divorced in 2017 and she remarried businessman Vishagan in 2019.\n\nControversy\n\nDocument forgery case \nAn FIR had been filed in Bangalore against Latha for allegedly forging documents to obtain a court order, restraining media from publishing news about her dispute with a commissioned producer over selling of rights of Kochadaiiyan on 15 June 2015.\n\nRent issues \nLatha operates a shop on CP Ramaswamy Road. Operating from a building owned by Chennai Corporation, Latha has for the past 25 years run a travel agency called \"Travel Exchange India\". The city Corporation recently increased the rent from ₹ 3702 to ₹ 21,160 per month. Latha filed a petition in the Madras High Court challenging the decision of the Greater Chennai Corporation. The Madras HC dismissed the petition filed on Latha Rajinikanth's behalf and ordered the corporation to take ownership of the property in a month if the due rental was not received.\n\nA school run by Latha Rajinikanth in Chennai was locked up by the landlord of the building in August 2017, who claimed he had not received two rent of ₹ 2 crore. The owner said in 2002 he leased out the school ground to be used. He said that over a year ago he had filed a case against the management of the school because they had not paid the rent amounting to ₹ 10 crore. On December 2020, the Madras High Court has ordered Latha to vacate the school by 30 April.\n\nFilmography\n\nAs producer\n Maaveeran (1986)\n Valli (1993)\n\nAs singer\n \"Netru Indha Neram\" - Tik Tik Tik\n \"Kadavul Ullame\" - Anbulla Rajinikanth\n \"Ding Dong\" - Valli\n \"Kukkukoo Koovum\" - Valli\n \"Manappenin Sathiyam\" - Kochadaiiyaan\n\nAs costume designer\n Valli (1993)\n\nReferences\n\nIndian women film producers\nFilm producers from Chennai\nLiving people\nSingers from Chennai\nTamil film producers\nTamil playback singers\nIndian women playback singers\n20th-century Indian singers\nFounders of Indian schools and colleges\nIndian women philanthropists\nIndian philanthropists\n20th-century Indian women singers\n21st-century Indian women singers\n21st-century Indian singers\nWomen musicians from Tamil Nadu\nBusinesswomen from Tamil Nadu\n20th-century Indian businesswomen\n20th-century Indian businesspeople\n1958 births",
        " When calculating square footage of a room, you need to make sure that you can access enough space along your walls to measure the length of that wall without any obstructions.\n\n\nSince some of the measurements will also need to be taken from the middle of the room, you may also need to move some items out from the middle. You could empty out the entire room to prevent any obstructions from getting in the way, but this is more work than you need to do. Instead, temporarily remove items from the middle of the room on an “as needed” basis to avoid putting in more effort than necessary.;\n, If your room is a giant rectangle, then you do not need to split it into parts. For rooms with a more complex shape, however, you should divide the space into simplified shapes. Split it into separate rectangles, squares, triangles, and circles or semi-circles.\n\n\n\n\n\n\n\n, If you need to maintain a visual guide of your room and its measurements as you work, you can sketch a rough blueprint of the room onto paper.\n\n\n\n\n\n\n\n\n\nIf you do not have paper handy or would rather see a visual map within the room itself, you could separate the sections off by laying flat yardsticks or painter's tape along the invisible edges.\n\n, Measure the length of each wall as well as the length of each invisible dividing line in your room. To keep matters simple, separate the space into its sections as you measure instead of writing the entire length of each wall down as one measurement.\n\n\n\n\n\n\n\n\n\nAfter taking your initial measurements, you should re-measure each edge again to verify that your original measurements are correct. This extra step can save your project if your initial measurements were off.\nTo simplify matters, the lengths of each edge are usually rounded off to the nearest foot or half foot.\n\n, Use the basic area formulas for rectangles, triangles, and circles or semi-circles to find the area of each individual section.\n\n\n\n\n\n\n\n\nCalculate the area of a rectangular section with A = L * W (length times width).\nCalculate the area of a triangular section with A = 1/2 * B * H (1/2 times the base times the height).\nCalculate the area of a circular section with A = π * r^2 (pi times the squared value of the radius).\nCalculate the area of a semi-circular section with A = 1/2 * (π * r^2) (half the value of pi times the squared value of the radius).\n\n, Once you have the square footage of each individual section, you can find the square footage or surface area of the entire room by adding the footages of each section into a single final measurement.\n\n\n\n\n\n\n\n\n, As indicated before, the area or square footage of a rectangular section can be found with the formula A = L * W. Use a tape measure or yardstick to find the length and width of your first rectangular section for this equation.\n\n\n\n\n\n\n\n\n\nRound your measurements up to the nearest foot or half foot.\n\n, Multiplying the length and width together is all you need to do to find the square footage of this section.\n\n\n\n\n\n\n\n\n\nExample: For a rectangular section with a length of 12 feet and a width of 10 feet:\n\nA = L * W = 12 * 10 = 120 sq ft\n\n\n\n, If you have other rectangular or square sections of your room, measure and multiply together the length and width of each.\n\n\n\n\n\n\n\n\n, Add together the square footages for each of individual section to come up with a total square footage for the entire room.\n\n\n\n\n\n\n\n\n\nIf your room can be divided into sections in other shapes (triangles or semi-circles), save the areas of your rectangular sections and add them to those of the other sections of your room to find the total square footage.\n\n, As indicated before, the area or square footage of a triangular section can be found with the formula A = ½ * B * H. Use a tape measure or yardstick to find the base and height of your first triangular section.\n\n\n\n\n\n\n\n\n\nRound your measurements up to the nearest foot or half foot.\n\n, Multiply the base and height measurements together to solve the first part of the equation.\n\n\n\n\n\n\n\n\n\nExample: For a triangular section with a base of 10 feet and a height of 9 feet:\n\nA = 1/2 * B * H\nB * H = 10 * 9 = 90 sq ft\n\n\n\n, Multiply the product of the previous step by 1/2 to find the area or square footage of that section.\n\n\n\n\n\n\n\n\n\nExample: For a triangular section with a base of 10 feet and a height of 9 feet:\n\nA = 1/2 * B * H = ½ * 90 = 45 sq ft\n\n\n\n, For any other triangular section of the room, measure and multiply together the base and height of the section. Multiply this value by 1/2 to determine the footage of that section.\n\n\n\n\n\n\n\n\n, Add together the square footages for each of individual section to come up with a total square footage for the entire room.\n\n\n\n\n\n\n\n\n\nIf your room can be divided into sections in other shapes (rectangles or semi-circles), save the areas of your triangular sections and add them to those of the other sections of your room to find the total square footage.\n\n, As indicated before, the area or square footage of a circular section can be found with the formula A = π*r^2. If finding the area of a semi-circle, that formula becomes A = 1/2 * (π*r^2). Use a tape measure or yardstick to find the radius of your first circular or semi-circular section.\n\n\n\n\n\n\n\n\n\nRound your measurement up to the nearest foot or half foot.\n\n, Complete the first part of your equation for area by squaring the radius of your section, or multiply the measurement by itself.\n\n\n\n\n\n\n\n\n\nFor a circular or semi-circular section with a radius of 5.5 feet:\n\nA = π * r^2\nr^2 = 5.5 * 5.5 = 30.25 sq ft\n\n\n\n, Complete the area measurement for a circular space simply by multiplying the squared value of the radius by π, 3.14.\n\n\n\n\n\n\n\n\n\nFor a circular or semi-circular section with a radius of 5.5 feet:\n\nA = π * r^2 = 3.14 * 30.25 = 95 sq ft\n\n\n\n, If dealing with a semi-circle instead of a full circle, you will need to complete the area measurement by cutting the previous value in half, or multiplying it by 1/2.\n\n\n\n\n\n\n\n\n\nFor a semi-circular section with a radius of 5.5 feet:\n\nA = (1/2) * π * r^2 = (1/2) * 95 = 47.5 sq ft\n\n\n\n, For any other circular or semi-circular section, measure and square the radius and multiply this value by π (3.14). If the section is semi-circular, cut this area measurement in half.\n\n\n\n\n\n\n\n\n, Add together the square footages for each of individual section to come up with a total square footage for the entire room.\n\n\n\n\n\n\n\n\n\nIf your room can be divided into sections in other shapes (triangles or rectangles), save the areas of your circular or semi-circular sections and add them to those of the other sections of your room to find the total square footage.\n\n, Measure the straight edges of each section. For the purpose of this example, consider a room that is divided into three rectangular sections, two triangular sections, and one semi-circular section.\n\n\n\n\n\n\n\n\n\nThe largest part of the room is a rectangle. Off the top and right sides of this rectangle are smaller rectangular sections extending the full length of the wall without connecting. A triangle connects off the right-side rectangle, spanning the rectangle's length, and a smaller triangle connects to a portion of the top section. A final semi-circular section lies along part of the bottom of the center rectangle in the room.\nThe large rectangle measures 12 feet by 10 feet.\nThe smaller top rectangle measures 12 feet by 2 feet.\nThe smaller right rectangle measures 10 feet by 3 feet.\nThe larger triangle has a base of 10 feet and a height of 9 feet.\nThe smaller triangle has a base of 5 feet and a height of 7.5 feet.\nThe semi-circle has a radius of 5.5 feet.\n\n, Multiply the lengths and widths of each section to find the square footage of each individual section.\n\n\n\n\n\n\n\n\n\nSection 1 = 12 * 10 = 120 sq ft\nSection 2 = 12 * 2 = 24 sq ft\nSection 3 = 10 * 3 = 30 sq ft\n\n, Multiply the base and height of each section together. Find the area of each section by multiplying this value by 1/2.\n\n\n\n\n\n\n\n\n\nSection 4 = (1/2) * 10 * 9 = (1/2) * 90 = 45 sq ft\nSection 5 = (1/2) * 5 * 7.5 = (1/2) * 37.5 = 18.75 sq ft\n\n, Square the radius of this section and multiply by pi, or 3.14. Multiply this value by 1/2 to find the area of the semi-circle.\n\n\n\n\n\n\n\n\n\nSection 6 = (1/2) * π * 5.5 * 5.5 = (1/2) * 3.14 * 30.25 = 95 (rounded up from 94.985) * (1/2) = 47.5 sq ft\n\n, Combine the surface area of the three rectangular sections, two triangular sections, and one semi-circular section. This sum will give you the square footage of the room.\n\n\n\n\n\n\n\n\n\nTotal square footage = 120 + 24 + 30 + 45 + 18.75 + 47.5 = 285.25 sq ft\n\n",
        "Hi!\nI think this is an outstanding paper. \nIt has greatly helped me towards understanding where failure modes of GANs come from, and I think it will also have great practical implications on how to train GANs. \n\nFollowing your ideas, I have played with added noise on GANs. I am able to get the generator to yield 'good looking' samples in much less G iterations than in the standard setting. Also, it appears that 'successful' training is a LOT less sensitive to the setting of hyperparameters. For example I am able to use 10 times higher learning rates, forget about gradient clipping, etc... so this is great! I think this paper paves the way to further research on the following points:\n- how to choose noise variance and schedule its decrease over training ?\n- what does 'training D to convergence' mean ? In practice it seems that training D 'more than G' but only a few steps is already sufficient. Can we get a theoretical understanding of this...\n\nAlso, in my (quick and dirty) first experiments, using instance noise helped training in less G iterations, but the mode dropping problem remained. In my understanding, your theoretical analysis demonstrates that the addition of noise helps D provide gradients to G so that at anytime, G is able to escape its current modes. However, and due to the fact that D outputs are calculated from single examples there is no 'coverage guarantee', for example G could keep switching between different modes. Is this right ? Minibatch discrimination appears to be a good first way to answer this, although not 100% satisfactory because the metric is combinatorial... What is your view on these practical considerations ?\n\nMany thanks !\n\nArnaud Sors",
        "  We present broadband NIR spectra of A0620-00 obtained with SpeX on the IRTF.\nThe spectrum is characterized by a blue continuum on which are superimposed\nbroad emission lines of HI and HeII and a host of narrower absorption lines of\nneutral metals and molecules. Spectral type standard star spectra scaled to the\ndereddened spectrum of A0620-00 in K exceed the A0620-00 spectrum in J and H\nfor all stars of spectral type K7V or earlier, demonstrating that the donor\nstar, unless later than K7V, cannot be the sole NIR flux source in A0620-00. In\naddition, the atomic absorption lines in the K3V spectrum are too weak with\nrespect to those of A0620-00 even at 100% donor star contribution, restricting\nthe spectral type of the donor star in A0620-00 to later than K3V. Comparison\nof the A0620-00 spectrum to scaled K star spectra indicates that the CO\nabsorption features are significantly weaker in A0620-00 than in field dwarf\nstars. Fits of scaled model spectra of a Roche lobe-filling donor star to the\nspectrum of A0620-00 show that the best match to the CO absorption lines is\nobtained when the C abundance is reduced to [C/H] = -1.5. The donor star\ncontribution in the H waveband is determined to be 82+-2%. Combined with\nprevious published results from Froning & Robinson (2001) and Marsh et al.\n(1994), this gives a precise mass for the black hole in A0620-00 of M_BH =\n9.7+-0.6 M_solar.\n",
        "  Short time-scale radio variations of compact extragalactic radio quasars and\nblazars known as IntraDay Variability (IDV) can be explained in at least some\nsources as a propagation effect; the variations are interpreted as\nscintillation of radio waves in the turbulent interstellar medium of the Milky\nWay. One of the most convincing observational arguments in favor of a\npropagation-induced variability scenario is the observed annual modulation in\nthe characteristic time scale of the variation due to the Earth's orbital\nmotion. So far there are only two sources known with a well-constrained\nseasonal cycle. Annual modulation has been proposed for a few other less\nwell-documented objects. However, for some other IDV sources source-intrinsic\nstructural variations which cause drastic changes in the variability time scale\nwere also suggested. J1128+592 is a recently discovered, highly variable IDV\nsource. Previous, densely time-sampled flux-density measurements with the\nEffelsberg 100-m radio telescope (Germany) and the Urumqi 25-m radio telescope\n(China), strongly indicate an annual modulation of the time scale. The most\nrecent 4 observations in 2006/7, however, do not fit well to the annual\nmodulation model proposed before. In this paper, we investigate a possible\nexplanation of this discrepancy.\n",
        "In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.",
        " Quail are small and fairly easy birds to look after but do need time for feeding, refilling water containers, cleaning the cage, checking up daily, and collecting eggs. You may also encounter many problems while raising quail for the first time.\n, Clear the space underneath in which to place straw. This will help you to collect and remove waste.\n\n\nYou can also decide whether to house your quail in a different sort of housing like a rabbit hutch or something alike. However, the cage above is set up in a way to prevent diseases since quail waste is high in ammonia.\n\n, Choose somewhere that has access to light, but is blocked from strong winds. Most quail cages are built of open mesh wire, since the birds need shelter, but plenty of air.\n\n\nThey should be housed away from predators including pets and the room shouldn't have any disturbances.\n\n, This will allow you to increase egg production in the fall and winter months. The birds need 15 hours of light per day to produce eggs but any more and they will get stressed from lack of sleep.\n\n, A mature bird can cost around $5, while you can get 50 eggs for about $20. Along with the eggs you'll have to prepare to spend a lot of time caring for a bunch of chicks and you will also have to purchase some extra equipment. If it's your first time raising quail you might want to purchase some mature birds instead of hatching chicks.\n\n, Figure out your weekly chicken egg consumption. It takes five quail eggs to equal one chicken egg and your mature females should lay one egg every day.\n\n\nPlan to get one female bird (through hatching eggs or pairs of mating birds) for each chicken egg you eat.\nQuail eggs can be consumed like chicken eggs; however, it requires more birds to produce the same amount.\n\n, Coturnix are highly recommended birds to keep. They aren't just good egg producers but they can also be kept as meat and are really easy to look after. If you're looking for quail that lay bigger eggs you should go for the jumbo coturnix.\n\n\nCoturnix quail are also known as japanese quail. You'll also get different coloured coturnix quail such as the tuxedo quail which is a white and brown colour.\nCoturnix usually start laying at 6-8 weeks when they mature. From then on they will lay 1 egg every day.\nIf you're not sure about coturnix you can buy other popular breeds such as the Scaled Quail, Gambel’s Quail or the Bobwhite Quail. However, coturnix quail is the recommended starter breed.\n\n, The best idea is to use contacts in the local livestock or urban farming community to get birds that are acclimated to your climate.\n\n, They can be sent via mail; however, you may suffer higher mortality rates than if you buy locally.\n\n, If they don’t get quail each spring with their chickens and guinea fowl, they may be able to order them specially for you.\n\n, A preponderance of females will ensure plenty of egg production in your flock. At the same time, you'll probably only be able to house one male in each cage; if two or more males are kept in a single cage, the dominant male may attempt to kill all other males to ensure that only he will be able to mate with the female quails.\n\n, You can also order them in advance online. Your incubator should include an egg turner.\n\n, Keep a humidifier or dehumidifier nearby to adjust the humidity. Humidity controls unnecessary loss of moisture in the egg.\n\n, It is essential that you keep it even at this temperature. It will take a Couternix egg about 16 to 18 days to hatch at this temperature, while other breeds take 22 to 25 days.\n\n, Then, the tray must turn 30 degrees each way every day to keep embryos from sticking to the shell., Gradually lower the temperature from 100ºF (40ºC) to room temperature by three degrees each day. Cold chicks will bunch on top of each other whilst hot chicks will pant.\n\n, Then, give them more space as they grow.\n\n, Chick starter provides more nutrients for your chick which is what they need, and when they get older they do not require this diet any more. You can purchase some turkey starter or some chicken starter and grind them up into smaller grains so that the quail chicks can eat it. Any feed that is high in protein should be alright.,, Clean and refill their water containers daily.\n\n\nMake sure that the chicks cannot drown in the water container and have easy access to it. Most breeders will put marbles in the water container to prevent the quail from drowning.\n\n, You can add some of it to your compost. Quail waste is high in ammonia, so it must be changed frequently.\n\n, Wash it once per week to avoid disease and illness.\n\n, Special laying food is available at most feed stores. Ask if it is good for laying birds before you buy it. If you are raising quail for meat, change their food to a finisher diet instead of a laying fowl mix.\n\n, The grit will help grind down the food and the oyster shells provide calcium for the quail to lay strong, healthy eggs., The females will start to lay and they will have poor egg production levels if they are exposed to other animals, noise or other disturbances.\n\n,",
        "**ELI5**: your body is cooled by air. The faster the air, the more your body is cooled. Without a blanket the air aroud you is faster, so it cools you more than the still air under the blanket. Plus the blanket retains some of your heat better.\n\n**ELI > 5**: I'll be using ~~IS~~ ~~SI units~~ Celsius degrees, pardon me if you're American.\n\nYour body is currently at 37°C circa, up to 40 when you're ill. When you get dressed, you create various \"microclimates\" of various temperatures in your body. Each layer of clothes creates a smaller temperature step with the outside world, and make it less traumatic for your body to live.\n\nNow, defining well-being of the body is not straightforward (the science behind it is called thermohigrometry), and it's a study on temperature and humidity. But, to make it simple, your body feels better the less heat it has to exchange with the outside world.\n\nThis means it does not only depend on temperature, but also on how the \"fluid\" you have around is capable of carrying heat (air is so-so capable, while water is much more. Liquid metal even more, but I don't think that's our problem).\n\nBack to our original problem. Room air is currently at a temperature ranging from 15 to 20°C. But it's also moving! This is due to hotspots in the room (you, a computer, a heater) that will create temperature differences in space (gradients) and ultimately, through convection, make the air move. Moving air creates more heat exchange, and more feeling of unpleasantness in your body. \n\nInside your warm sheets the air is not at room temperature, but somewhere in between. Actually, it's more shifted towards your body temperature the thicker the sheet will be. Plus, it's quite stationary, since it doesn't have anywhere to go (it's a thin layer after all). Hence the feeling of well-being.\n\nPs: I'll add some few more words about convection, since it's a fascinating concept.\n\nAny fluid (air, water, helium, liquid lead, hot chocolate, sperm, anything that's not solid) has a \"convection coefficient\", which we'll call h, that indicates how well a fluid carries heat, say between you and the cold walls. This, as said before, is enhanced by the  velocity of the fluid.\n\nIt's for this reason that you blow on the soup to make it colder: from a logic point of view you're blowing air on the soup which is hotter than room air, which is stupid. But you're greatly enhancing h, hence you're actually making the soup colder faster than before!\n\nWhen you're hot, you turn on the fan. Which is not introducing colder air in the environment, but actually hotter (it's heated by the electrical resistances of the engine). But! You'll feel better with it, because the slightly hotter air is greatly faster, thus it will exchange more heat.\n\nI have thousands of amazing day-to-day applications of thermodynamics, but I'm realising I'm wasting too much time on this. Farewell!\n\nEDIT: I FORGOT ABOUT THE MONSTERS. That's the most accurate scientific explanation.\n\nEDIT2: by great popular demand I reformulated the answer. I hope this way some people will get less frustrated about a slightly more in-depth answers. Cheer up! :D",
        "This paper describes a simple but clever method for allowing variable amounts of computation at each time step in RNNs. The new architecture seems to outperform vanilla RNNs on various sequence modelling tasks. Visualizations of the assignment of computational resources over time support the hypothesis that the model is able to learn to assign more computations whenever longer longer term dependencies need to be taken into account.\n\nThe proposed model is evaluated on a multitude of tasks and its ability to outperform similar architectures seems consistent. Some of the tasks allow for an interesting analysis of the amount of computation the model requests at each time step. It’s very interesting to see how the model seems to use more resources at the start of each word or ASCII character. I also like the investigation of the effect of imposing a pattern of computational budget assignment which uses prior knowledge about the task. The superior performance of the architecture is impressive but I’m not yet convinced that the baseline models had an equal number of hyperparameters to tune. I’ll come back to this point in the next paragraph because it’s mainly a clarity issue.\n\nThe abstract claims that the model is computationally more efficient than regular RNNs. There are no wall time measurements supporting this claim. While the model is theoretically able to save computations, the points made by the paper are clearly more conceptual and about the ability of the model to choose how to allocate its resources. This makes the paper interesting enough by itself but the claims of computational gains are misleading without actual results to back them up. I also find it unfortunate that it’s not clear from the text how the hyperparameter \\bar{m} was chosen. Whether it was chosen randomly or set using a hyperparameter search on held-out data influences the fairness of a comparison with RNNs which did not have a similar type of hyperparameter for controlling regularization like for example dropout or weight noise (even if regularization of RNNs is a bit tricky). I don’t consider this a very serious flaw because I’m impressed enough by the fact that the new architecture achieves roughly similar performance while learning to allocate resources but I do think that details of this type are too important to be absent from the text. Even if the superior performance is due to this extra regularization controlling parameter it can actually be seen as a useful part of the architecture but it would be nice to know how sensitive the model is to its precise value.\n\nTo my knowledge, the proposed architecture is novel. The way the amount of computation is determined is unlike other methods for variable computation I have seen and quite inventive. Originality is one of this paper’s strongest points. \n\nIt’s currently hard to predict whether this method for variable computation will be used a lot in practice given that this also depends on how feasible it is to obtain actual computational gains at the hardware level. That said, the architecture may turn out to be useful for learning long-term dependencies. I also think that the interpretability of the value m_t is a nice property of the method and that it’s visualizations are very interesting. It might shed some more light into what makes certain tasks difficult for RNNs. \n\nPros:\nOriginal clever idea.\nNice interesting visualizations.\nInteresting experiments.\n\nCons:\nSome experimental details are not clear.\nI’m not convinced of the strength of the baseline.\nThe paper shouldn’t claim actual computational savings without reporting wall-clock times.\n\nEdit:\nI'm very positively impressed by the way the authors ended up addressing the biggest concerns I had about the paper and raised my score. Adding an LSTM baseline and results with a GRU version of the model significantly improves the empirical quality of the paper. On top of that, the authors addressed my question about some experimental detail I found important and promised to change the wording of the paper to remove confusion about whether the computational savings are conceptual or in actual wall time. I think it's fine that they are conceptual only as long as this is clear from the paper and abstract. I want to make clear to the AC that since the changes to the paper are currently still promises, my new score should be assumed to apply to an updated version of the paper in which the aforementioned concerns have indeed been addressed. \n\nEdit: \nSince I didn't know that the difference with the SOTA for some of these tasks was so large, I had to lower my score again after learning about this. I still think it's a good paper but with these results I cannot say that it stands out.\n",
        "this paper proposes a model for representing unseen words in a neural language model. the proposed model achieves poor results in LM and a slight improvement over a baseline model. \n\nthis work needs a more comprehensive analysis:\n- there's no comparison with related work trying to address the same problem\n- an intrinsic evaluation and investigation of why/how their work should be better are missing.\n- to make a bolder claim, more investigation should be done with other morphologically rich languages. Especially for MT, in addition to going from En-> Language_X, MRL_X -> En or MRL_X -> MRL_Y should be done.\n",
        "Update: I thank the authors for their comments! After reading them, I still think the paper is not novel enough so I'm leaving the rating untouched.\n\nThis paper proposes a domain adaptation technique for time series. The core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation (at the last time step).\n\nPros:\n\n1. The authors consider a very important application of domain adaptation.\n\n2. The paper is well-written and relatively easy to read.\n\n3. Solid empirical evaluation. The authors compare their method against several recent domain adaptation techniques on a number of datasets.\n\nCons:\n\n1. The novelty of the approach is relatively low: it’s just a straightforward fusion of the existing techniques.\n\n2. The paper lacks any motivation for use of the particular combination (VRNN and RevGrad). I still believe comparable results can be obtained by polishing R-DANN (e.g. carefully penalizing domain discrepancy at every step)\n\nAdditional comments:\n\n1. I’m not convinced by the discussion presented in Section 4.4. I don’t think the visualization of firing patterns can be used to support the efficiency of the proposed method.\n\n2. Figure 1(c) looks very suspicious. I can hardly believe t-SNE could produce this _very_ regular structure for non-degenerate (non-synthetic, real-world) data.\n\nOverall, it’s a solid paper but I’m not sure if it is up to the ICLR standard.",
        "There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).",
        ";\n,,,,,,, Keep doing that each 5 centimeters (2.0 in) away from each round until the half sphere is completed to the ground.,,,, Soak newspapers in the glue you choose to use without ruining them, than stretch them on the outer side of the parabola, until it's all covered, let it dry a day, than rotate it outside down and do the same inside, repeat outer and inner layers until you have some 10 layers total. reinforce the pipes area well. Allow to dry a few days, Impermeable the whole thing by painting it with waterproof paint., Cut it in two, and in between the two parts weld a L iron based frame 30 X 30 CM or larger, on which you will place the cooking pot, or the lava stone sheet to grill upon, etc, this is your stove surface.\n\n, This should do for reflecting a lot of sun., Make a support of 3 regular legs to bolt it beneath and another small support (the center of a rim that fits) to make the upper center to which you weld two pipes of 500 mm diameter 60 mm wall thickness 5 mm with their center on opposite sides and parallel among them.\n\n, You put two 100 mm long pipes of 100 out on the ends of the support and weld them to the 2 pipes coming from the rim on one side and the other two to the other pipe the opposite side. Block the outer support to the cooking support with nuts and bolts so your soup won't be in danger. Nor the whole support from the rim up rotates horizontally, and the parabola rotates around the central support. Done.\n\n,,, Easy to make with a flex grinder and a welding machine.",
        "This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). \nA second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. \nThe framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.\nOverall, the paper is very rich with ideas so I think it would be a great contribution to the conference. \n",
        "The authors proposes an interesting idea of connecting the energy-based model (descriptor) and \nthe generator network to help each other. The samples from the generator are used as the initialization \nof the descriptor inference. And the revised samples from the descriptor is in turn used to update\nthe generator as the target image. \n\nThe proposed idea is interesting. However, I think the main flaw is that the advantages of having that \narchitecture are not convincingly demonstrated in the experiments. For example, readers will expect \nquantative analysis on how initializing with the samples from the generator helps? Also, the only \nquantative experiment on the reconstruction is also compared to quite old models. Considering that \nthe model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison \nto that model. \n\n** Minor\n- I'm wondering if the analysis on the convergence is sound when considering the fact that samples \nfrom SGLD are biased samples (with fixed step size). \n- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?\n",
        "This paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings. There are reasonable scenarios in which such a strategy could come in helpful, so I feel this paper addresses an interesting problem. The paper is mostly well executed but somewhat lacks in evaluation. It would have been nice if a stronger downstream task had been attempted.\n\nThe inverted Softmax idea is very nice.\n\nA few minor issues that ought to be addressed in a published version of this paper:\n\n1) There is no mention of Haghighi et al (2008) \"Learning Bilingual Lexicons from Monolingual Corpora.\", which strikes me as a key piece of prior work regarding the use of CCA in learning bilingual alignment. This paper and links to the work here ought to be discussed.\n2) Likewise, Hermann & Blunsom (2013) \"Multilingual distributed representations without word alignment.\" is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data.\n3) It would have been nicer if experiments had been performed with more divergent language pairs rather than just European/Romance languages\n4) A lot of the argumentation around the orthogonality requirements feels related to the idea of using a Mahalanobis distance / covar matrix to learn such mappings. This might be worth including in the discussion\n5) I don't have a better suggestion, but is there an alternative to using the term \"translation (performance/etc.)\" when discussing word alignment across languages? Translation implies something more complex than this in my mind.\n6) The Mikolov citation in the abstract is messed up",
        "I appreciate the authors putting a lot of effort into the rebuttal. But it seems that all the reviewers agree that the local trend features segmentation and computation is adhoc, and the support for accepting the paper is lukewarm.\n \n As an additional data point, I would argue that the model is not end-to-end since it doesn't address the aspect of segmentation. Incorporating that into the model would have made it much more interesting and novel.",
        " If you are as young as 14, some companies will not hire you. Get a job (or at least a paper route).;\n, Example: Instead of going on a date to dinner and a movie, go for a stroll through the park and have a picnic. (much more romantic anyway)\n\n, Then you won't be pressured to go to the mall or the movies and spend money with them.\n\n, Tell them that you would be much more encouraged to study harder and put more effort into your work if there was a cash reward. Tell them that it is your future, but you are just a kid. Kids need to work hard at school and have fun. If you had a cash reward for good grades, then you could both be happy.\n\n, Seriously. But write down where you hid it. Or tell someone. Then only allow yourself a certain amount of money a month.\n\n, Give yourself more time to save the money.\n\n, Whatever you are good at you can get paid for. You can tutor kids in a subject area you're especially good at: mow lawns, babysit, clean houses, run errands, anything.\n\n, Compare your electric bill to the same month as last year (try to get them to subtract inflation) and see if they'll pay you the balance. Also do dishes, sweep. Keep your room clean.\n\n, Have a friend burn a CD for you or go without. Don't buy extra junk food when you aren't really hungry; wait until you get home to eat.\n\n, For example, if you need a pen, don't buy one that is going to wear out the next day. By the same token, don't buy the most expensive pen out there.\n\n, You may find money still in old wallets or under the bed, or between sofa cushions.\n\n, Also, keep in mind that places such as TJ Max take clothes from name-brand stores and sell them for drastically cheaper than the original price. Thrift stores are also good for hidden treasures.\n\n, If you have some gift cards that you won't use, but know your parents can use them to purchase household cleaning items, discount clothing, etc.,then ask them if they will pay you in cash for whatever is left on the card. This can add up if you have lots of gift cards! The second use of gift cards is to use them. Go through all of your old wallets and take out all of the gift cards you find. Put them on a surface and separate into two piles:wanted and unwanted gift cards. Use the ones you wanted to keep.This will help you save up much extra money.\n\n,, Think a penny doesn't count? Think again. Keep all of your spare change in a piggy bank of some form, and when the pig is full take it to your local bank. You'll be surprised at how quickly those loose coins add up.\n\n, The Coin Master takes 8-10 cents per dollar which will really add up! If you originally had $100 worth of coins, then you took it to a coin star machine, it would take ten dollars or more out of your money. To get your change converted to dollar bills, go to a nearby bank and ask for some paper coin rolls. It is actually kind of fun to count all the change by hand. You never know how much it really is, and end up surprising yourself! Be sure to wash your hands when you are done.\n\n,,, Put all extra money you earn in a piggy bank. You will be surprised at how much it mounts up!\n\n,",
        "This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of “weak learners”, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.\n\nThe approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a “training set” worth of samples from the previous strong learner, where samples are obtained via MCMC.\n\nThe experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.\n\nWith regards to novelty and prior work, there is also a missing reference to “Self Supervised Boosting” by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.\n\nOverall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.\n\n[R1] ",
        "Andriza (or Andrew) Mircovich (,  1879 – May 14, 1913) was an Austro-Hungarian national of Serb descent. He was the only prisoner ever to be executed by shooting in the US state of Nevada. He had been sentenced to death for the premeditated murder of John Gregovich in Tonopah, Nevada. Mircovich felt that he was owed more money from Gregovich's administration of his late cousin's estate and resorted to settling the matter by stabbing Gregovich to death.\n\nA 1911 statute was passed by the Nevada Legislature that allowed a death row inmate to opt to die by shooting or hanging; that statute was repealed in 1921. Mircovich was insistent on shooting, claiming that hanging often took too long. After the Nevada State Prison warden, George W. Cowing, was unable to find five men to form a firing squad, a shooting machine was requisitioned and built to carry out the execution.\n\nBackground\n\nAndriza Mircovich was a 31-year-old miner and a recent immigrant to the US from Austria-Hungary. Mircovich spoke little English and was barely literate in his native Serbian dialect. His cousin Christopher Mircovich had died in a fire in the Tonopah Belmont Mine on February 23, 1911. Because Christopher did not have a will, his estate was turned over to Nye County public administrator Arthur H. Keenan. At the recommendation of District Attorney James A. Sanders, Andriza Mircovich and Christopher Mircovich's surviving siblings, Vasso and Maria, met with John Gregovich on May 10, 1911. Gregovich, who also went by the surname Greggory, was a fellow Serb from Castellastva, (now Petrovac), who was handling the cases of other Serbian miners who had died in the fire. Mircovich, who was unfamiliar with the probate laws in Nevada, began cursing Gregovich and Sanders because he was frustrated over being unable to take sole control of the estate.\n\nAt the time of his death, Christopher Mircovich's estate included $520 in cash, and a settlement of $2,000 from the Tonopah Belmont Company. On July 17, 1911, Gregovich issued a check for $50 to Andriza Mircovich, followed by another check for $1,742.50 to Vasso and Maria on July 22. $507.32 was added to the estate for money orders held in Christopher's name. After legal and administrative fees were paid, $966.25 in undisbursed funds remained in the estate. Mircovich believed that he was owed more money and his demands escalated into threats against Gregovich. On November 14, 1911, Gregovich petitioned District Judge Peter J. Sommers, to be released as executor of the estate. Feeling cheated, Mircovich vowed to take matters into his own hands. At that time, South Slavic immigrants traditionally settled disagreements in mining camps by the use of a knife.\n\nDeath of John Gregovich\n\nIn the early morning of May 14, 1912, Gregovich was at the Tonopah and Goldfield train depot to collect a grocery bill and had a man called J. R. Masterson in conversation.  Mircovich had come to the depot from the cemetery to see who was arriving on the morning train. When he saw Gregovich on the station platform, Mircovich said, \"I will get you, you old son-of-a-bitch!\" He then stabbed Gregovich with a knife in the chest and groin, puncturing his lung and severing his femoral artery. Masterson was able to clamp the artery with a hemostat from his medical bag. Deputy Sheriff William Walker immediately apprehended Mircovich, who told him that the murder weapon was his and that Gregovich had taken his money. Despite the efforts of doctors, Gregovich died of shock at 1:00 p.m. at Miner's Hospital. At the Nye County Jail, Mircovich told Sheriff Ed Malley that he \"wanted to make John Gregovich die\", although Malley did not advise him that such statements could be used against him in court.\n\nTrial and sentencing\n\nAt the grand jury hearing on the next day, Nye County Judge Mark R. Averill denied bail, as Mircovich's case involved capital murder. George B. Thatcher was Mircovich's court-appointed attorney, but had to leave town on June 1, 1912, for the Democratic State Convention in Fallon. Averill then appointed Patrick M. Bowler on behalf of Mircovich until Thatcher returned on June 5. The case was prosecuted by District Attorney Sanders. Witnesses testified that Mircovich had made threats against Gregovich's life, and was once thrown out of Gregovich's business premises. In his closing statement Sanders challenged the all-male jury to have the \"manhood\" to \"defend the law of my country and its liberty-loving people\" or else \"we might just as well dynamite this old courthouse. We might just as well take his Honor off the bench and say we have no law in Nye County.\"\n\nOn June 15, 1912, Mircovich was convicted of premeditated murder. He was sentenced to death by Judge Averill. Mircovich boarded a train to Nevada State Prison in Carson City on June 17 while escorted by Sheriff Malley and prison warden George W. Cowing. Mircovich parted with the statement: \"They will treat me to a shower of cold lead.\" He became nauseated during the trip and begged his captors to shoot him right away, not understanding that attorneys would pursue appeals on his behalf.\n\nAttorney J.E. McNamara argued to the Nevada Supreme Court that Sanders unfairly prejudiced the jury with the statement: \"Why, gentlemen of the jury, if you cannot pronounce by your verdict the death penalty upon this defendant, I say, let's resurrect old Casey [that killed Mrs. Hislop in Goldfield] and let him live again.\" However, the appeal was denied and the lower court decision was upheld, with Justice Pat McCarran abstaining from the opinion.\n\nExecution\nNevada State Prison had been the state-designated facility for hangings since 1903. At the urging of the Mormon population, the Nevada Legislature passed a statute in 1910 that became effective in January 1911, which allowed condemned prisoners to choose between execution by shooting or hanging. Only Mircovich and one other inmate selected shooting. However, the other prisoner's sentence was commuted. Warden Cowing tried to talk Mircovich out of his decision. Mircovich did not want to be hanged and insisted on being shot, claiming it would be quicker. Cowing was faced with a predicament in meeting the scheduled execution date of August 29, 1912, because he was unable to find five marksmen willing to participate in a firing squad.\n\nThe shooting machine\n\nThe state then ordered a rack of rifles that was to be set up on a frame by an Eastern ordnance foundry. The 1000-pound (450 kg) execution machine, which was called the \"shooting gallery of steel\", included three Savage Model 1899 .30-30 caliber rifles with Maxim silencers. When the device arrived at the prison, Cowing no longer wanted to have any part of the execution and resigned. Denver S. Dickerson, a former Nevada governor and prison reformer, was appointed as the new warden. On May 13, 1913, Prison Chaplain Lloyd B. Thomas of St. Peter's Episcopal Church, Carson City, Nevada, made an unsuccessful appeal for commutation on Mircovich's behalf with the Board of Pardons.\n\nThe \"shooting machine\" was designed to be loaded with two lethal rounds and a blank cartridge, each connected to a coiled spring mechanism. The device could be fired by cutting three strings, only one of which would fire the rifles. This design would prevent the three randomly selected prison guards from knowing who would be responsible for triggering the lethal shot. On the morning of May 14, the three guards entered the firing area and then the 12 invited witnesses were admitted to a designated enclosed space in the yard. Mircovich refused an offer for a blindfold from Warden Dickerson and shook his hand, stating: \"I much obligated to you. You be good man to me.\" Mircovich was then strapped to a chair bolted onto a platform in front of the machine. The prison doctor, Donald T. McLean, placed a heart-shaped target on Mircovich's chest. The aim on the loaded rifles were sighted on the defendant's heart, and checked by each guard. Mircovich cursed the name of Judge Averill and said \"I die like a soldier\" before he was shot to death.\n\nAftermath\n\nDoctor McLean declared that the death was instantaneous. An autopsy found the two soft-nosed ball cartridges within 2/3 inch (17mm) of each other in Mircovich's heart. Reverend Thomas conducted an informal service at the prison cemetery that afternoon. Mircovich's body was buried in a pine coffin on the same day.\n\nIn a 1915 report to the Nevada Legislature, Dickerson stated that despite his personal reservations, \"executions by shooting are a trifle less barbarous than by hanging and have the further merit of eliminating many of the possibilities of bungling.\" However, the shooting machine was never used again and was placed in storage by the prison quarry. In July 1942, it was donated with 50 tons of scrap metal for a collection drive during World War II. The rifles from the machine were found during an inventory of the prison armory in June 1977 and were donated to the Nevada State Museum in Carson City.\n\nSee also\n\nCapital punishment in Nevada\nCapital punishment in the United States\nGee Jon\nMiranda warning\n\nReferences\n\nFurther reading\n\nExternal links \n Nevada State Prison Inmate Case Files: Andriza Mircovich at Nevada State Library and Archives\n State of Nevada v. Andriza Mircovich, 35 Nev. 485 - 1913\n\n1870s births\nYear of birth uncertain\n1913 deaths\n19th-century American people\n19th-century Austrian people\nAustro-Hungarian emigrants to the United States\nPeople convicted of murder by Nevada\nMontenegrin people executed abroad\nAustrian people of Slavic descent\nExecuted Montenegrin people\n20th-century executions by Nevada\nPeople executed for murder\nPeople executed by Nevada by firing squad\nPeople murdered in Nevada\nDeaths by firearm in Nevada\nAmerican miners",
        "  We present X-ray, infrared and radio observations of the field centered on\nX-ray source 1E 1547.0-5408 in the Galactic Plane. A new Chandra observation of\nthis source shows it is unresolved at arc-second resolution, and a new XMM\nobservation shows that its X-ray spectrum is best described by an absorbed\npower-law and blackbody model. A comparison of the X-ray flux observed from\nthis source between 1980 and 2006 reveals that its absorbed 0.5-10 keV X-ray\nflux decreased from ~2x10^-12 ergs cm-2 s-1 to ~3x10^-13 ergs cm-2 during this\nperiod. The most recent XMM observation allows us to put a 5 sigma confidence\nupper limit of 14% for the 0.5-10 keV peak-to-peak pulsed fraction. A\nnear-infrared observation of this field shows a source with magnitude Ks =\n15.9+/-0.2 near the position of 1E 1547.0-5408, but the implied X-ray to\ninfrared flux ratio indicates the infrared emission is from an unrelated field\nsource, allowing us to limit the IR magnitude of 1E 1547.0-5408 to >17.5.\nArchival radio observations reveal that 1E 1547.0-5408 sits at the center of a\nfaint, small (4' diameter) radio shell, G327.24-0.13, which is possibly a\npreviously unidentified supernova remnant. The X-ray properties of 1E\n1547.0-5408 suggest that this source is a magnetar - a young neutron star whose\nX-ray emission is powered by the decay of its extremely strong magnetic field.\nThe spatial coincidence between this source and G327.24-0.13 suggests that 1E\n1547.0-5408 is associated with a young supernova remnant, supporting a neutron\nstar interpretation. Additional observations are needed to confirm the nature\nof both 1E 1547.0-5408 and G327.24-0.13, and to determine if these sources are\nassociated. If so, this pair will be an important addition to the small number\nof known associations between magnetars and supernova remnants.\n",
        "Demobilization after World War I for a large portion of men serving in the U.S. Army was quite quick; only one-half of Army forces mobilized for the war were overseas by the end of 1918. 58 divisions had been mobilized for service, and 41 plus one partial one went overseas, of which 12 saw no combat, being used as \"depot divisions\" to provide replacements for units at the front. 16 divisions formed too late to go overseas, of which 8 had less than half their men on Armistice Day. The process of demobilization in itself was not handled very efficiently and was instituted too quickly to be as effective as possible; only a single man, Colonel C.H. Conrad, was entrusted to form a plan, and he only started his work a month before the war ended. Four plans were initially considered;\n\n* Demobilization by length of service; men who had served the longest discharged first\n\n* Demobilization by civilian occupation; men most critical in their industry being discharged first \n\n* Demobilization by locality; men being sent to their local draft boards to be discharged\n\n* Demobilization by unit\n\nAfter mulling over the pros and cons of each (and seeing several in action), the fourth plan, demobilization by unit, was chosen. Including the several divisions serving as occupation forces in the Rhineland, all U.S. divisions were home by the summer of 1919, having traveled on one or several troopships or converted cruise liners as soon as they were available. A maximum of 102,000 men a week passed through demobilization centers on continental Europe by the middle of June 1919, and by August 1919, 80 percent of the troops in the \"emergency\" U.S. Army had begun the process of returning home.\n\nThe U.S. Third Army, having been created to occupy the Rhineland, was disbanded on July 2, 1919 and renamed American Forces in Germany, and only a few thousand American troops remained there until 1923.\n\nThe process of demobilization stateside occurred at designated camps throughout the United States which had previously been newly constructed to train the millions of draftee and volunteer troops in 1917 and 1918; many of these camps would later become the familiar forts of today's U.S. Army. Divisions of the Volunteer Army (National Army) and the National Guard of the United States had been raised from men that lived in groups of adjacent states, and an effort was made to demobilize these divisions at a camp which was closest to a place where most of the men lived (which was not necessarily the camps where the divisions were raised). Divisions were sometimes demobilized at a camp near a port of embarkation and the sub-units returned home individually, especially if they were explicitly state-based like National Guard units.\n\n**Regular Army Divisions**\n\nDivision|Camp Mobilized At|Camp Demobilized at|Demobilized in U.S.\n:--|:--|:--|:--\n1st|Jay, NY|George G. Meade, MD|September 1919\n2nd|Bourmont, France|Mills, NY|September 1919\n3rd|Greene, NC|Merritt, NJ|August 1919\n4th|Greene, NC|Merritt, NJ|August 1919\n5th|Logan, TX|Merritt, NJ|July 1919\n6th|McClellan, AL|Mills, NJ|June 1919\n7th|Wheeler, GA|Mills, NJ|June 1919\n8th (headquarters only)|Fremont, CA|Lee, VA|January 1919\n\n**National Guard Divisions:**\n\nDivision|States Recruited From|Camp Mobilized At|Camp Demobilized at|Demobilized in U.S.\n:--|:--|:--|:--|:--\n26th|ME, NH, VT, MA, RI, CT|Greene, NC|Devens, MA|May 1919\n27th|NY|Wadsworth, SC|Upton, NY|April 1919\n28th|PA|Hancock, GA|Dix, NJ|May 1919\n29th|NJ, VA, MD, DE, Wash., D.C.|McClellan, AL|Dix, NJ|May 1919\n30th|TN, NC, SC|Sevier, SC|Jackson, SC|May 1919\n31st|GA, AL, FL|Wheeler, GA|Gordon, GA|December 1918\n32nd|MI, WI|MacArthur, TX|Custer, MI|April 1919\n33rd|IL|Logan, TX|Grant, IL|May 1919\n34th|MN, IA, NE, ND, SD|Cody, NM|Grant, IL|December 1918\n35th|MO, KS|Doniphan, OK|Funston, KS|May 1919\n36th|TX, OK|Bowie, TX|\"\"|June 1919\n37th|OH, WV|Sheridan, AL|Sherman, OH|June 1919\n38th|IN, KY|Shelby, MS|Zachary Taylor, KY|June 1919\n39th|LA, MS, AR|Beauregard, LA|\"\"|May 1919\n40th|CA, NV, UT, CO, AZ, NM|Kearny, CA|\"\"|June 1919\n41st|WA, OR, MT, WY|Fremont, CA|Dix, NJ|June 1919\n42nd|26 states and Washington, D.C.|||May 1919\n\n**Volunteer Army Divisions:** \n\nDivision|States Recruited From|Camp Mobilized At|Camp Demobilized at|Demobilized in U.S.\n:--|:--|:--|:--|:--\n76th|ME, NH, VT, MA, RI, CT|Devens, MA|\"\"|May 1919\n77th|Metropolitan New York City|Upton, NY|“”|April 1919\n78th|NY, northern PA|Dix, NJ|\"\"|June 1919\n79th|Southern PA|George G. Meade, MD|Dix, NJ|June 1919\n80th|NJ, VA, MD, DE, Wash., D.C.|Lee, VA|\"\"|May 1919\n81st|TN, NC, SC|Jackson, SC|Port of Hoboken, NJ|June 1919\n82nd|GA, AL, FL|Jackson, SC|Mills, NY|April 1919\n83rd|OH, WV|Sherman, OH|\"\"|October 1919\n84th|IN, KY|Zachary Taylor, KY|\"\"|January 1919\n85th|MI, WI|Custer, MI|\"\"|April 1919\n86th|IL|Grant, IL|Custer, MI|January 1919\n87th|AR, LA, MS|Pike, AR|\"\"|January 1919\n88th|MN, IA, NE, ND, SD|Dodge, IA|\"\"|June 1919\n89th|MO, KS, CO|Funston, KS|\"\"|May 1919\n90th|TX, OK, AZ, NM|Travis, TX|Bowie, TX|June 1919\n91st|ID, MT, WY|Lewis, WA|Presidio of San Francisco, CA|April 1919\n92nd|Nationwide|Funston, KS|“”|May 1919\n93rd|Nationwide|Funston, KS|“”|February 1919\n\nUnder Section 1406 of the [Revenue Act of 1918](_URL_2_), honorably discharged soldiers were paid an immediate bonus of $60.00 ($855.48 in 2017 dollars) upon discharge. This is different than the promised bonus which caused the [Bonus Army](_URL_1_) conflict of 1932. Soldiers were instructed not to waste it;\n\n >  This money is intended to bridge over the period between your leaving the army and getting a position in civil life, and should not be squandered. It is not a windfall, but comes directly or indirectly from the hard earned savings of your family and from the war taxes and liberty bonds to which they contributed.\n\nSince most soldiers probably had sold or neglected their civilian clothing during their time in service, or might have had only a few articles of clothing to begin with; they were allowed to keep the following;\n\n* 1 overseas cap (for all enlisted men who have had service overseas) or, 1 hat and 1 hat cord (for all other enlisted men)\n\n* 1 olive drab shirt\n\n* 1 woolen service coat and ornaments [medals and awards]\n\n* 1 pair woolen breeches\n\n* 1 pair shoes\n\n* 1 pair canvas or spiral leggings (canvas if available)\n\n* 1 waist belt\n\n* 1 slicker [raincoat]\n\n* 1 overcoat\n\n* 2 suits underwear\n\n* 4 pairs stockings\n\n* 1 pair gloves\n\n* 1 gas mask and helmet (for all officers and enlisted men to whom they were issued overseas)\n\n* 1 set toilet articles; this includes 1 hairbrush, 1 comb, 1 toothbrush, 1 shaving brush, 1 razor, 1 small steel mirror, and 2 towels (if in possession of soldier at time of discharge)\n\n* 1 barrack bag\n\n* 3 scarlet chevrons; to be sewed on uniform prior to discharge when practicable\n\nThe scarlet chevrons were used to indicate that a soldier had been honorably discharged, and were to be placed one each on the [left sleeve of the shirt, coat, or overcoat, point up](_URL_0_). The  [National Defense Act of 1916](_URL_4_) prescribed several regulations on the wear of the military uniform. Soldiers were allowed to wear their uniforms for up to three months post-discharge. It was unlawful to wear the military uniform without the identifying mark (chevrons), if the soldier had received them. Unlawful wear of the military uniform or impersonation was punished by a fine of up to $300.00 and/or a jail sentence of up to six months. \n\nUnder the Act, train tickets or other means of transportation were offered to enlisted soldiers at a fare of 3 1/2 cents per mile to their homes. Men were compelled to purchase tickets immediately after their arrival at the demobilization camp via only having a reduced rate (often 2 cents) offered there and at the date of discharge, and being marched directly to the ticket window after their arrival, to prevent them from lingering in urban areas and make them go home as quickly as possible. Meals could be had on trains for seventy-five cents. Soldiers could reasonably expect to be back at home within two or three weeks, even less if they lived next to the area where demobilization took place.\n\nAnother result of demobilization was a flooding of the labor market; civilians in many towns and cities complained about the supposed large number of discharged soldiers in their communities looking for employment by January 1919 (the New York Employment Board claimed 25 to 30% of soldiers seeking jobs in the city had never lived there, but the real figure was 1.6%), and had to be reassured by the government.\n\n**Sources:**\n\nClay, Steven E. *U.S. Army Order of Battle 1919-1941 Volume 1: The Arms: Major Commands and Infantry Organizations*. Fort Leavenworth: Combat Studies Institute Press, 2010.\n\n[Combat Chronicles of U.S. Army Divisions](_URL_3_)\n\n“Demobilization After the First World War.” Social Service Review 18, no. 2 (1944): 248-250.\n\nE. Jay Howenstine, Jr., \"Demobilization After the First World War.\" The Quarterly Journal of Economics 58, No. 1 (Nov., 1943): 91-105.\n\n*United States Army in the World War 1917-1919, American Occupation of Germany, Volume 11*. Washington: United States Army Center of Military History, 1948.\n\nUnited States. United States Army. *History of Personnel Demobilization in the United States Army.* By John C. Sparrow, Major, Quartermaster Corps, United States Army. Washington, D.C. Department of the Army, 1952.\n\n*Valuable Information for Discharged Soldiers of the United States Army*, Compiled Under the Direction of Major General W.A. Holbrook, U.S. Army, Commanding Camp Grant, Illinois, by Major Francis B. Eastman, Camp Morale Officer\n\nWilson, John B. *Maneuver and Firepower:\nThe Evolution of Divisions and Separate Brigades*. Washington: United States Army Center of Military History, 1998.",
        "William J. Henry (December 2, 1867  November 26, 1955) was an American hymn writer and evangelist.\n\nHe originated from Ohio. He was an early leader in the Holiness Movement near Boyertown, Pennsylvania. From 1889 to 1895, he toured eastern Pennsylvania and New Jersey for several months each year, holding revival meetings. In 1893, he traveled to Liverpool, England, and founded a small congregation there. He later founded a congregation in Springfield, Missouri, which he served as pastor for 13 years.\n\nHe wrote more than 300 hymns, several of which continue to be included in hymnals. Some have been translated into German and Scandinavian languages. One of his hymns, \"Fully Saved Today\" (1911), is the basis of the gospel blues song \"Church, I'm Fully Saved To-Day\" (1930) by the Texan Blind Willie Johnson.\n\nReferences \n\n1867 births\n1955 deaths\nAmerican Christian hymnwriters\nSongwriters from Ohio\nPeople from Toledo, Ohio",
        "Along the paper we publish a little tutorial. It contains the basic functionalities. \n\n",
        " You will need to take this nationally standardized test for entrance into any graduate program--master's or doctorate. The current GRE has a verbal, math, and essay section. Most humanities departments, such as English and/or Literature, will focus on the verbal and writing scores.Acquire a practice test and take some sample tests to help get a better score on the actual test.\nCheck your prospective program's guidebooks, usually on the department's website, for average accepted scores, as the scoring scheme on these tests changes from time to time.\nGRE scores are valid for 5 years after the year in which you took the test. The year runs from July 1-June 30.If your scores are more than 5 years old, you will likely need to retake the GRE.\nSubmit the scores on your tests to the correct school(s) you are applying to during the actual test. You can order up to four scores at the time that you take the test. If you are applying to more than four programs, you will need to order additional score reports for a fee.;\n, Not all doctoral programs require this subject test, but many do. Check the website or contact your prospective program's graduate coordinator to find out whether you need to take this test. If you do, you will almost certainly need to take it in addition to the GRE General Test.This subject test consists of about 230 questions that cover the full range of literature in English, including a few questions on important works of \"world lit\" that are routinely translated into English, such as Homer's Iliad or the Christian Bible.The test also covers basic knowledge about important cultural and historical contexts, as well as literary criticism.\nThe subject test is only offered on paper and is only offered at certain times of the year, which means that you need to schedule your exam well in advance. Additionally, you should allow up to six weeks for your scores to become available to your chosen graduate program(s)., The humanities can cover a wide variety of subjects. You do not have to have a master's degree in English to apply for an English Ph.D. program, but you should consider getting your earlier degrees in topics related to English/Literature.Some Ph.D. programs will admit students directly from undergraduate programs, without requiring a master's degree first. You can investigate this option too.\n\n\nLook through your prospective master's program's degree offerings for an English and/or Literature department.\nContact the graduate coordinator for the master's program you're considering to see if she recommends the program as a starting point for an academic career in English and/or literary studies. If not, she may be able to direct you to better departments or programs.\nLook into related fields like history, anthropology, philosophy, and theology for potential leads on programs and classes that will benefit your breadth and depth of study.\nIf your master's degree is in a completely unrelated field, such as computer programming, don't despair. English is a very flexible field and welcomes people with all types of backgrounds. However, you will need to carefully and convincingly explain why you want to do a Ph.D. in English and how your experience has prepared you for the program.\n\n, If you're already in a master's program for English/Literature, you might ask about staying at your current school for the doctorate degree. Some schools will allow students to combine some master's degree achievements into their doctorate program.If you have a \"fast track\" option you can and want to pursue, ask your advisor if you're in a position academically to move onto it by your program's deadline. Some will admit you only if you intend to pursue a doctorate.\nAcquire an accurate list of the requirements of the doctorate program in comparison to your existing master's program. This likely means increased credit hours, differences between a thesis and a dissertation (the final written composition you will produce), and changes to your course selection.\nEven if you do not have a \"fast track\" option, you may investigate the choice of staying in your current school for the Ph.D. after achieving your master's degree. However, this may require a fresh application, just as if you were a new student to the school.\n\n, Most humanities doctorate programs in the United States, English/Literature included, require at least reading knowledge of at least one foreign language. If you are considering a prestigious graduate program, you may need to demonstrate competency in two or three languages.\n\n\nIf your thesis or dissertation involves incorporating multinational and older works, you may need additional languages beyond the program requirements.\nCheck your prospective program's course lists for what languages are regularly offered.\nGet a schedule of when the official knowledge reading exams or equivalent tests are given each semester.\nConsult with your prospective program about their requirements for passage of the language credit, especially if a class or just an exam is needed to pass the credit.\n\n, The thesis is the major writing composition at the end of a master's degree. These projects, at least for the master of arts in English/Literature, are lengthy original works in poetry, fiction, drama, rhetoric and/or similar genres.Choose something that will inform your dissertation later. Don't \"reinvent the wheel,\" as this will take you too long. Write about something that's already been done but look for new inspiration for your subject matter to write about or find a different angle within an established topic.\nKeep a portfolio of all your work. This will be helpful in applying for doctoral programs, teaching jobs, and/or other forms of employment should you pursue them.\n\n, Look at your prospective schools' department lists to see if they have an English/Literature department.You may want to take popular rankings systems, such as U.S. News & World Report's \"Best Grad Schools,\" into account.Take a close look at the faculty list of the top few programs you are most interested in and narrow down the list to professors that cover the sub-fields of English/Literature you are interested in.Consider highly ranked schools first. Students who earn their doctorates from the top 10 programs are significantly more likely to find tenure-track positions after graduating.Apply to the most prestigious programs you think you have a shot at. It is also wise to have a \"fall-back school,\" which is well-ranked but more within reach.\n\n, This professor will most likely direct both your program of study and dissertation.\n\n\nIntroduce yourself and your ideas for the theoretical project you would like to work on with this faculty member.\nExplain about the work you've already done, including compositions from your master's program. This is a good chance to engage the faculty member on literary, rhetorical, and pedagogical (teaching) ideas and see if you can form a good professional relationship.\nThe professor you pair up with finally is frequently referred to as your \"major professor\" for various purposes. Sometimes they will also be called your dissertation director when that time comes.\nOn rare occasions, major professors will leave a department/university for one reason or another. It is possible to still work with them remotely if you can get permission from your department/program. Otherwise, you will have to find another professor in the department to take you on.\n\n, If you've narrowed your prospective program choices down to just a handful, then physically going to them may help make your decision about where to spend the next few years it may take to get your degree.Schedule time to meet with the professor(s) you may work with. See if you can at least with your potential major advisor to go over likely dissertation subjects, courses, and program expectations.\nTake a walk around the department. Find out what the other English, Literature, and Creative Writing professors and grad students are studying. They may contribute to your work and vice-versa.\nFind out if there are any writing clubs, literary workshops, poetry reading groups, book clubs or similar organizations that work with the department or university that could be of use to your topic. Get their contact information at the very least.Ask to talk with current doctoral students in the program. Many graduate programs will organize informal lunches, dinners, or social times with current grad students so that you can ask questions about what their experience is like. These can be just as informative as speaking with the faculty, if not more so. Ask about the camaraderie of the department, work/life balance, and whether there are any \"problem\" faculty you should be aware of.\n\n, You will need these for your applications to other graduate schools. These letters will, ideally, highlight your aptitude and work ethic, among other positive traits observed by those that know your work and character.Try to give your chosen writers at least a month's advance notice so they can take their time writing the letter. Consider providing your writers with a copy of your curriculum vitae (C.V.) to help remind them of your work. Be transparent and let them know exactly what the letter is for and the school you intend to apply for. They can tailor the letter better for your needs.\nPick writers that know your work, such as teachers and employers. Make sure you seek letters that include some from your master's program faculty--especially your thesis advisor. Your program may have specific requirements about who writes the letters.\nChoose not to see the letters before they are sent. Sometimes you are given the option of viewing sealed or unsealed letters before they are sent to where you apply, and some schools will invalidate the letters if they are unsealed. Additionally, requesting to examine the letter may seem as though you do not trust the sender. If the faculty member seems hesitant to recommend you, it is better to find someone else than take a half-hearted letter.\nConfirm with the school if the letters need to be sent from the university, on university letterhead, submitted electronically, or should be sent with your application packet.\nIf there is a physical mailing involved, provide the properly addressed envelope and postage to your writers. If there is an electronic submission involved, make sure all the web addresses are correct, and get confirmations.\nIf it's approaching the deadline and your recommenders have not provided their letters, it's acceptable to give them a polite reminder about your request for a letter of recommendation.\n\n, Your personal statement, sometimes called a statement of purpose, is a major component of your application. It introduces you to the admissions committee and makes your case as to why you want to pursue a doctorate in English/Literature. You should prepare to write several drafts of this important document and get feedback from mentors. The personal statement may be anywhere from 500 words to two pages.Discuss your academic background, research interests, and professional aspirations. If you have already conducted extensive research or have peer-reviewed publications, mention them here. Talk in detail about what you want to research and how the degree will further your career goals.Mention specific attractions about the program to which you’re applying. For example, do they have an extensive rare manuscripts library? Is there a faculty member you are keen to work with? You don’t need to “suck up” to the committee, but making informed statements about why you want to study at each particular program will help convince the committee that you have prepared for the work ahead.Avoid cliches. Do not talk about how you have always liked to read, or how you burn with a hard, gem-like flame for poetry. Do not use a dictionary definition or famous quote to open your essay. Convey that you are engaging with the field at a professional level.\nShow, don’t tell. Use specific examples whenever possible. For example, don’t say “I am passionate about literature.” Say “In my spare time I volunteer as a reading group coordinator for underprivileged schoolchildren because I want to share with them my passion for literature.”Try to keep your writing polished but approachable. You don’t want to come across as stuffy, but you also don’t want to come across as unsuited for the rigors of a Ph.D. program.\n\n, You will need to submit a writing sample to your doctoral program. This sample displays your intellectual abilities, the quality and rigor of your work, and your promise as a scholar. Ask for help from a mentor or faculty member in choosing what essay you will use as your writing sample.You may be asked to prepare several writing samples, especially if you are applying to a doctoral program with a creative writing emphasis. Choose samples that reflect the variety of your work as well as the quality of it.\nKeep your sample to the length requested. If this means you have to cut chunks out of it, do so. Do not submit a sample significantly longer than requested. The committee may well simply stop reading it., You will likely need to submit your CV along with your other application materials. If you are applying to the Ph.D. directly from your undergraduate degree, you may need to include some accomplishments from your high school years. If you are applying from your master’s program, make sure that you don’t include too much early stuff -- this can make it appear as though you are padding your CV. However, unlike with a traditional resume, the lengthier your CV, the better, so do not leave off important information.Include the following sections:Heading. This includes your name, address, and contact information\nEducation. This includes your degrees, beginning with those in progress or most recently earned. Include the name of the institution, city/state, degree type and major, and the month and year the degree was (or will be) awarded. If you graduated with honors, it is acceptable to mention it. If you are coming from a master’s program, state your thesis title and advisor’s name.\nRelevant Experience. List positions that reflect your experience and expertise. If you have prior teaching or tutoring experience, highlight this, as you will likely need to teach to support yourself in the doctoral program. Include your job title, organization name, city/state, and the dates you held each position. Include a brief summary of your responsibilities. Don’t include non-academic jobs here.\nPublications. If you have already published research or creative work, list it here. If you do not have publications, you can title this section “Research Experience” and discuss any major research projects you have conducted or assisted with.\nProfessional Presentations. If you have attended conferences or other professional events, list them here.\nHonors and Awards. List any competitive scholarships or fellowships you have received. You may also list scholastic honors and other awards.\n\n, Some will be printed, some will be online. Make sure you have mailing supplies (large yellow envelopes, postage stamps) if it's a print application. Make sure you have the correct browser software and settings according to the instructions if it's online.\n\n\nThese can be large and detailed forms, so take your time. If it's a paper application, print a couple of copies in case you fill out something incorrectly and need to redo a page. Hold off stapling the pages until the end.\nFor online applications, make sure you set aside plenty of time since they often are done in one sitting. Save your work frequently if possible.\nUse a large yellow envelope to mail your application materials to the university in the case of a print application. If you are unsure of the amount of postage you need, take the entire pack to your local post office and they will weigh it, let you know the postage amount, and mail it for you if request.\nSubmit your application online if that is the method available.\n\n, Ask friends to proofread your application as well. Print out your documents and read them in hard-copy. You are applying to the highest-level degree in your field, and the admissions committee will notice minor errors. Sloppy proofreading suggests a lack of commitment., You should consider applying to at least half a dozen schools, if not more. These departments can vary considerably in size and funding, so it's hard to predict how much competition you will be facing.\n\n\nMost programs will have fees associated with applying, so take that into consideration with the number of applications you submit.\nBe prepared that you may get staggered responses from your different applications. Some may accept, wait-list, or reject you earlier than others.\nIf you are accepted or wait-listed fairly early, be alert to response deadlines since most programs are aware that students they accepted sometimes refuse admission. This may open up a position for a wait-listed student, but also may compel you to make a decision on one school that accepted you before others have responded.\n\n, Make sure they arrived at your prospective doctoral program by its application deadline.\n\n\nSome schools will have this service for free, some will have you pay for it. But to be official, they must be submitted by your school to the destination. Some schools will ask for copies from you in the interim, but those will be unofficial.\n\n, The names of the major field may differ from school to school, but some examples of major concentrations include: medieval, Renaissance, Restoration and 18th century (British/American), 19th century (British/American), criticism, theory, race and ethnicity, gender and sexuality, and rhetoric.Most universities require a major field to be taken along with several minor fields to improve a graduate's breadth of academic coverage. So you may focus on one area, but still take a few classes in others you weren't initially interested in.\n\n, These will be professors that lead your minor fields and consult for your comprehensive exams in addition to your major advisor. They may or may not continue onto your dissertation committee.\n\n\nTake classes with at least some of your committee members if possible. It will help them learn your work, and facilitate collegiality (professional friendship).\nSome universities require an \"out of area\" member--a professor from another department to ensure your department is not giving students a free pass. Ask your major professor who they recommend, or if you have a professor in mind, run the choice by your advisor first.\nOccasionally committee members will retire, move, or for whatever reason leave the department. If this happens you can arrange with your department to conduct academic business with them remotely, or consult the procedures to replace them.\n\n, These professionals are the source of information and guidance throughout your program.\n\n\nCheck with your department's academic advisor especially for administrative information and deadlines. Also the advisor can help make sure you're on time with fulfilling the required credits for your major and minor fields.\nMeet with your major professor frequently for guidance on theoretical ideas, professional development, and to make sure you are meeting program requirements in conjunction with the academic advisor.\nCheck your program rules for how many credits you need for each major and minor concentration. You might want to keep your own chart in addition to the department's records.\nSee if some classes \"double count\" for major and minor fields, two minor fields, etc., with your advisor. This can free up room to take other classes you need or might be interested in.\nDo not be averse to taking classes out of your \"comfort zone.\" This can lead to helpful collaborations with other colleagues, introduce rich ideas into your work, and provide breadth and depth to your professional applications later.\n\n, In most doctorate of English/Literature programs, a minimum of 2 years or 30 credit hours of full-time coursework is required for the degree beyond the master's degree. But this can vary widely.Pick mostly classes that also help your dissertation along with the writing that you will complete for them.\n\n, Most graduate programs fund their students through assistantships of some kind. Frequently, teaching assistants provide grading or some other form of assistance to professors and the department on a part-time basis during the semester.\n\n\nTake advantage of opportunities to instruct a class if you get them. This will be practice for a teaching position should you get one, and experience you can put on a job application later. Note that even if you are the instructor of record, meaning you are entirely in charge of teaching the class, you will likely still be called a “Teaching Assistant.”\n\n, Most programs require students to pass a written and/or oral comprehensive exam to become a Ph.D. candidate, at which point all coursework is recognized as complete and only the dissertation remains for degree completion.\n\n\nPrepare for this test to cover the major and minor concentration fields. In some programs, you may get to choose which areas you take exams in. Other programs may have pre-established examination areas.\nGet questions or prepare with your committee members ahead of time.\nExpect the written essay parts to last several hours, whether in take-home form or on-site in a classroom/computer lab form. If you know graduate students that have taken exams from these professors before, you should consider asking them what to expect--but remember your experience could still be different, their experience should just be a guide.\nOral exams typically involve the entire committee in the room together with you, asking you questions. This can be very intimidating, but typically you will not be allowed to enter comprehensive exams unless your major professor thinks you are ready. If at any point you are having serious questions, consult your major professor first.\n\n, You should do this to plan your dissertation's development. Your status at this point frequently will be called \"ABD\" (all but dissertation).\n\n\nDiscuss fellowship and grant funding opportunities for your dissertation work.\nEstablish your dissertation committee, keeping in mind it may differ from your committee leading up to comprehensive exams. You should pick committee members that contribute to your work's theoretical framework, but also can help with the practical writing critique.\nGet an idea of time-frame for completion. This will likely change, but you should keep in mind your program's deadline to complete your degree requirements. Otherwise, you may need to retake your comprehensive exams or file an extension (to allow more time to complete the degree) with your university.\nDiscuss other professional activities you might do while \"ABD,\" including book reviews, conferences, teaching assistance, etc.\n\n, Consult frequently with your major advisor about your work. Ideally, you can build off of ideas you generated earlier in your master's program and/or classes, but either way you will be doing a considerable amount of additional writing.Outline your rhetorical ideas, characterizations, plots, inspirations, philosophical arguments, methods, and keep a list of works you may cite.\nPlan a more detailed time frame for completing each step of your dissertation including time to travel if needed to gather resources, when you might complete chapters, and potential defense dates.\nExplain how your work fits in genre of literature you seek to enter.\n\n, Outline the dissertation itself including chapters, subheadings, and a works cited section. This will organize your thoughts, and keep your workload manageable.\n\n\nConsider dropping in headings of works you have already done as potential chapter titles\nHaving sections to work on will allow you to more easily budget your time to work on the dissertation. You can work on a section, take a break, move onto another, and repeat.\nEnsure an overall theme or current of thought is running through these sections you are setting up. If you set this up in your outline, it will focus your ability to maintain coherent thoughts in the body of the dissertation.\n\n, There are numerous funding opportunities for humanities and graduate students. Consult with your program's and college's listings as well as any academic societies you may belong to. These can be competitive, so look for and apply to multiple sources.Find funding for the right stage of your work \"pre-dissertation,\" while you're researching, and others while you're in the writing phase near completion.Ensure that your project falls within the grant/fellowship scope, and write any application letters to argue for such a case.\nMake a note of deadlines and notification time frames as they tend to move from semester to semester and year to year.\nCheck to see if you can satisfy the grant/fellowship commitments beyond your own work. Some will ask you to conduct lectures, present your progress, travel, etc...\n\n, Like the thesis, this will be mostly original work. Bring in the work you used for your master's degree if applicable. Look for contributions that will enrich your content.\n\n\nAsk experienced writers for help on other techniques, plot devices, and/or writing strategies that might help your work along.\nPresent work you do at conferences to get early feedback before it’s in the final form. You may also get ideas on more concepts you had not considered.\n\n, Keeping a steady writing schedule can be extremely difficult, especially as one is likely to have other distractions from life crop up by the time the dissertation writing phase arrives.\n\n\nTry to write just a little each day, maybe a couple of hours. If you can't, then maybe set aside blocks of time you know you won't be disturbed on specific days.\nShop around parts of what you write to your major advisor or graduate students at the same level of progress as yourself. Putting together a group of students at the dissertation phase can be mutually motivating.\n\n, Upon completion of the dissertation, each student meets with a dissertation committee--usually the major advisor, a few others from your department, and one \"out of area\" faculty member. If the committee approves the dissertation and defense, you will have completed the degree.\n\n\nMake sure you have consulted with your committee on a date and gotten a location for the defense. Frequently these defenses are budgeted to last two hours, but may be shorter.\nIf one professor is abroad, see if teleconferencing them in is an option.\nCheck with your college office/advisor to see if your dissertation has met all formatting requirements.\n\n",
        "Authors propose a neural pruning technique starting from trained models using an approximation of change in the cost function and outperform other criteria. Authors obtain solid speedups while maintaining reasonable accuracy thanks to finetuning after pruning. Comparisons to existing methods is weak as GFLOPS graphs only show a couple simple baselines and no prior work baselines. I would be more convinced of the superiority of the approach with such comparison.",
        "It's important for you to realize that Energy is not a thing. It's not a material entity, not something you can look at a microscope and say: \"here, look, that is energy!\". Energy is a number, nothing more than that! There is matter, like electrons, photons, quarks and so on... and all this matter has labels, such as mass, spin and momentum, and energy is just another label!\n\nWhy do we need this number? It turns out that the universe was kind enough to give us conservation laws, and it turns out that all of physics can be written in terms of these conservation laws. Whenever there is a continuous symmetry in the laws of physics, we can derive one number that is conserved because of this symmetry.\n\nSince by definition, the laws of physics are invariant under time translations, we have a conserved quantity under this symmetry, which is nothing more than energy. So energy is special, because it's *the number that globally never changes*. \n\nSo, in a more ELI5 way to put it: there is matter: photons, electrons, quarks, apples, trees, planets, etc etc. And we want to describe the world in terms of how this matter interacts. It doesn't matter if you're writing a theory of apples falling from trees or particle collisions, and the end of the day you need to find good quantities in order to parametrize your processes. The best quantities are always those that \"change less\", because this invariance allows you to write \"final variables\" in terms of the \"initial ones\". So that's why we have energy, simply because of accounting.",
        "Simply put the Roman allies system was superior. \n\nRome slowly but surely made its allies part of its system, for every defeat of a neighbouring village Rome allowed the defeated village to become a part of the Roman system. Subdued foes fought for it as part of the Roman army, and while subdued elites were not Senators and didn't have citizenship they still very much formed part of the Roman system and reaped the rewards from being part of it. \n\nThis gave the Romans a huge pool of manpower to play with compared to other states. Especially when Rome started to be willing to hand out citizenship.\n\nCarthage on the other hand was a city state, it relied on a couple of field armies led by a small core if its citizens and never made any particular attempt to expand its citizenship or rights to its subjects. Allies served as just allies for the campaign or war they were involved in. Foreign elites were bribed and given gifts but never really integrated. Soldiers were hired as soldiers rather than serving as part and parcel of an integrated army. \n\nIts also worth noting that territory is not equal to resources. \n\nLets take the Second Punic War as an example: \n\nWhile on paper Carthage was larger in the Second Punic War you need to examine which bits were under actual control and how long that was the case. \n\nCarthage controlled much of North Africa and Spain, however only a chunk of North Africa was actually Carthaginian territory by itself, the rest of it was held by subject Numidians who had a frequently antagonistic relationship with Carthage. Meanwhile in Spain it was Hannibals father who had done much of the legwork in turning it into \"Carthaginian territory\" if we look at Spain at the time though it was made of a plethora of tribes and villages who were locked into a raiding and prestige lifestyle. Again this ensured the territory was in no way actually Carthaginian. These people were fighters, they fought for honour and for money with each other and against the Carthaginians. As long as the Carthaginians could give them money and show they were strong they would fight for them, the minute weakness was shown then there would be no incentive to fight, which is just what happened when the Romans started to make inroads to the area. \n\nCompare this to Italy where Rome controlled a much more dense web of allies in the Latin states immediately around it, these areas where very much part of the system and willingly sent men to fight for Rome and kept on doing so throughout the dark days of Hannibal right until there were literally no more men to send. Its in the South where Hannibal made some gains amongst the former Greek city-states but even then surprisingly few of them turned and none of them were especially useful at reinforcing Hannibal when they did turn. In a way they crippled him because the more defected the more Hannibal had to protect and he had only a single army to do that with. Rome could very much grind him down and take city after city and leave him with no good choices to make. \n\nCouple that with the crippling logistics issues with trying to get Carthage to actually reinforce him and you see why he couldn't win. \n\nOn that note we'll move onto logistics and politics. \n\nNow logistically sailing in the ancient world relied on access to food and water for the crews, any attempt to reinforce Hannibal in Italy would rely on sailing for several days via hostile territory and in the face of enemy resistance and landed a few thousand more troops. Not exactly the easiest thing to do... \n\nPolitically there was also the clear difference between the Roman and Carthaginian senate. Each year the Romans could and did give clear priorities to one theatre or another and allocate resources for the entire state, this was light years ahead of the Carthaginian effort which only seemingly knew what Hannibal was about to do when the Romans arrived and asked to them to ensure he didn't cross the Ebro and attack Roman allies. There was a distinct level of infighting and not knowing what the left hand was doing compared to the Roman method of clear allocation and command responsibilities in the war effort. \n\nMain source: \nThe Punic Wars - Adrian Goldsworthy",
        "  Stochastic Loewner evolution also called Schramm Loewner evolution\n(abbreviated, SLE) is a rigorous tool in mathematics and statistical physics\nfor generating and studying scale invariant or fractal random curves in two\ndimensions. The method is based on the older deterministic Loewner evolution\nintroduced by Karl Loewner, who demonstrated that an arbitrary curve not\ncrossing itself can be generated by a real function by means of a conformal\ntransformation. In 2000 Oded Schramm extended this method and demonstrated that\ndriving the Loewner evolution by a one-dimensional Brownian motion, the curves\nin the complex plane become scale invariant; the fractal dimension turns out to\nbe determined by the strength of the Brownian motion. SLE fills a gap in our\nunderstanding of the critical properties of a variety of lattice models in\ntheir scaling limits and supplements the result obtained by means of conformal\nfield theory. In this paper we attempt to provide a simple and heuristic\ndiscussion of some of the important aspects of SLE.\n",
        "This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.\n\nOn balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.\n\nDetailed/minor points below:\n\n1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.\n2) The tables need better and more descriptive labels.\n3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?\n4) Why was \"Enriched CBOW\" not included in the analogy task?\n5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.",
        "Edit: this post was locked way too soon. We're cops with thick skin, we can handle it. Sorry to those I didn't get to answer your questions. \n\nLaw enforcement here. Such a question is done for a number of reasons. \n\nFirst off and probably most important is to establish a dialogue to accertain the persons level of nervousness. Almost everybodies nervous when they get pulled over, however, through experience you quickly start to tell the difference between the type of nervousness that's \"oh shit, I'm about to get a ticket\" and \"fuck, I hope he doesn't find the brick of cocaine in the trunk\" and most importantly \"should I pull out my gun and shoot this pig\" this question usually leads to follow up questions and gives you a baseline of what type of person you're dealing with really quickly and if they might be a threat and if you need to call for backup. Please understand that we may ask you silly, even obvious questions. And sometimes people get really offended when asked where they are going. We don't give a shit where you're going personally, we are painting a picture trying to decide if you're engaging in something more than a simple traffic stop or if you're about to stick a gun in our face. I have asked people what they have for breakfast. It's all about that baseline and observing people's responses to questions they won't usually lie about, vs questions they will lie about. \n\nSecondly, as some have already said, it puts you on the spot and might elicit a confession. As police, we have a lot of encounters with a lot of people and when we get called to court which is sometimes months down the road, most of the time we only have our notes to remind us of these traffic stops (who has time to pull and watch dash cams for a simple traffic ticket that your salary isn't dependent on). If someone comes right off the bat and admits they were speeding, you write that in your notes and at court you get to say, I don't recall our conversation but according to my notes, the person admitted they were speeding. Boom, guilty. \n\nAnother thing, which is probably more of a preference thing, you want to get an idea on what the persons attitude is. As a police officer (and I'll be honest, some take a long time to realize this, some never realize it) the biggest power you have is the power of discretion. Personally, I always took the attitude of 'what did the person do to get themselves in this situation, and what kind of response is needed from me to get them to correct it\" \n\nSo what I mean by this, when you pull over somebody and they start to downplay why you pulled them over, you're obviously going to have a measured response against that. For example, you pull somebody over in a construction zone for speeding. They start telling you something like, \"well I didn't see any workers\". That's somebody that NEEDS to see a ticket, plus the double fine that construction zone usually brings. That's somebody that clearly doesn't realize how many construction workers and other personnel die every year because of people like them. Where as if somebody responds like, \"yes I'm sorry. I realize I was going to fast in a construction zone, I got complacent\", you can say to yourself that they probably feel bad enough getting pulled over and may likely, at the very least, slow down in the future going through construction zones, so you can just give a simple warning or at least not tack on the construction zone enhancement. \n\nMost cops do have a pet peeve though. For example, children jumping around in a vehicle. Don't give a damn want the excuse is, your getting a ticket Everytime. Seat belts and car seats. No other excuse is getting you out of that one. I can't even start to get into how horrifying any type of crime scene is when it involves children, especially motor vehicle accidents.",
        "The paper proposes an online variant of segment to segment transducers, which allows to circumvent the necessity of observing whole sentence, before making target predictions. Authors mostly build on their previous work, allowing additionally to leverage independent priors on the target hypotheses, like the language grammar or sentence length.\n\nStrong points:\n- well written, interesting idea of combining various sources of information in a Bayesian framework for seq2seq models\nHandling something in an online manner typically makes things more difficult, and this is what the authors are trying to do here - which is definitely of interest to the community\n- strong experimental section, with some strong results (though not complete: see weak points)\n\nWeak points:\n- Authors do not improve on computational complexity (w.r.t Tillmann proposal), hence the algorithms may be found difficult to apply in scenarios where inputs may be long (this already takes into account a rather constrained model of alignment latent variables)\n- What about the baseline where you only combine direct, LM and bias contributions (no channel)? Was there any (non-obvious) algorithmic constraint why - this has not been included?\n\nSome other (minor) comments:\n\n- Related to the first weak point: can you elaborate more on how the clue of your work is conceptually different from the work of Tillmann et al. (1997) (except, of course, the fact you use connectionist discriminative models to derive particular conditional probabilities). \n- How sensitive is the model to different choices of hyper-parameters in eq (3). Do you naively search through the search space of those, or do something more clever?\n- Some more comments on details of the auxiliary direct model would be definitely of interest.\n- How crucial is the correct choice of the pruning variables (K1 and K2)? \n- Sec. 2: makes no Markovian assumptions -> no first-order Markovian assumption?\n\nTypos:\nTable 1: chanel -> channel (one before last row)\n\nApologies for late review.",
        "Updated version:\n- added a section with generative results, including movies showing what the network has learned over the course of training\n- used the ICLR style file",
        "The Girenspitz (2,394 m) is a mountain in the Rätikon range of the Alps, located north of Schiers in the Swiss canton of Graubünden. It is the culminating poing of the group lying between the valleys of the Taschinasbach and the Schraubach.\n\nReferences\n\nExternal links\n Girenspitz on Hikr\n\nMountains of the Alps\nMountains of Switzerland\nMountains of Graubünden\nTwo-thousanders of Switzerland",
        "Update: I thank the author for his comments! At this point, the paper is still not suitable for publication, so I'm leaving the rating untouched.\n\nThis paper proposes a transfer learning method addressing optimization complexity and class imbalance.\n\nMy main concerns are the following:\n\n1. The paper is quite hard to read due to typos, unusual phrasing and loose use of terminology like “distributed”, “transfer learning” (meaning “fine-tuning”), “softmax” (meaning “fully-connected”), “deep learning” (meaning “base neural network”),  etc. I’m still not sure I got all the details of the actual algorithm right.\n\n2. The captions to the figures and tables are not very informative – one has to jump back and forth through the paper to understand what the numbers/images mean.\n\n3. From what I understand, the authors use “conventional transfer learning” to refer to fine-tuning of the fully-connected layers only (I’m judging by Figure 1). In this case, it’s essential to compare the proposed method with regimes when some of the convolutional layers are also updated. This comparison is not present in the paper.\n\nComments on the pre-review questions:\n\n1. Question 1: If the paper only considers the case |C|==|L|, it would be better to reduce the notation clutter.\n\n2. Question 2: It is still not clear what the authors mean by distributed transfer learning. Figure 1 is supposed to highlight the difference from the conventional approach (fine-tuning of the fully-connected layers; by the way, I don’t think, Softmax is a conventional term for fully-connected layers). From the diagram, it follows that the base CNN has the same number of convolutional filters at every layer and, in order to obtain a distributed ensemble, we need to connect (for some reason) filters with the same indices. This does not make a lot of sense to me but I’m probably misinterpreting the figure. Could the authors revise the diagram to make it clearer?\n\nOverall, I think the paper needs significant refinement in order improve the clarity of presentation and thus cannot be accepted as it is now.",
        "The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.\nAlthough the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.\nThe proposed approach relates to Batch Norm and weight decay.\nExperiments are given on \"low-shot\" settting.\nThere seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?\nRegarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?\nOverall, the idea is simple but feels like preliminary: while it is supposed to be a \"soft BN\", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?\n\n-- edits after revised version:\n\nThank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:\n- on Omniglot, the paper is still significantly far from the current state of the art.\n- the new experiments do not really confirm/infirm the relationship with BN.\n- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.\nI'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.\n\n",
        ";\n,\n\n, At the top of the page, next to the ‘All Newsletters’ heading, click the \"Create a new email\" button.\n\n,\n\n, On the \"Automatically sent…\" line, leave the first dropdown set as \"When there’s new content…\" which will trigger an email to be sent at a time depending on what you set in the second drop down on that line. Your choices are:\n\n\n\"once a day at…\"\n\"weekly on…’\n‘monthly on the…\"\n\"monthly every…\"\n\"immediately…\"\nThe first four choices will bring up another set of drop downs with which you may set the date and time parameters based on the choice. If you select ‘immediately’ there will be no further choices.\n\n, Select what you’d like your email’s subject to be. You can also add some placeholder tokens that will automatically populate dynamic data into your subject line. Here are placeholder tokens you may use:\n\n\ndisplays number of posts.\nwill display the name of your latest post.\nwill show the issue number of your newsletter.\n\n, You must now select which list you’d like your automatic emails to send to. If it’s general subscribers to your WordPress site, i.e. they haven’t signed up for a specific newsletter, select the list called \"WordPress Users.\"\n\n\nOnce you’ve selected the list, click the \"Next Step\" button at the bottom of the page.\n\n, On the next page is your email. By default, it has the \"Automatic latest content\" already added. Hover over it, or any of the pre-populated text boxes, and click the \"Edit automatic latest conten\" button.\n\n\nHere, you can select whether posts or pages get sent, the maximum number of posts or pages per email, and you can also select which post categories get sent.\nFor any of the other pre-populated text boxes in the email, click the \"Edit Display\" button that appears. To remove them, click the \"X\" that appears at the upper right when you hover over it.\n\n, To the right of the email is a box with four tabs: the Content, Images, Styles, and Themes.\n\n\nOn the \"Content\" tab are four widgets you may click and drag into the email to the left. They are:\n\nTitles & Text\nAutomatic latest content\nDivider\nSocial Bookmarks\n\n\nOn the \"Images\" tab, you may click the \"Add Images\" button and upload pictures, or select them from your media library.\n\"Styles\" is where you set the colors, fonts, and other style parameters. ‘Themes’ is where you can select an overall style template for your newsletters.\n\n, When you’re happy with the appearance of your email, scroll to the bottom of the page and click the \"Next Step\" button.\n\n, On the \"Final step: last details\" page, you will have the opportunity to review everything you did on the \"First step: main details\" page.\n\n, On the \"Sender\" line, enter the name and email address from which this email is being sent.\n\n,, Below the \"Reply-to name & email\" line, enter your email address in the text box, and click the \"Send preview\" button. Check your email, and make sure your email looks like you expect it should.\n\n, Click \"Activate Now\" and you have an automatic email that sends a message when your WordPress site is updated.\n\n,,,, On the page, this is next to \"What type of newsletter is this?\"\n\n, On the \"Automatically sent…\" line, click the dropdown and select, \"When a new WordPress user is added to your site.\"\n\n\nThe next dropdown will let you select what level of user will receive the newsletter (i.e. subscriber, contributor, editor, administrator, or any), and on the next dropdown, select when will the new user receive this email.\n\n, On the next line, add your \"Subject.\" There are no dynamic tokens for this type of message, so whatever the subject is will be as you write it.\n\n, Editing the email is the same as listed above, EXCEPT you may also select a particular post or page to add to this email by clicking and dragging the \"WordPress post\" bar into the email, in the box to the right of the template.\n\n\nIf you want to add a particular post or page, go to the next step, otherwise skip to step 9.\n\n, Click and drag the \"WordPress post\" widget bar into your email. A dialog box will open.\n\n\nAt the top left of it you may select whether you’re adding a post or a page, then you may filter by category and whether it’s published or not.\nIn the box below, select which post or page you’d like added to this email, and then click ‘Insert selected’ at the bottom right of the dialog box.\n\n, When you’re happy with the appearance of your email, click \"Next Step\" at the bottom of the page.\n\n, On the \"Final step: last details\" page, you will have the opportunity to review everything you did.\n\n,,, Enter your email address in the text box, and click the \"Send preview\" button. Check your email, and make sure your email looks like you expect it should.\n\n, Click \"Activate Now\" and you have an automatic email that sends a message when your WordPress site is updated.\n\n",
        "  Using local morphological measures on the sphere defined through a steerable\nwavelet analysis, we examine the three-year WMAP and the NVSS data for\ncorrelation induced by the integrated Sachs-Wolfe (ISW) effect. The steerable\nwavelet constructed from the second derivative of a Gaussian allows one to\ndefine three local morphological measures, namely the signed-intensity,\norientation and elongation of local features. Detections of correlation between\nthe WMAP and NVSS data are made with each of these morphological measures. The\nmost significant detection is obtained in the correlation of the\nsigned-intensity of local features at a significance of 99.9%. By inspecting\nsigned-intensity sky maps, it is possible for the first time to see the\ncorrelation between the WMAP and NVSS data by eye. Foreground contamination and\ninstrumental systematics in the WMAP data are ruled out as the source of all\nsignificant detections of correlation. Our results provide new insight on the\nISW effect by probing the morphological nature of the correlation induced\nbetween the cosmic microwave background and large scale structure of the\nUniverse. Given the current constraints on the flatness of the Universe, our\ndetection of the ISW effect again provides direct and independent evidence for\ndark energy. Moreover, this new morphological analysis may be used in future to\nhelp us to better understand the nature of dark energy.\n",
        "The paper provides a first study of customized precision hardware for large convolutional networks, namely alexnet, vgg and googlenet. It shows that it is possible to achieve larger speed-ups using floating-point precision (up to 7x) when using fewer bits, and better than using fixed-point representations. \n\nThe paper also explores predicting custom floating-point precision parameters directly from the neural network activations, avoiding exhaustive search, but i could not follow this part. Only the activations of the last layer are evaluated, but on what data ? On all the validation set ? Why would this be faster than computing the classification accuracy ?\n\nThe results should be useful for hardware manufacturers, but with a catch. All popular convolutional networks now use batch normalization, while none of the evaluated ones do. It may well be that the conclusions of this study will be completely different on batch normalization networks, and fixed-point representations are best there, but that remains to be seen. It seems like something worth exploring.\n\nOverall there is not a great deal of novelty other than being a useful study on numerical precision trade-offs at neural network test time. Training time is also something of interest. There are a lot more researchers trying to train new networks fast than trying to evaluate old ones fast. \n\nI am also no expert in digital logic design, but my educated guess is that this paper is marginally below the acceptance threshold.",
        " For example, people from his era would have thought differently about how we fight wars than we do now.;\n, Admiral Halsey graduated from the US Naval Academy in 1904., \"Bull\" studied medicine for one year at the United States Naval Academy, and admitted he learned little while there. Also, he enrolled in that university as his best friend was there, and earned several athletic honors., Awarded the Navy Cross for meritorious naval service, he earned the Navy Cross for distinguished service in WWI. During that time, he commanded the USS Shaw and specialized in torpedo boats and destroyers.\n\n, Halsey never took the easy road to success. In the 1930s, before taking command of the aircraft carrier USS Saratoga, he insisted on taking a more difficult and lengthy aviator course, and was the last one in the class to graduate as an aircraft pilot.\n\n, For example: Vice Admiral Halsey was at sea during Pearl Harbor, commanding the USS Enterprise. It is said that he stated: \"Before we're through with 'em, the Japanese language will only be spoken in hell.\" While such talk would be considered inappropriate today, it was not so in those days of war and revenge, after the massive bombing sneak attack by Japan at Pearl Harbor while the U.S. was still at peace with Japan.\n\n, Research his victories, and their importance in winning WWII in the Pacific. He helped secure Guadalcanal for the Allies in a crucial victory, and was also instrumental in victories in the Solomon Islands, Rabaul, the Bismark Archipelago, Palaus, Leyte and Luzon. He was also involved in raids on Japanese bases, such as in Formosa, China and Vietnam.\n\n, Admiral Halsey sailed to intercept a Japanese decoy fleet observed by U.S. spotter aircraft, and left the San Bernardino Strait unguarded where the Japanese main fleet attacked the US supply flight. From this, a small US escort aircraft carrier and three smaller ships were destroyed. While a heroic resistance by American sailors and aircraft pilots from several escort carriers prevented immense tragedy, an important Japanese fleet was able to escape when it might have been defeated soundly.\n\n, Also consider how the truly \"greats\" get back on their feet and move on. For example, in another blunder, known as \"Halsey's Typhoon\", Admiral Halsey did not retreat from powerful Typhoon Cobra but turned his ships into the storm, and three destroyers sank with many other ships damaged. 800 men died. A Navy court found that he committed an error of judgment, but he was not sanctioned.\n\n,, Some say that this humane treatment by victorious Allied forces was unprecedented in human history.\n\n, who after he was promoted to Fleet Admiral on 12/1945, retired from active duty on 3/1947. He and his wife are buried in Arlington National Cemetery.\n\n,",
        "Misiano's directed episodes of  45 different series including Chicago PD, Agents of S.H.I.E.L.D., The Blacklist, West Wing, Prison Break, Medium and Third Watch.  He was elected to three terms as National Vice-President of the Directors Guild of America.\n\nCareer\nMisiano began his directing career working in advertising . His first episodic directing assignment  CAME W ITH A 1995 episode of Law and Order. Two more episodes of L&O  led to two episodes of Ally McBeal in 1998. He became a regular director for the short-lived CBS drama Now and Again, directing seven first season episodes over 1999 and 2000. He directed a first season episode of That's Life in 2000. Later in 2000 he directed two episodes of Level 9.\n\nIn 2001 he directed an episode of the X-Files spin-off The Lone Gunmen. 2002 saw a significant increase in demand for his work. He directed single episodes of The Education of Max Bickford, MDs, and Ed. He also began a working relationship with ongoing series The West Wing and Third Watch. He directed a third season episode of The West Wing and returned in fall 2002 for a fourth season episode. Similarly, he directed a third season episode of Third Watch and returned for a further fourth season episode.\n\nMisiano continued his work on the fourth season of The West Wing into 2003 with a further episode. In fall 2003 he directed a fifth season episode of Third Watch and an episode of Judging Amy. In 2004 Misiano directed a fourth episode of The West Wing episode-7, \"Change is Gonna Come,\" this time for the shows sixth season. He also directed an episode of Hack. 2005 saw the end of Third Watch with its sixth season, Misiano directed his fourth episode of the show before it ended. Misiano became a regular director for Medium later in 2005. He directed two episodes of the first season and then returned in the fall to direct an episode of the second season. He also directed three second season episodes of The 4400. He worked on another political drama, Commander in Chief, and directed two episodes.\n\nIn 2006 Misiano directed the third season premiere of The 4400. He directed two episodes of short-lived legal drama Conviction, and single episodes of both Killer Instinct and Dr. Vegas. In fall 2006 he became involved with the second season of Fox action series Prison Break. He directed one second season episode in 2006 and another in 2007. He also directed a late third season episode of Medium in Spring 2007. In fall 2007 he directed a third season episode of Prison Break.\n\nIn Spring 2008 Misiano directed a first season episode of Eli Stone and three fourth season episodes of Medium. In fall 2008 Misiano returned to Eli Stone to direct a second season episode. He directed another second season episode in 2009 before the show's cancellation. He continued to direct for 'Medium, directing a fifth season episode in 2009.\n\nIn 2014, he directed episodes of the premiere of the second season and the first part of the two-parter season finale of Marvel's Agents of SHIELD, ABC's Resurrection, Agent Carter and three episodes of The Mysteries of Laura''.\n\nExternal links\n\nLiving people\nAmerican television directors\nYear of birth missing (living people)",
        "This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.\n\nThe main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.\n\nRegarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.\n\nRegarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. \n\nOverall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.",
        "So for starters, we know there has to be a lot of iron at the center of the planet. \n\nIf we just measure the mass of Earth and its size, then we can find our planet has an average density of 5.5 grams per cubic centimeter. Rock alone has a density of around 3 grams per cubic centimeter, so there must be quite a bit of something much denser for the total to average out to 5.5, and iron (with a density of 7.9) fits the bill nicely.\n\nOn top of that, we know we have a very strong magnetic field. That requires some kind of swirling magnetic fluid at the core to generate that field, which again points to iron as the culprit.\n\nOkay, but then how do we know the outer core is liquid and the inner core is solid? For starters, dynamo theory suggests you can't generate a magnetic field that strong without some kind of solid-liquid interaction. For example, Venus has no intrinsic magnetic field; it remains unknown if this is because the core is all solid, all liquid, or just doesn't have enough of a temperature gradient, but the point remains that you need that solid-liquid interaction.\n\nSo we know that there has to be some solid iron inner core, but how do we know how big it is? This is where earthquakes help us out. Seismic waves generated by earthquakes can travel deep into the Earth's core and back out again. Exactly where those waves are detected, which kind of waves are detected, and how strong they are can give us a lot of clues. \n\nWe see a sudden shadow of pressure waves about 105 degrees away from an earthquake's epicenter. If you back out the geometry, this gives you a zone of intense refraction from a sphere 3400 km in radius - that's the liquid iron outer core. On top of this, we also see a secondary zone of refraction, as well as some pretty intense reflection from a separate sphere 1200 km in radius - that's the solid iron inner core. The path of that reflection off the inner core maps out similar to the black lines in [this diagram](_URL_1_)\n\nAdditionally, we can verify this by studying the orbits of satellites around Earth. By measuring changes to their orbits very carefully, using some gravitational calculations we can back out the depths of sudden changes in density, similar to what's shown in [this diagram](_URL_0_). We see sudden density discontinuities for both a 1200 km inner solid core as well as a 3400 km liquid outer core.\n\n**TL;DR**: There has to be a lot iron just based on our density, we know some of it has to be solid based on our magnetic field, and we know its size based on seismic wave reflection. This is also verified by gravity measurements.",
        "  In the present paper we present results of calculations obtained with the use\nof the theoretical method described in our preceding paper [1] and perform\ndetail analysis of alpha helix-random coil transition in alanine polypeptides\nof different length. We have calculated the potential energy surfaces of\npolypeptides with respect to their twisting degrees of freedom and construct a\nparameter-free partition function of the polypeptide using the suggested method\n[1]. From the build up partition function we derive various thermodynamical\ncharacteristics for alanine polypeptides of different length as a function of\ntemperature. Thus, we analyze the temperature dependence of the heat capacity,\nlatent heat and helicity for alanine polypeptides consisting of 21, 30, 40, 50\nand 100 amino acids. Alternatively, we have obtained same thermodynamical\ncharacteristics from the use of molecular dynamics simulations and compared\nthem with the results of the new statistical mechanics approach. The comparison\nproves the validity of the statistical mechanic approach and establishes its\naccuracy.\n",
        "\"Guitar Tango\" is a song originally recorded in French in 1961 as \"Guitare-Tango\". It was written by Georges Liferman, Norman Maine and Jacques Plaint and there were versions recorded by Dario Moreno, Tino Rossi and Maya Casabianca. However, the song is best known for the instrumental version released the following year by British group the Shadows which peaked at number 4 on the UK Singles Chart.\n\nTrack listings\n7\": Columbia / DB 4870\n \"Guitar Tango\" – 2:59\n \"What a Lovely Tune\" – 2:17\n\n7\": Columbia / C 22 243 (Germany)\n \"Guitar Tango\" – 2:59\n \"Driftin'\" – 2:31\n\n7\": Columbia / SCMH 5130 (Netherlands)\n \"Guitar Tango\" – 2:59\n \"Nivram\" – 3:19\n\nPersonnel\n Hank Marvin – acoustic lead guitar\n Bruce Welch – acoustic rhythm guitar\n Jet Harris – electric bass guitar\n Brian Bennett – drums\nNorrie Paramor Orchestra – all other instrumentation\n\nCharts\n\nReferences\n\n1962 singles\nRock instrumentals\n1962 songs\nSong recordings produced by Norrie Paramor\nColumbia Graphophone Company singles\nThe Shadows songs\n1960s instrumentals",
        "  It has been claimed that the observed magnitude of the vacuum energy density\nis consistent with the distribution predicted in anthropic models, in which an\nensemble of universes is assumed. This calculation is revisited, without making\nthe assumption that the CMB temperature is known, and considering in detail the\npossibility of a recollapsing universe. New accurate approximations for the\ngrowth of perturbations and the mass function of dark haloes are presented.\nStructure forms readily in the recollapsing phase of a model with negative\nLambda, so collapse fraction alone cannot forbid Lambda from being large and\nnegative. A negative Lambda is disfavoured only if we assume that formation of\nobservers can be neglected once the recollapsing universe has heated to T > 8\nK. For the case of positive Lambda, however, the current universe does occupy a\nextremely typical position compared to the predicted distribution on the\nLambda-T plane. Contrasting conclusions can be reached if anthropic arguments\nare applied to the curvature of the universe, and we discuss the falsifiability\nof this mode of anthropic reasoning.\n",
        "Cap and Gown Club, founded in 1890, is an eating club at Princeton University, in Princeton, New Jersey, United States. Colloquially known as \"Cap\", the club is one of the \"Big Four\" eating clubs at Princeton (the others are The Ivy Club, University Cottage Club, and Tiger Inn). Members are selected through a selective process called bicker. Sometimes known as \"the Illustrious Cap and Gown Club,\" it was the first of the currently selective eating clubs to accept women. Though personalities of eating clubs certainly change throughout the years, Cap and Gown is described in F. Scott Fitzgerald's This Side of Paradise as \"anti-alcoholic, faintly religious and politically powerful.\"\n\nCap was the most bickered eating club in 2011, 2013, 2014, and 2015. It has been the most selective club since 2013, with 287 students bickering in Spring 2019, thirty-five percent of whom were offered membership.\n\nHistory\nCap is located at 61 Prospect Avenue between Cloister Inn and the University Cottage Club. It is the only Princeton eating club to have stayed in the same geographic location for its entire existence. Three Cap clubhouses have occupied this location. The first was completed in 1892. In 1895 when the club outgrew this clubhouse, the structure was moved across the street, and William Ralph Emerson was commissioned to design the second clubhouse (completed in 1896). Ten years later, Cap was ready to expand again. The Emerson building was moved away, and Raleigh Gildersleeve designed the clubhouse that Cap still occupies today.  A major renovation and expansion of the clubhouse to increase the size of the clubhouse in step with its growing membership was completed in February 2011.\n\nOn December 9, 2020, the Cap and Gown Club Board of Trustees unanimously approved a new financial aid policy that provides a grant to every member of Princeton University financial aid, guaranteeing that no member on full financial aid pays any out-of-pocket costs for club membership.\n\nNotable Cap and Gown alumni include Dean Cain '88, Brooke Shields '87, and Donald Rumsfeld '54. Neurosurgeon Wilder Penfield, who pioneered the concept of the brain homunculus, was also a member of Cap and Gown.\n\nReferences\n\nExternal links\nCap and Gown Club\n\nEating clubs at Princeton University\nHistoric district contributing properties in Mercer County, New Jersey",
        "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016.",
        "Just adding a bit to the answer given by /u/brandonsmash. If you are asking if the test explosion in North Korea physically triggered this earthquake in South Korea, you can think of it in the same terms of natural earthquake triggering. There are two physical mechanisms where earthquakes can trigger one another. One of these is static stress triggering and one is dynamic stress triggering. \n\nEarthquakes have triggered other earthquakes across long distances (1000+ km) through dynamic stress triggering. This occurs when an already stressed fault is near the breaking point, then the surface waves of a large earthquake pass by and throw that last straw on top to make the fault rupture. Remember that there are earthquakes much, much larger than these tests that do not seem to trigger any seismicity. Some do, and you can read about those in this recent paper published in Science [[summary article](_URL_1_) / [Fan and Shearer, Science, 2016](_URL_0_)].\n\nThere is also static triggering which is more like a domino effect. That's where one bit of fault ruptures and puts added stress on an adjacent fault, pushing it over the brink and causing it to rupture. This is only relevant over distances of about one fault length, so over a few hundred kilometers for a big M8 or 9 but a much smaller distance (~10 kilometers) for an M6 or less.\n\nSo because the North Korean test was relatively small and about 500 km from the South Korean earthquake the strain transfer would be too localized for static triggering, and because the earthquake occurred days after the test the timing is too late for dynamic triggering.",
        "While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",
        "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",
        "  We provide a new estimate of the local supermassive black hole mass function\nusing (i) the empirical relation between supermassive black hole mass and the\nSersic index of the host spheroidal stellar system and (ii) the measured\n(spheroid) Sersic indices drawn from 10k galaxies in the Millennium Galaxy\nCatalogue. The observational simplicity of our approach, and the direct\nmeasurements of the black hole predictor quantity, i.e. the Sersic index, for\nboth elliptical galaxies and the bulges of disc galaxies makes it\nstraightforward to estimate accurate black hole masses in early- and late-type\ngalaxies alike. We have parameterised the supermassive black hole mass function\nwith a Schechter function and find, at the low-mass end, a logarithmic slope\n(1+alpha) of ~0.7 for the full galaxy sample and ~1.0 for the early-type galaxy\nsample. Considering spheroidal stellar systems brighter than M_B = -18 mag, and\nintegrating down to black hole masses of 10^6 M_sun, we find that the local\nmass density of supermassive black holes in early-type galaxies rho_{bh,\nearly-type} = (3.5+/-1.2) x 10^5 h^3_{70} M_sun Mpc^{-3}, and in late-type\ngalaxies rho_{bh, late-type} = (1.0+/-0.5) x 10^5 h^3_{70} M_sun Mpc^{-3}. The\nuncertainties are derived from Monte Carlo simulations which include\nuncertainties in the M_bh-n relation, the catalogue of Sersic indices, the\ngalaxy weights and Malmquist bias. The combined, cosmological, supermassive\nblack hole mass density is thus Omega_{bh, total} = (3.2+/-1.2) x 10^{-6} h_70.\nThat is, using a new and independent method, we conclude that (0.007+/-0.003)\nh^3_{70} per cent of the universe's baryons are presently locked up in\nsupermassive black holes at the centres of galaxies.\n",
        "This work showed that word representation learning can benefit from sememes\nwhen used in an appropriate attention scheme. Authors hypothesized that sememes\ncan act as an essential regularizer for WRL and WSI tasks and proposed SE-WL\nmodel which detects word senses and learn representations simultaneously.\nThough experimental results indicate that WRL benefits, exact gains for WSI are\nunclear since a qualitative case study of a couple of examples has only been\ndone. Overall, paper is well-written and well-structured.\n\nIn the last paragraph of introduction section, authors tried to tell three\ncontributions of this work. (1) and (2) are more of novelties of the work\nrather than contributions. I see the main contribution of the work to be the\nresults which show that we can learn better word representations (unsure about\nWSI) by modeling sememe information than other competitive baselines. (3) is\nneither a contribution nor a novelty.\n\nThe three strategies tried for SE-WRL modeling makes sense and can be\nintuitively ranked in terms of how well they will work. Authors did a good job\nexplaining that and experimental results supported the intuition but the\nreviewer also sees MST as a fourth strategy rather than a baseline inspired by\nChen et al. 2014 (many WSI systems assume one sense per word given a context).\nMST many times performed better than SSA and SAC. Unless authors missed to\nclarify otherwise, MST seems to be exactly like SAT with a difference that\ntarget word is represented by the most probable sense rather than taking an\nattention weighted average over all its senses. MST is still an attention based\nscheme where sense with maximum attention weight is chosen though it has not\nbeen clearly mentioned if target word is represented by chosen sense embedding\nor some function of it.\n\nAuthors did not explain the selection of datasets for training and evaluation\ntasks. Reference page to Sogou-T text corpus did not help as reviewer does not\nknow Chinese language. It was unclear which exact dataset was used as there are\nseveral datasets mentioned on that page. Why two word similarity datasets were\nused and how they are different  (like does one has more rare words than\nanother) since different models performed differently on these datasets. The\nchoice of these datasets did not allow evaluating against results of other\nworks which makes the reviewer wonder about next question.\n\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by\nusing CBOW word embeddings.\n\nReviewer needs clarification on some model parameters like vocabulary sizes for\nwords (Does Sogou-T contains 2.7 billion unique words) and word senses (how\nmany word types from HowNet). Because of the notation used it is not clear if\nembeddings for senses and sememes for different words were shared. Reviewer\nhopes that is the case but then why 200 dimensional embeddings were used for\nonly 1889 sememes. It would be better if complexity of model parameters can\nalso be discussed.\n\nMay be due to lack of space but experiment results discussion lack insight into\nobservations other than SAT performing the best. Also, authors claimed that\nwords with lower frequency were learned better with sememes without evaluating\non a rare words dataset.\n\nI have read author's response.",
        ";\n, If you use origami-paper, make sure the pattern-side is outside and will be visible later.\n\n,, This is often called a cupboard fold, or book fold.\n\n,, Make sure to fold over both 'doors' of the cupboard fold.\n\n,, Remember to fold over both 'doors' of the cupboard fold. This makes a parallelogram.\n\n,, You will end up with a small square.\n\n,,, You've made your first unit!\n\n, If you have three different colors of paper, make ten of each color.\n\n, The surface of the stellated icosahedron is made up of a number of pyramids (in fact, if you take the regular icosahedron - a solid with twenty triangular faces - and make each face the base of a triangular pyramid, you get a stellated icosahedron). So we start by using the units to make a series of connected pyramids. To get a more simple idea of this complex cube you are now going to build, just think of an only 12-faced regular pentagonal dodecahedron (a Platonic solid), and imagine each of its 20 vertices (5 at the top, 5 at the bottom and 10 around in-between) will be replaced by a pyramid. With 30 units you will shape together those 20 pyramids.\n\n, The triangular ends of each unit are called 'tabs', and the square in the center of the unit contains 'pockets' made up by the cupboard fold that goes along the diagonal. Begin by putting the tab of one unit (pictured in orange below) into the pocket of another (pictured in yellow).\n\n, Congratulations - you made your first pyramid!\n\n, Continue by taking a new unit (orange in this case), and put its tab in the yellow unit's second pocket.\n\n, Tada! You now have two adjacent pyramids.\n\n, If you are using three unit colors, make sure to never put the tab of a unit into a pocket of another unit of the same color. This ensures a regular, colorful pattern on your stellated icosahedron.\n\n, You may need to feel your way around a bit to make sure that you never end up having to put a tab into a pocket of the same color. Don't worry if you do - you can carefully prise your units apart and rearrange them until you find a proper arrangement.\n\n, The last unit is tricky - you'll have to make sure that both its tabs go into pockets, and both its pockets are filled by the two remaining free tabs. Proceed with care.\n\n, You now have a fully formed, colorful, stellated icosahedron.\n\n",
        "  We use large cosmological N-body simulations to study the subhalo population\nin galaxy group sized halos. In particular, we look for fossil group candidates\nwith typical masses ~10-25% of Virgo cluster but with an order of magnitude\nless substructure. We examine recent claims that the earliest systems to form\nare deficient enough in substructure to explain the luminosity function found\nin fossil groups. Although our simulations show a correlation between the halo\nformation time and the number of subhalos, the maximum suppression of subhalos\nis a factor of 2-2.5, whereas a factor of 6 is required to match fossil groups\nand galaxies. While the number of subhalos depends weakly on the formation\ntime, the slope of the halo substructure velocity function does not. The\nsatellite population within Cold Dark Matter (CDM) halos is self-similar at\nscales between galaxies and galaxy clusters regardless of mass, whereas current\nobservations show a break in self-similarity at a mass scale corresponding to\ngroup of galaxies.\n",
        "This paper introduces pointer-network neural networks, which are applied to referring expressions in three small-scale language modeling tasks: dialogue modeling, recipe modeling and news article modeling. When conditioned on the co-reference chain, the proposed models outperform standard sequence-to-sequence models with attention.\n\nThe proposed models are essentially variants of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), which have been modified to take into account reference chains. As such, the main architectural novelty lies in 1) restricting the pointer mechanism to focus on co-referenced entities, 2) applying pointer mechanism to 2D arrays (tables), and 3) training with supervised alignments. Although useful in practice, these are minor contributions from an architectural perspective.\n\nThe empirical contributions are centred around measuring perplexity on the three language modeling tasks. Measuring perplexity is typical for standard language modeling tasks, but is really an unreliable proxy for dialogue modeling and recipe generation performance. In addition to this, both the dialogue and recipe tasks are tiny compared to standard language modeling tasks. This makes it difficult to evaluate the impact of the dialogue and recipe modeling results. For example, if one was to bootstrap from a larger corpus, it seems likely that a standard sequence-to-sequence model with attention would yield performance comparable to the proposed models (with enough data, the attention mechanism could learn to align referring entities by itself). The language modeling task on news article (Gigaword) seems to yield the most conclusive results. However, the dataset for this task is non-standard and results are provided for only a single baseline. Overall, this limits the conclusions we can draw from the empirical experiments.\n\n\nFinally, the paper itself contains many errors, including mathematical errors, grammatical errors and typos:\n- Eq. (1) is missing a sum over $z_i$.\n- \"into the a decoder LSTM\" -> \"into the decoder LSTM\"\n- \"denoted as his\" -> \"denoted as\"\n- \"Surprising,\" -> \"Surprisingly,\"\n- \"torkens\" -> \"tokens\"\n- \"if follows that the next token\" -> \"the next token\"\n- In the \"COREFERENCE BASED LANGUAGE MODEL\" sub-section, what does $M$ denote?\n- In the sentence: \"The attribute of each column is denoted as $s_c, where $c$ is the c-th attribute\". For these definitions to be make sense, $s_c$ has to be a one-hot vector. If yes, please clarify this in the text.\n- \"the weighted sum is performed\" -> \"the weighted sum is computed\"\n- \"a attribute\" -> \"an attribute\"\n- In the paragraph on Pointer Switch, change $p(z_{i,v} |s_{i,v}) = 1$ -> $p(z_{i,v} |s_{i,v}) = 0$.\n- In the \"Table Pointer\" paragraph, I assume you mean outer-product instead of cross-product? Otherwise, I don't see how the equations add up.\n\n\nOther comments:\n- For the \"Attention based decoder\", is the attention computed using the word embeddings themselves or the hidden states of the sentence encoder? Also, it applied only to the previous turn of the dialogue or to the entire dialogue history? Please clarify this.\n- What's the advantage of using an \"Entity state update\" rule, compared to a pointer network or copy network, which you used in the dialogue and recipe tasks? Please elaborate on this.\n- In the Related Work section, the following sentence is not quite accurate: \"For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly.\". There are task-oriented dialogue models which do query databases during natural language generation. See, for example, \"A Network-based End-to-End Trainable Task-oriented Dialogue System\" by Wen et al.",
        "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n",
        "The Great Britain national under-18 basketball team is the national representative for Great Britain in international under-18 basketball competitions. They are administered by British Basketball. The team competes at the FIBA U18 European Championship, with the opportunity to qualify for the FIBA Under-19 World Cup.\n\nSee also\nGreat Britain men's national basketball team\nGreat Britain men's national under-16 basketball team\n\nReferences\n\nExternal links\nOfficial website\nFIBA profile\n\nB\nMen's national under-18 basketball teams",
        " Talk to him/her about your idea of a new uniform before you go ahead and do anything! You could get into serious trouble for doing something without their approval. Make an appointment and bring your design portfolio if you already have one. Talk to them about the kind of style you like and ask them what style they like and decide on both. Also, find a colour scheme that you both like.;\n, If you can't afford it, maybe ask for a sketchbook for your birthday or Christmas present. Use your portfolio like a scrapbook diary for your inspiration and ideas - drawing up design pages, dress patterns, rough sketches, magazine cutouts, collages, photos...anything.\n\n, Vintage clothing is any clothing that resembles the 1920s - 1960s time period. For example, you choose the 1950s. Take careful note of the particular kind of style, popular fabrics and the trends. Keep in mind that the fashion style depends on the country - American 1950s was very different compared to French 1950s. Scribble down any other information of what was considered fashionable in that time period - and it's a smart idea to do some rough sketches.\n\n, Don't forget to compare them with the current uniform. If you're lucky, you may even get to see real samples of the old uniform...When you do, scribble down the colors, so you have some inspiration.\n\n, Don't forget to bring a notebook and a pen so you can write down the fabric used, sewing technique - see how you can reference it to your fashionable time period. Draw the uniform and annotate it. Make sure your annotations are meaningful.\n\n, Alternatively, copy a still life object. For example, you choose a flower. Draw that flower in a variety of different ways. Do a couple of pencil studies and don't forget to do some colour studies too. And then after that, draw a stylized version of that object.\n\n, Look at the different fabrics. Do you want the fabric to be patterned, tie-dyed or a solid block of colour? Touch it. Do you like the feel? Take note of the cost. Is it expensive? Will it be a crisp white linen? Cotton? Polyester? Rayon? What colour will it be? White? Red? Green? Blue? Navy?\n\n, Be as creative as you like - but keep within that colour scheme that you and your principal decided on. Fill up the entire page. The pages should have a 1950s flavour about them or a sense of whatever time period you have researched.\n\n, If you can, use an artist's colour wheel, as it will really help you in working out what colours go well together.\n\n, Make sure each annotation is meaningful - ask yourself questions. Does it enhance the sense of style - yes or no? Does it match the time period? Does it have line? Have you used a good colour range? What motifs are you using? Where are you going to put the motifs? What silhouette does it have? Which body shape does it best fit?\n\n,, If you are terrible at sewing with a machine, either hire a sewing machine (if you can afford it)and teach yourself how to use it. Or, borrow one from someone who knows how to sew - and get them to teach you how to use it. Know how to set it up, know the different settings, types of stitches.\n\n, Yes, I said sew it! Many fashion designers do this, believe it or not. Make adjustments if you need to.\n\n, But if they don't like your uniform, keep drawing new designs and keep sewing doll-sized uniform replicas until they are satisfied.\n\n, Send the word around school.\n\n,",
        "We fixed minor typographical error in author's name and Section. 4. etc.\nOur policy restricts to reveal much more details about the internal dataset and results of the end-user satisfaction measure, however, we did our best to introduce how our idea is to be used for multi-label learning in an application to computer vision, especially e-commerce industry. ",
        "The paper reads well and the idea is new.\nSadly, many details needed for replicating the results (such as layer sizes of the CNNs, learning rates) are missing. \nThe training of the introspection network could have been described in more detail. \nAlso, I think that a model, which is closer to the current state-of-the-art should have been used in the ImageNet experiments. That would have made the results more convincing.\nDue to the novelty of the idea, I recommend the paper. I would increase the rating if an updated draft addresses the mentioned issues.",
        ";\n, At this time, the Samsung Galaxy Ace can only be updated using Samsung Kies 2.6, which is only compatible with Windows-based computers.,,,,, Samsung Kies will launch, and is now ready to update your Samsung Galaxy Ace., Samsung Kies will take a moment to recognize your device.\n\n,, This will back up and save all personal data on your device to Samsung Kies; updating your firmware may result in data loss.\n\n\nAlternately, you can choose to save your personal data to Google’s servers, your computer, or to another cloud storage service to prevent data loss.\n\n,, Your phone’s firmware will now be updated., In some cases, faulty hardware can prevent your device being from recognized.\n\n,\nReplace the battery, power on your device, and connect the phone to your computer using a USB cable.\nLaunch Samsung Kies, click on “Tools,” then select “Firmware Upgrade and Initialization.”\nType “SGH-S5830” using all caps into the field labeled “Model name.” This is the model number of the Samsung Galaxy Ace.\nClick on “OK,” then enter the serial number for your Galaxy Ace.\nClick on “OK,” then click on “Start Upgrade.” Samsung Kies will now update your device.\n\n, In some cases, a corrupt installation of Samsung Kies can prevent you from being able to successfully update your device.\n\n, A mounted SD card can sometimes interfere with the installation of new updates.Tap on Menu and select “Settings” on your Samsung Galaxy Ace.\nTap on “SD card and phone storage,” then tap on “Unmount SD card.”\nTap on “OK” to confirm you want to unmount your SD card.\n\n, A hard reset can help restore your device back to the original factory settings and resolve any software problems.\n\n\nPower off your Samsung Galaxy Ace.\nPress and hold the Home, Volume Up, and Power buttons at the same time until the phone boots up and displays the Recovery Menu.\nUse the volume keys to select “Wipe data / factory reset,” then press the Power button to make your selection.\nSelect “Yes” to confirm you want you want to reset your device. Your device will restart when the reset is complete.\n\n",
        "This paper proposes to use an empirical Bayesian approach to learn the parameters of a neural network, and their priors.\nA mixture model prior over the weights leads to a clustering effect in the weight posterior distributions (which are approximated with delta peaks). \nThis clustering effect can exploited for parameter quantisation and compression of the network parameters.\nThe authors show that this leads to compression rates and predictive accuracy comparable to related approaches. \n\nEarlier work [Han et al. 2015] is based on a three-stage process of pruning small magnitude weights, clustering the remaining ones, and updating the cluster centres to optimise performance. The current work provides a more principled approach that does not have such an ad-hoc multi-stage structure, but a single iterative optimisation process.\n\nA first experiment, described in section 6.1 shows that an empirical Bayes’ approach, without the use of hyper priors, already leads to a pronounced clustering effect and to setting many weights to zero. \nIn particular a compression rate of 64.2 is obtained on the LeNet300-100 model.\nIn section 6.1 the text refers to figure C, I suppose this should be figure 1.\n\nSection 6.2 describes an experiment where hyper-priors are used, and the parameters of these distributions, as well as other hyper-parameters such as the learning rates, are being optimised using Spearmint (Snoek et al., 2012). Figure 2 shows the performance of the  different points in the hyper-parameter space that have been evaluated (each trained network gives an accuracy-compressionrate point in the graph). The text claims that best results lie on a line, this seems a little opportunistic interpretation given the limited data. Moreover, it would be useful to add a small discussion on whether such a linear relationship would be expected or not. Currently the results of this experiment lack interpretation.\n\nSection 6.3 describes results obtained for both CNN models and compares results to the recent results of (Han et al., 2015) and (Guo et al., 2016).\nComparable results are obtained in terms of compression rate and accuracy. \nThe authors state that their current algorithm is too slow to be useful for larger models such as VGG-19, but they do briefly report some results obtained for this model (but do not compare to related work). It would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach, and how the proposed algorithm scales in terms of the relevant quantities of the data and the model.\n\nThe contribution of this paper is mostly experimental, leveraging fairly standard ideas from empirical Bayesian learning to introduce weight clustering effects in CNN training.\nThis being said, it is an interesting result that such a relatively straightforward approach leads to results that are on par with state-of-the-art, but more ad-hoc, network compression techniques.\nThe paper could be improved by clearly describing the algorithm used for training, and how it scales to large networks and datasets.\nAnother point that would deserve further discussion is how the hyper-parameter search is performed ( not using test data I assume), and how the compared methods dealt with the search over hyper-parameters to determine the accuracy-compression tradeoff. Ideally, I think, methods should be evaluated across different points on this trade-off.\n\n",
        "  We present a simple implementation of the dynamical mean-field theory\napproach to the electronic structure of strongly correlated materials. This\nimplementation achieves full self-consistency over the charge density, taking\ninto account correlation-induced changes to the total charge density and\neffective Kohn-Sham Hamiltonian. A linear muffin-tin orbital basis-set is used,\nand the charge density is computed from moments of the many body\nmomentum-distribution matrix. The calculation of the total energy is also\nconsidered, with a proper treatment of high-frequency tails of the Green's\nfunction and self-energy. The method is illustrated on two materials with\nwell-localized 4f electrons, insulating cerium sesquioxide Ce2O3 and the\ngamma-phase of metallic cerium, using the Hubbard-I approximation to the\ndynamical mean-field self-energy. The momentum-integrated spectral function and\nmomentum-resolved dispersion of the Hubbard bands are calculated, as well as\nthe volume-dependence of the total energy. We show that full self-consistency\nover the charge density, taking into account its modification by strong\ncorrelations, can be important for the computation of both thermodynamical and\nspectral properties, particularly in the case of the oxide material.\n",
        "\nThis paper explores transfer in reinforcement learning between agents that may be morphologically distinct. The key idea is for the source and target agent to have learned a shared skill, and then to use this to construct abstract feature spaces to enable the transfer of a new unshared skill in the source agent to the target agent. The paper is related to much other work on transfer that uses shared latent spaces, such as CCA and its variants, including manifold alignment and kernel CCA. \n\n\nThe paper reports on experiments using a simple physics simulator between robot arms consisting of three vs. four links. For comparison, a simple CCA based approach is shown, although it would have been preferable to see comparisons for something more current and up to date, such as manifold alignment or kernel CCA. A three layer neural net is used to construct the latent feature spaces. \n\nThe problem of transfer in RL is extremely important, and receives less attention than it should. This work uses an interesting hypothesis of trying to construct transfer based on shared skills between source and target agent. This is a promising approach. However, the comparisons to related approaches is not very up to date, and the domains are fairly simplistic. There is little by way of theoretical development of the ideas using MDP theory. \n",
        "Kivdo-Tyukan () is a rural locality (a selo) in Rabochy posyolok Bureya of Bureysky District, Amur Oblast, Russia. The population was 31 as of 2018. There are 5 streets.\n\nGeography \nKivdo-Tyukan is located 17 km northwest of Novobureysky (the district's administrative centre) by road. Kivdinsky is the nearest rural locality.\n\nReferences \n\nRural localities in Bureysky District",
        "  We analyze the transmitted flux in a sample of 17 QSOs spectra at\n5.74<zem<6.42 to obtain tighter constraints on the volume-averaged neutral\nhydrogen fraction, xHI, at z~6. We study separately the narrow transmission\nwindows (peaks) and the wide dark portions (gaps) in the observed absorption\nspectra. By comparing the statistics of these spectral features with Lyalpha\nforest simulations, we conclude that xHI evolves smoothly from 10^{-4.4} at\nz=5.3 to 10^{-4.2} at z=5.6, with a robust upper limit xHI<0.36 at z=6.3. The\nfrequency and physical sizes of the peaks imply an origin in cosmic underdense\nregions and/or in HII regions around faint quasars or galaxies. In one case\n(the intervening HII region of the faint quasar RD J1148+5253 at z=5.70 along\nthe LOS of SDSS J1148+5251 at z=6.42) the increase of the peak spectral density\nis explained by the first-ever detected transverse proximity effect in the HI\nLyalpha forest; this indicates that at least some peaks result from a locally\nenhanced radiation field. We then obtain a strong lower limit on the foreground\nQSO lifetime of tQ>11 Myr. The observed widths of the peaks are found to be\nsystematically larger than the simulated ones. Reasons for such discrepancy\nmight reside either in the photoionization equilibrium assumption or in\nradiative transfer effects.\n",
        "Quality, Clarity: \n  The paper is well written. Further revisions have been made upon the original.\n \n Originality, Significance:\n  The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. This is done using a mix of (a) architectural modifications; (b) jumpy predictions; and (c) particular training schemes. The experimental validation is extensive, now including additional comparisons suggested by reviewers.\n There is not complete consensus about the significance of the contributions, with one reviewer seeking additional technical novelty. Overall, the paper appears to provide interesting and very soundly-evaluated results, which likely promises to be the new standard for this type of prediction problem.",
        "We thank the reviewers and the area chair for their insightful feedback. We have incorporated all comments into our current draft. We first discuss the updates to the paper and address common questions raised by the reviewers. We have also posted individual comments to the reviewers to address specific questions.\n\nUpdates\n=====\n\na) We have updated the experimental section of the paper with new experiments on MNIST, CIFAR-10 and two datasets on RNNs (PTB and char-LSTM).\n\nb) We have modified the algorithm to introduce a technique called \"scoping\". This increases the scope parameter \\gamma as training progresses instead of fixing it and has the effect of exploring the parameter space in the beginning of training (Sec. 4.3). As a result of this, we can now train all our networks with only the local entropy loss instead of treating it as a regularizer (Eqn. 6).\n\nc) For a fair comparison of the training time, we now plot the error curves in Figs. 4, 5 and 6 against the \"effective number of epochs\", i.e., the number of epochs of Entropy-SGD is multiplied by the number of Langevin iterations L (we set L=1 for SGD/Adam). Thus the x-axis is a direct measure of the wall-clock time agnostic to the underlying implementation and is proportional to the number of back-props as suggested.\n\nWe obtain significant speed-ups with respect to our earlier results due to scoping and the wall-clock training time for all our networks with Entropy-SGD is now comparable to SGD/Adam. In fact, Entropy-SGD is almost twice as fast as SGD on our experiments on RNNs and also obtains a better generalization error (cf. Fig. 6). The acceleration for CNNs on MNIST and CIFAR-10 is about 20%.\n\nTable 1 (page 11) summarizes the experimental section of the paper.\n\nd) Improved exposition of the algorithm in Sec. 4.2 that includes intuition for hyper-parameter tuning. We have expanded the discussion of experiments in Sec. 5.3, 5.4 to provide more details and insights that relate to the energy landscape of deep networks.\n\ne) Appendix C discusses the possible connections to variational inference (this is the same material as the discussion with the AC below). Sec. C.1 presents an experimental comparison of local entropy vs. SGLD. We note here that our results using Entropy-SGD for both CNNs and RNNs are much better than vanilla SGLD in significantly smaller (~3-5x) wall-clock times.\n\nResponse to the reviewers:\n================\n\n>> smoothing of the original loss vs. local entropy\nWe discuss this in detail in the related work in Sec. 2 and Appendix C. While smoothing the original loss function using convolutions or averaging the gradient over perturbations of weights reduces the ruggedness of the energy landscape, it does not help with sharp, narrow valleys. Local entropy introduces a measure that focuses on wide local minima in the energy landscape (cf. Fig. 2 which has a \"global\" minimum at a wide valley); this is the primary reason for its efficacy. Smoothing the loss function can also, for instance, generate an artificial local minimum between two close by sharp valleys, which is detrimental to generalization.\n\n>> unrealistic eigenvalue assumption in Sec. 4.3\nWe have clarified this point in Remark 4. Our analysis employs an assumption that the Hessian \\nabla^2 f(x) does not have eigenvalues in the set [-2\\gamma-c, c] for some c > 0. This is admittedly unrealistic, for instance, the eigenspectrum of the Hessian in Fig. 1 has a large fraction of its eigenvalues almost zero. Let us note though that Fig. 1 is plotted at a local minimum, from our experiments, the eigenspectrum is less sparse in the beginning of training.\n\nWe would like to remark that the bound on uniform stability in Thm. 3 by Hardt et al. assumes global conditions on the smoothness of the loss function; one imagines that Eqn. 9 remains qualitatively the same (in particular, with respect to the number of training iterations) even if this assumption is violated to an extent before convergence happens. Obtaining a rigorous generalization bound without this assumption requires a dynamical analysis of SGD and seems out of reach currently.",
        "Monflanquin (; Languedocien: Montflanquin) is a commune in the Lot-et-Garonne department in south-western France. Built in 1256 as a military bastide town on a strategic north-south route, it changed hands several times during the Hundred Years' War.\n\nThe village is a member of the Les Plus Beaux Villages de France (\"The most beautiful villages of France\") association.\n\nDemography\n\nSee also\nCommunes of the Lot-et-Garonne department\n\nReferences\n\nExternal links\n\nMonflanquin bastide modèle - Georges Odo\n\nCommunes of Lot-et-Garonne\nPlus Beaux Villages de France",
        "  When modeling the distribution of a set of data by a mixture of Gaussians,\nthere are two possibilities: i) the classical one is using a set of parameters\nwhich are the proportions, the means and the variances; ii) the second is to\nconsider the proportions as the probabilities of a discrete valued hidden\nvariable. In the first case a usual prior distribution for the proportions is\nthe Dirichlet which accounts for the fact that they have to sum up to one. In\nthe second case, to each data is associated a hidden variable for which we\nconsider two possibilities: a) assuming those variables to be i.i.d. We show\nthen that this scheme is equivalent to the classical mixture model with\nDirichlet prior; b) assuming a Markovian structure. Then we choose the simplest\nmarkovian model which is the Potts distribution. As we will see this model is\nmore appropriate for the case where the data represents the pixels of an image\nfor which the hidden variables represent a segmentation of that image. The main\nobject of this paper is to give some details on these models and different\nalgorithms used for their simulation and the estimation of their parameters.\n  Key Words: Mixture of Gaussians, Dirichlet, Potts, Classification,\nSegmentation.\n",
        "Sriboonruang Wittayakarn School  (commonly called: Sibunrueang; ; ) is a large high school in Nong Bua Lamphu. The school is located in Mu 13, Ban Santisuk, Muang Mai, Sriboonrueng District, Nongbua Lamphu province. It is in the BuaBan campus and under the Office of the Secondary Educational Service Area Office 19.\n\nHistory \nBefore 1974, most of the graduates of Sriboonrueng Nongbua Lamphu province No education opportunities One of the reasons is that the secondary school is located far away. It was not easy to travel until 2512 is planned to establish a secondary school in Si Bun Rueang. However, this project must be stopped for a period until 1973, the district has brought various information to the Department of Education. And the Ministry of Education was established on May 23, 1974 to be named Sriboonruang Wittayakarn School under the Department of General Education. Ministry of Education Sriboonrueng School The first student was opened on May 17, 1974. In the first year, two classes were offered. There were 161 students enrolled. The school received only 90 students, and the number of classrooms and students increased every year to the present.\nIt is currently the secondary school of Sriboonrueng and is a large international standard school.\n\nCourse\n\nJunior high school level\n\nHigh school level\n\nDirector names\n\nO-NET average  \nThis is O-NET average of Sriboonruang Wittayakarn School in 5 Basic Subject. They are Math, Science, Social study, English and Thai language\n\nReferences\n\nExternal links \n \n Another website for the school from The Secondary Education Area office 19\n School buildings\n\nSecondary schools in Thailand\nSchools in Nong Bua Lamphu Province",
        "Charles Ray Hatcher (July 16, 1929December 7, 1984) was an American serial killer who confessed to having murdered 16 people between 1969 and 1982.\n\nChildhood and youth\nCharles Ray Hatcher was born in Mound City, Missouri, a small town  north of St.Joseph. He was the youngest of Jesse James and Lula Novada (Bomar) Hatcher's four children. His father was a bootlegger, an ex‑convict and an abusive alcoholic. Hatcher was bullied in school, and in turn bullied his classmates and would carry this practice into adulthood.\n\nIn the spring of 1935, he and his older brothers were flying a kite with copper wire they had found in an old ModelT Ford. His oldest brother, Arthur Allen, was about to hand the kite to him when it hit a high‑voltage power line and electrocuted him. Arthur was pronounced dead at the scene. Soon afterward, his father left home and divorced his mother. His mother remarried several times, and in 1945, Hatcher moved with his mother and her third husband to St.Joseph.\n\nCrimes\n\n1947–1963\nIn 1947, Hatcher was convicted of auto theft in St.Joseph after stealing a logging truck from Iowa–Missouri Walnut Company, his employer of two weeks. He received a two‑year suspended sentence. In 1948, he was convicted of auto theft a second time for stealing a 1937 Buick in St.Joseph. Hatcher was sentenced to two years in the Missouri State Penitentiary.\n\nOn June 8, 1949, Hatcher was released from prison after serving a little more than half of his time. He was back in prison in just a few months, after being convicted of forging a $10 check at a gas station in Maryville, Missouri. On March 18, 1951, Hatcher escaped from prison and attempted a burglary. He was caught and received an extra two years in prison.\n\nAfter serving his additional time, Hatcher was released from prison on July 14, 1954. He stole a 1951 Ford in Orrick and was subsequently sentenced to four years in prison. Before he was sentenced, Hatcher attempted to escape from the Ray County Jail in Richmond and received an additional two years. On March 18, 1959, Hatcher was released from prison after the sixth prison sentence of his life.\n\nOn June 26, 1959, Hatcher attempted to abduct Steven Pellham, a 16‑year‑old in St.Joseph who delivered newspapers, threatening him with a butcher knife. Pellham reported the crime, and Hatcher was arrested when the police stopped him in a stolen vehicle.\n\nHatcher was sentenced to five years in the Missouri State Penitentiary for the attempted abduction and auto theft under the Habitual Criminal Act. While Hatcher was waiting to be transported to prison, he unsuccessfully attempted to break out of the Buchanan County Jail. When Hatcher arrived at the Missouri State Penitentiary, he claimed to be the most notorious criminal in northwest Missouri since Jesse James.\n\nOn July 2, 1961, a 26-year-old inmate named Jerry Tharrington was found raped and stabbed to death on the floor of the prison kitchen loading dock. He had been repeatedly stabbed in the back. Hatcher was the only individual missing from the kitchen crew at the time of the murder. He was sent to solitary confinement for Tharrington's murder, but there was not enough evidence to convict him in court.\n\nWhile in solitary confinement for the murder, Hatcher wrote a note claiming that he needed psychiatric treatment; however, the prison psychologist felt that it was simply a scheme to get out of solitary and possibly out of prison early. Treatment was refused, and Hatcher was returned to the general population. His sentence was reduced to three‑quarters the original time, and he was released on August 24, 1963.\n\n1969–1977\nHatcher confessed to abducting a 12‑year‑old named William Freeman in Antioch, California on August 27, 1969. He claimed he had told the boy to come with him, took him to a creek, and strangled him.\n\nOn August 29, 1969, six‑year‑old Gilbert Martinez was reported missing in San Francisco. According to the six‑year‑old girl with whom he was playing, Martinez walked away with a man who offered him ice cream. He was found by a man walking his dog as the boy was being beaten and sexually assaulted. Police arrived and arrested the assailant, who identified himself as Albert Ralph Price, although he carried identification with the name Hobert Prater. Martinez survived the assault, and Federal Bureau of Investigation records later identified the man as Charles R. Hatcher.\n\nStill going by the name Albert Price, Hatcher was charged with assault with attempt to commit sodomy and kidnapping. He was ordered to undergo competency evaluations to determine his competence to stand trial. A complete psychological evaluation was ordered when Hatcher was unresponsive during the preliminary evaluations. During this time, he claimed to hear voices, and faked delusions and suicide attempts to avoid prison.\n\nIn December 1970, Hatcher was sent back and forth between the courts and hospital multiple times. One psychiatrist diagnosed him as having a passive–aggressive personality with paraphilia and pedophilia. It was reported that the hospital staff felt Hatcher was fabricating or exaggerating the symptoms of his mental disorders. He was examined by two psychiatrists in January 1971. He was declared insane by the first one, who recommended vigorous treatment in a secure hospital. The second psychiatrist declared him to be incompetent to stand trial and sent him back to the hospital.\n\nOn May 24, 1971, Hatcher was sent to trial and pleaded not guilty by reason of insanity. He was sent to a different hospital for more evaluations, where it was determined that he was unfit to stand trial. On June 2, Hatcher escaped from the hospital. He was caught a week later in Colusa, California and arrested for suspected auto theft under the name Richard Lee Grady. Hatcher was returned to the California State Hospital for a mental evaluation. In April 1972, hospital staff determined that his treatment was unsuccessful and that he was a danger to other patients, after which he was sent to the prison state hospital in Vacaville.\n\nIn August 1972, Hatcher was transferred to San Quentin State Prison to stand trial, three years after the crime. He was ordered to undergo two final examinations: one declared him competent to stand trial and the other determined him to be sane at the time of the crime.\n\nIn December 1972, Hatcher was tried for, and convicted of the abduction and molestation of Martinez. In January 1972, he was committed to the California State Hospital as a \"mentally disordered sexual offender\".\n\nOn March 28, 1973, security guards found Hatcher hiding in a cooler near the hospital's main courtyard with two sheets stuffed into his pants, after which he admitted to an escape attempt. He was sent back to court for sentencing after doctors determined he was still a threat to society. In April, Hatcher was sentenced to one year to life and sent to a medium‑security prison in Vacaville.\n\nIn May 1973, a psychologist found Hatcher to be a \"manipulative institutionalized sociopath\". In June 1973, he attempted suicide by slashing his wrists after it was recommended that he be transferred to a maximum security prison. A psychiatrist diagnosed him with paranoid schizophrenia, and he remained at Vacaville.\n\nIn August 1975, guards reported good behavior at Hatcher's parole review. In June 1976, the California Parole Board found that Hatcher had improved dramatically through his time in prison and set a parole date of December 25, 1978. As a result of the passage of a bill giving inmates credit for time spent in jails and mental hospitals, Hatcher received a modified parole date in January 1977. He was released to a halfway house in San Francisco on May 20, 1977.\n\n1978–1982\nOn September 4, 1978, Hatcher was arrested under the name Richard Clark in Omaha, Nebraska for sexually assaulting a 16‑year‑old boy. He was sent to the Douglas County Mental Hospital and released in January 1979.\n\nOn May 3, 1979, Hatcher was arrested for assault and attempted murder after he tried to stab seven‑year‑old Thomas Morton. He was sent to Norfolk Regional Center, a mental health facility, after the charges were dropped.\n\nIn May 1980, Hatcher was released from the facility, but was sent back two months later for another assault. He escaped in September.\n\nOn October 9, 1980, Hatcher was arrested as Richard Clark in Lincoln, Nebraska for the attempted assault and sodomy of a 17‑year‑old boy. He was sent to another mental health facility and released after 21 days.\n\nOn January 13, 1981, Hatcher was arrested as Richard Clark in Des Moines, Iowa after a knife fight. He spent time in several mental health facilities and was released to a Davenport Salvation Army shelter in April.\n\nMelvin Reynolds\nOn May 26, 1978, four‑year‑old Eric Christgen disappeared from downtown St. Joseph, Missouri. His body was later found along the Missouri River; he had been sexually abused and died of suffocation. The police questioned more than 100 possible suspects, including \"every known pervert in town\", to no avail.\n\nOne of them was Melvin Reynolds, a 25‑year‑old man of limited intelligence who had been sexually abused himself as a child and who had homosexual experiences as an adolescent. Reynolds, although agitated by the investigation, cooperated through several interrogations over a period of months, including two polygraph examinations and one interrogation under hypnosis. In December 1978, he was questioned under amobarbitalbelieved at the time to be a truth serumand made an ambiguous remark that intensified police suspicion. Two months later, in February 1979, the police brought the still cooperative Reynolds in for another round of interrogation: fourteen hours of questions, promises, and threats. Finally, Reynolds gave in and said, \"I'll say so if you want me to\".\n\nIn the weeks that followed, Reynolds embellished this confession with details that were fed to him, deliberately or otherwise. That was enough to convince the prosecutor to charge Reynolds, and to convince a jury to convict him of second‑degree murder. He was sentenced to life imprisonment. Four years later, Reynolds was released when Charles Hatcher confessed to three murders to an FBI agent, including that of Eric Christgen.\n\nDeath\nOn July 29, 1982, 11‑year‑old Michelle Steele was reported missing from St.Joseph. The day after, her uncle found her nude, ravaged body on a bank of the Missouri River. She had been beaten and strangled to death.\n\nHatcher was arrested the following day, as he tried to check in at the St.Joseph State Hospital. While awaiting trial, he confessed to fifteen other murders dating from 1969. The first victim, 12‑year‑old William Freeman, had disappeared from Antioch, California in August of that year, one day before Hatcher was charged with child molestation in nearby San Francisco. In another case, Hatcher penned a crude map that led searchers to the remains of 28-year-old James Churchill, buried on the grounds of the Rock Island Army Arsenal, near Davenport, Iowa. It was then that he also confessed to the murder of Eric Christgen.\n\nHe was convicted of the Christgen homicide in October 1983, and drew a term of life imprisonment with no parole for at least 50 years. Facing his second Missouri conviction a year later for the murder of Michelle Steele, Hatcher requested a death sentence, but the jury refused, recommending life on December 3, 1984. Four days later, Hatcher hanged himself in his cell, at the Missouri State Penitentiary in Jefferson City.\n\nSee also \n List of serial killers in the United States\n List of serial killers by number of victims\n\nNotes\n\nReferences\n\nCited works and further reading\n \n \n\n1929 births\n1984 suicides\n20th-century American criminals\nAmerican escapees\nAmerican male criminals\nAmerican murderers of children\nAmerican people convicted of murder\nAmerican people convicted of theft\nAmerican people who died in prison custody\nAmerican prisoners sentenced to life imprisonment\nAmerican rapists\nAmerican serial killers\nAmerican sex offenders\nEscapees from Missouri detention\nFugitives\nLGBT people from Missouri\nMale serial killers\nPeople convicted of murder by Missouri\nPeople from Mound City, Missouri\nPeople from St. Joseph, Missouri\nPeople with antisocial personality disorder\nPrisoners and detainees of California\nPrisoners sentenced to life imprisonment by Missouri\nPrisoners who died in Missouri detention\nSerial killers who committed suicide in prison custody\nSuicides by hanging in Missouri\nViolence against men in North America",
        "The authors extend GANs by an inference path from the data space to the latent space and a discriminator that operates on the joint latend/data space. They show that the theoretical properties of GANs still hold for BiGAN and evaluate the features learned unsupervised in the inference path with respect to performance on supervised tasks after retraining deeper layers.\n\nI see one structural issue with this paper: Given that, as stated in the abstract, the main purpose of the paper is to learn unsupervised features (and not to improve GANs), the paper might spent too much space on detailing the relationship to GANs and all the theoretical properties. It is not clear whether they actually would help with the goal of learning good features. While reading the paper, I actually totally forgot about the unsupervised features until they reappeared on page 6. I think it would be helpful if the text of the paper would be more aligned with this main story.\n\nStill, the BiGAN framework is an elegant and compelling extension to GANs. However, it is not obvious how much the theoretical properties help us as the model is clearly not fully converged. To me, especially Figure 4 seems to suggest that G(E(x)) might be doing not much more than some kind of nearest neighbour retrival (and indeed one criticism for GANs has always been that they might just memorize some samples). By the way, it would be very interesting to know how well the discriminator actually performs after training.\n\nComing back to the goal of learning powerful features: The method does not reach state-of-the-art performance on most evaluated tasks (Table 2 and 3) but performs competitive and it would be interesting to see how much this improves if the BiGAN training (and the convolutional architecture used) would be improved.\n\nThe paper is very well written and provides most necessary details, although some more details on the training (learning rates, initialization) would be helpful for reproducing the results.\n\nOverall I think the paper provides a very interesting framework for further research, even though the results presented here are not too impressive both with respect to the feature evaluation (and the GAN learning).\n\nMinor: It might be helpful to highlight the best performance numbers in Tables 2 and 3.",
        "This paper presents an interesting model for reading comprehension, by\ndepicting the multiplicative interactions between the query and local\ninformation around a word in a document, and the authors proposed a new\ngated-attention strategy to characterize the relationship. The work is quite\nsolid, with almost state of art result on the whole four cloze-style datasets\nachieved. Some of the further improvement can be helpful for the similar tasks.\n\n\nNevertheless, I have some concerns on the following aspect:\n\n1. The authors have referred many papers from arXiv, but I think some really\nrelated works are not included. Such as the works from Caiming Xiong, et al.\nhttps://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et\nal. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on\nenhancing the attention operation to modeling the interaction between documents\nand queries. Although these works are not evaluated on the cloze-style corpus\nbut the SQuAD, an experimental or fundamental comparison may be necessary.\n\n2. There have been some studies that adopts attention mechanism or its variants\nspecially designed for the Reading Comprehension tasks, and the work actually\nshare the similar ideas with this paper. My suggestion is to conduct some\ncomparisons with such work to enhance the experiments of this paper.",
        "The paper presents a neural model for predicting SQL queries directly from\nnatural language utterances, without going through an intermediate formalism.\nIn addition, an interactive online feedback loop is proposed and tested on a\nsmall scale.\n\n- Strengths:\n\n1\\ The paper is very clearly written, properly positioned, and I enjoyed\nreading it.\n\n2\\ The proposed model is tested and shown to perform well on 3 different\ndomains (academic, geographic queries, and flight booking)\n\n3\\ The online feedback loop is interesting and seems promising, despite of the\nsmall scale of the experiment.\n\n4\\ A new semantic corpus is published as part of this work, and additionally\ntwo\nexisting corpora are converted to SQL format, which I believe would be\nbeneficial for future work in this area.\n\n- Weaknesses / clarifications:\n\n1\\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice\nof the length of span for querying the search engine. Why and how is it\nprogressively reduced? (line 333).\n\n2\\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback\nloop (algorithm 1) is *not* used for these experiments. If this is indeed the\ncase, I'm not sure when does data augmentation occur. Is all the annotated\ntraining data augmented with paraphrases? When is the \"initial data\" from\ntemplates added? Is it also added to the gold training set? If so, I think it's\nnot surprising that it doesn't help much, as the gold queries may be more\ndiverse.  In any case, I think this should be stated more clearly. In addition,\nI think it's interesting to see what's the performance of the \"vanilla\" model,\nwithout any augmentation, I think that this is not reported in the paper.\n\n3\\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. \nDoes the accuracy measure the correctness of the execution of the query (i.e.,\nthe retrieved answer) as the text seem to indicate? (Line 471 mentions\n*executing* the query). Alternatively, are the queries themselves compared? (as\nseems to be the case for Dong and Lapata in Table 2). If this is done\ndifferently for different systems (I.e., Dong and Lapata), how are these\nnumbers comparable? In addition, the text mentions the SQL model has \"slightly\nlower accuracy than the best non-SQL results\" (Line 515), yet in table 2 the\ndifference is almost 9 points in accuracy.  What is the observation based upon?\nWas some significance test performed? If not, I think the results are still\nimpressive for direct to SQL parsing, but that the wording should be changed,\nas the difference in performance does seem significant.\n\n4\\ Line 519 - Regarding the data recombination technique used in Jia and Liang\n(2016): Since this technique is applicable in this scenario, why not try it as\nwell?  Currently it's an open question whether this will actually improve\nperformance. Is this left as future work, or is there something prohibiting the\nuse of this technique?\n\n5\\ Section 6.2 (Three-stage online experiment) - several details are missing /\nunclear:\n\n* What was the technical background of the recruited users?\n\n* Who were the crowd workers, how were they recruited and trained?\n\n* The text says \"we recruited 10 new users and asked them to issue at least 10\nutterances\". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in\ntotal (1 for each).\n\n* What was the size of the initial (synthesized) training  set? \n\n* Report statistics of the queries - some measure of their lexical variability\n/ length / complexity of the generated SQL? This seems especially important for\nthe first phase, which is doing surprisingly well. Furthermore, since SCHOLAR\nuses SQL and NL, it would have been nice if it were attached to this\nsubmission, to allow its review during this period.\n\n6\\ Section 6.3 (SCHOLAR dataset)\n\n* The dataset seems pretty small in modern standards (816 utterances in total),\nwhile one of the main advantages of this process is its scalability. What\nhindered the creation of a much larger dataset?\n\n* Comparing performance - is it possible to run another baseline on this newly\ncreated dataset to compare against the reported 67% accuracy obtained in this\npaper (line 730).\n\n7\\ Evaluation of interactive learning experiments (Section 6): I find the\nexperiments to be somewhat hard to replicate as they involve manual queries of\nspecific annotators. For example, who's to say if the annotators in the last\nphase just asked simpler questions? I realise that this is always problematic\nfor online learning scenarios, but I think that an effort should be made\ntowards an objective comparison. For starters, the statistics of the queries\n(as I mentioned earlier) is a readily available means to assess whether this\nhappens. Second, maybe there can be some objective held out test set? This is\nproblematic as the model relies on the seen queries, but scaling up the\nexperiment (as I suggested above) might mitigate this risk. Third, is it\npossible to assess a different baseline using this online technique? I'm not\nsure whether this is applicable given that previous methods were not devised as\nonline learning methods.\n\n- Minor comments:\n\n1\\ Line 48 - \"requires\" -> \"require\"\n\n2\\ Footnote 1 seems too long to me. Consider moving some of its content to the\nbody of the text.\n\n3\\ Algorithm 1: I'm not sure what \"new utterances\" refers to (I guess it's new\nqueries from users?). I think that an accompanying caption to the algorithm\nwould make the reading easier.\n\n4\\ Line 218 - \"Is is\" -> \"It is\"\n\n5\\ Line 278 mentions an \"anonymized\" utterance. This confused me at the first\nreading, and if I understand correctly it refers to the anonymization described\nlater in 4.2. I think it would be better to forward reference this. \n\n- General Discussion:\n\nOverall, I like the paper, and given answers to the questions I raised above,\nwould like to see it appear in the conference.\n\n- Author Response:\n\nI appreciate the detailed response made by the authors, please include these\ndetails in a final version of the paper.",
        "While this area chair disagrees with some reviewers about (1) the narrowness of the approach's applicability and hence lack of relevance to ICLR, and also (2) the fairness of the methodology, it is nonetheless clear that a stronger case needs to be made for novelty and applicability.",
        "On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI",
        "Jacinto Caamaño Moraleja (1759-1825?) was the leader of the last great Spanish exploration of Alaska (then Russian America) and the Coast of British Columbia. He was a Knight of the Military Order of Calatrava.\n\nEarly life\n\nBorn in Madrid, he came from an aristocratic Galician family, whose homestead was near Santiago de Compostela. His father was Juan Fernández de Caamaño, and his mother, Mariana Moraleja Alocen. He entered the Spanish Navy (Armada) at the age of 18, and two years later he was an Ensign (Alférez de Navio).\n\nExplorations\nA few years later he formed part of a politico-commercial expedition to Constantinople to establish business relations with Turkey, Poland, and Russia. After a quick trip to Cuba in 1787, he was chosen by Juan Francisco de la Bodega y Quadra to go to the Pacific Coast of Mexico and the naval base of San Blas, the headquarters for the exploration of the Pacific Northwest. He was a Frigate Lieutenant (Teniente de Fragata) at the time. Alongside went his brother-in-law, Francisco de Eliza, who would distinguish himself as Governor of Fort San Miguel on Vancouver Island's Nootka Sound. On their ship to Mexico was the recently named Viceroy of New Spain, Don Juan Vicente de Güemes Padilla Horcasitas y Aguayo, 2nd Count of Revillagigedo. On February 3, 1790, Caamaño took part on an expedition to the Pacific Northwest. He was commanding the Nuestra Señora del Rosario (also known as the La Princesa), a 189-ton frigate built in San Blas for the exploration of the North. He did not go beyond Nootka on this voyage, but on his next one, in 1792, came as far as Bucareli Bay commanding the frigate Aránzazu, a corvette built at Cavite in the Philippines. By this time he had been promoted to Ship Lieutenant (Teniente de Navío). This expedition did a thorough study of the coast from Bucareli to Nootka, marking the map of Alaska and British Columbia with many names which are there today. Sailing from Nootka on June 13, 1792, Caamaño explored Bucareli Bay, off Prince of Wales Island, Alaska, and anchored in Dixon Entrance on July 20. Then he explored southward, passing through Principe Channel, Nepean Sound, Whale Sound, near the Estevan Group archipelago, then into Caamaño Sound and south through Laredo Channel, between Aristazabal Island and Princess Royal Island, returning to Nootka on September 7, 1792. He named Principe Channel, Laredo Channel, Campania Island, Campania Sound, Aristazabal Island, Gil Island and Gravina Island after Federico Gravina). George Vancouver adopted these names for his chart.\n\nLater career and family\nHaving successfully completed his trip to Alaska, Caamaño was sent, after a brief stay in San Blas, across the Pacific Ocean to the Philippines. From 1794 to 1807 he served at various posts between Mexico and Peru, having married on one of those trips the Ecuadorian, Francisca de Arteta Santistevan, who gave him eight children. In 1820 he was still living in Guayaquil, the birthplace of his youngest daughter, but nothing is known of the date and place of his death. Some of his descendants, the Caamaño family, have been historically prominent in Ecuador, most notably his grandson, José Plácido Caamaño, who became president of the Republic. Also noted is his great-grandson, Jacinto Jijón y Caamaño, a noted aristocrat, historian and politician; as well as another great-grandson, the poet Ernesto Noboa y Caamaño, among others.\n\nLegacy\nCamano Island, an island in Puget Sound, was named to honor Jacinto Caamaño, as was Caamaño Sound, British Columbia, on the northern limit of which there is also Jacinto Island.  Other Spanish names in the immediate vicinity of Caamaño Sound are Campania Sound, Estevan Point (named by Juan José Pérez Hernández) and Aristazabal Island.  Also named for him is Caamaño Passage northwest of Prince Rupert (between Dundas and Zayas Islands).\n\nSee also\nList of ships in British Columbia\n\nReferences\nWagner, Henry R., \"The Journal of Jacinto Caamaño\" in the British Columbia Historical Quarterly. 2(3), 189-222 & 2(4), 265-300.\nWagner, Henry R., Northwest Coast, pp. 233–235.\nHayes, Derek, Historical Atlas of the Pacific Northwest, p. 76.\nHowgego, Raymond, Encyclopedia of Exploration I, C1, p. 167.\nKathleen E Dalzell, The Queen Charlotte Islands Vol. 2: Of Places and Names (Queen Charlotte Islands)\n\nNotes\n\nFurther reading\n \n\n1759 births\n1820s deaths\nPeople from Madrid\nSpanish explorers of North America\nExplorers of British Columbia\nSpanish explorers\nSpanish history in the Pacific Northwest\n18th-century explorers\n18th-century Spanish people\n19th-century Spanish people\nExplorers of Alaska",
        "  The four-body Yakubovsky equations in a Three-Dimensional approach with the\ninclusion of the three-body forces is proposed. The four-body bound state with\ntwo- and three-body interactions is formulated in Three-Dimensional approach\nfor identical particles as function of vector Jacobi momenta, specifically the\nmagnitudes of the momenta and the angles between them. The modified three\ndimensional Yakubovsky integral equations is successfully solved with the\nscalar two-meson exchange three-body force where the Malfliet-Tjon-type\ntwo-body force is implemented. The three-body force effects on the energy\neigenvalue and the four-body wave function, as well as accuracy of our\nnumerical calculations are presented.The four-body Yakubovsky equations in a\nThree-Dimensional approach with the inclusion of the three-body forces is\nproposed. The four-body bound state with two- and three-body interactions is\nformulated in Three-Dimensional approach for identical particles as function of\nvector Jacobi momenta, specifically the magnitudes of the momenta and the\nangles between them. The modified three dimensional Yakubovsky integral\nequations is successfully solved with the scalar two-meson exchange three-body\nforce where the Malfliet-Tjon-type two-body force is implemented. The\nthree-body force effects on the energy eigenvalue and the four-body wave\nfunction, as well as accuracy of our numerical calculations are presented.\n",
        "Edeowie may refer to:\n\nEdeowie, South Australia, a ghost town\nEdeowie glass, a natural glass found in South Australia\nEdeowie  Station, a pastoral lease in South Australia\nHundred of Edeowie, a cadastral unit in South Australia",
        "Lupine may be one of several things:\n\nSomething that is like, or relating to, a wolf (Canis lupus).\nLupinus, a genus of flowering plants\nLu Pine Records, a record label in Detroit.\n\nSee also\n Lupin (disambiguation)",
        " This will snag the fibers and make them frayed. Also, never use your own brush on the doll's hair. The oil that comes off your hair will damage the doll.;\n,, Don't go in the backyard and play in the dirt with your doll, it will get all messy. Keep your doll clean so you don't have to wash it. If you have a playhouse or slide, go up high. Bratz dolls can be washed, without damaged use a mild shampoo and conditioner for their hair.\n\n, It is hard to pull the pants up so instead of ripping them you can put water on your dolls legs and it will slip up easier.\n\n, Be careful who you let use your dolls.\n\n, Make sure that you keep your dolls accessories in a safe place, like a ziploc bag, plastic container or a special Bratz case so they won't get lost. Earrings, rings, bracelets and sunglasses are easy to lose.\n\n, Make sure you keep an eye on your Bratz because someone could take your Bratz doll. It could even be stolen (or sold) by one of your siblings!\n\n, They will leave gashes in the doll's body and teeth marks. If you have a cat, they might think the Bratz hair are pieces of yarn., It should not get tangles after that. Also, don't let the doll sleep on your bed, due once again to the oil from your hair damaging the hair and you may also damage it by tossing and turning in your sleep.\n\n, Leaving them on the floor or lying around somewhere within reach of small children or pets may cause damage to your doll.\n\n\nBrush curly hair softly and gently - it may get puffy.\n\n,",
        " Buying clothes and jewelry made and directly sold by people you've actually met or talked to ensures that credit is given where it’s due. Artists from many different Native American cultures sell beautiful clothes, jewelry and other accessories. Check online for Native American retailersor shop locally for more options.\n\n\nThis way you can also get a better understanding of the historical and cultural significance of the items you wear. For example, if you buy a piece of jewelry directly from the person who made it, you can ask him or her to tell you more about what’s behind the design.\nMaking sure Native American artists and designers are in control of the sales of their own goods is really important. When Native American symbols and designs get manufactured and sold out of context, their significance gets lost, and that's detrimental to the individual cultures responsible for these highly sought after items.;\n,, If there's a brand making a Native American-inspired item that catches your eye, do some research to see how it's being manufactured and what the company's business practices are. Even if the company isn't Native-American owned, its owners might be using practices that you can get behind. See if you can find out the following:\n\nDoes the company credit the artist or group of people who originally inspired the design or product they're selling? Do they get the proceeds?\nDoes the product perpetuate Native American stereotypes of any kind, or does its design and manufacture honor the spirit in which it was created?\n\n, Since Native American-inspired designs have become so popular, it’s easy to find mass-produced “Native American” products in every mall. But these items aren’t authentic, and more importantly, the people who originally created the designs don’t benefit from their sales.\n\n\nAvoid fake versions of Native American items, like plastic headdresses or cheap imitations of animal pendant jewelry, especially if they were factory made in a different country. These were not made by people who understand the cultural significance behind such items.The same goes for traditional patterns and trademarks that some cultures hold sacred. For example, items like underwear and liquor flasks with Navajo-inspired prints don't honor Navajo culture., Do you know the origin of that Cherokee-inspired pattern on the t-shirt you bought? If not, take the time to find out. If you wear something created by another culture and to which you have no personal connection, you might be committing a huge faux pas without even realizing it.\n\n\nBefore you decide to wear something around, think about what it means, and conduct some research to dig deeper.Whether it’s a pair of moccasins with the classic thunderbird beading, feather earrings or a sweater with a bold native-inspired design, it’s important to think about the history of clothes and jewelry that have Native American roots, so you can be respectful to those who created them.\nIf you choose to wear something you do know has cultural significance without acknowledging it, that’s a form of cultural appropriation - taking something that’s not yours and pretending like it is.\n\n, Some items are fine to wear, but some might have sacred meaning or a history that makes them ill-advised to wear unless you're part of a particular tradition. In these cases it’s important to be willing to not wear something, even if you think it’s beautiful and have no intention to cause any harm.\n\n\nFor example, feathers, headdresses and warbonnets have particular spiritual meaning. Some of these items must be earned and received through special ceremonies. Therefore, they are not to be worn casually by people unfamiliar with the cultures from which these practises stem.,, Trying to look as though you’re part of a culture other than your own is not a good idea. If you’re attracted to Native American designs and art, educate yourself about Native American history and learn how to honor Native American values without appropriating them.\n\nEven famous fashion designers, musicians, sports teams and movie stars are guilty of perpetuating stereotypes and crossing the line.But many groups have spoken out against these practices and called on non-Native Americans to stop. For example, the “Change the Mascot” campaign calls on the National Football League to stop using an offensive mascot for the NFL team in Washington, DC.Making your best effort to be well informed will help you express yourself through fashion without hurting other people.\n\n, If you dress “Native American” from head to toe but you don’t have Native American heritage, you’re making a statement that you’re someone you’re not. Wearing a pair of moccasins or some turquoise jewelry is a fine choice, but decking yourself out in every item you own at once is probably over the top - and a little too close to trying to “look” Native American.\n\n, Don’t dress up like a stereotypical version of a Native American for any reason - even if it’s for a costume party or Halloween. Even if you think it’s all in good fun, it’s disrespectful and ultimately harmful. You might mean well, but even if you’re dressing up as a favorite character from a book, show or movie, it’s still not OK to put on this type of costume. Your costume choice could be very offensive to some people.",
        "Puycornet (; ) is a commune in the Tarn-et-Garonne department in the Occitanie region in southern France.\n\nSee also\nCommunes of the Tarn-et-Garonne department\n\nReferences\n\nCommunes of Tarn-et-Garonne",
        "The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.\n\nHowever, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. ",
        "*Not* only is there still the strong anti-Castro pro-Republican contingent lobbying hard to keep the embargo AND no motivation to lift it, but it is a matter of continued doctrine, principle, and preparation for the demise (of old age, probably) of the Castro brothers.\n\nIn Cuba, a single strong dictator confronted us directly both ideologically and (granted, re-actively) militarily by allowing launchers so close to US soil. We lost face in the Bay Of Pigs incident (long forgotten except in Miami) and then cut a deal with the Soviets (which continued with the Russians, btw) that we would not invade militarily. So basically the U.S. government took the position that we will win this confrontation the easy way; through attrition and time. This is actually a good tactic because the U.S. believes that the only thing that holds the Cuban nation together at this point is the Castro family.   Once Raul is gone, the system will *likely* fall apart within a couple of years.  A large (and rich) ex-pat community will then pour much needed capital and political leadership into the island nation.\n\nFidel, though many have wished him dead of old age for years now, is still hammering away ideologically from the sidelines, but it is Raul that is the military strongman now. Behind him is...nothing. Well, at least nothing with their force of personality or the Castro family name-brand attached to it.  In short, within a few years of Raul's death, no matter who takes over, the island will revert to where it was before Communism took over and we will not have needed to fire a shot.\n\nThe embargo keeps their economy weak and prepares the way for the wave of Cuban-heritage politicians and business people from the U.S. that will provide the next government and economic base once the Castro brothers die off.  These 1st  &  2nd generation Cubans have *vast* wealth. Others have political power  &  experience in local Florida as well as national-level politics. Let us not forget that we have THREE U.S. Senators that are of Cuban heritage. Senators. [One of those, Marco Rubio](_URL_0_), will likely be a Republican nominee for PRESIDENT OF THE UNITED STATES as well. That is some serious pro-embargo political clout at the highest levels of our government.\n\nSome ex-pat Cubans have something even better... money AND political power.  Combined with the very generous hand-outs and recommenced trade from a newly friendly U.S. government, these people will probably turn Cuba from one of the poorest nations in the Caribbean to one of the wealthiest in less than a decade.\n\nTLDR: The embargo keeps them weak  &  paves the wave for 2nd generation Cubans in the US to transform it almost over-night into a rich  &  (more importantly) US-friendly ally once the Castro brothers die.\n\nEdit: I seem to have drawn the ire of Mtl_dood.  I am not trying to justify the embargo or predict the future; I was simply answering the question by explaining the US rational for keeping it in place.",
        "\nAfter the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\"\n",
        "tldr: The authors compare a wide variety of approaches towards sub-word\nmodelling in language modelling, and show that modelling morphology gives the\nbest results over modelling pure characters. Further, the authors do some\nprecision experiments to show that the biggest benefit towards sub-word\nmodelling is gained after words typically exhibiting rich morphology (nouns and\nverbs). The paper is comprehensive and the experiments justify the core claims\nof the paper. \n\n- Strengths:\n\n1) A comprehensive overview of different approaches and architectures towards\nsub-word level modelling, with numerous experiments designed to support the\ncore claim that the best results come from modelling morphemes.\n\n2) The authors introduce a novel form of sub-word modelling based on character\ntri-grams and show it outperforms traditional approaches on a wide variety of\nlanguages.\n\n3) Splitting the languages examined by typology and examining the effects of\nthe models on various typologies is a welcome introduction of linguistics into\nthe world of language modelling.\n\n4) The analysis of perplexity reduction after various classes of words in\nRussian and Czech is particularly illuminating, showing how character-level and\nmorpheme-level models handle rare words much more gracefully. In light of these\nresults, could the authors say something about how much language modelling\nrequires understanding of semantics, and how much it requires just knowing\nvarious morphosyntactic effects?\n\n- Weaknesses:\n\n1) The character tri-gram LSTM seems a little unmotivated. Did the authors try\nother character n-grams as well? As a reviewer, I can guess that character\ntri-grams roughly correspond to morphemes, especially in Semitic languages, but\nwhat made the authors report results for 3-grams as opposed to 2- or 4-? In\naddition, there are roughly 26^3=17576 possible distinct trigrams in the Latin\nlower-case alphabet, which is enough to almost constitute a word embedding\ntable. Did the authors only consider observed trigrams? How many distinct\nobserved trigrams were there?\n\n2) I don't think you can meaningfully claim to be examining the effectiveness\nof character-level models on root-and-pattern morphology if your dataset is\nunvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I\nappreciate that finding transcribed Arabic and Hebrew with vowels may be\nchallenging, but it's half of the typology.\n\n3) Reduplication seems to be a different kind of phenomenon to the other three,\nwhich are more strictly morphological typologies. Indonesian and Malay also\nexhibit various word affixes, which can be used on top of reduplication, which\nis a more lexical process. I'm not sure splitting it out from the other\nlinguistic typologies is justified.\n\n- General Discussion:\n\n1) The paper was structured very clearly and was very easy to read.\n\n2) I'm a bit puzzled about why the authors chose to use 200 dimensional\ncharacter embeddings. Once the dimensionality of the embedding is greater than\nthe size of the vocabulary (here the number of characters in the alphabet),\nsurely you're not getting anything extra?\n\n-------------------------------\n\nHaving read the author response, my opinions have altered little. I still think\nthe same strengths and weakness that I have already discussed hold.",
        " You may have wondered before how to get new game modes: Team Slayer, King of the Hill, Juggernaut. Here's how to do it.\n\n,,, Just select CTF.\n\n, Now, you should see Computer's Desktop screen again.\n\n,, Copy that by right-clicking and selecting Copy.\n\n,,,,, click OK.\n\n, Now you're back at the Halo screen.\n\n,, What you did in the previous steps was essentially replace the information the file available to you (CTF, or Folder 32) with information from a file unavailable to you (Team Slayer, or Folder 34). You'll always paste the information into Folder 32. To access these different modes, copy the information out of the folder number listed and paste into Folder 32.\n\n\n'00' for Classic Slayer (original)\n'01' for Classic Slayer Pro\n'02' for Classic Elimination\n'03' for Classic Phantom\n'04' for Classic Endurance\n'05' for Classic Rockets\n'06' for Classic Snipers\n'07' for Classic Oddball\n'08' for Classic Reverse Tag\n'09' for Classic Accumulate\n'10' for Classic Juggernaut\n'11' for Classic Stalker\n'12' for Classic King\n'13' for Classic King Pro\n'14' for Classic Crazy King\n'15' for Classic Race\n'16' for Classic Rally\n'17' for Classic CTF\n'18' for Classic Invasion\n'19' for Classic Iron CTF\n'20' for Classic CTF Pro\n'21' for Classic Team Race\n'22' for Classic Team Rally\n'23' for Classic Team Ball\n'24' for Classic Team King\n'25' for Classic Team Slayer\n'26' for Slayer\n'27' for Oddball\n'28' for Juggernaut\n'29' for King\n'30' for Crazy King\n'31' for Race\n'32' for CTF (original)\n'33' for Assault\n'34' for Team Slayer\n'35' for Team Oddball\n'36' for Team King\n'37' for Team Race\n\n, You do not have to save the blam.lst in another folder. The only difference is that one you close the online game, you will need to shut down the game and then restart the game and it will be back to normal.\n\n",
        "This paper proposes joint CTC-attention end-to-end ASR, which utilizes both\nadvantages in training and decoding. \n\n- Strengths:\nIt provides a solid work of hybrid CTC-attention framework in training and\ndecoding, and the experimental results showed that the proposed method could\nprovide an improvement in Japanese CSJ and Mandarin Chinese telephone speech\nrecognition task. \n\n- Weaknesses:\nThe only problem is that the paper sounds too similar with Ref [Kim et al.,\n2016] which will be officially published in the coming IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.\nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,\nand this paper proposes joint CTC-attention using MTL+joint decoding for\nJapanese and Chinese ASR tasks. I guess the difference is on joint decoding and\nthe application to Japanese/Chinese ASR tasks. However, the difference is not\nclearly explained by the authors. So it took sometimes to figure out the\noriginal contribution of this paper.\n\n(a) Title: \nThe title in Ref [Kim et al., 2016] is “Joint CTC- Attention Based End-to-End\nSpeech Recognition Using Multi-task Learning”, while the title of this paper\nis “Joint CTC-attention End-to-end Speech Recognition”. I think the title\nis too general. If this is the first paper about \"Joint CTC-attention\" than it\nis absolutely OK. Or if Ref [Kim et al., 2016] will remain only as\npre-published arXiv, then it might be still acceptable. But since [Kim et al.,\n2016] will officially publish in IEEE conference, much earlier than this paper,\nthen a more specified title that represents the main contribution of this paper\nin contrast with the existing publication would be necessary. \n\n(b) Introduction:\nThe author claims that “We propose to take advantage of the constrained CTC\nalignment in a hybrid CTC-attention based system. During training, we attach a\nCTC objective to an attention-based encoder network as a regularization, as\nproposed by [Kim at al., 2016].“ Taking advantage of the constrained CTC\nalignment in a hybrid CTC-attention is the original idea from [Kim at al.,\n2016]. So the whole argument about attention-based end-to-end ASR versus\nCTC-based ASR, and the necessary of CTC-attention combination is not novel.\nFurthermore, the statement “we propose … as proposed by [Kim et al,\n2016]” is somewhat weird. We can build upon someone proposal with additional\nextensions, but not just re-propose other people's proposal. Therefore, what\nwould be important here is to state clearly the original contribution of this\npaper and the position of the proposed method with respect to existing\nliterature\n\n(c) Experimental Results:\nKim at al., 2016 applied the proposed method on English task, while this paper\napplied the proposed method on Japanese and Mandarin Chinese tasks. I think it\nwould be interesting if the paper could explain in more details about the\nspecific problems in Japanese and Mandarin Chinese tasks that may not appear in\nEnglish task. For example, how the system could address multiple possible\noutputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input\nwithout using any linguistic resources. This could be one of the important\ncontributions from this paper.\n\n- General Discussion:\nI think it would be better to cite Ref [Kim et al., 2016] from\nthe official IEEE ICASSP conference, rather than pre-published arXiv:\nKim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech\nRecognition Using Multi-task Learning\", IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear.",
        "  We consider processes with second order long range dependence resulting from\nheavy tailed durations. We refer to this phenomenon as duration-driven long\nrange dependence (DDLRD), as opposed to the more widely studied linear long\nrange dependence based on fractional differencing of an $iid$ process. We\nconsider in detail two specific processes having DDLRD, originally presented in\nTaqqu and Levy (1986), and Parke (1999). For these processes, we obtain the\nlimiting distribution of suitably standardized discrete Fourier transforms\n(DFTs) and sample autocovariances. At low frequencies, the standardized DFTs\nconverge to a stable law, as do the standardized sample autocovariances at\nfixed lags. Finite collections of standardized sample autocovariances at a\nfixed set of lags converge to a degenerate distribution. The standardized DFTs\nat high frequencies converge to a Gaussian law. Our asymptotic results are\nstrikingly similar for the two DDLRD processes studied. We calibrate our\nasymptotic results with a simulation study which also investigates the\nproperties of the semiparametric log periodogram regression estimator of the\nmemory parameter.\n",
        "Summary: The authors propose an input switched affine network to do character-level language modeling, a kind of RNN without pointwise nonlinearity, but with switching the transition matrix & bias based on the input character. This is motivated by intelligibility, since it allows decomposition of output contribution into these kappa_s^t terms, and use of basic linear algebra to probe the network.\n\nRegarding myself as a reviewer, I am quite sure I understood the main ideas and arguments of this paper, but am not an expert on RNN language models or intelligibility/interpretability in ML.\nI did not read any papers with a similar premise - closest related work I'm familiar with would be deconvnet for insight into vision-CNNs.\n\nPRO:\nI think this is original and novel work. This work is high quality, well written, and clearly is the result of a lot of work.\nI found section 4.5 about projecting into readout subspace vs \"computational\" subspace most interesting and meaningful.\n\nCON:\n+ The main hesitation I have is that the results on both parts (ISAN model, and analysis of it) are not entirely convincing:\n   (1) ISAN is only trained on small task (text8), not clear whether it can be a strong char-LM on larger scale tasks,\n   (2) nor do the analysis sections provide all that much real insight in the learned network.\n\n(1b) Other caveat towards ISAN architecture: this model in its proposed form is really only fit for small-vocabulary (i.e. character-based) language modeling, not a general RNN with large-vocab discrete input nor continuous input.\n\n(2a) For analysis: many cute plots and fun ideas of quantities to look at, but not much concrete insights.\n(2b) Not very clear which analysis is specific to the ISAN model, and which ideas will generalize to general nonlinear RNNs.\n(2c) Re sec 4.2 - 4.3: It seems that the quantity \\kappa_s^t on which analysis rests, isn't all that meaningful. Elaborating a bit on what I wrote in the question:\nFor example: Fig 2, for input letter \"u\" in revenue, there's a red spot where '_' character massively positively impacts the logit of 'e'. This seems quite meaningless, what would be the meaning of influence of '_' character? So it looks ot me that the switching matrix W_u (and prior W_n W_e etc) are using previous state in an interesting way to produce that following e. So that metric \\kappa_s^t just doesn't seem very meaningful.\nThis remark relates to the last paragraph of Sec4.2.\n\nEven though the list of cons here is longer than pro's, I recommend accept; specifically because the originality of this work will in any case make it more vulnerable to critiques. This work is well-motivated, very well-executed, and can inspire many more interesting investigations along these lines.",
        " This is a good way to tell if your ankle is fractured or just sprained. If your bone is fractured, moving it is nearly impossible, while you will still be able to move it if it is sprained. Pain, swelling, and the inflammatory cells causing your ankle to swell contribute to this loss of function.Moreover, proprioception is altered during injury. Proprioception is the awareness of the brain in terms of a position of a part of your body without the aid of vision. Therefore, coordination between your brain and your ankle is damaged when you fracture your ankle.;\n, The muscles, ligaments, and tendons surrounding your ankle are highly peppered with pain receptors. Once injury occurs, these receptors send signals of pain to your brain. The brain reciprocates and lets the affected area feel pain—which you are probably experiencing right now.While pain can be associated with sprains too, fracturing a bone in your ankle will probably feel very painful, unless you have a high pain tolerance.\n\n, If you have fractured your ankle, your body will try to protect the injured ankle from further damage by sending a lot of inflammatory (“healing”) cells to the affected area. The inflammatory cells carry repair kits to mend the injury. Unfortunately, these cells also cause swelling and discomfort.Believe it or not, your injured ankle actually benefits from swelling indirectly, even if it causes you feelings of pain and frustration. Swelling prohibits large degrees of movement of the involved ankle. Therefore, it promotes speedy recovery.\n\n, Redness or flushing of the affected ankle means that there is a rush of blood in the area. Blood carries recovery cells to the area to promote faster healing., When blood rushes to your ankle, it will also cause your ankle to feel really hot. You may feel like you have a fever that you can only feel in your ankle.\n\n, PRICE stands for: Protect, Rest, Ice, Compress, and Elevate. How to do the things in this acronym will be explained in the following steps. You should continue to follow these rules after you have seen a doctor while your ankle heals.\n\n, If you think your ankle may be fractured but cannot go to a doctor immediately, put a splint on it. The most basic splint is a sturdy flat object, like a ruler, that you can press up against your broken bone. You can keep it in place with gauze or medical tape; this helps to keep the broken bone from moving.To learn how to properly splint an ankle, click here.\n\n\nIf you do not have a splint but are wearing shoes that support your ankle, like hiking boots or regular boots, tighten the laces as tightly as they can go to keep your ankle immobile while you make your way to the hospital.\n\n, Continuing to walk and bear weight on the affected ankle will cause injury. You should stay put while waiting for help. If you are in a situation where you need to leave the area you are in, ask others to support you while you hobble away, or find a sturdy branch or pole that you can use as a crutch.\n\n\nRest should be continued even after your initial treatment. The first three days after injury should be dedicated to rest. Wait for the doctor’s clearance before resuming any activities that involve using your ankle.\n\n, Use an ice pack, bag of ice, or even a bag of frozen veggies to cool down your injured ankle. Ice decreases the flow of blood to your ankle, thus reducing swelling. The cold also masks the pain you feel. Ice your ankle for 15 to 20 minutes every hour.If you can, use crushed ice because it can follow the contours of your ankle.\nAvoid applying the ice directly to your skin as really cold temperatures can burn your skin just like heat will.\n\n, Compression of the affected area reduces blood flow to the injured site. Hence, less inflammatory cells will be available to cause swelling. If you have put a splint on your ankle and wrapped it with elastic bandage, you are already compressing your ankle.You can also skip the splint and wrap your ankle in elastic bandages while you make your way to the hospital.\n\n, When you elevate your ankle, you reduce the flow of blood to that area. Because of this, the swelling in your ankle will decrease. You can elevate your ankle while sitting or lying down.Sitting: Your ankle must be elevated higher than your hip.\nLying down: Your ankle must be elevated higher than your heart and chest.\n\n, On top of using the PRICE acronym, you can also take acetaminophen to regulate the pain. Take 325 to 650 mg tablets, 1 tablet every 4 hours, unless you have previous orders from a doctor to avoid acetaminophen.\n\n\nDo not take other painkillers like ibuprofen, until after you have gone to the hospital. Painkillers like ibuprofen thin your blood and could cause problems if you have to get surgery.\n\n, Once you have taken some precautions for your ankle by following the acronym outlined in Part Two, you should go to the hospital to get your ankle treated medically. The form of treatment you receive will depend on how severe the break is in your ankle. A doctor will take an x-ray of your ankle to determine what to do.\n\n, If you have a mild break (which means a fracture without a dislocated ankle) you will most likely have a cast put on your ankle. A cast is a cement boot that immobilizes your foot and allows the broken ends of the bone in your ankle to unite naturally.You will most likely have to wear the cast for four to eight weeks.\n\n, Closed reduction is a maneuver performed by doctors to restore a bone in its proper place without having to perform surgery. It is important get the ankle back into its normal location so that you are able to move around like normal once your ankle heals. An orthopedic surgeon will usually do the maneuver.Casting is required after reduction to ensure that the bones stay in place. In extreme cases of ankle joint instability, metal plates and screws have to be inserted externally (called external fixation) or internally (called internal fixation) to hold the bones in place.\n\n, Getting surgery done will help put your bones back into their proper places, and keep them in those proper places. On the positive side of things, getting surgery will speed up your recovery time—while you have to wait out a cast, you will be on the mend within days after your surgery.There are two parts to ankle surgery. First, you would undergo open reduction, during which your bones would be put back into their proper places. Then, through external fixation, metal plates are drilled into the bone and screws are placed to ensure very minute movement of the affected bones.\n\n, Regardless of what kind of treatment you receive, you should give yourself a break for the next couple of days. Your body has been through a traumatic experience and needs time to recover. Follow the PRICE acronym outlined in Part Two as a way to make sure you are giving your ankle the tender care it needs.\n\n, If you are experiencing a lot of pain after your medical treatment, take some painkillers like acetaminophen or ibuprofen. If your doctor prescribed a different painkiller, follow his or her instructions.\n\n, In the first few days, you should take short walks in your house, using the crutches that the doctor gave you. It might be very uncomfortable in the beginning, but you will get used to them. You must not put pressure on your leg, unless stated otherwise by your doctor. You should not even put your leg on the floor for the first few days.\n\n, if you wet your cast, immediately contact your orthopedist for a new one. This is very important because if water accumulates inside the cast (between the cast and your skin), your skin can be affected and an infection can occur very quickly.\n\n\nAnother issue with a wet cast is that it can get loose, which means it won’t hold your ankle in place properly.\n\n, As your ankle recovers, you will need to start gaining strength back in that ankle. To do this, you will need to go to physical therapy and perform exercises that will help to build your ankle’s mobility and flexibility. This will be covered in Part Five.\n\n, Having a cast or surgery bring your bones back together is only half of the recovery process. Rehabilitation (aka physical therapy) is equally important to recover the stability, mobility, and function of your ankle. Rehabilitation includes ankle joint stability and mobility exercises, calf muscle strengthening, and stretching.Stability should be trained before mobility. Stability exercises reinforce the maximum allowable degree of movement in the ankle without risking injury. Stability is also necessary to withstand external pressure applied to the ankle.\n\n, Ankle joint stability exercises are usually done within a week after surgery or when the cast is removed. This is a walkthrough of a simple ankle joint stability exercise:\n\n\nStand in front of a wall so that you are an arm’s length away from it. Push against the wall. Look straight, keep your shoulder blades down and back and make a double chin. Standing like this will make sure that your spine is properly aligned.\nSuck in your gut as if someone were trying to punch you in the stomach. Squeeze your butt muscles together. Doing this maneuver will train the core and posterior chain muscles. Both are vital for training the proper alignment of the ankle joint and preventing re-injury.\nRaise the healthy foot off the floor. Maintain this position for 30 seconds. Standing on one leg will introduce an unstable situation. This will train the injured ankle to resist unstable forces. Resist the urge to look at your foot. Looking in a straight direction throughout the movement will train proprioception as well.\nIt is common to experience wobbling in the first few tries. Rest for 1 minute and then repeat this process twice. Do this once with the other leg to make sure that it receives the same strength training.\n\n, Ankle joint mobility is very important in everyday activities such as walking, climbing a flight of stairs, driving and more. The goal is to restore ankle mobility to normal movement patterns. Ankle joint mobility exercises are usually done when ankle joint stability is established. “The Alphabet” is an example of ankle joint mobility exercise:\n\n\nSit in a chair and extend your leg with the injured ankle so that it is parallel to the ground. Pretend that the affected foot is a pen and draw the letters of the alphabet in the air from A-Z and reverse.\nYou might feel stiffness in your ankle. Work through the stiffness but not so hard that you feel pain. Your focus should be on the movement of your ankle and not the smoothness of the strokes.\nRest for 2 minutes and repeat the movement twice.\n\n, The calf raise targets the calf muscles, Achilles tendon, and the ankle joint ligaments. It mimics the motion that our foot makes for everyday activities such as walking, running, climbing a flight of stairs, and reaching for tall objects. To do it:\n\n\nStand with the balls of your feet on the edge of a step on your stairs with your heels hanging off. Keep the gut sucked in, stand tall, and rest your hands against a wall or handrail.\nRise on the balls of your feet in a tip-toe fashion. Try to stand as high on your tip-toes as you can. You heels should lift above the step. Exhale while you do this to keep your blood pressure normal.\nSlowly descend until your heels are a few inches below the edge of a step. Inhale while you do this. This works the calf muscle to its whole range of motion leaving no weak point behind. Repeat 10 times for one set. Rest for one minute and do 2 more sets.\n\n, The soles of the foot contain a lot of small foot muscles. The small foot muscles are enclosed in a sheath called the plantar fascia. The mobility of the plantar fascia is very crucial in preventing injury of the ankle. The towel curls will help the fascia stay loose despite everyday stress to the foot. To do the towel curl:\n\n\nSit in a chair or on the couch. You can even do this while watching TV or reading a wikiHow article. Place one medium-sized thin towel on the floor lengthwise. Put your foot on the end of the towel.\nDraw the other end of the towel towards you by curling only your toes. The heel should be planted to the ground at all times. Repeat this exercise three more times and switch with the other foot.\n\n, Flexibility is as important as strength when it comes to the health of your ankles. The calf stretch mainly targets the calf muscles and Achilles tendon. Both are vital to great ankle flexibility. To do this:\n\n\nStand in front of a wall and push against it with your arms. Your arms should be at chest level and shoulder-width apart. The shoulder blades should be drawn back and down.\nPut your right foot behind your left foot and keep your right leg straight. The heel should come in contact with the ground and toes should point forward throughout the stretch.\nBend the left foot forward to stretch the right calf muscle. You should feel a stretch or a slight discomfort that is tolerable in the calf muscles of your right leg. If you feel pain, stop.\nHold this position for 30 seconds for people younger than 40 years old. Those older than 40 years old should hold their stretch for 60 seconds. The muscles become stiffer as we age. Keeping longer stretches will help loosen the muscles up.\n\n, Tennis ball rolls relieve accumulated stress of the foot muscles and plantar fascia. To do them:\n\n\nPlace a regular tennis ball on the floor. Sit in a chair and place the middle part of your foot on the tennis ball.\nRoll the ball in circles. Do it in clockwise fashion for 1 minute followed by counter-clockwise motion for 1 minute as well. Roll the ball back and forth for 1 minute. Roll the ball side-to-side for 1 minute.\nSwitch to your other foot. Do this process three times on both feet.\n\n",
        "This paper is an extension of Lenc&Vedaldi15 paper, showing CNN representations at FC7 layer are to certain extent equivariant to various classes of transformations and that training with a certain group of transformation makes the representations more equivalent.\n\nAuthors performed a large amount of experiments, training over 30 networks with different forms of jitter, which is quite impressive. However it is rather difficult to find a main message of this work. Yes, authors measured the properties on a different layer than the Lenc&Vedaldi15, however it is hard to find some novel insights other than the known fact that jitter helps to achieve invariance. The evaluation seems to be mostly correct, however the paper does not seem to be solving the task advertised in its title really well.\n\nMajor issues are in the experiments with the representation distances:\n* The selection of only FC7 is a bit controversial - it is followed only by a single classification layer (FC8) to the common output - class likelyhoods. Because the FC8 is just a linear projections, what the equivalence map does is just to re-project the FC8 weights of the attached network to the weights of the original network. Probably performing similar experiments but on more layers may be more useful (as the networks are already trained).\n* The experiment with representation distance is missing what is the classification error on the testing dataset. This would answer whether the representations are actually compatible up to linear transformation at all...\n* It is not clear for the experiment with K-NN whether this is measured per each test set example? After training the equivalence map? More clear would be to show that networks trained on similar group of jitter transformations are more compatible on the target task.\n* The proposed method does not seem to improve equivariance consistently on all tasks. Especially with \\lambda_1 and \\lambda_2 having such small values, the loss is basically equal to simple data jitter as it just adds up the loss of the original and transformed image. Maybe the issue is in the selection of the FC7 layer?\n\nIn general, this paper shows some interesting results on the FC7 equivariance, but it does not seem to be drawing many interesting new observations out of these experiments. Due to some issues with the equivalence experiments and the finetuning of equivariance, I would not recommend acceptance of this manuscript. However, refining the experiments on already trained networks and restructuring this manuscript into more investigative work may lead to interesting contribution to the field.\n\nThere are also few minor issues:\n* It is not experimentally verified that the new criterion for equivariance mapping helps to gain better results.\n* The angles on page 1 and 5 are missing units (degrees?).\n* On page three, \"In practice, it is difficult... \", it is not M_g which is maximised/minimised, but the loss over the M_g\n* Page 4, footnote 2 - if you are just halving the activations, it is hard to call it a dropout as this constant factor can be passed to the following/preceding weights\n* Is the network for RVL-CDIP the same architecture as Alexnet?\n* On page 7, Figure 3a+3b - in my opinion, turning the diagonal elements to white is really misleading, and probably even incorrect, as the distance between the same representations should be zero (which is also a way how to verify that the experiments are performed correctly).",
        "Dear Reviewer, \n\nThank you very much for your positive comments and scores!  As stated before, we are using Adadelta as the basic optimizer, rather than simple SGD. Very sorry for the misunderstanding brought by the term `Plain SGD’ in the paper. It just means `Adadelta without any data filtration'. In addition we are working on new datasets and conducting more qualitative analysis.\n",
        "Kleidocerys is a genus of seed bugs in the family Lygaeidae. There are about 17 described species in Kleidocerys.\n\nSpecies\nThese 17 species belong to the genus Kleidocerys:\n\n Kleidocerys costaricensis Cervantes & Brailovsky, 2010\n Kleidocerys denticollis (Stal, 1874)\n Kleidocerys dimidiatus Barber, 1953\n Kleidocerys ericae (Horvath, 1909)\n Kleidocerys franciscanus (Stal, 1859)\n Kleidocerys hispaniola Baranowski, 2005\n Kleidocerys modestus Barber, 1953\n Kleidocerys nubilus (Distant, 1883)\n Kleidocerys obovatus (Van Duzee, 1931)\n Kleidocerys ovalis Barber, 1953\n Kleidocerys pallipes Brailovsky, 1976\n Kleidocerys privignus (Horvath, 1894)\n Kleidocerys punctatus (Distant, 1893)\n Kleidocerys resedae (Panzer, 1793) (birch catkin bug)\n Kleidocerys suffusus Barber, 1947\n Kleidocerys truncatulus (Walker, 1872)\n Kleidocerys virescens Fabricius, 1794\n\nReferences\n\nFurther reading\n\nExternal links\n\n \n\nLygaeidae\nArticles created by Qbugbot",
        "In this paper, citations are appearing with the authors' first initials and last names, e.g. (Hinton, G. et al., 2012a) instead of the authors last names and no initials, e.g. (Hinton et al., 2012a).  I find the first initials to be very distracting.  Please reformat the paper to match the citation style of the ICLR 2017 template.\n",
        "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  \n\nPresentation: \nAlthough that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network – for me that would be a more intuitive exposition of the new algorithm and findings. \n\nSmall concern in general case derivation: \nSection 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.\n\nResults:\nA comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)\n\nOverall: strong paper, good theoretical insights. ",
        "Mahesh Anand (13 August 1961 – 9 February 2019) was an Indian actor, dancer and martial artist who worked in Hindi, Tamil, Telugu and Malayalam films. He is remembered for playing villainous roles in Hindi films. He was a black belt in Karate and was a model and a trained dancer before he started acting. His debut movie was Karishmaa 1984 while his last on-screen appearance was in the 2019 comedy-drama Rangeela Raja. Before acting in Karishmaa he performed for the opening sequence of Sanam Teri Kasam 1982 with his dance in silhouette.\n\nFilmography\n\nTelugu\n\nTamil\n\nMalayalam\n\nPersonal life \nAnand has been married five times and also has a son. He first married actor Reena Roy’s sister Barkha Roy and went on to marry Miss India International, Erica Maria D’Souza in 1987, Madhu Malhotra in the year 1992, Usha Bachani in the year 2000 and a woman of Russian origin in 2015.\n\nDeath \nOn 9 February 2019, his maid failed to get any response from him after ringing the bell of his residence many times. She then immediately informed his sister who came there with Versova Police. Anand was found dead, sitting on a sofa and a bottle of alcohol & a food plate was found lying on a table beside him.\n\nReferences\n\nExternal links\n \n\n1961 births\n2019 deaths\n20th-century Indian male actors",
        "The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n",
        "Kaipattoor Bridge is a concrete bridge in Pathanamthitta, Kerala that connects Kaipattoor and Omalloor over the Achankovil River.\n\nReferences\n\nBridges in Kerala",
        "Quality, Clarity:\n \n The work is well motivated and clearly written -- no issues there.\n \n Originality, Significance:\n \n The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.\n \n pros:\n - the problem is difficult and relevant: good solutions would have impact\n \n cons:\n - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines\n - the influence of the initial controller on the results\n - the work may gain better appreciation at a robotics conference\n \n I am very much on the fence for this paper.\n It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.\n Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track.",
        " Many rental or fleet aircraft have one which may have all recent service, inspection, and maintenance information updated for each aircraft. Here are some things you may see:\n\n\nFlight hours. Because aircraft maintenance is usually scheduled at flight hour intervals, the actual hours flown are logged to accommodate scheduling service dates for these activities.\nPilot observations. When more than one pilot is likely to operate an aircraft, it is beneficial to have input from each pilot as to the flight characteristics of the plane. One pilot may notice a vibration, shudder, or other physical anomaly, or unusual readouts from gauges while in flight, that the next pilot should be aware of.\nService schedule. If a component (airframe, engine, etc) has a scheduled 100 hrs inspection due after 5 more flight hours, a longer flight could be postponed until the service is performed, or another aircraft utilized.;\n,\n\n\nMake sure the aircraft registration, certification, and other paperwork is in the cabin and up to date.\nMake sure the ignition switch is in the off position.\nTurn on the master power switch.\nCheck the fuel gauges. If the fuel is low, call for the service truck to bring your fuel out while you continue to do the rest of your checks.\nListen to the sounds of equipment powering on. Radio cooling fans, instrument gyros, and other equipment make noticeable sounds, and when unusual sounds are heard, it may be a warning an instrument or radio could fail in-flight.\nCheck flaps, landing gear lockdown levers, and other flight controllers for smooth, normal function.\n\n,\n\n\nAs you climb out of the cabin, take a look at the seat support rails (on small aircraft) to make sure the seat is anchored securely and all fasteners are in place.\nCheck the cabin door to be sure it operates smoothly and shuts securely. Worn hinges or latches that do not secure properly can cause an inflight emergency. If the door does not move smoothly into open and closed positions, it may indicate the airframe and internal structure are damaged.\n\n,, Look at the flight control surfaces, flaps, and ailerons. Make sure that items are secure and have no loose fasteners.\n\n, Replace the fuel cap securely.\n\n,, Continue looking at the aircraft surface. Be particularly careful to observe defects or loose fasteners in the aircraft surface.\n\n, While at the tail, you may remove the wheel chock or tail tie-down. Look at the elevators and the rudder. As with all control surfaces, these should be tight, with no loose motion or free play.\n\n, Give the gear suspension a once-over, too, making sure any boots or covers are in place, and that all support cables are tight.\n\n, Remove the fuel cap and look into the tank to be sure it is fueled, replace the tank cap securely, and check out the control surfaces (again, the ailerons and flaps).\n\n,, Look at the alternator belt to make sure it is tight, and the air intake to be sure it is not obstructed.\n\n, Never place your body in the swing radius of an airplane propeller. Check the \"spinner\" for lubricant leaks, missing bolts and pins, or other problems. Look at the propeller itself, to be sure the blades are not cracked, bent, delaminated, or damaged in other ways.\n\n, Any leaks or appearance of fluid on the aircraft skin should be inspected by a trained maintenance person prior to flight.\n\n, Look for loose fittings, tires which may have tread separation, low inflation pressure, and other defects.\n\n,",
        "The authors propose the use of a vertical and horizontal one-dimensional RNN (denoted as L-RNN module) to capture long-range dependencies and summarize convolutional feature maps. L-RNN modules are an alternative to deeper or wider networks, 2D RNNs, dilated (Atrous) convolutional layers, and a simple flatten or global pooling layer when applied to the last convolutional layer for classification. L-RNN modules are faster than 2D RNNs, since rows and columns can be processed in parallel, are easy to implemented, and can be inserted in existing convolutional networks. The authors demonstrate improvements for classification and semantic segmentation.\n\nHowever, further evaluations are required that show for which use cases L-RNNs are superior to alternatives for summarizing convolutional feature maps:\n\n1. I suggest to use a fixed CNN with as certain number of layers, and summarize the last feature map by a) a flatten layer, b) global average pooling, c) a 2D RNN, d) and dilated convolutional layers for segmentation. The authors should report both the run-time and number of parameters for these variants in addition to prediction performances. For segmentation, the number of dilated convolutional layers should be chosen such that the number of parameters is similar to a single L-RNN module.\n\n2. The authors compare classification performances only on 32x32 CIFAR-10 images. For higher resolution images, the benefit of L-RNN modules to capture long-range dependencies might be more pronounced. I therefore suggest evaluating classification performances on one additional dataset with higher resolution images, e.g. ImageNet or the CUB bird dataset.\n\nAdditionally, I have the following minor comments:\n\n3. The authors use vanilla RNNs. It might be worth investigating LSTMs or GRUs instead.\n\n4. For classification, the authors summarize hidden states of the final vertical recurrent layer by global max pooling. Is this different from more common global average pooling or concatenating the final forward and backward recurrent states?\n\n5. Table 3 is hard to understand since it mingles datasets (Pascal P and COCO C) and methods (CRF post-processing). I suggest, e.g., using an additional column with CRF ‘yes’ or ‘no’. I further suggest listing the number of parameters and runtime if possible.\n\n6. Section 3 does not clearly describe in which order batch-normalization is applied in residual blocks. Figure 2 suggest that the newer BN-ReLU-Conv order described in He et al. (2016) is used. This should be mentioned in the text.\n\nFinally, the text needs to be revised to reach publication level quality. Specifically, I have the following comments:\n\n7. Equation (1) is the update of a vanilla RNN, which should be stated more clearly. I suggest to first describe (bidirectional) RNNs, to reference GRUs and LSTMs, and then describe how they are applied here to images. Figure 1 should also be referenced in the text.\n\n8. In section 2.2, I suggest to describe Bell at al. more clearly. Why are they using eight instead of four RNNs? \n\n9. Section 4 starts with a verbose description about transfer learning, which can be compressed into a single reference or skipped entirely.\n\n10. Equation (6) seems to be missing an index i.\n\n11.In particular section 5 and 6 contain a lot of clutter and slang, which should be avoided:\n11.1 page 8: ‘As can be seen’, ‘we turn to that case next’\n11.2 page 9: ‘to the very high value’, ‘as noted earlier’,  ‘less context to contribute here’\n11.3 page 10: ‘In fact’, ‘far deeper’, ‘a simple matter of’, ‘there is much left to investigate.\n\n\n\n\n",
        "  We study the effects of different forms of information feedback associated\nwith mass media on an agent-agent based model of the dynamics of cultural\ndissemination. In addition to some processes previously considered, we also\nexamine a model of local mass media influence in cultural dynamics. Two\nmechanisms of information feedback are investigated: (i) direct mass media\ninfluence, where local or global mass media act as an additional element in the\nnetwork of interactions of each agent, and (ii) indirect mass media influence,\nwhere global media acts as a filter of the influence of the existing network of\ninteractions of each agent. Our results generalize previous findings showing\nthat cultural diversity builds-up by increasing the strength of the mass media\ninfluence. We find that this occurs independently of the mechanisms of action\n(direct or indirect) of the mass media message. However, through an analysis of\nthe full range of parameters measuring cultural diversity, we establish that\nthe enhancement of cultural diversity produced by interaction with mass media\nonly occurs for strong enough mass media messages. In comparison with previous\nstudies a main different result is that weak mass media messages, in\ncombination with agent-agent interaction, are efficient in producing cultural\nhomogeneity. Moreover, the homogenizing effect of weak mass media messages are\nmore efficient for direct local mass media messages than for global mass media\nmessages or indirect global mass media influences.\n",
        "The likely reason would probably go like this:\n\nTap water tends to contain a decent amount of dissolved air in it, and gases are less soluble in hot liquids than cold ones. So boiling water removes most of the dissolved gas, which you can actually see if you look closely at boiling water, as there are no small bubbles around once it's boiled for a while. \n\nIf you heat water in the microwave, it heats up much faster. It'd also typically be in a smaller container, with ~~less~~ more surface area for the bubbles to form (nucleate) on. For which reason you have to be a bit careful with microwaving water, as it might not form steam bubbles (i.e. boil) at all either. Instead becoming superheated, only to suddenly and dangerously flash-boil once you stick a spoon or something into it. So you're more likely to end up with a significant amount of air still dissolved in your microwaved water.\n\nWhen you then pour the sugar into the hot microwaved water, you're suddenly providing a whole lot of nucleation sites and allowing the still-dissolved air to escape in a fizz, while in the stove-boiled water it's already escaped. \n\nIt's much the same thing as the famous menthos-in-coke phenomenon, except that it's a small amount of air and not a large amount of CO2 being released.",
        "Major changes in the revision\n-- Figures 2 are replaced with tables for better readability\n-- Related works have been expanded\n-- Incorporated the reviewers’ comments in the experiment section.\n-- Experiments for GRAM+ and RNN+ in heart failure prediction were re-run\n\nMore details\nWe discovered that, for GRAM+ and RNN+ in HF prediction with varying amounts of training data, we initialized the basic embeddings e_i’s and the embedding matrix W_emb with GloVe vectors that were trained always on the 100% training data instead of the downsampled training data. This could have exaggerated the AUC of both models. Therefore we conducted a correct experiment where the initialization was done with GloVe vectors trained on the downsampled training data. We found that GRAM+’s performance did not show noticeable difference, but RNN+’s performance dropped approximately 0.5-1% AUC.\n",
        "This work presents an empirical study of the influence of different types of data augmentation on the performance of CNNs. It also proposes to incorporate additional loss functions to encourage approximate invariance or equivariance, and shows there are some benefits.\n\nThe paper reads well and the objectives are clear. The study of invariances in CNNs is a very important topic, and advances in this area are greatly appreciated. The paper splits itself in two very different parts -- the empirical study of equivariances in existing CNNs, and the proposal of equivariance objectives. However, taken separately each of these two parts could be better executed.\n\nOn the empirical study, its breath is relatively limited, and it's hard to draw any far-reaching conclusions from it:\n- Only one network is studied; at least one other architecture would have made for better generalization.\n- Only one layer (fc7) is studied; this presents issues as the top layer is the most invariant. At least one convolutional layer (possibly more) should have been considered.\n- The reliance on the scanned text dataset does not help; however the ImageNet results are definitely very encouraging.\n\nIt is nice to see how performance degrades with the degree of transformations, and the authors do interpret the results, but it would be better to see more analysis. There is only a limited set of conclusions that can be drawn from evaluating networks with jittered data. If the authors could propose some other interesting ways to assess the invariance and equivariance, they would potentially draw more insightful conclusions from it.\n\nOn the proposed loss function, only a very quick treatment of it is given (Section 4, half a page). It does not differ too much from known invariance/equivariance objectives studied in the literature previously, e.g. Decoste and Scholkopf, \"Training Invariant Support Vector Machines\", Machine Learning, 2002.\n\nI'm not sure that dividing the paper into these two different contributions is the best approach; they both feel a bit incomplete, and a full treatment of only one of them would make for an overall better paper.\n\n",
        "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, François Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413–456.\n\nAmy Isard, 2016. “The Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts” , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portorož, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline on the two corpora could disappear if Wen et al.'s\nsubstantially higher-scoring method were employed.  The assumption that this\nrelative difference would remain even with fancier methods should be made\nexplicit, e.g. by acknowledging the issue in a footnote.  Even with this\nlimitation, the comparison does still strike this reviewer as a useful\ncomponent of the overall comparison between the datasets.\n\nRe whether a paper about dataset creation should be able to get into ACL\nwithout system results:  though this indeed not unprecedented, the key issue is\nperhaps how novel and important the dataset is likely to be, and here this\nreviewer acknowledges the importance of the dataset in comparison to existing\nones (even if the key advance is in the already published content selection\nwork).\n\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the\nrole of domain dependence and what it means to be \"wide coverage\" in the final\nversion of the paper, if accepted.",
        "While I wasn't able to find much, I was able to find some sources that brought some light to your question. \n\nEarnst Hess, the Protestant son of a Jewish mother served as Corporal Hitler's commanding officer during WW1. Being the son of a Jewish mother, under Nazi Race Laws, Hess was considered a \"full-blooded Jew\", which would have inevitably made him a target under Hitler's oppression. However, in a letter from August 27, 1941 to the Dusseldorf Gestapo, Heinrich Himmler, one of the architects of the Final Solution, instructed the secret police to grant Hess \"the relief and the protection as per the Fuhrer's wishes\". \n\n >  Hess's daughter Ursula, said her father had few memories of Hitler other than that he had no friends in the regiment.\n\nFritz Wiedemann, a former member of his unit, served as Hitler's personal adjutant from 1934 to 1939.\n\nAugust Kubizek, a childhood friend of Hitler's was hired in 1938 by the Nazi Party to write two short propaganda booklets called *Reminiscences* about his youth with Hitler.",
        " Though as the owner of a nursing agency you don't have to be a licensed nurse, it will give you a better understanding of customers' and employees' situations.;\n,, Include your start-up costs, initial payroll costs, market, operating strategy, taxes, and advertisement campaign. It's advisable to have an accountant review your business plan for any omissions or mistakes.\n\n, Starting a nursing agency requires a relatively small start-up sum, but you will need to have sufficient money in reserve to pay your nurses in case clients pay their invoices late.\n\n, You will also need to apply for separate license from your state to supply skilled nursing or CNAs staff\n\n, In today's digital world, you can have a virtual office. Google apps for instance makes it very easy to work from anywhere.\n\n, The best way to do this is by having a local lawyer draw them up in compliance with all local and national laws. As a new agency, you might need to use local healthcare facilities for new clients and advertise in local magazines for new nurses and CNAs. Most hospitals and nursing homes consume a lot of CNAs than agency nurses which is a good way to start. You can utilize staff engagement services to help you engage your new nurses once you receive work requests. This cuts down on costs of hiring a scheduler. Hospitals and nursing homes loves fast turn around. You can use services Make sure you understand every clause to avoid problems later.\n\n,, You can also utilize online Software as Service companies (Saas). You pay a small monthly fee.\n\n, Make sure their nursing licenses are current and their records are clean.\n\n,, Once all of the details have been agreed upon, sign the contracts and send out your nurses to the appropriate clients.\n\n",
        "The enforcement of prohibition on a national scale (as opposed to a statewide level) represented an unprecedented expansion in the federal government's authority. The federal government would have to stop the importation of alcohol (a massive project in and of itself) while also ensuring that its manufacture and sale stopped and stayed that way. This would mean a nation wide policing effort in ensuring that all beer, wine, and spirit producers closed shop and destroyed their leftover stock. Up until this point the federal government had never taken on such a responsibility and it was not clear that the constitution granted the power to do so. \n\nLet me know if I've missed the mark and I shall try to clear up any more questions as best I can.\n\n\nSources:\n\n McGirr, Lisa. *The War on Alcohol: Prohibition and the Rise of the American State.* New York: W.W. Norton  &  Company, 2016.\n\nOkrent, Daniel. *Last Call: The Rise and Fall of Prohibition.* New York: Scribner, 2010.\n\nEDIT:\nLet's get in to the efforts of the Anti-Saloon League (ASL) and the Woman's Christian Temperance Union (WCTU). The Anti-Saloon League, as the name indicates, was initially opposed to the institution that served alcohol rather than alcohol itself. The 19th century saloon was a cesspool of gambling, prostitution, and addiction. Alcohol's direct ties to such a seedy organization helped to perpetuate the idea that alcohol was responsible for societal problems like the spread of venereal disease, loss of family income, and domestic abuse. The WCTU used these points to argue that getting rid of alcohol would lead to a better society.\n\nThese were the two most powerful and influential temperance societies in terms of influencing lawmakers and the voting public to consider the merits of prohibition. This is not to say that they were united in their ideas of how to achieve prohibition. Should they focus on individual pledges of sobriety until there was no one left to drink? Bring up ballot measures at the state and local level? Push for national reform? Many members were outright hostile to a national solution, so they pursued action at the state level. They were quite successful. By 1919, 28 states had some kind of prohibition laws on the books.\n\nThen, in 1913, the constitutional campaign was launched which aimed at adding a prohibition amendment to the constitution. This is a stark departure from the previous efforts at the statewide level, and I would argue, that it is because of the success of the Progressive movement in other areas. The 16th and 17th amendments (both passed in 1913) demonstrated that change could be effected nationally in one fell swoop. Similar hopes were held for a prohibition amendment. If the forces of prohibition could win on a national scale, then all lives (so they argued) would be immediately improved without having the uncertainties and the delays that were related to the state-by-state campaign. It also meant that the Prohibitionists did not have to spend their time and money attempting to convince cities (filled with immigrant, working class wets) to give up an integral part of their leisure culture.\n\nWorld War One made prohibition a reality. As the United States entered the war in 1917 everything was turned over to the war effort. People joined war industries, were expected to scrimp and save their food and ration their other consumable goods so that everything possible could be used to win the war. In such an environment it isn't difficult to see how turning good, bread-making wheat into beer would be seen as a waste of resources and actively damaging the war effort. It also didn't help that most of the companies making the beer bore German names such as Pabst, Busch, and Schlitz. Wartime patriotism fueled the prohibition amendment to success under conditions that were wholly unimaginable before the war began. \n\nIn my earlier comment I made a remark that the 18th amendment \"was crafted\" to address a perceived lack in constitutional authority to regulate alcohol on a federal scale. I was intentionally vague on this point because I felt that it was too difficult to say any one person or organization wrote the amendment. I hope that I have included enough information about the efforts of temperance societies in the 19th, and early 20th centuries to demonstrate that the evolution of their tactics from individual pledges to local and state option laws to full blown prohibition was a factor of their early success and that of the Progressive movement at large.\n\nSources: Same as above, except I have also taken a look at:\n\nRose, Kenneth D. *American Women and the Repeal of Prohibition.* New York: New York University Press, 1996.",
        "  [ABRIDGED] We report the results of a 944-epoch survey for transient sources\nwith archival data from the Very Large Array spanning 22 years with a typical\nepoch separation of 7 days. Observations were obtained at 5 or 8.4 GHz for a\nsingle field of view with a full-width at half-maximum of 8.6' and 5.1',\nrespectively, and achieved a typical point-source detection threshold at the\nbeam center of ~300 microJy per epoch. Ten transient sources were detected with\na significance threshold such that only one false positive would be expected.\nOf these transients, eight were detected in only a single epoch. Two transients\nwere too faint to be detected in individual epochs but were detected in\ntwo-month averages. None of the ten transients was detected in longer-term\naverages or associated with persistent emission in the deep image produced from\nthe combination of all epochs. The cumulative rate for the short timescale\nradio transients above 370 microJy at 5 and 8.4 GHz is 0.07 < R < 40 deg^-2\nyr^-1, where the uncertainty is due to the unknown duration of the transients,\n20 min < t_char < 7 days. A two-epoch survey for transients will detect 1.5 +/-\n0.4 transient per square degrees above a flux density of 370 microJy. Two\ntransients are associated with galaxies at z=0.040 and z=0.249. These may be\nsimilar to the peculiar Type Ib/c radio supernova SN 1998bw associated with GRB\n980428. Six transients have no counterparts in the optical or infrared (R=27,\nKs=18). The hosts and progenitors of these transients are unknown.\n",
        ", Now inside the folder please run the Minecraft server.exe you download, now you may be thinking what's happened? don't panic just go into the EULA file it generated and change \"false\" to true the save the text document and re-run the java file.(please read the terms and conditions of the EULA before typing true)., Now look for your IPv4 address under your used network adapter and copy your IPv4 address into the IP address setting in your text document called server., (If you have 8GB of RAM, don't exceed more than 4GB of RAM taken in for use in RAM.)., This is what you want to choose for your server so if you want 1GB only copy everything before the 2GB of ram same applies to the higher GB's.Bat Server Commands:1GB RAM:@echo offjava -Xms1024M -Xmx1024M -jar minecraft_server.1.8.1.exe guiPause2GB RAM:@echo offjava -Xms2048M -Xmx2048M -jar minecraft_server.1.8.1.exe guiPause3GB RAM:@echo offjava -Xms3072M -Xmx3072M -jar minecraft_server.1.8.1.exe guiPause4GB RAM:@echo offjava -Xms4096M -Xmx4096M -jar minecraft_server.1.8.1.exe guiPauseOnce chosen how much you want go and save the text document as a bat file just by typing \".bat\" at the end of the named text document. Now instead of running the Minecraft server.exe run the bat file instead. And type stop when you want to end the server.(if this tutorial is out of date from Minecraft 1.8.1 change the bat command to whatever version it is), Once you have your default gateway type it into your web browser and you will access your routers' settings. However you will need a username and password for this so go to:http://portforward.com/default_username_password/and find your routers default username and password and then type it in. When done you will have access now, so look for something called \"port forwarding\" or \"port triggering\" and click onto it. Then now we can add our own custom service so click something like \"add custom service\" and name the service whatever you want to, change the service type to TCP/UDP, and change the start and finish port to \"25565\".Now look for something called \"firewall rules\" or \"internal IP address\" or \"outbound services\" and \"inbound services\" once found change the \"internal IP address\" to your IPv4 address., So now your in you may ask \" how do my friends join?\" well go into your browser and type \"IP\" and this number is what you give to people you trust not strangers ).So give the public IP to your friends.",
        "[Here](_URL_2_) is the famous pic about how much space you would need to cover power needs with solar. Of course it looks tiny but the logistics to get there would be insanely complicated.\n\nWhile we certainly would need to install back-up panels and due to weather, day/night and so forth we would need to cover at least double that area but it's still tiny compared to total earth surface. So no, it will not have a measurable effect.\n\nEDIT:\n\nSince I'm getting this many replies, yes I'm aware that this is entirely hypothetical, the region is not stable, you would have huge loss of power due to long-distance cables, the projection is probably not all that accurate and so forth. I only wanted to make the point that covering your needs with solar would not have a negative effect on global temperatures (albeit yeah, I obviously don't have proof for this, no one really has). \n\nAnd even if the area looks tiny it's still bigger than a small country, all filled up with solar panels. the logistics would be insane. I'm actually specifically against wasting money on solar and wind and use it for nuclear, especially IFR research / commercialization. And for US actually [insulating your homes](_URL_2_) would just safe massive amount of energy, yes also in warm places because you then need less AC.",
        "This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. \nI do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.",
        "  We combined sensitive near-infrared data obtained with ground-based imagers\non the ESO NTT and VLT telescopes with space mid-infrared data acquired with\nthe IRAC imager on the Spitzer Space Telescope to calculate the extinction law\nA_\\lambda/A_K as a function of \\lambda between 1.25 and 7.76 micron to an\nunprecedented depth in Barnard 59, a star forming, dense core located in the\nPipe Nebula. The ratios A_\\lambda/A_K were calculated from the slopes of the\ndistributions of sources in color-color diagrams \\lambda-K vs. H-K. The\ndistributions in the color-color diagrams are fit well with single slopes to\nextinction levels of A_K ~ 7 (A_V ~ 59 mag). Consequently, there appears to be\nno significant variation of the extinction law with depth through the B59 line\nof sight. However, when slopes are translated into the relative extinction\ncoefficients A_\\lambda/A_K, we find an extinction law which departs from the\nsimple extrapolation of the near-infrared power law extinction curve, and\nagrees more closely with a dust extinction model for a cloud with a total to\nselective absorption R_V=5.5 and a grain size distribution favoring larger\ngrains than those in the diffuse ISM. Thus, the difference we observe could be\npossibly due to the effect of grain growth in denser regions. Finally, the\nslopes in our diagrams are somewhat less steep than those from the study of\nIndebetouw et al. (2005) for clouds with lower column densities, and this\nindicates that the extinction law between 3 and 8 micron might vary slightly as\na function of environment.\n",
        "\nThis paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.\n\nThe model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.\n\nOverall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. \n\nComments\n\nThis contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)\nIn section 3, it is unclear why the authors refer the entity as a ‘topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. \nIs it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.\nIn equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.\nLearning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).\nIt would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ",
        "  We have carried out a comparative statistical study for the displacement of\nthe Sun from the Galactic plane (z_\\odot) following three different methods.\nThe study has been done using a sample of 537 young open clusters (YOCs) with\nlog(Age) < 8.5 lying within a heliocentric distance of 4 kpc and 2030 OB stars\nobserved up to a distance of 1200 pc, all of them have distance information. We\ndecompose the Gould Belt's member in a statistical sense before investigating\nthe variation in the z_\\odot estimation with different upper cut-off limits in\nthe heliocentric distance and distance perpendicular to the Galactic plane. We\nfound z_\\odot varies in a range of ~ 13 - 20 pc from the analys is of YOCs and\n~ 6 - 18 pc from the OB stars. A significant scatter in the z_\\odot obtained\ndue to different cut-off values is noticed for the OB stars although no such\ndeviation is seen for the YOCs. We also determined scale heights of\n56.9(+3.8)(-3.4} and 61.4(+2.7)(-2.4) pc for the distribution of YOCs and OB\nstars respectively.\n",
        "  An integral PBW-basis of type $A_1^{(1)}$ has been constructed by Zhang [Z]\nand Chen [C] using the Auslander-Reiten quiver of the Kronecker quiver. We\nassociate a geometric order to elements in this basis following an idea of\nLusztig [L1] in the case of finite type. This leads to an algebraic realization\nof a bar-invariant basis of $\\uq2$. For any affine symmetric type, we obtain an\nintegral PBW-basis of the generic composition algebra, by using an algebraic\nconstruction of the integral basis for a tube in [DDX], an embedding of the\nmodule category of the Kronecker quiver into the module category of the tame\nquiver, and a list of the root vectors of indecomposable modules according to\nthe preprojective, regular, and preinjective components of the Auslander-Reiten\nquiver of the tame quiver. When the basis elements are ordered to be compatible\nwith the geometric order given by the dimensions of the orbit varieties and the\nextension varieties, we can show that the transition matrix between the\nPBW-basis and a monomial basis is triangular with diagonal entries equal to 1.\nTherefore we obtain a bar-invariant basis. By a orthogonalization for the\nPBW-basis with the inner product, we finally give an algebraic way to realize\nthe canonical bases of the quantized enveloping algebras of all symmetric\naffine Kac-Moody Lie algebras.\n",
        "Thanks for your questions. We shall try to adress them in the following:\n\nQuestion 1: There are many modules in the system that are trained separately. Furthemore, fine-tuning for e.g. the image model appears to only take place on the FC7 layer. Jointly training all the components of the model would presumably improve performance, perhaps considerably. Did you try this? If not, why not?\n\nNo we did not. We were primarily interested in knowing the additional performance one can gain by merging pretrained models and re-specializing them to the task. We agree that for the paper it would have been a worthy experiment but we did not get to it due to time constraints. We will try to add it in the next paper update.\n\nQuestion 2: For the joint product embedding, it was stated that a fully connected hidden layer that mixes the individual embeddings wasn't used in order to have a smaller number of parameters and increase comprehensibility. But presumably it could also perform better than the chosen approach. Did you try it?\n\nWe made some attempts but the convergence speed was bad. It is also on the to-do list for the next paper update.\n\nQuestion 3: Could you provide some more details on the way TextCNN was used? Was it used on full product descriptions, even when they consist of multiple sentences? Were titles and descriptions embedded separately?\n\nSure! We decided to not embed the titles and descriptions separately, but concatenate the two and keep the first ten words without stop words (with empty word padding for shorter sequences). This method can definitely be extended to the full description since text CNN are quite robust to the length of the sequence.\n\nQuestion 4:  In the table of results at the end, I didn't see any results for a model trained on both the books and movies data and tested on both in the cold start scenario. Did you evaluate this case?\n\nYes, we did try and as expected the results were not great, since the current version of the model does not support category-specific embeddings. Therefore, the resulting overall distance between products is a compromise between the two category-specific distances. If interested, we can provide you the results we achieved.\n\nQuestion 5: Earlier work on content-based recommendation focused on not just performance but also on the interpretability of the resulting models. Can you say anything about whether effective interpretations of the models latent dimensions can be gleaned when learning in this way?\n\nWe only explored the text interpretability by looking at the top activating word embeddings  for each of the text convolutions. What we saw is that most of the 100 unigram filters used in the convolution layer corresponds to a topic (could be clusters corresponding to a genre, a date or an author). It looks like while learning the right text representation for book pairwise similarity prediction the model discovers naturally occuring book topics. The paper was already long so we decide not to publish them, but if you are interested, we can provide them.\n\nQuestion 6: Is Prod2Vec realistic in cold-start scenarios? Presumably co-purchase information wouldn't be available for cold-start products and thus an embedding couldn't be estimated.\n\nThe reason why we keep Prod2vec only in Content2vec + is because prod2vec only works when you have available collaborative filtering signal on at least some of the products in test. That is why prod2vec and content2vec+ algorithm results are only reported in the two datasets with some CF signal.\n\nQuestion 7: I didn't understand why different sets of baselines seem to appear across the three tables, but maybe I missed some detail here.\n\nWe used the hard cold start dataset to do the extensive analysis of our architecture VS other available methods. We dropped crossfeat because we made the case in the hard cold start dataset.\n\nFeel free to comment if you have any other questions.\n\nElena, Flavian & Thomas",
        "One Piece Film: Strong World or simply Strong World is a 2009 Japanese animated fantasy action adventure film directed by Munehisa Sakai. It is the tenth feature film based on the shōnen manga series One Piece by Eiichiro Oda.\n\nThe film features Naoto Takenaka (in Japanese) and Scott McNeil (in English) as Shiki, the evil captain of his crew who kidnaps Nami to force her to join his crew and intends to conquer the East Blue. Monkey D. Luffy and his crew must stop Shiki from carrying out his plans.\n\nPlot \n uses his Devil Fruit powers to destroy marine ships and warn Monkey D. Garp and Fleet Admiral Sengoku. On a floating island, Monkey D. Luffy is chased by a genetically-enhanced animal. The monster is overpowered by the other monsters before Luffy defeats the fourth monster. The Straw Hats have been separated into three groups: Sanji with Usopp, Roronoa Zoro with Tony Tony Chopper, and Nico Robin with Franky and Brook. Shiki tells Nami that she has been taken to the island against her will and a brief flashback is shown: several days earlier, the Straw Hats read news of an attack on East Blue. Luffy vows to protect the East Blue before witnessing Shiki's ship overhead. After escaping a storm, Shiki meets Nami and reveals his powers to make any inanimate object he touches float. After learning it was Nami that delivered the warning, Shiki offers to take them there before abducting Nami. The others try to rescue her, but Shiki makes the pirates scatter on the island.\n\nShiki asks Nami to become his navigator but she refuses. His minion  demonstrates an evolved bird called , who can produce electricity, but Shiki rejects it after Dr. Indigo is electrocuted. He reveals that a plant, called IQ, can cause animals to evolve instantly and to increase strength along the way. Nami protects Billy, and the bird is left with her as Shiki and his men leave. Meanwhile, Sanji and Usopp battle various animals while Sanji searches for Robin and Nami. Meanwhile, Zoro and Chopper rescue a young girl, , and are led to her village and are told about the large poisonous plants around the village. However, long term exposure to the plants is poisonous to humans, and the girl's grandmother has become ill by it. Xiao was looking for the cure which is the IQ plant, but Shiki has stolen the IQ plants for his experiments. Sanji and Usopp learn that Shiki also takes all the men and young women to his royal palace, leaving the village with only the very young and old, before meeting up with Zoro and Chopper.\n\nNami flees with the help of Billy, and finds the Thousand Sunny along with Luffy. Robin's group discovers that Shiki is planning to release the animals on the island into East Blue to force the World Government's surrender and that he is planning a demonstration against a village on the floating island to show their power. The two join the others at the village, and they also learn of the plan from the village residents. Shiki confronts and defeats the Straw Hats and offers Nami to rejoin him on the condition that the Cocoyashi Village will be spared. Robin's group arrives and rejoin the rest of the crew. Xiao gives them a tone dial and they replay Nami's farewell message to Luffy, but he angrily leaves before the end.\n\nMeanwhile, Nami attempts to destroy the plants protecting his palace, but gets poisoned herself. Shiki traps her near the plants and heads off to meet the pirate captains gathering. While greeting them, the Straw Hats launch a preemptive strike against Shiki and his henchmen. The group manages to defeat them while Chopper and Usopp are ordered to search for Nami. Nami is found by Billy who helps destroy the plants just as Usopp and Chopper arrive. Chopper soon realizes the only way to save Nami is to find the IQ medicine, but Shiki attempts to stop them. Luffy engages Shiki in a duel. The two find the IQ plant, but find the medicine is being held by Dr. Indigo. Zoro manages to defeat Dr. Indigo and Nami recovers. Sanji and Brook, meanwhile, witness another of Shiki's henchmen, , attempting to kiss Robin, but Sanji defeats Scarlet.\n\nNami, Usopp and Chopper trick Shiki into redirecting his ship to the island, forcing his crew to flee. The Straw Hats rig the palace with explosives. Shiki refocuses his attention on the Straw Hats, but Luffy uses an electric charge and knocks Shiki to the ground, leaving Luffy victorious. The other Straw Hats escape with the Thousand Sunny, using Shiki's pirate sail as a parachute. Luffy is recovered by Billy while the villagers are shown flying away using the wings on their arms. The Marines capture the retreating pirates, including Shiki. As the Marines witness the islands crash into the sea, now free of Shiki's power, they spot the Thousand Sunny. However, the Straw Hats escape. Luffy later learns that Nami's message was actually a coded SOS directed at him that the crew took as a love confession; he tries to listen to the end, but Nami throws it overboard in embarrassment.\n\nVoice cast\n\nProduction \nOda personally supervised the production of Strong World, created the film's original story and over 120 pages of rough drawings. Furthermore, he placed his own name on the film's credits to indicate his desire for a film that is different from its nine predecessors. The actual director of the film is Munehisa Sakai, who is also a former director of the One Piece anime television series. The Japanese rock band Mr. Children performed the film's theme song, \"Fanfare\". Oda had personally offered them the opportunity.\n\nPromotion \nAn English-language teaser trailer of 45 seconds length was shown on the Tokyo International Anime Fair in March 2009 and later placed on the official website of the One Piece franchise's anime films, when it was relaunched around July 2009. The website began streaming a 96 seconds long trailer on August 8, 2009. Yet more footage from the film was shown on the 2009 Jump Super Anime Tour and later posted on the website of Weekly Shōnen Jump.\n\nIn the 49th issue of Shueisha's Weekly Shōnen Jump, the manga anthology that has been publishing One Piece ever since the series' premiere, the magazine announced that it would publish the prequel to the film's story, depicting a confrontation between the Pirate King Gol D. Roger and Shiki the Golden Lion, which the first 1.5 million Japanese moviegoers where promised to receive in form of a One Piece manga \"Volume 0\", in its 53rd issue and that it will eventually be animated. The One Piece anime television series' episodes 426 through 429 formed a sub-series of special episodes depicting a prelude to the events in the film.\n\nCommemorating the release of the 56th volume of One Piece, on November 4, 2009, almost within a week to Strong World'''s premiere, the Friday morning issue of the major Japanese newspaper Asahi Shimbun contained nine full-page spreads, showing One Piece characters and advertisements for Weekly Shōnen Jump. On December 10, 2009, only two days before the film's premiere, Shueisha's fashion magazine Men's Non-No released its January issue, its cover adorned by an Oda-drawn Luffy in a look by stylist Shinichi \"Miter\" Mita, which not only marked the first time that Oda drew the cover for a non-manga magazine, but also the first time that a manga character has been on the cover in the magazine's 24-year history. The first eight pages of the issue are occupied by photographs of models, resembling Luffy, Robin, Nami, Zoro, and Sanji, dressed in sea and pirates themed clothes. Furthermore, the issue contains interviews with Oda and Kōsuke Kitajima.\n\nThe promotions surrounding Strong World boosted the sales of the One Piece manga during the week of December 7 through 13, causing all 56 then published volumes to be listed in Oricon's Top 200 chart of weekly Japanese manga sales.\n\n Release \nOn December 12, 2009, Strong World opened on 188 screens throughout Japan. For comparison, Ponyo set the record of screens for a domestic film to 481 in July 2008 and Harry Potter and the Half-Blood Prince premiered in Japan in July 2009 on 844 screens.\n\nThe first 1.5 million moviegoers received the \"0th volume\" of the One Piece manga series, containing a prequel story that depicts events from 20 years into the past of the One Piece world, as well as the materials Oda created for Strong World's production. After the film's success on its first weekend of showing, Toei decided to extend the offer by another million copies of the manga.\n\nThe Blu-ray + DVD Combo Pack release of the film was released on August 27, 2010.\n\nThe film has been licensed, along with Season 5 of One Piece, in North America by Funimation. Funimation announced on July 3, 2013, at Anime Expo that Strong World would be released in the United States on November 19, 2013. The Region 1 release omits the ending theme, \"Fanfare\" by Mr. Children, due to rights issues, though it is mentioned to be the ending in both the English and Japanese credits at the end of the film. Both Blu-ray + DVD Combo Pack and the single DVD begin with a disclaimer explaining the removal of the song.\n\nOne Piece Strong World was released in France on August 24, 2011. It was the first One Piece film to be released in cinemas in France. Selecta Visión released the film in Spain on DVD and Blu-ray on November 30, 2016, featuring Japanese and Spanish audio, as well as subtitles in Spanish.\n\n Related media \nHamazaki Tatsuya adapted the film's story into a 208 pages light novel, released on December 14, 2009. An art book to the film of 120 A4 pages, published on December 18, 2009, entered to weekly Japanese comic sales ranking on place 21 with 42,076 copies sold.\n\n Reception \n\n Box office \nOn its first weekend of showing, Strong World was seen 820,000 times on 188 screens throughout Japan, 103 of which had sold out over the entire weekend, resulting in a per-screen average of 5,520,000 Japanese yen (approx. 62,200 United States dollars), which is the record for a nationwide-released film in Japan, and a gross revenue of ¥1,038,000,000 (approx. $11.7 million), ¥553,000,000 (approx. $6.24 million) on the first and ¥485,000,000 (approx. $5.47M) on the second day, thus topping both Ponyo, which took in ¥1,025,000,000 (approx. $11.55 M), and Harry Potter and the Half-Blood Prince, which made ¥990,000,000 (approx. $11.2 M), on their first weekends of showing, as well as earning more than the ¥920,000,000 (approx. $11 M) its predecessor, One Piece: Episode of Chopper + Fuyu ni Saku, Kiseki no Sakura, made in its whole time of showing. Anime News Network attributes part of this success, which includes a 1st place on the Japanese and a fourth place on the international box office over the time frame December 11 through 14, to Toei's giving-away of the \"One Piece Volume 0\" manga.\n\nOver the weekend of December 19 and 20, Strong World topped the Japanese box office for a second time in a row and, with $22,500,000 earned in its first eight days of showing, set a new company record for Toei, beating Aibo, the record holder from 2008, which needed two more days to reach the same amount and eventually finished with a total gross revenue of $50,000,000, a sum Toei expects Strong World to exceed by $7,000,000.\n\nIn its third week, Strong World fell to the 3rd place on Kogyo Tsushinsha's Japanese box office chart and to the 4th place on the charts of Variety and Rentrak Theatrical. Shown on 193 screens, it increased its total gross revenue by $2,576,258 to a new total of $32,238,129. In its fourth week, it fell to fourth place. Shown on 194 screens, it grossed another $2,611,102, creating a new total of\n$39,439,879. The film remained in fourth place during its fifth week, earning another $1,753,517 on 194 screens to a new total of $44,506,849. Falling to the 6th place on its sixth weekend, Strong World'' still grossed an additional $1,063,584 on 193 screens, increasing its total to $47,918,186, before falling off the Top 10 in the following week. The worldwide total box office is about .\n\nReviews \nStrong World received largely positive reviews from critics, praising the film's story and character designs, as well as Oda's involvement in the film. Chris Beveridge of The Fandom post described the film as \"a very streamlined Oda story that would have gone on for twenty or thirty episodes if it was done as a regular arc with the TV series\", adding that the storyline was \"predictable\" but also \"well polished\". Beveridge also praised the film for having a stronger connection to the One Piece TV series than the franchises previous films. Rebecca Silverman of Anime News Network awarded the film a 'B' rating, praising the storyline and character design. Silverman commented that \"no one can come up with weird laughs or monsters quite like Oda\", although unfavorably comparing some of the animation to \"stop-motion animation\". Silverman also commented on the English dubbing of the film, saying that Ian Sinclair's debut as Brook \"does a very good job with the loopy skeleton\", although she found Scott McNeil's \"pseudo-Caribbean\" accent as Shiki \"a little off-putting\". Despite being largely unfamiliar with the franchise, Kyle Mills of DVD Talk called FUNimations release \"Highly Recommended\", praising the \"terrific cast of characters\" and \"giant kick ass final fight\". Jeffrey Kauffman of Blu-ray.com also recommended Strong World, describing it as \"loud, frenetic and frequently nonsensical,\" but also \"kind of crazily entertaining at the same time\" Kauffman also added that \"while Strong World won't necessarily be incomprehensible to newcomers\", \"those with a solid grounding in the background of the story and its many characters will reap the most rewards from this particular outing\".\nStrong World also currently has a rating of 7.1 out of 10 collated from both film critics and users on IMDb.\n\nAwards and nominations \nIt won the award for Excellent Animation of the Year at the 34th Japan Academy Prize and is nominated for Animation of the Year.\n\nSee also\n List of One Piece films\n List of One Piece media\n List of 2009 box office number-one films in Japan\n\nNotes\n\nReferences\n\nExternal links\nOfficial website of Toei Animation \n\n2009 films\nJapanese films\n2009 anime films\nFunimation\nToei Animation films\nStrong World\nFilms scored by Kohei Tanaka",
        "  We study the Laplacian operator of an uncorrelated random network and, as an\napplication, consider hopping processes (diffusion, random walks, signal\npropagation, etc.) on networks. We develop a strict approach to these problems.\nWe derive an exact closed set of integral equations, which provide the averages\nof the Laplacian operator's resolvent. This enables us to describe the\npropagation of a signal and random walks on the network. We show that the\ndetermining parameter in this problem is the minimum degree $q_m$ of vertices\nin the network and that the high-degree part of the degree distribution is not\nthat essential. The position of the lower edge of the Laplacian spectrum\n$\\lambda_c$ appears to be the same as in the regular Bethe lattice with the\ncoordination number $q_m$. Namely, $\\lambda_c>0$ if $q_m>2$, and $\\lambda_c=0$\nif $q_m\\leq2$. In both these cases the density of eigenvalues\n$\\rho(\\lambda)\\to0$ as $\\lambda\\to\\lambda_c+0$, but the limiting behaviors near\n$\\lambda_c$ are very different. In terms of a distance from a starting vertex,\nthe hopping propagator is a steady moving Gaussian, broadening with time. This\npicture qualitatively coincides with that for a regular Bethe lattice. Our\nanalytical results include the spectral density $\\rho(\\lambda)$ near\n$\\lambda_c$ and the long-time asymptotics of the autocorrelator and the\npropagator.\n",
        "This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of \"rate\" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.\n\nThe authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific \"form\" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.\n",
        "The paper reports several connections between the image representations in state-of-the are object recognition networks and findings from human visual psychophysics:\n1) It shows that the mean L1 distance in the feature space of certain CNN layers is predictive of human noise-detection thresholds in natural images.\n2) It reports that for 3 different 2-AFC tasks for which there exists a condition that is hard and one that is easy for humans, the mutual information between decision label and quantised CNN activations is usually higher in the condition that is easier for humans.\n3) It reproduces the general bandpass nature of contrast/frequency detection sensitivity in humans. \n\nWhile these findings appear interesting, they are also rather anecdotal and some of them seem to be rather trivial (e.g. findings in 2). To make a convincing statement it would be important to explore what aspects of the CNN lead to the reported findings. One possible way of doing that could be to include good baseline models to compare against. As I mentioned before, one such baseline should be reasonable low-level vision model. Another interesting direction would be to compare the results for the same network at different training stages.\n\nIn that way one might be able to find out which parts of the reported results can be reproduced by simple low-level image processing systems,  which parts are due to the general deep network’s architecture and which parts arise from the powerful computational properties (object recognition performance) of the CNNs.\n\nIn conclusion, I believe that establishing correspondences between state-of-the art CNNs and human vision is a potentially fruitful approach. However to make a convincing point that found correspondences are non-trivial, it is crucial to show that non-trivial aspects of the CNN lead to the reported findings, which was not done. Therefore, the contribution of the paper is limited since I cannot judge whether the findings really tell me something about a unique relation between high-performing CNNs and the human visual system.\n\nUPDATE:\n\nThank you very much for your extensive revision and inclusion of several of the suggested baselines. \nThe results of the baseline models often raise more questions and make the interpretation of the results more complex, but I feel that this reflects the complexity of the topic and makes the work rather more worthwhile. \n\nOne further suggestion: As the experiments with the snapshots of the CaffeNet shows, the direct relationship between CNN performance and prediction accuracy of biological vision known from Yamins et al. 2014 and Cadieu et al. 2014 does not necessarily hold in your experiments. I think this should be discussed somewhere in the paper.\n\nAll in all, I think that the paper now constitutes a decent contribution relating state-of-the art CNNs to human psychophysics and I would be happy for this work to be accepted.\n\nI raise the my rating for this paper to 7.",
        "\n\nFor this tutorial to be effective, one must first calculate the degrees of freedom to determine whether or not this is a feasible solution to solve for these unknowns.;\n,, If this is equal to zero, the following instruction are applicable. If this does not equal zero, then a revision of the problem statement and/or determination of additional specifications must be found., Using the Sample Problem, the degrees of freedom are determined to be zero:\n\nNumber of Equations: Three (3) equations total.\n\nEquation 1: 4𝑥+2𝑦+𝑧=5\nEquation 2: 7𝑦+5𝑧=17\nEquation 3: 15𝑥+25𝑦+8𝑧=11\n\n\nNumber of Unknowns: Three (3) unknowns total.\n\nUnknown 1: x\nUnknown 2: y\nUnknown 3: z Unknowns total.\n\n\n𝐷𝑒𝑔𝑟𝑒𝑒𝑠 𝑜𝑓 𝐹𝑟𝑒𝑒𝑑𝑜𝑚=𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝐸𝑞𝑢𝑎𝑡𝑖𝑜𝑛𝑠−𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑈𝑛𝑘𝑜𝑤𝑛𝑠 𝐷𝑒𝑔𝑟𝑒𝑒𝑠 𝑜𝑓 𝐹𝑟𝑒𝑒𝑑𝑜𝑚=3−3=0\n\n,\n\nSSE is defined as: Σ(𝑌𝑖−𝑓(𝑥𝑖))2𝑛𝑖=1\n\nWhere Y = the known value of the equation\nf(x) = the value of the equation calculated as a function of the variables.\n\n\n\n,, Can be found towards the top of the page.,\n\nIf the Excel programming being used has the Solver Add-in, skip this part.\n\n, A separate window labelled Excel Options should appear,, Click on Analysis ToolPak and Solver Add-in.,,,\n\nLabels, borders, and placement are personal preferences\n\n,,,,, The set objective should be the cell dedicated to the sum of squared error. Click the “Min” bubble. For the “By changing cells section” highlight the three cells containing the unknown variable values. Make sure the “make unconstrained variables non-negative” box is un-checked.,,,\nIf solution was not found, try entering different sample values and make sure your equations were formulated correctly.\n\n",
        " This is so you can tell apart the white from the blue thread which makes up denim. If you skip this step then your whole shorts could get ruined because all the threads will be white and impossible to tell apart.;\n, If you look closely at the denim you can see white and blue threads running horizontal against each other. Cut two horizontal lines across the shorts using scissors, leaving a strip of fabric in between the slits. Cut them the length you want your distressed/worn patch to be. Using a tool you want to remove the blue threads from the fabric you just cut. You could use a needle, safety pin, or food/salad clamps to remove the blue threads, you need to do this constantly until all the blue thread has gone and you only have white strings showing, it takes quite a while so do this when you have a bit of time, don't rush otherwise you could be left with just a frayed hole.\n\n, You may want to leave some white and just cut it short to leave threads hanging to give an even better look!\n\n, All you need to do is put the shorts in the wash and that will separate the threads even more to have more exaggerated holes and to remove all the excess thread that's not needed.\n\n, Don't use a tumble dryer as the threads can get caught and break from the shorts, once they're done you're ready to start bleaching! Always wear protective gloves as bleach does burn straight through skin! Now, I wouldn't personally recommend any brand of bleach because you're not looking for the hygiene job, just how thick the bleach is. Of course you can't test the bleach in-store so you're going to have to find the best brand for you out of trial and error. Buy the thickest bleach you can afford and water it down when its in the bowl/container.\n\n, If you only want a half an half effect there's no need for bands. Don't use red rubber bands otherwise the red dye used in the bands will leak onto the shorts leaving red stains and patches. Here are the methods to get the look you want-\n\n\nAll of the shorts in- All shorts will turn a whitey-blue colour.\nHalf of the shorts in- A two tone white and blue effect.\nTie rubber bands around shorts in a ball- Random spotty tie dye effect.\nTie rubber band knots randomly- Small clouds of white.\n\n, Don't use any other methods to dry the shorts otherwise the effect and colour could change. Leave the shorts for 3 days to make sure they are definitely dry otherwise you could burn yourself testing if the shorts are still wet.\n\n, But you can add more items to your shorts! Lots of DIY and accessory stores sell chains, studs and gems which you can sew or glue onto your shorts. Even though studding may take a lot of time it make shorts look even better and shop bought. You can write or design anything using the gems/studs! For example your initial, just covering a pocket or making designs like a cross or a skull. Id recommend HobbyCraft for their choice in clothes decorations.\n\n, Blue isn't enough this summer and colour range all over the rainbow! Fabric dye can be very easy to get hold of and its not expensive either, you can buy fabric dye at all hobby stores or general DIY stores. Fabric dye doesn't contain anymore bleach so your shorts won't get ruined.\n\n, Lay shorts in liquid for 30 minutes then take out and lye on newspaper to prevent staining. Leave shorts to dry naturally without any heat and there you have it! Your own pair of bleached, distressed jeans! Enjoy!\n\n,",
        "This paper proposes a neural architecture for answering non-factoid questions. The author's model improves over previous neural models for answer sentence selection. Experiments are conducted on a Japanese love advice corpus; the coolest part of the paper for me was that the model was actually rolled out to the public and its answers were rated twice as good as actual human contributors! \n\nIt was hard for me to determine the novelty of the contribution. The authors mention that their model \"fills the gap\nbetween answer selection and generation\"; however, no generation is actually performed by the model! Instead, the model appears to be very similar to the QA-LSTM of Tan et al., 2015 except that there are additional terms in the objective to handle conclusion and supplementary sentences. The structure of the answer is fixed to a predefined template (e.g., conclusion --> supplementary), so the model is not really learning how to order the sentences. The other contribution is the \"word embedding with semantics\" portion described in sec 4.1, which is essentially just the paragraph vector model except with \"titles\" and \"categories\" instead of paragraphs. \n\nWhile the result of the paper is a model that has actually demonstrated real-life usefulness, the technical contributions do not strike me as novel enough for publication at ICLR.\n\nOther comments:\n- One major issue with the reliance of the model on the template is that you can't evaluate on commonly-used non-factoid QA datasets such as InsuranceQA. If the template were not fixed beforehand (but possibly learned by the model), you could conceivably evaluate on different datasets. \n- The examples in Table 4 don't show a clear edge in answer quality to your model; QA-LSTM seems to choose good answers as well.\n- Doesn't the construction model have an advantage over the vanilla QA-LSTM in that it knows which sentences are conclusions and which are supplementary? Or does QA-LSTM also get this distinction?",
        "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it’s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it’s hard to interpret the squared error scores in Figure 2c. It’s not clear to me what the term ‘unrollings’ refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I’d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It’s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing.",
        "[Here](_URL_1_) is a Boston University article skimming over women's rights in the USSR from WWI to the 90s. Essentially, the USSR granted them rights to vote and divorce, but they had to take up jobs in factory and field. In addition to that, Russian culture is very macho, and men long refused to take part in any housework, whether that meant chores, fetching water, or even a bit of shopping. So Soviet women after the Revolution were now not only saddled with all the demands of maintaining a family and house but also with the same intensity of work their husbands were doing. Men were preferred to women in most jobs, and very few women served in higher government positions, which the article below claims confirms the USSR as a classic patriarchy.\n\n[This](_URL_0_) article comes off as a bit biased, but offers good basic information on the rights of working mothers and the increasing apathy of Soviet husbands, who could no longer take pride in being a breadwinner (as Soviet wages were kept low, meaning all couples had to work equally).\n\nAnyone who has more information, please, speak.",
        " If you are allergic to citrus, these methods won't be appropriate. As with the oil remedies, it can take a few weeks before you notice an improvement, so keep using.\n\n\nDo not use citrus if you have cuts, open skin, hangnails or sores, as the citrus can sting.\n\n,,,, You can use this method once per day.\n\n, Use a ceramic or glass bowl.\nSoak your nails in this mixture for 15 minutes. Tip the mixture out after soaking.\nWash your hands. Then dip in a cup of olive oil, using the same bowl and soak for 5 minutes.\nUse twice a week for two weeks, or until you notice an improvement.\n\n, They are high in vitamin C. Lemons also have the ability to brighten and remove yellow color from nails, and leave them looking shiny.\n\n,,, Soak for 10 minutes.\n\n,, This can help to brighten the nails and clean them. Rub cut lemon slices on your fingernails for 15 minutes before rinsing off with warm water. Pat dry and add moisturizer as usual.\n\n,,,, Soak your nails in the mixture for 20 minutes.\n\n,,,,\n\n\nYou can do this method once each day to get better results.\n\n",
        "In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.\n\n-----\n\nThis paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.\n\nStrengths:\n- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.\n- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.\n- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.\n- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.\n\nWeaknesses:\n- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.\n- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are \"significant\" (even in an informal sense).\n- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?\n\nI have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.\n\nFor what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.",
        "The Java Research License (JRL) is a software distribution license created by Sun in an effort to simplify and relax the terms from the \"research section\" of the Sun Community Source License.  Sun's J2SE 1.6.0, Mustang, is licensed under the JRL as well as many projects at Java.net.\n\nAlthough the JRL has elements of an open source license, the terms forbid any commercial use and are thus incompatible with both the Free Software Definition and the Open Source Definition. The JRL is a research license to be used for non-commercial academic uses.\n\nSee also \n Sun Microsystems\n Java Research License – full text of the JRL\n\nExternal links\n\nSoftware licenses\nSun Microsystems",
        "This paper proposes and tests two ideas. (1) a method of pruning networks by identifying highly correlated neuron pairs, pruning one of the pair, and then modifying downstream weights to compensate for the removal (which works well if the removed neurons were highly correlated). (2) a method, dubbed NoiseOut, for increasing neuron correlation by adding auxiliary noise target outputs to the network during training.\n\n\nThe first idea (1) is fairly straightforward, and it is not clear if it has been tried before. It does seem to work.\n\n\nThe second idea (2) is of unclear value and seems to this reviewer that it may merely add a regularizing effect. Comments in this direction:\n - In Fig 4 (right), the constant and Gaussian treatments seem to produce the same effect in both networks, right? And the Binomial effect seems the same as No_Noise. If this is true, can we conclude that the NoiseOut targets are simply serving to regularize the network, that is, to reduce its capacity slightly?\n - To show whether this effect is true, one would need to compare to other methods of reducing the network capacity, for example: by reducing the number of neurons, by applying L2 regularization of various values, or by applying Dropout of various strengths. Fig 7 makes an attempt at this direction, but critically misses several comparison treatments: “Pruned without any regularization”, “Pruned with only L2”, and “Pruned with only DropOut”. Have these experiments been run? Can their results be included and used to produce plots like Fig 5 and Fig 7?\n\nWithout these comparisons, it seems impossible to conclude that NoiseOut does anything but provide similar regularization to DropOut or L2.\n\n\nThe combined ideas (1) + (2) DO produce a considerable reduction in parameters, but sadly the experiments and exposition are somewhat too lacking to really understand what is going on. With a little more work the paper could be quite interesting, but as is it should probably not be accepted.\n\n\nAdditional comments:\n - Section 4 states: “In all of these experiments, the only stop criteria is the accuracy decay of the model. We set the threshold for this criteria to match the original accuracy; therefore all the compressed network have the same accuracy as the original network.” Is this accuracy the train accuracy or test accuracy? If train, then test accuracy needs to be shown (how much test performance is lost when pruning?). If test, then this would typically be referred to as “cheating” and so the choice needs to be very clearly stated and then defended.\n - Lowercase rho is used to indicate correlation but this is never actually specified, which is confusing for. Just state once that it indicates correlation.\n - How do these results compare to other pruning methods? No numerical comparison is attempted.",
        "This was a borderline paper. However, no reviewers were willing to champion the acceptance of the paper during the deliberation period. Furthermore, in practice, initialization itself is a hyperparameter that gets tuned automatically. To be a compelling empirical result, it would be useful for the paper to include a comparison between the proposed initialization and a tuned arbitrary initialization scale with various tuning budgets. Additionally, other issues with the empirical evaluation brought up by the reviewers were only partially resolved in the revisions. For these reasons, the paper has been recommended for rejection.",
        "Pflügers Archiv: European Journal of Physiology is a peer-reviewed scientific journal in the field of physiology. A continuation of a journal founded in 1868 by the German physiologist, Eduard Friedrich Wilhelm Pflüger, Pflügers Archiv () is the oldest physiological journal. Pflügers Archiv is currently published by Springer, with 11 issues per year.\n\nThe journal publishes molecular and cellular studies across the physiological sciences; topics include the physiology of the heart, muscle and sensory systems, transport physiology, neuroscience, signalling, ion channels and receptors. It aims to publish \"innovative work that focuses on mechanistic insight into basic physiological functions\".\n\nHistory\nPflügers Archiv is the oldest physiological journal. It was founded in 1868 by the German physiologist, Eduard Friedrich Wilhelm Pflüger, under the title Archiv für die gesamte Physiologie des Menschen und der Tiere. It was published in German. The first issue of the journal contains 26 articles, with contributors including Hermann Rudolph Aubert, Julius Bernstein, Johann Nepomuk Czermak, Franciscus Donders, Sigmund Exner, Siegmund Mayer, Peter Ludvig Panum, William Thierry Preyer, Salomon Stricker, Hermann von Helmholtz, Friedrich Wilhelm Zahn and Nathan Zuntz. It includes the earliest accurate description of the action potential, by Julius Bernstein, using an apparatus called a \"differential rheotome\".\n\nThe journal was mainly published annually until 1874; there were two or three volumes annually in 1874–1890. From 1891, the volume was split into issues, with five or six issues per volume, and initially three volumes published a year; volumes often did not commence in January. The frequency of volumes increased, with five volumes in 1909.\n\nIn 1910, after Pflüger's death, the journal was retitled Pflüger's Archiv für die gesamte Physiologie des Menschen und der Tiere (). It was published by Springer in German, with some English translations of summaries. Several other German physiology journals merged into the publication: in 1919, Archiv für Anatomie und Physiologie. Physiologische Abt (), which had been founded in 1877; in 1921, Zentralblatt für Physiologie, which had been founded in 1887; and also Archiv für Physiologie. Publication was interrupted in 1945–46. The journal assumed its current title in 1968. It was published in a mixture of English, French and German until 1997, when it became an exclusively English-language journal.\n\nOther key developments published in the journal include Bernstein's explanation of the resting membrane potential; the demonstration of the requirement for sodium ions for nervous excitation, and the prediction of the existence of a lipid cell membrane by Ernest Overton; and early descriptions of conductance in excitable tissues by Ludwig Hermann. In 1978, Erwin Neher and Bert Sakmann published the first description of the patch clamp technique in Pflügers Archiv.\n\nModern journal\nPflügers Archiv's 2009 impact factor is 3.158. It is also indexed in Academic OneFile, BIOSIS Previews, CAB Abstracts, Chemical Abstracts, Current Contents/Life Sciences, EBSCO, EMBASE, MEDLINE, ProQuest and Scopus, among other services. All issues are available online as PDFs, with text versions additionally available from 2000; access is by subscription. Additionally, authors can pay to have their articles released freely online as part of a hybrid open access scheme.\n\n, the editor in chief is Bernd Nilius (Katholieke Universiteit Leuven, Belgium).\n\nKey papers\n\nReferences\n\nExternal links\n \n\nSpringer Science+Business Media academic journals\nEnglish-language journals\nGerman-language journals\nHybrid open access journals\nPublications established in 1868\nPhysiology journals\nMonthly journals",
        "Emily Batty (born June 16, 1988) is a Canadian cross-country mountain biker. She won a bronze medal at the 2016 World Championships. Batty is the current Pan American Games champion and was the silver medalist at the 2014 Commonwealth Games. She won the gold medal at the Pan Am games in Toronto in 2015 in her home country.\n\nCareer\nBatty started racing in 1999 and raced in the Canada Cup Series by 2001. She competed for Trek World Racing in the 2010 UCI Mountain Bike World Cup season. Batty switched to the Subaru-Trek team in 2011.\n\nAt the 2012 Summer Olympics, she competed with a broken collar bone and bruised shoulders in the Women's cross-country at Hadleigh Farm, finishing in 24th place. After the Olympics Emily broke through onto the podium at the 2014 Commonwealth Games in Glasgow. There she finished second on the podium behind teammate Catharine Pendrel. Following the race Pendrel said \"I knew from training and the nationals that Emily was on fire, so I'm proud she got silver.\"\n\nThe next multi-games competition for the Canadian riders was the 2015 Pan American Games on home soil in Toronto. There Batty and Pendrel again found themselves in a one-two position, however this time Batty finished on top with a six-second advantage over Pendrel, winning the title of Pan Am Games champion. After the race Batty said \"it's really just pushing each other. It doesn't matter who's first on the day as long as it's a Canadian. So we got gold and silver, which was amazing.\"\n\nFor the 2016 Summer Olympics Batty competed for Canada, coming in as World Championships bronze medallist. At the Games she finished a close fourth to teammate Pendrel. After she said \"after London with a broken collarbone, to being 10 metres from a bronze medal, it is a heartbreak. My preparation was amazing. I raced clean and I rode incredibly strong and just missed a medal by a couple of bike lengths so I have some mixed emotions.\"\n\nPersonal\nBorn in Brooklin, Ontario, Batty grew up in a racing family. She has two older brothers and a younger sister, all of whom race. During her competitions, Batty wears a pearl necklace discovered among her mother's jewelry when she was 11 years old. Batty raced on the team Trek Factory Racing between 2013 and 2020. For 2021 Batty will race for Canyon. She is coached by Adam Morka, who is also her husband.\n\nReferences\n\nExternal links\n\nCross-country mountain bikers\nCanadian female cyclists\nLiving people\nOlympic cyclists of Canada\nCyclists at the 2012 Summer Olympics\nCyclists at the 2016 Summer Olympics\nSportspeople from Whitby, Ontario\n1988 births\nCyclists from Ontario\nCyclists at the 2014 Commonwealth Games\nCyclists at the 2015 Pan American Games\nCanadian mountain bikers\nCommonwealth Games silver medallists for Canada\nPan American Games gold medalists for Canada\nCommonwealth Games medallists in cycling\nPan American Games medalists in cycling\nCyclists at the 2018 Commonwealth Games\nMedalists at the 2015 Pan American Games",
        "  Fifteen years ago, a structural analysis of the hormone insulin and the\nretinoblastoma tumor suppressor protein (RB) revealed that they may physically\ninteract with one another. Subsequently, an RB peptide corresponding to the\nproposed RB binding site for insulin was found to recognize full-length insulin\nin vitro. As part of efforts aimed at developing this RB peptide into an\nanti-cancer drug, this molecule was chemically coupled to a cellular\ninternalization signal and termed \"MCR peptide\". Meanwhile, several such MCR\npeptide variants have been demonstrated to restrain the proliferation of\ndifferent human cancer cells in vitro and in vivo. Moreover, one of the MCR\npeptides coined MCR-10 was shown to be capable of interfering with the complex\nformation between insulin and RB in HepG2 human hepatoma cells, as monitored by\nimmunofluorescence. This latter result indicating an in vivo association\nbetween insulin and RB was confirmed by a follow-up study combining the methods\nof co-immunoprecipitation and immunoblotting. Here, we provide evidence for the\nexistence of the insulin-RB complex in A549 human non-small cell lung cancer\ncells. Specifically, we demonstrate this heterodimer by means of a magnetic\nbeads-based immunoprecipitation approach and equally show that this dimer can\nbe disrupted by MCR-4 or MCR-10 each of which is known to possess\nantiproliferative properties, yet to a much lesser extent by a control peptide.\nThus, this investigation has yielded another important proof for the occurrence\nof the insulin-RB dimer and, furthermore, its validity as a target for\nantineoplastic MCR peptides.\n",
        "  We present millimeter and radio observations of 13 SDSS quasars at reshifts\nz~6. We observed eleven of them with the Max-Planck Millimeter Bolometer Array\n(MAMBO-2) at the IRAM 30m-telescope at 250 GHz and all of them with the Very\nLarge Array (VLA) at 1.4 GHz. Four sources are detected by MAMBO-2 and six are\ndetected by the VLA at >=3 sigma level. These sources, together with another 6\npublished in previous papers,yield a submillimeter/millimeter and radio\nobserved SDSS quasar sample at z~6. We use this sample to investigate the\nfar-infrared (FIR) andradio properties of optically bright quasars in the early\nuniverse. We compare this sample to lower redshift samples of quasars observed\ninthe submillimeter and millimeter wavelengths ((sub)mm), and find that the\ndistribution of the FIR to B band optical luminosity ratio (L_FIR/L_B) is\nsimilar from z~2 to 6. We find a weak correlation between the FIR luminosity\n(L_FIR) and B band optical luminosity (L_B) byincluding the (sub)mm observed\nsamples at all redshifts. Some strong (sub)mm detections in the z~6 sample have\nradio-to-FIR ratios within the range defined by star forming galaxies, which\nsuggests possible co-eval star forming activity with the powerful AGN in these\nsources. We calculate the rest frame radio to optical ratios (R*_1.4=L_{v,\n1.4GHz}/L_{v, 4400A}) for all of the VLA observed sources in the z~6 quasar\nsample. Only one radio detection in this sample, J083643.85+005453.3, has\nR*_1.4~40 and can be considered radio loud. There are no strong radio sources\n(R*_1.4>=100) among these SDSS quasars at z~6. These data are consistent with,\nalthough do not set strong constraints on, a decreasing radio-loud quasar\nfraction with increasing redshift.\n",
        "  The formation of macroscopic reconnected magnetic structures (islands) have\nbeen observed in advanced experiments on weakly collisional, well confined\nplasmas while established theories of the drift-tearing modes, which depend\nstrongly on the electron temperature gradient and can describe the formation of\nthese structures, had predicted practically inaccessible excitation thresholds\nfor them in these regimes. The relevant theoretical dilemma is resolved as\nmesoscopic modes that depend critically on the ratio of the transverse (to the\nmagnetic field) to the longitudinal thermal\nconductivity${D^e_{\\perp}/D^e_{\\|}$, can produce large scale magnetic\nreconnection. These modes are envisioned to emerge from a background, which can\nbe coherent, of collisionless microscopic reconnecting modes driven by the\nelectron temperature gradient, that create a sequence of adjacent strings of\nmagnetic islands and increase considerably the ratio ${D^e_{\\perp}/D^e_{\\|}$\nover its classical value. The mesoscopic reconnecting mode is treated by a\nsingular perturbation analysis involving three asymptotic regions and the small\nparameters ${(D^e_{\\perp}/D^e_{\\|})}^{1/4}$ and ${\\epsilon}^{1/4}_{*}$, where\n${\\epsilon}_{*} {\\equiv}D_m/D_A$, $D_m$ is the magnetic diffusion coefficient,\n$D_A\\sim\\texttt{v}^{2}_{A}r_{Te}/(D_Bk_{\\perp})$,\n$r_{Te}\\equiv(-d\\texttt{ln}T_e/dr)^{-1}$, $k_{\\perp}$ is the transverse mode\nnumber, $\\texttt{v}^{2}_{A}=B^{2}/(4\\pi{nm}_{i})}$ and $D_B=cT_e/(eB)$.\n",
        "Dear Reviewers and AreaChair, \n\nApproaching to the deadline, I'd like to remind the reviewers of the fact that the revised version of the paper (that we uploaded in Dec. 27) has significant improvements with respect to most of the comments pointed by the reviewers, which are mainly in terms of the writing clarity in Section 3.",
        "  Within a general theoretical framework we study the effective,\ndeformation-induced interaction between two colloidal particles trapped at a\nfluid interface in the regime of small deformations. In many studies, this\ninteraction has been computed with the ansatz that the actual interface\nconfiguration for the pair is given by the linear superposition of the\ninterface deformations around the single particles. Here we assess the validity\nof this approach and compute the leading term of the effective interaction for\nlarge interparticle separation beyond this so-called superposition\napproximation. As an application, we consider the experimentally relevant case\nof interface deformations owing to the electrostatic field emanating from\ncharged colloidal particles. In mechanical isolation, i.e., if the net force\nacting on the total system consisting of the particles plus the interface\nvanishes, the superposition approximation is actually invalid. The effective\ncapillary interaction is governed by contributions beyond this approximation\nand turns out to be attractive. For sufficiently small surface charges on the\ncolloids, such that linearization is strictly valid, and at asymptotically\nlarge separations, the effective interaction does not overcome the direct\nelectrostatic repulsion between the colloidal particles.\n",
        "  It is conjectured that time intervals of any kind are proportional to the age\nof the Universe taken at the time we are considering the interval. If this is\nthe case then the speed of light, in fact any speed, must decrease inversely\nproportional to this age. The immediate consequence is that energy is not\nconserved: the hypothesis that time is a homogeneous property implies\nconservation of energy (the theorem of Noether). Nonconservation of energy\nfollows from the condition that any time interval is proportional to the\ncosmological time, and therefore time can not be homogeneous. From the\nuncertainty principle, taking the constant of Planck as a real constant, time\nindependent, it follows that any energy in the Universe decreases linearly with\ntime. We then prove that Schroedinger equation does not change, except for the\npotential energy term. The future of the Universe gives for the wave functions\na long sinusoidal spatial solution, so that everything becomes unlocalized. The\nrelativistic absolute interval remains the same, even with a changing speed of\nlight, and the Universe turns out to be nonexpanding. A Mass-Boom effect is\nconfirmed.\n",
        ",,,,\n\n\nThere are 4 sets of goals and each set contains 5 goals, so you can create a maximum of 20 goals.\n\n,,\n\n\nThe \"On\" radio button will be selected by default, but you can select the \"Off\" radio button if you want to activate your goal at a later time.\n\n,\n\n\nThe Goal Position field can be useful if you have multiple goals and want them to appear in a specific order in your Analytics reports.\n\n,\n\n\nChoose \"URL Destination\" if your goal is to have visitors land on a specific page on your website. This goal option will allow you to set up a funnel, which is useful if you want to track the progress of visitors as they click through pages to reach the goal destination.\nChoose \"Time on Site\" if you want to measure the amount of time a visitor spends on your website.\nChoose \"Pages per Visit\" if you want to view how many pages visitors browsed while they were on your website.\n\n,,\n\n\nThe Match Type field determines how you want the URL used by the visitor to match with the URL you identify for this goal type. In some cases, a URL can change slightly depending on where the visitor is coming from. An Exact Match is when all your URLs remain the same and never change. A Head Match allows you to designate a specific string of characters in your URL that you want matched and can be useful if visitors have an identification number when they are logged into your website. Regular Expression enables wild card matching of certain characters in the URL and is helpful if visitors land on the URL from a sub-domain.\n\n,\n\n\nInclude the part of your URL that follows after your domain; do not provide your domain. For example, if your website ends in \".com,\" include all the characters following \".com,\" including the forward slash.\n\n,\n\n\nIf your domain or URL requires certain upper-case or lower-case letters for it to work, place a check mark in the field next to \"Case Sensitive.\"\n\n,\n\n\nFor example, if your URL Destination is a checkout page for visitors that buy a product from you, enter the amount of total transactions you want to occur.\n\n,\n\n\nClick on \"Yes, create a funnel for this goal\" to open your funnel options.\nProvide each URL in the series of URLs for the funnel by typing in the characters following your website's domain.\n\n,",
        "The paper describes a modification to the output layer of recurrent neural\nnetwork models which enables learning the model parameters from both gold and\nprojected annotations in a low-resource language. The traditional softmax\noutput layer which defines a distribution over possible labels is further\nmultiplied by a fully connected layer which models the noise generation\nprocess, resulting in another output layer representing the distribution over\nnoisy labels. \n\nOverall, this is a strong submission. The proposed method is apt, simple and\nelegant. The paper reports good results on POS tagging for eight simulated\nlow-resource languages and two truly low-resource languages, making use of a\nsmall set of gold annotations and a large set of cross-lingually projected\nannotations for training. The method is modular enough that researchers working\non different NLP problems in low-resource scenarios are likely to use it.\n\nFrom a practical standpoint, the experimental setup is unusual. While I can\nthink of some circumstances where one needs to build a POS tagger with as\nlittle as 1000 token annotations (e.g., evaluations in some DARPA-sponsored\nresearch projects), it is fairly rare. A better empirical validation of the\nproposed method would have been to plot the tagging accuracy of the proposed\nmethod (and baselines) while varying the size of gold annotations. This plot\nwould help answer questions such as: Does it hurt the performance on a target\nlanguage if we use this method while having plenty of gold annotations? What is\nthe amount of gold annotations, approximately, below which this method is\nbeneficial? Does the answer depend on the target language?\n\nBeyond cross-lingual projections, noisy labels could potentially be obtained\nfrom other sources (e.g., crowd sourcing) and in different tag sets than gold\nannotations. Although the additional potential impact is exciting, the paper\nonly shows results with cross-lingual projections with the same tag set. \n\nIt is surprising that the proposed training objective gives equal weights to\ngold vs. noisy labels. Since the setup assumes the availability of a small gold\nannotated corpus, it would have been informative to report whether it is\nbeneficial to tune the contribution of the two terms in the objective function.\n\n\nIn line 357, the paper describes the projected data as pairs of word tokens\n(x_t) and their vector representations \\tilde{y}, but does not explicitly\nmention what the vector representation looks like (e.g., a distribution over\ncross-lingually projected POS tags for this word type). A natural question to\nask here is whether the approach still works if we construct \\tilde{y} using\nthe projected POS tags at the token level (rather than aggregating all\npredictions for the same word type). Also, since only one-to-one word\nalignments are preserved, it is not clear how to construct \\tilde{y} for words\nwhich are never aligned.\n\nLine 267, replace one of the two closing brackets with an opening bracket.",
        "Glyphipterix hyperlampra is a species of sedge moth in the genus Glyphipterix. It was described by Alfred Jefferis Turner in 1913. It is found in Australia.\n\nReferences\n\nMoths described in 1913\nGlyphipterigidae\nMoths of Australia",
        "Snowfall is an American crime drama television series, created by John Singleton, Eric Amadio, and Dave Andron, that was first broadcast on FX on July 5, 2017. Set in Los Angeles in 1983, the series revolves around the first crack epidemic and its impact on the city. The series follows the stories of several characters whose lives are fated to intersect: 20-year-old drug dealer Franklin Saint, Mexican luchador Gustavo \"El Oso\" Zapata, CIA operative Teddy McDonald, and a Mexican crime boss's niece, Lucia Villanueva.\n\nThe series, which was first set up at Showtime in 2014, was picked up by FX for a ten-episode season on September 30, 2016. On August 9, 2017, the network renewed Snowfall for a second season, which premiered on July 19, 2018. On September 19, 2018, the series was renewed for a third season, which premiered on July 10, 2019. On August 6, 2019, FX renewed the series for a fourth season which was originally scheduled to premiere in 2020, but filming was temporarily suspended due to the COVID-19 pandemic. The fourth season premiered on February 24, 2021. On March 23, 2021, FX renewed the series for a fifth season which premiered on February 23, 2022.\n\nCast\n\nMain\nDamson Idris as Franklin Saint, a 21-year-old drug kingpin and patriarch of The Family, a crew of crack cocaine producers based in Los Angeles\nCarter Hudson as Theodore \"Teddy\" McDonald/ Reed Thompson, a CIA operative working undercover for the American government in the war against communism\nEmily Rios as Lucia Villanueva, the daughter of a Mexican crime boss and heiress to the Villanueva Cartel, a Mexican drug cartel (seasons 1–2)\nSergio Peris-Mencheta as Gustavo \"El Oso\" Zapata, a former Mexican luchador affiliated with the Villaneuva Cartel \nMichael Hyatt as Cissy Saint, Franklin's mother and a veteran real estate agent\nAmin Joseph as Jerome Saint, Franklin's uncle and a member of The Family who introduces him to the criminal lifestyle\nAngela Lewis as Louise \"Louie\" Saint, Jerome's girlfriend and a member of The Family\nJuan Javier Cardenas as Alejandro Usteves, a Nicaraguan Contra soldier and pilot who works with Teddy as a CIA asset (season 1)\nIsaiah John as Leon Simmons, Franklin's best friend and second-in-command of The Family\nFilipe Valle Costa as Pedro Nava, Lucia's cousin and a member of The Villanueva Cartel (seasons 1–2)\nAlon Aboutboul as Avi Drexler, an Israeli druglord and arms dealer who used to work with Mossad\nMalcolm Mays as Kevin Hamilton, Franklin and Leon's best friend and a member of The Family (seasons 1–2)\nMarcus Henderson as Andre Wright, a sergeant in the LAPD, Melody's father and Franklin's neighbor (season 3; recurring seasons 1–2)\nKevin Carroll as Alton Williams, Franklin's estranged father and a former member of the Black Panthers who runs a homeless shelter (season 4; recurring seasons 1–3)\nDevyn A. Tyler as Veronique, Franklin's girlfriend who works for him as a real estate manager (season 5)\n\nRecurring\nJudith Scott as Claudia Crane, owner of a local nightclub and Louise's lover (seasons 1–3)\nReign Edwards as Melody Wright, Franklin's on/ off girlfriend and Andre's daughter (seasons 1–4)\nPeta Sergeant as Julia, Teddy's ex-wife and a CIA agent (seasons 1–3, 5)\nTaylor Kowalski as Rob Volpe, Franklin's friend from high school, a crack bagger and a member of The Family (seasons 1–5)\nCarlos Linares as Mauricio Villanueva, Lucia's father and the patriarch of the Villanueva Cartel (season 1)\nJosé Zúñiga as Ramiro Nava, Lucia's uncle and the second-in-command of the Villanueva Cartel (season 1)\nTony Sancho as Eduardo \"Stomper\" Castillo, the leader of Los Monarcas, a Mexican street gang (seasons 1–2)\nMarkice Moore as Ray-Ray, a thief from Compton (seasons 1–2)\nCraig Tate as Lenny, a thief from Compton and Ray-Ray's friend (season 1)\nJustine Lupe as Victoria Grelli, a young woman who befriends Teddy whilst looking for her missing sister  (season 1)\nMichael Ray Escamilla as Hernan Zapata, Gustavo's brother who is confined to a wheelchair (seasons 1–4)\nWade Allain-Marcus as Diego, co-leader of the Cali Cartel, a Columbian crime syndicate (seasons 1–2, 4)\nIzzy Diaz as Danilo, Diego's brother and co-leader of the Cali Cartel (seasons 1–2, 4)\nAdriana Barraza as Mariela Villanueva, Lucia's mother and the matriarch of the Villanueva Cartel (seasons 1, 3)\nJonathan Tucker as Matt McDonald, Teddy's older brother, a Vietnam War veteran and a pilot (seasons 2–3)\nGail Bean as Wanda Bell, Leon's ex-girlfriend who develops a strong addiction to crack cocaine (season 2–present)\nDeRay Davis as Peaches, a Vietnam War veteran and a member of The Family (season 2–present)\nAdriana DeGirolami as Lorena Cardenas/ Soledad Caro, Pedro's fiancé and an undercover DEA agent (seasons 2–3)\nMarcelo Olivas as Santiago \"Conejo\" Estrada, a soldado who works with Los Manarcas (season 2) \nScott Subiono as Tony Marino, a DEA Agent and Lorena's handler (seasons 2–4)\nAlanna Ubach as Gabriella Elias, the leader of La Fuerza, a high ranking gang in the Mexican Mafia (season 2)\nMatthew Alan as Stephen Havemeyer, a CIA handler and Teddy's boss (season 2–present)\nJordan Coleman as Thaddeus \"Fatback\" Barber, Leon's bodyguard and a member of The Family (seasons 3–4)\nBentley Green as C.J., an up-and-coming member of The Family (seasons 3–4)\nMelvin Gregg as Drew \"Manboy\" Miller, Franklin's associate and the leader of the Compton Crips (seasons 3–4)\nCalvin Clausell Jr. as Bootsy, the second-in-command of the Compton Crips (seasons 3–4)\nChristian Tappan as Rigo Vasco, a drug lord associated with the Medellín Cartel, a Colombian crime syndicate (season 3)\nJesse Luken as  Herb \"Nix\" Nixon, a rogue corporal in the LAPD and Andre's friend (seasons 3–4)\nDe'Aundre Bonds as Terrence \"Skully\" Brown, an OG and the Leader of the Inglewood Bloods (seasons 3–present)\nCorr Kendricks as Cornrows, the second-in-command of the Inglewood Bloods (season 3–present)\nSuzy Nakamura as Irene Abe, a journalist working for the Los Angeles Herald Examiner who investigates the ongoing drug trade in Los Angeles (season 4)\nStephen Ruffin as Wilson, Irene's assistant at the Los Angeles Herald Examiner (season 4)\nKwame Patterson as Lurp, a freelancer, Franklin's bodyguard and a member of The Family (season 4)\nGeffri Maya as Khadija Brown, Skully's wife, the mother of his daughter, Tianna Brown, and the sister of Manboy (season 4)\nAdrianna Mitchell as Tanosse, Franklin's ex-girlfriend from high school whom he reconnects with (season 4)\nAntonio Jaramillo as Oscar Fuentes, the corrupt police chief of the Tijuana Police Department (season 4)\nJeremiah Birkett as John Baxter, a homeless crack addict and single father at Alton's homeless shelter (season 4)\nBrent Jennings as Henry Nelson, Irene's ex-husband and a journalist (season 4)\nQuincy Chad as Deon \"Big Deon\" Barber, an OG and enforcer for the PJ Watts Crips (season 4–present)\nKamron Alexander as Einstein, the brains behind Deon's Crip set (season 4–present)\nBrandon Jay McLaren as Beau Buckley, a leading detective in the LAPD involved in the C.R.A.S.H. unit (season 5)\n\nEpisodes\n\nSeason 1 (2017)\n\nSeason 2 (2018)\n\nSeason 3 (2019)\n\nSeason 4 (2021)\n\nSeason 5 (2022)\n\nReception\n\nCritical response\nOn Rotten Tomatoes, the series is rated as 87% fresh, though its first season has an approval rating of only 62% based on 63 reviews, with an average rating of 6.19/10. The site's critical consensus for the first season reads, \"Snowfall struggles to create a compelling drama from its separate storylines, despite Singleton's accurate recreation of 1983 Los Angeles and a strong lead performance from Damson Idris.\" On Metacritic, the series has a score of 62 out of 100, based on 38 critics, indicating \"generally favorable reviews\". A 100% approval rating for the second season was reported by Rotten Tomatoes, with an average rating of 7/10 based on 7 reviews. A 100% approval rating for the third season was reported by Rotten Tomatoes with an average rating of 9/10 based on 5 reviews.\n\nAlan Sepinwall of Uproxx gave a mixed review of the series, criticizing the show's clichéd storytelling and pacing; the latter he observed, \"oddly feels sluggish and rushed at the same time, lingering over certain tasks and story beats... but then oddly jumping over story points in a way that had me frequently checking to be sure I hadn't skipped an episode by mistake.\" He further emphasized: \"Snowfalls not a bad drama at this stage, just more generic than it should be, especially on a channel known for making old TV ideas feel brand-new.\" \n\nMatt Zoller Seitz of Vulture, however, gave a positive review to Snowfall, praising \"the attention it pays to the sights, sounds and textures of people's lives in 1983 Los Angeles, and to fine details of characterization — in other words, the sort of stuff that would never get a dramatic series a green light unless drugs and violence were attached to it.\" He further stated: \"rather than go for a vibe like The Wire or Steven Soderbergh's Traffic, which explored the drug trade with a newspaperman's anthropological detachment, Snowfall aims for a bouncier, more seductive vibe.\" Seitz also praised the \"phenomenal\" acting of Idris, Peris-Mencheta, Hudson, and Rios.\n\nRatings\n\nSeason 1\n\nSeason 2\n\nSeason 3\n\nSeason 4\n\nSeason 5\n\nBroadcast\nOutside of the United States, Snowfall premiered on BBC Two in the United Kingdom on October 8, 2017. It is also available for view on BBC iPlayer.\n\nNotes\n\nReferences\n\nExternal links\n\n2017 American television series debuts\n2010s American crime drama television series\nEnglish-language television shows\nFX Networks original programming\nTelevision productions suspended due to the COVID-19 pandemic\nTelevision series about the Central Intelligence Agency\nTelevision series about illegal drug trade\nTelevision series about organized crime\nTelevision series set in 1983\nTelevision shows about cocaine\nTelevision shows set in Los Angeles\nWorks about African-American organized crime\nWorks about Jewish-American organized crime\nWorks about Colombian drug cartels\nWorks about Mexican drug cartels",
        "Sasiny  is a village in the administrative district of Gmina Małdyty, within Ostróda County, Warmian-Masurian Voivodeship, in northern Poland.\n\nReferences\n\nSasiny",
        "  We present the results from an optical spectroscopic analysis of the massive\nstars HD 37366 and HD 54662. We find that HD 37366 is a double-lined\nspectroscopic binary with a period of 31.8187 +/- 0.0004 days, and HD 54662 is\nalso a double lined binary with a much longer period of 557.8 +/- 0.3 days. The\nprimary of HD 37366 is classified as O9.5 V, and it contributes approximately\ntwo-thirds of the optical flux. The less luminous secondary is a broad-lined,\nearly B-type main-sequence star. Tomographic reconstruction of the individual\nspectra of HD 37366 reveals absorption lines present in each component,\nenabling us to constrain the nature of the secondary and physical\ncharacteristics of both stars. Tomographic reconstruction was not possible for\nHD 54662; however, we do present mean spectra from our observations that show\nthat the secondary component is approximately half as bright as the primary.\nThe observed spectral energy distributions (SEDs) were fit with model SEDs and\ngalactic reddening curves to determine the angular sizes of the stars. By\nassuming radii appropriate for their classifications, we determine distance\nranges of 1.4 - 1.9 and 1.2 - 1.5 kpc for HD 37366 and HD 54662, respectively.\n",
        "Summary:\nThis paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well. Authors of the paper show results over several different datasets.\n\nOverview of the Review:\n    Pros:\n        - The idea is very interesting.\n        - The diverse set of results on different datasets.\n    Cons:\n        - The justification is not strong enough.\n        - The paper is not well-written.\n        - Experiments are not convincing enough.\n\nCriticisms:\n\nI liked the idea and the intuitions coming from the paper. However, I think this paper is not written well. There are some variables introduced in the paper and not explained good-enough, for example in 2.3, the authors start to talk about p without introducing and defining it properly. The only other place it appears before is Equation 6. The Equations need some work as well, some work is needed in terms of improving the flow of the paper, e.g., introducing all the variables properly before using them.\n\nEquation 6 appears without a proper explanation and justification. It is necessary to explain it what it means properly since I think this is one of the most important equation in this paper. More analysis on what it means in terms of optimization point of view would also be appreciated.\n\n$\\phi$ is not a parameter, it is a function which has its own hyper-parameter $\\alpha$. \n\nIt would be interesting to report validation or test results on a few tasks as well. Since this method introduced as an additional cost function, its effect on the validation/test results would be interesting as well.\nThe authors should discuss more on how they choose the hyper-parameters of their models. \n\nThe Figure 2 and 3 does not add too much to the paper and they are very difficult to understand or draw any conclusions from. \n\nThere are lots of Figures under 3.4.2 without any labels of captions. Some of them are really small and difficult to understand since the labels on the figures appear very small and somewhat unreadable.\n\n\nA small question:\n\n* Do you also backpropagate through $\\tilde{\\mW}_i^t$?",
        "The Maio Champion's Cup or the Maio Champion's Trophy (Portuguese: Taça da Campeões de Maio, Capeverdean Creole: ALUPEK: Tasa da Kampionis di Maiu) is a single knockout football (soccer) competition that is played each season in the island of Maio, Cape Verde. The competition features the champion from the Premier Division and the champion of the Second Division.  The trophy competition is organized by the Maio Regional Football Association (Associação Regional do Maio, ARFM). Its current winner is Onze Unidos who won their only title.\n\nThe Champion's Cup (or Trophy) was introduced in 2016.  The first edition took place in November, 2016 and featured Académico 83 (Premier Division) and Real Marítimo (Second Division) which were the first club to participate.\n\nWinners\n\nPerformance By Club\n\nPerformance by area\n\nSee also\nMaio Premier Division\nMaio Second Division\nMaio Island Cup\nMaio Super Cup\nMaio Opening Tournament\n\nReferences\n\nExternal links\nMaio Regional Football Association which includes the Champion's Cup (or Trophy) \n\nSport in Maio, Cape Verde\nFootball cup competitions in Cape Verde\n2016 establishments in Cape Verde\nRecurring sporting events established in 2016",
        "This paper basically applies A3C to 3D spatial navigation tasks. \n\n- This is not the first time A3C has been applied to 3D navigation. In fact the original paper reported these experiments. Although the experimental results are great, I am not sure if this paper has any additional insights to warrant itself as a conference paper. It might make more sense as a workshop paper\n\n-  Are the graphs in Fig 5 constructed using a single hyper-parameter sweep? I think the authors should report results with many random initializations to make the comparisons more robust\n\n- Overall the two main ideas in this paper -- A3C and curriculums -- are not really novel but the authors do make use of them in a real system. ",
        "It seems that you use inconsistent notation - the variable 't' is used for different time scales: in Eq. (1) t represents strided time frames, whereas in x_t above it enumerates frames directly.",
        "**ELI5 version:**\nA hedge fund is just a fancy name for someone who takes the savings you give them (its your choice, you don't have to), and then they buy things that they believe will increase in value over time.  Eventually they sell what they bought and get even more money back.  They then give this money back to you, so your savings is larger.  You don't have to give your savings to a hedge fund, but the reason some people do is because they want more money later in their life and they think this is one way to do that.  You could take your savings and try to buy things that will go up in value over time, but for a variety of complicated adult reasons you either wouldn't be allowed to buy everything a hedge fund can, and also a lot of people think that a hedge fund is smarter than they are and has a better chance of getting them more money over time.  Now, why would a hedge fund agree to take your savings, make it bigger, and then give it back to you? Seems too nice, right? Well, they also take a little bit of your money AND if your money is worth a lot more they get a little bit of that extra money too.  Because of this, a person who owns a hedge fund makes more money for themselves because when they take your savings to buy things, they also get to keep a little bit for themselves and never have to give it back.  The reason some people don't like to give their savings to hedge funds is because if the savings is used to buy something that loses value over time, you then don't have as much savings in the future. \n\n[Khan Academy does a great job explaining this via video in under 4mins](_URL_2_), but are slightly above ELI5 level.\n\n**ELI25 version:**\n\n* How big is the industry?  It's growing, a lot. Even growing after the financial crisis when many thought it would shrink in size. Because the industry isn't broadly regulated, there's not a great central source of information.  Most people in finance and reporters rely on reports compiled by Hedge Fund Research, which said over the first nine months of 2013 that all hedge funds combined managed a total of $2.51 trillion USD ([source](_URL_0_))\n\n* Who starts a hedge fund? Generally people who worked at other hedge funds previously or worked as a trader (usually in a very senior role) at an investment bank.  They need some prior position with a track record (e.g. they can demonstrate they made people a bunch of money in the past) which gives them credibility. To get hired at a hedge fund you need to have gone to an Ivy League level colleague undergrad, have received very good grades (3.8/4.0 GPA or higher), have worked at a \"top tier\" investment bank as an analyst/associate for at least 2 years, and be very good at fundamental financial analysis and financial modeling (the investment bank would train you or someone to train you.  [Training The Street](_URL_5_) and [Wall Street Prep](_URL_1_) are the two industry standard training organizations most people use.  They're growing in popularity too ([source](_URL_4_)). Fees for training can exceed $1000/day per student. Here's a Q & A w/ a hedge fund analyst about the hedge fund recruiting process [source])_URL_6_).\n\n* Are all hedge funds basically doing the same thing?  Sort of, in the sense that the whole industry isn't regulated and can broadly investment wherever and however it wants.  Just like nobody tells a private investor who they can/can't buy (I'm generalizing here), there isn't a government body that tells a hedge fund they can/can't buy specific things.  All hedge funds will have a \"strategy\" they've told investors they will use to make money, like some are long some stocks and short others (long/short strategy), or some they look for companies they think will be bought by another company and invest in them before they actually are are go up in value (called merger arbitrage). There are a variety of other strategies [explained here](_URL_3_)). Probably the \"sexiest\" strategy most popular since the financial crisis/crash is \"Global Macro\", which involves trying to make investments that will benefit from major macroeconomic events (like an predicting an economy crashing, a central bank rate changing, a foreign exchange rate movement, etc.)\n\n* How much do hedge funds charge?  Typically they take 2% of the total assets they manage each year as an administrative fee (\"hey, we gotta pay for office space\") plus 20% of the profits made.  This is why hedge funds like to be as large as possible, although there's some belief you can be too big (\"there's not enough stuff to buy will all this money that will go up in value?!\").  A typical fund size I would say is in the $50-200 million range, and most people in the industry call $200 million or less \"small\", $500 million \"decent sized\" and anything over $1 billion as \"large\". If your hedge fund is over $8 billion I would call you quite successful, and there aren't that many that meet that category.  \n\nI'm getting tired and can write more later, or if any questions just reply.  It's a really fascinating industry and has been in the press a lot, especially with the trial this week of the former portfolio manager from SAC Capital, one of the larger and most successful hedge funds over time (successful enough they charge 3% management fee yearly + keep 50% of profits...these are insanely high numbers even for this industry).",
        " There are 2 ways to search, by using an image you already have as the search term or using text terms to find image results.;\n, For example, images are given names and captions that help people find them.\n\n\nAdjusting your search to a more specific event or location may trigger more words that were associated with the original image.\nKeep in mind that an image about a foreign country could be characterized in another language. Use foreign words to increase the accuracy of your search.\n\n, Images usually take 1 to 2 weeks to appear in the front of the image results for any given search terms. If you are looking for something new, it may not appear in the first few pages, unless it is trending.\n\n, These are certain words or punctuation that increase the accuracy of a search. They can also save time.\n\n,, For example, historical buildings not jail.\n\n, It will broaden your search to include both terms equally.\n\n, For example, (child or children).\n\n, In 2013, the most popular sites are Google.com and Bing.com. Go to the site of your choice.\n\n,,, Be specific, but keep in mind that the search engine will look through image names, captions and descriptions.\n\n,,, Keep in mind that images are frequently copyrighted, so in certain cases, it may not be used for commercial purposes without copyright infringement.\n\n, You will be redirected when you ask to go directly to the website.\n\n, You can also find an image and copy its URL.\n\n, Find the camera icon to the right of the text box where you type in search terms.\n\n,, Click \"Upload an Image.\" Use the browser to select and upload the image.\n\n,, First, the image details should appear, then websites with links to the photo or similar subjects should appear. Click on the result of your choice.\n\n",
        "Clinidium kochalkai is a species of ground beetle in the subfamily Rhysodinae. It was described by R.T. Bell & J.R. Bell in 1985. It is named for arachnologist John A. Kochalka, friend and former student of the Bells. Clinidium kochalkai is known from Sierra Nevada de Santa Marta, Colombia. It measures  in length.\n\nReferences\n\nClinidium\nBeetles of South America\nArthropods of Colombia\nEndemic fauna of Colombia\nBeetles described in 1985",
        " Lightly coat a 2-quart (1.89 liters) casserole dish with some butter. Set the dish aside.;\n, Fill a large pot with 4 quarts (3.79 liters) of water and bring it to a boil. Add some salt and macaroni, and cook it for about 5 to 7 minutes. The macaroni will continue cooking in the oven.\n\n, Place a large strainer or colander in the sink and pour the macaroni into it. Shake the strainer a little to get rid of any excess water.\n\n, Melt the butter in the saucepan and stir in the flour, salt, and pepper using a whisk. Cook the roux over medium heat for 1 to 2 minutes, or until the flour turns a light brown and bubbles start to form.The saucepan has to be large enough to hold the milk, cheese, and macaroni. You will be mixing everything in this.\n\n, Pour in 1 cup (240 milliliters) of milk and stir it with a whisk to combine. Pour in the remaining milk and continue stirring.Be sure to break up any lumps or clumps of flour.,,, Switch the whisk out for a wooden spoon for this so that the macaroni doesn't get caught in it.\n\n, Tilt the saucepan over the casserole dish, and use your wooden spoon to help spread it in an even layer across the bottom. of the dish. If you'd like, you can sprinkle some extra cheese on top for that extra-crispy layer.\n\n, If you do not want the top to be crispy, cover the dish with a sheet of tin foil for the first 30 minutes of baking.Once the casserole is baked, put it out of the oven using oven mitts.\n\n,",
        "This paper provides an interesting idea, which extends GAN by taking into account bidirectional network. Totally, the paper is well-written, and easy to follow what is contribution of this paper. From the theoretical parts, the proposed method, BiGAN, inherits similar properties in GAN. The experimental results show that BiGAN is competitive with other methods. A drawback would a non-convex optimization problem in BiGAN, this paper is still suitable to be accepted in my opinion. ",
        "  We have explored a sample of suspected A-type binaries in a systematic way,\nboth spectroscopically and photometrically. Due to their location in the H-R\ndiagram, indications of pulsation and/or chemical peculiarities among these\nsuspected binary (or multiple) systems may be found. High-resolution\nspectroscopy obtained with the ELODIE and MUSICOS spectrographs was used in\ncombination with a few nights of differential CCD photometry in order to search\nfor pulsation(s). Of the 32 investigated targets, eight are spectroscopic\nbinaries, one of which is a close binary also showing eclipses, and three have\nbeen identified as Delta Scuti pulsators with rapid line-profile variations.\nAmong the latter stars, HD 217860 reveals interesting multiperiodic photometric\nand spectroscopic variations, with up to eight frequencies common to two large\nphotometric data sets. We suggest that at least one radial overtone mode is\nexcited among the two most dominant frequencies. We furthermore found evidence\nfor a strong modulation of the amplitude(s) and/or the (radial) frequency\ncontent of this intriguing Delta Scuti star.\n",
        "  In the reaction e+e- -> WW -> (q_1 qbar_2)(q_3 qbar_4) the usual\nhadronization models treat the colour singlets q_1 qbar_2 and q_3 qbar_4 coming\nfrom two W bosons independently. However, since the final state partons may\ncoexist in space and time, cross-talk between the two evolving hadronic systems\nmay be possible during fragmentation through soft gluon exchange. This effect\nis known as Colour Reconnection. In this article the results of the\ninvestigation of Colour Reconnection effects in fully hadronic decays of W\npairs in DELPHI at LEP are presented. Two complementary analyses were\nperformed, studying the particle flow between jets and W mass estimators, with\nnegligible correlation between them, and the results were combined and compared\nto models. In the framework of the SK-I model, the value for its kappa\nparameter most compatible with the data was found to be: kappa_{SK-I} =\n2.2^{+2.5}_{-1.3} corresponding to the probability of reconnection P_{reco} to\nbe in the range 0.31 < P_{reco} < 0.68 at 68% confidence level with its best\nvalue at 0.52.\n",
        "This is actually a difficult question to address and needs a bit of background in order to adequately approach. First, it must understood that the characterizing traits of the Japanese POW administration were incompetence, negligence, and disinterest. Operating a POW camp was a hated job, often held by outcast officers, those men passed over for promotion to general (\"permanent\" colonels) or seen as unfit for front line command. While these men were ostensibly managed by higher ranking officers in the Military Affairs bureau (Army office), in reality, such oversight was few and far between. Far more common was the POW camp managed by one officer, and his subordinates, operating as an independent entity. This lead to spectacular diversity in conditions at each camp. \n\nThe \"regular\" POW camps (i.e. those managed directly by the Army office) were characterized by relatively high living conditions, communication of prisoner name and condition to the International Committee of the Red Cross, and payment of an officer salary to those imprisoned. In such a situation, where some mutual respect can be assumed to exist between captor and captive, it is possible that prisoners improved their status in camp through such collaboration. I was unable to find any sources though, outside of Unbroken (a problematic source), that record specific instances of prisoners becoming actors in Japanese propaganda. There were, however, certain cases where prisoners greatly improved prison living conditions, for themselves and their fellow prisoners, by applying knowledge and skills from previous occupations. One shining example is of Sir Norman Alexander, a Professor of Physics and prisoner at Changi prison in Singapore, who designed and built a salt evaporation plant and small industrial plant capable of producing surgical spirit for the prison hospital. \n\nThe horror stories, such as the labor camps of the Burma-Thailand Railway and those associated with the Bataan death march, are found with the \"irregular\", independently-managed camps. It is extremely unlikely that prisoners in these camps, weakened by routine beatings, starvation rations, disease and injury, would have had the capability, never mind the opportunity to collaborate with their captors. These places were unpredictable and brutal, managed by mob rule, more than any military hierarchy. Low ranking Japanese soldiers, hearts hardened by the constant corporeal punishment dealt out on them by their superiors, were overjoyed to have someone lower on the totem pole than themselves. In some ways, punishing POWs became an outlet for the frustration and anguish contained, pressurized, within these soldiers. This is not to say that some irregular camp supervisors or government officials did not try to improve camp conditions. However, the lack of a dedicated, coordinated effort to improve and standardize POW camps resulted in far too many camps run under deplorable conditions. Such a situation, amidst the miasma of the Showa period, resulted in the deaths of nearly 1 in 3 prisoners. I sincerely hope that some of the prisoners in these irregular camps were able to improve their conditions, through collaboration or some other method, because otherwise their outlook was grim. \n\nSources: \n_URL_0_\n_URL_1_\nHavers, R. P. W. Reassessing the Japanese Prisoner of War Experience: The Changi POW Camp, Singapore, 1942-5. London: RoutledgeCurzon, 2003. Print.",
        "- Strengths: This paper explores a relatively under-explored area of practical\napplication of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian\ntreatment of the parameters of RNNs, it is possible to incorporate benefits of\nmodel averaging during inference. Further, their gradient\nbased sampling approximation to the posterior estimation leads to a procedure\nwhich is easy to implement and is potentially much cheaper than other\nwell-known techniques for model averaging like ensembling.  \nThe effectiveness of this approach is shown on three different tasks --\nlanguage modeling, image captioning and sentence classification; and\nperformance gains are observed over the baseline of single model optimization.\n\n- Weaknesses: Exact experimental setup is unclear. The supplementary material\ncontains important details about burn-in, number of epochs and samples\ncollected that should be in the main paper itself. Moreover, details on how the\ninference is performed would be helpful. Were the samples that were taken\nfollowing HMC for a certain number of epochs after burn in on the training data\nfixed for inference (for every \\tilda{Y} during test time, same samples were\nused according to eqn 5) ? Also, an explicit clarification regarding an\nindependence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X),\nwhich lets one use the conditional RNN model (if I understand correctly) for\nthe potential U(\\theta) would be nice for completeness.\n\nIn terms of comparison, this paper would also greatly benefit from a\ndiscussion/ experimental comparison with ensembling and distillation methods\n(\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble\nof Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are\nintimately related by a similar goal of incorporating effects of model\naveraging.\n\nFurther discussion related to preference of HMC related sampling\nmethods over other sampling methods or variational approximation would be\nhelpful.\n\nFinally, equation 8 hints at the potential equivalence between dropout and the\nproposed approach and the theoretical justification behind combining SGLD and\ndropout (by making the equivalence more concrete) would lead to a better\ninsight into the effectiveness of the proposed approach.  \n\n- General Discussion: Points addressed above.",
        "People were absolutely aware that Mazovian aurochs are the last of their species. Jędrzej Święcicki have written in mid 16th century *Topographia sive Masoviae descriptio* (Topography or description of Mazovia, published in 1634), in this book he included one of the most valuable descriptions of aurochs:\n\n >  In Jaktrów forest one can find herds of enormous aurochs. They are restricted solely for kings: under penalty of death no one is allowed to kill them. In ancient times, the animal had its habitats in the Germanic forests, nowadays (as far as I know) nowhere in Europe occurs, can be found only on this small patch of land, which is a remnant of the ancient Hercynia\n\nEarlier (1565) papal legate to Poland Fulvius Ruggierii have written that aurochs \"are to be found only in the Mazovian forests near\nRawa, there are not many of them in Prussia\". Venetian envoy Hieronymus Lippomano (1575) also noted that \"They say that aurochs can be found only in the forests of Mazovia, where they are guarded and keep for the king's hunts\".\n\nAurochs were restricted only for monarchs, we know about the laws from XIII century issued by Mazovian dukes that prohibit hunting on aurochs, and reserve them only for dukes. Polish kings and Lithuanian grand dukes were doing the same in their possesions and in Mazovia when it was included into Poland in late 15th century. \n\nPeasants living in surrounding villages were obliged to take care of the forest and especially of the aurochs. Their responsibility was provide them enough food, some of them called *łowczy* were responsible of fighting off poachers. They were free from any other obligations, tributes or servitudes. At first it was just to keep royal herds intact for hunting but since 16th century it was mainly to keep the species alive (similiary protected was żubr - european bison). There was also other factor, *tur*/aurochs was regarded as national symbol and pride of the kingdom. When king Sigismund III was granting to Stanisław Radziejowski the position of starosta of Sochaczew he reminded him to:\n\n >  Do his best in breeding of aurochs, not sparing any efforts and keeping guards in the fields, because they belong ad famam Regni, it is to the fame of the Kingdom\n\nAurochs or their horns were often granted as gifts to distinguished guests or foreign monarchs, like emperor or pope. They often were also invited on hunting. \n\nSince the half of 16th century aurochs were counted so we know exactly how many of them were still alive. It looks like royal warnings were not enough because during first years of Radziejowski rule almost all animals died out (from 24 in 1599 to just 4 in 1602). Radziejowski was tried by royal judges, but eventually he remained on his position. His fault was evident but it looks like the herd was already so small that basically condemned to extinction.",
        "This is an interesting paper about quantized networks that work on temporal difference inputs.  The basic idea is that when a network has only to process differences then this is computational much more efficient specifically with natural video data since large parts of an image would be fairly constant so that the network only has to process the informative sections of the image (video stream). This is of course how the human visual system works, and it is hence of interest even beyond the core machine learning community. \n\nAs an aside, there is a strong community interested in event-based vision such as the group of Tobi Delbrück, and it might be interesting to connect to this community. This might even provide a reference for your comments on page 1.\n\nI guess the biggest novel contribution is that a rounding network can be replaced by a sigma-delta network, but that the order of discretization and summation doe make some difference in the actual processing load. I think I followed the steps and \nMost of my questions have already been answers in the pre-review period. My only question remaining is on page 3, “It should be noted that when we refer to “temporal differences”, we refer not to the change in the signal over time, but in the change between two inputs presented sequentially. The output of our network only depends on the value and order of inputs, not on the temporal spacing between them.”\n\nThis does not make sense to me. As I understand you just take the difference between two frames regardless if you call this temporal or not it is a change in one frame. So this statement rather confuses me and maybe should be dropped unless I do miss something here, in which case some more explanation would be necessary.\n\nFigure 1 should be made bigger.\n\nAn improvement of the paper that I could think about is a better discussion of the relevance of the findings. Yes, you do show that your sigma-delta network save some operation compared to threshold, but is this difference essential for a specific task, or does your solution has relevance for neuroscience?",
        "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5",
        "The time frame matters here. In the very short term (hours) thirst is easier to ignore than hunger, and I'll get to that below. In the longer term, dehydration has a much bigger impact: if you don't have any fluid or any water, you will die of dehydration (~3 days) much faster than you will die of starvation (~3 weeks).  \n\nIn the short term, why can we ignore our thirst cravings? The answer is, we don't know! From wikipedia:\n  >  However, the true neuroscience of this conscious craving is not fully clear. In general, the end-result is towards behavior of drinking for hydration, but this can to some degree be resisted, such as in voluntary fluid restriction.\n\nThirst is complicated. It has to do with how much fluid you have in your body total, and also, how concentrated with salts is that fluid. A lot of body systems are involved (excretory, endocrine, cardiopulmonary, etc). Here is some speculation: I suspect that those systems are trying to work together to find a balance that works for the organism as a whole, so maybe no one system fully takes over -compelling you to drink- until you're quite dehydrated. \n\nHunger is slightly less complicated. It has to do with the cells of your body getting the nutrients they need. Each cell is like a car that needs fuel, and that fuel comes from food. When they don't have easy access to nutrients, they have to rely on less optimal means of operation. Your body's response to that is like, \"Eat something, ya jackass\". The digestive system releases a chemical called [ghrelin](_URL_0_), which causes your stomach to contract, aka \"hunger pain\" as a reminder to eat.",
        "The 2014 Mae Lao earthquake occurred at  on May 5. The epicenter was located at a point  south of Mae Lao District,  southwest of Chiang Rai, Thailand. One person was killed as a result.\n\nEffects\n\nThe earthquake was a recorded as having a maximum intensity of strong (MMI VI), shaking both northern Thailand and Myanmar in the evening. People in many northern provinces (including Chiang Rai, Chiang Mai, and Lampang) sensed the quake. Windows, walls and roads as well as temples all suffered damage from the quake. Originally no casualties were reported, but later there were news reports of one death and several injuries. It was the strongest earthquake ever recorded in Thailand according to National Disaster Warning Center Director Somsak Khaosuwan.\n\nChiang Rai International Airport, located near the epicenter, immediately evacuated people from its terminal. Airport general manager Damrong Klongakara said the runway and flights had not been affected by the quake. Even so, the airport was closed for a while.\n\nIn Phan district of Chiang Rai, a road was split by serious cracks. A Buddha statue's head at the Udomwaree Temple fell off due to the quake and a residential building of the temple suffered exterior cracks and ceiling damage. Several other temples were also damaged.\n\nA Chiang Rai police officer reported that goods in shops were scattered, cracks appeared in buildings, and some provincial roads proved to have \"large cracks\".\n\nIn Bangkok, tall buildings swayed as the earthquake occurred. Tremors were felt as far away as in Yangon, Myanmar.\n\nApproximately one hundred repeated aftershocks were reported by the Asian Disaster Preparedness Center.\n\nSee also\n List of earthquakes in 2014\n\nReferences\n\nExternal links\n Likelihood of earthquakes in Thailand\n \n\nMae Lao earthquake\nEarthquakes in Thailand\nChiang Rai province\n2014 in Thailand",
        " Prop their arm up on your knee or pillow to loosen the pectoral, anterior deltoid area to work through it more easily. You can start at the center of their chest by the clavicle and work towards you to clear the area thumb-width by thumb-width like combing through their muscle bit-by-bit. Then change their arm position to above their head and prop it up again; this changes the fibers of each muscle area, go through the area again for a more detailed, deeper effect. Place arm down to the side and go through again.;\n, Forearm flexors and extensors and then the hands, working with steady pressure in the palm, and gentle twist/pull for each finger.\n\n, Working knee to hip then hip to knee, then knee to hip slowly clearing the muscles deeper as they let you go through, like a wave in water, letting the muscle relax and move as you go through with your thumbs. If you work the inner or outer thighs, proceed with care, as they tend to be quite sensitive.\n\n\nAbdomen and Hips. If you work this area, you must be patient and gentle, as it tends to be sensitive. Start at the bottom of the ribs on the left and right abdominal muscles and work your way down to a couple inches above the front of the pubic bone, then work back up. Then slowly go back down and up a bit further out; repeat until you get to the obliques. Also work gently inside the iliac crest, and you can work quite a bit on the outside edge of the hip between the iliac area and the joint at the head of the femur.\n\n\n\n\n\n\n\n,, Calves the same way, including the muscles lateral to the shinbones in front, then feet; they are easier since the bottom of their feet can be propped up on a firm pillow. Use a couple run-throughs of increasing pressure in the arch and ball of the foot, then the toes as done to the fingers; You can have them turn their knee out and bend their leg out so that their illio-tibial band is up to work on; this also changes their hip to work on again around their trochanter (leg bone). End with their legs straight of course.\n\n,, You can start with their shoulder or hip, it doesn’t matter, all will be worked on. When a person is turned on their side, the muscles change their structure, some loosening, some tightening, work on the loose muscles deeper and slowly, concentrating and clearing those areas. Have them place their arm down to side to get at infraspinatus and rear deltoid easier; this also loosens trapezius as well. Carefully work and loosen the side of their neck on top, then have them turn their head so that their nose is down toward table and work the underside of their neck, by turning their head it puts a slight twist in the neck and shoulder muscles to catch and go through slowly and more thorough. Have them turn their head back to a comfortable place to go again through the shoulder, then lats, then rib cage and quadratus lumborum, by working this area from the side you can slowly sink through and clear to spine.\n\n,,, Here, start at the temples and work your way out, then through the ocular region, down onto the cheeks and jaw, and where the jaw meets the neck below the ears. Then work outward from the center of the forehead, and slowly running your fingertips longitudinally around the skull from front to back.",
        " If you're just starting out, you can find many of these things by looking at gift sets that include all or most of them, sometimes in a nice wooden storage box or a table easel. What you will need at a minimum is:\n\n\nA stretched canvas the size of the painting you would like to do. It's a good thing to also purchase several small \"canvas boards\" for practice and preliminary studies. You can also use canvas paper or canvas that comes in pads, as long as they say they're suited for oil painting and are gessoed. Try to choose a small board with the exact proportions of the stretched canvas but if it's not, get one larger so that you can mark up that shape on it.\nTubes of oil paint in a basic palette. If you're purchasing a set, it probably has all the most essential colors. The smallest essential palette has red, blue, yellow, Burnt Sienna and a large tube of white. If it's Winsor and Newton open stock, get Lemon Yellow, Permanent Rose and Ultramarine or French Ultramarine (they are chemically close.) If it's choosing primaries out of a set with more colors, use Alizarin Crimson or whichever the more purple cast red is, not the orange red. You could do without the Burnt Sienna but there's a reason for it besides mixing. If your set doesn't have it, use the reddish brown.\nBuy the oil and thinner. Linseed oil is a traditional oil painter's medium. Some artists like walnut oil better. If you want your painting to dry faster, choosing a medium like Winsor & Newton's \"Liquin\" will make the oil painting dry faster. You will also need turpentine, or odorless turpentine substitute, sometimes called turpenoid, or white mineral spirits. This is a thin liquid that has a strong or slight odor, it's a paint thinner as opposed to a medium. Odorless paint thinners, like Weber's Turpenoid or Gamsol, are reputedly healthier to use, but always have proper ventilation when using volatiles. Oil paint itself is not toxic in the way that turpentine is in that it doesn't give off toxic fumes. But some oil paints contain toxic ingredients like cadmium and cobalt that can be quite harmful if ingested. Never eat, drink or smoke while using oil paint.\nBuy some removable artist grade varnish such as Damar varnish intended for oil paintings. Varnish will probably have some toxic fumes and should be applied outdoors or in a well ventilated area. Definitely choose a removable artist grade varnish. Varnish is supposed to be added after the oil painting has completely dried all the way through and chemically changed to \"cure.\" At that point a clear removable varnish gets added to give it a nice glossy finish and protect the paint layer. Every 25 to 30 years, the varnish should be removed by a conservator (or the artist or owner) with a varnish remover solution and reapplied, because the varnishes become yellow over time and aren't intended to be permanent. This is why very old oil paintings turn brown. They often just need cleaning and a clear new coat of varnish to look as bright as if they were painted last year. You don't need to buy the varnish before finishing the painting, since you won't use it till the painting's done and completely dried. \"Retouch varnish\" can be used as soon as a painting is touch dry. It doesn't hurt the paint layer, but the painting should feel thoroughly dry and you should wait a good month before using it. That gives a temporary finish if you want to sell the painting sooner.\nBuy the brushes. Stiff ones are preferred. Bristle brushes are less expensive at the cheap end but good ones of either white synthetic fiber that's as stiff as bristle brushes are just as good. Some oil painters also use a soft sable brush with a long handle for different effects. Get a range of sizes, large medium and small, for blocking in areas, painting in the forms and objects and quite small ones for final details if you like detailed realism. A soft \"rigger\" brush with very long thin soft hairs gets used for ship's rigging, cat's whiskers and other long linear details in realism, it holds a lot of very thin paint and can be used to write your name small or do long smooth lines. For a beginner, it's recommended that you try a variety pack of bristle or synthetic bristle brushes with different shapes and sizes to discover the style each one creates.\nPalette knife, painting knife or non serrated butter knife to serve as one for mixing paint. Palette knives are pretty cheap though if you get the plastic ones. The nicer metal ones don't stain and will last for years if kept clean. Painting knives have different shapes like trowels and angled things, each has a different effect and you can use those instead of brushes to do your whole painting.\nCharcoal or a violet pastel pencil to sketch on the canvas.\nA palette to put your oil paints on while using them. This can be an actual palette with a thumb hole or you can improvise with a cheap plain ceramic, glass or melamine plate. Something that could stand up to being washed off with turpentine is good. Many artists prefer a gray palette because the colors show up truest on gray. If you use a flat piece of glass on a table (very cheap if you take it out of a cheap photo frame) you can put gray paper under it to have a gray easily cleaned palette for every time you need it.\nTwo small cups for oil (or Liquin) and thinner. Some sets come with a \"double dipper\" that clips onto a palette, if so then your set probably also has a palette.\nPainting rags. These can be any kind of clean rags. Strong paper towels will work but cloth rags are reusable if washed. Cloth baby diapers that have been used and washed, even worn out stained ones, make really good painting rags. Paper towels wear out fast––it's better to use old clothes that are soft like old t-shirts and stuff like that, actual rags. Try not to use fuzzy ones that shed though, since you may be wiping out painted areas with the rags. Use rags that are about at the end of their usefulness, unless you want to wash them out and keep reusing stained ones over and over.\nAn easel to work at, either a table easel set up on a table or a standing easel. This doesn't need to be expensive. The cheapest \"display easel\" will hold up any reasonably sized canvas at a comfortable working angle and its legs will adjust to a standing or sitting height. Unless you're disabled by age, disease or injury limiting the amount of time you can stay on your feet, it's much healthier to stand at the easel. This will also let you stand back every few strokes to see how the painting looks before adding to it, which makes for a better painting. You can also prop up the painting against a chair or other support, or otherwise improvise something. A \"painting horse\" is a bench with a board sticking up at the end that you straddle and prop the canvas into a groove.\nSketching supplies to plan the painting - pencil or charcoal, sketchbook or drawing paper or even scrap paper. They don't need to be archival since these are working sketches but if you like your sketches, you might as well get an actual sketchbook and use a soft pencil or even a pen or marker for it. Just something to sketch with and something to sketch on, your favorites. Your usual sketchbook and favorite drawing tools.\nA safe, dust free place to put the wet painting to dry where nothing is going to bang into the wet side to smear it. Drying times for oil paintings vary from a few days to several months. Some types of oil painting take up to a year to \"cure\" before they can be varnished.;\n, If it's a square, that's square. If it's rectangular or oval, decide if it's going to be vertical \"portrait\" orientation or horizontal \"landscape\" orientation. Do the notan drawings very small, just to place the light, dark and medium areas on the design. They can range from a large postage stamp to a business card size - the idea is to see it as if it was at a distance or a thumbnail. Do lots of them till you find the best design without worrying about the details.\n\n, It can be quite detailed and shaded carefully or just loose to show you where the shadows and highlights are. This partly depends on how detailed and realist you want the painting with. A looser painting style can have a sketchier value sketch, but should still have one with more than \"white middle and black\" so that you can tell where there are at least five values - white accents, light value, medium, dark, black accents. Some painters like to not use pure black and white but just use \"light, light middle, middle value, middle dark, dark\" for the five values. It depends on the effect you want. If you don't like the sketch keep trying different versions of it till you get one you like.\n\n\nIn the sketch, make sure the light falling on the person, objects or landscape elements is all going in the same direction. Pay attention to where the shadows go. They should all go the same direction and are shorter when the sun or lamp is high, longer if it's later or earlier in the day and the sun is low (or lamp is low). Directional lighting will make all the objects look more three dimensional. Draw the shapes of the shadows carefully and most of your subjects will look three dimensional at that point. This makes for good Impressionism or realism.\nIf you want to do an abstract, do the pencil sketch loosely and work out where you want particular effects like spattering or strong texture strokes. Or skip the sketch stage on paper and proceed to the next.\nSketch the subject on the canvas board, canvas paper or canvas pad. Use charcoal or your violet pastel pencil. Mark up the exact proportions of the canvas on the board or pad if it's not exactly the same shape, so everything's placed the way it is in the planning sketches. Do this drawing as pure outlines. You can get detailed for realism by marking up eyes, mouth, any important shapes on it or you can keep it very simple just to the main shapes and main shadow shapes. Either way it should look like a Paint By Numbers canvas when the sketch is done. If you make mistakes, wipe off the charcoal or pastel pencil with a damp cloth, let that area dry and draw it again. Very correctable.\n\n, Set out your yellow, blue, red and a larger dab of white with some distance between them. Optional, use Burnt Sienna as well. Leave all the other colors in the box if it was a gift set.\n\n, Just paint right over the sketch into the areas of each color. Because this doesn't need to be detailed, you can try painting the color study with the palette knife or painting knife. If you don't like any of your color choices, use the palette knife to scrape off that bit and put the mixed-up paint off to the side on your palette in case you need some muddy brown. The mix of all three primaries will harmonize throughout the painting and so the mixed up paint can be separated and mixed with a little more to turn it into pale or dark browns and grays. No waste with a simple primary palette. Keep playing with the Color Study until you like it as a simple, bold painting done with a fairly big brush and not much detail. If necessary, do more than one of them till you work out what mixtures and colors you like. You're doing this little practice painting with the paint right out of the tube. It doesn't need either thinner or oil for this technique. If you like the look, you can do the big painting the same way just by using the palette knife and tube paint with bold strokes onto the canvas, no extra oil and no thinned out layer. That's a style of oil painting that's fast and powerful.\n\n, On a landscape painting, using a violet pastel pencil is a very good choice because that color blends well with all the landscape colors without darkening or staining light colors as much as black. Charcoal and the violet pastel pencil are both easily corrected with a damp tissue or rag, so don't worry about making changes to the sketch! Draw it in, if you get it wrong wipe off the wrong bit and try again.\n\n, Wipe your brushes and palette knife clean. Wash the brush you used if you used it for the color study, using the turpenoid - just dip it in the thinner and squeeze it out with a painting rag.\n\n, Or if there isn't any white or much white in the three-color mud mix, use that for your brown thin layer. Thin it out by dipping your brush into the thinner, turpentine or turpenoid or Sansodor (the Winsor & Newton brand is good). Dip the wet brush into a little bit of paint and squeeze it around till you have very thin, transparent paint that's light. Paint in the light areas on your painting following the notan. Using a little more paint, do the medium light and successively darker areas with the Burnt Sienna, still thinning it till it's like ink in texture. Even the dark areas should have a fair amount of paint thinner in them. The more thinner you use, the faster this transparent Burnt Sienna value layer will dry.\n\n\nWow. The transparent value painting in Burnt Sienna usually looks pretty cool at this stage. It's still easy to change if you got it too dark somewhere or too light somewhere. Take a rag and wipe off the part you don't like and redo it the right value, or add a little more color. Or wipe out and change the shape. Gee, you thought oil painting had to be perfect, nope, it's very easy to correct and make changes all the way through. This stage will dry pretty fast, within a few minutes to half an hour. The thinnest parts may be touch dry by the time you finish the other corner. It only needs to be touch dry.\n\n, This is structural. That first layer - the value sketch you paint over - was very lean - almost all turpentine or turpentine substitute, very little oil. Just the amount of oil in a little bit of staining paint to make it show up. It looks almost like watercolor on paper at that thinner layer. You can do successive washes in different colors if you want a fun technique at the \"wash\" layer. The next layer is \"Alla Prima\" or paint right from the tube the way you did the color study. That's sort of medium fatness, like someone who's not fat or skinny. After that, the more oil or Liquin that you add to the paint, the fatter it is. The problem of Lean Over Fat is that the oiliest layers dry the slowest, so the faster drying paint should be under it. Otherwise the outside will dry before the inside and the inside might remain squishy and sealed.\n\n\nWorst case, a painting that has Lean over Fat can slide off the canvas on a hot day, losing all paint cohesion. This happened at least once to a past student of a teacher who told the story.\nNever use oil pastels under oil paint because their oil formula includes mineral oil that never dries. You can optionally add oil pastel marks on the last layer of an oil painting when it's touch dry.\n\n, Mix your colors half on the palette, half on the canvas. Start with getting the main areas of light and shadow blocked in with the right general colors, then add in more paint to modify them. Shade gradually and blend them gently where you want the paint to be smooth without showing much brush strokes. Dab on lots of it and leave it where you want strong textures like an Impressionist painting, or use knife strokes to make bold textures. Contrasting smooth textures and bold ones so that some parts of the painting are raised \"impasto\" texture and others are smooth and carefully painted is very lively. So vary the amount of \"alla prima\" texture you put on. Mix some of the oil into the paint if you want to lay it on thin and brush out the brush strokes to keep it smooth. As long as it's still wet, you can mix more oil or more paint in to make that layer fatter or thinner. But if it starts to dry or skin over, don't put anything leaner over what has fat in it.\n\n\nUnless you want a really ugly special effect, like painting a zombie's face and putting a big pocket of fat in on the cheek, then letting it dry wrong, then ripping it open to have the paint skin dangle down and the clump of brownish-red fat paint hit the air and dry solid, maybe dripping over the rip. Almost any mistake can be turned into a special effect once you know how it works.\n\n, This means you can paint all day, fool around with it, go to bed, put an empty box over the palette so your cat won't walk in it, start over tomorrow and keep making changes while it's wet. You can use the palette knife to scrape off whole areas of it before it dries and start over. Oil paint's slow drying time allows plenty of changes before you decide it's done and let it dry.\n\n, It would take at least two weeks unless you used Liquin as your medium. Liquin dries faster than paint from the tube, so use at least a bit of it into all the paint so that it all bonds well. It's not fat, but oil right from the tube is. You can also get alkyd oils that have alkyd (the main ingredient of Liquin medium) right in the tube paint, where the painting may take only a couple of days to a week to touch dry depending on how thick the paint is.\n\n, Start as described here, do the thin Burnt Sienna layer, then using tube texture paint and carefully brushing it, do a realistic black and white painting with all the details of your subject just using Ivory Black and Titanium White. Let that \"grisaille\" or \"dead layer\" dry thoroughly. It will look like a black and white photo in a way, very detailed. Then start mixing oil with all your colors, using them very thin, start painting over the grisaille layer. Covering the black and white painting with various transparent colors will let the light bounce back and forth within the dried layers and give it a unique luminosity. Only slow, heavily layered colored pencil rendering comes close to the effect. It's one of the things oil painting is famous for.\n\n\nYou can try this method if you have a lot of time to let each glazed layer dry before doing the next. But if you don't want to take that long, just let the grisaille dry, add a bit of oil, paint over it in the right colors and add one final glaze when that layer's dry. You can get as elaborate or as simple as you like with oil painting.\n\n, Repeat several times till almost all the paint is off before swishing them in thinner, otherwise it wastes thinner. Store your painting rags and supplies away from any open flame or electrical circuits or heaters or anything that can start a fire. Seal them in a metal can if you have one handy. If you store your palette with paint squeezed on it in the fridge, it will slow the drying and you can use the squeezed out paint longer. But don't let anyone mistake it for food.\n\n, You can build a vertical drying rack with a home built cabinet where you put peg board panels a couple of inches apart to lean one wet painting into each slot. If you do a lot of oil painting, this is a good DIY project to leave you more garage space. Since you are creating fumes with the thinner, it's a good idea to use the garage and other areas that people don't spend as much time in or have very good studio ventilation. Storing them in vertical slots reduces the amount of dust that falls on the painting while it's drying, it'll mostly accumulate on the top edge instead of the front of the painting.\n\n, Just paint the sides too, either wrap the painting around or paint them black or put a design, do something fun with it. Then you don't need to buy a frame to either sell it in a gallery or give it as a present, it's ready to hang when it's dry and varnished.\n\n, Some colors dry matte and flat, others shiny, it can be annoying till the varnish is on. Then wait another eleven months to add Damar varnish or any other removable conservator varnish and let that dry for a few days. Your painting will now last longer than you will.\n\n",
        "This paper proposed the group sparse auto-encoder for feature extraction. The author then stack the group sparse auto-encoders on top of CNNs to extract better question sentence representation for QA tasks. \n\nPros: \n- group-sparse auto-encoder seems new to me.\n- extensive experiments on QA tasks. \n\nCons:\n- The idea is somewhat incremental.\n- Writing need to be improved. \n- Lack of ablation studies to show the effectiveness of the proposed approach. \n\nMoreover, I am not convinced by the author's answer regarding the baseline. A separate training stages of CNN+SGL for comparison is fine. The purpose is to validate and analyze why the proposed SGA is preferred rather than group lasso, e.g. joint training could improve, or the proposed group-sparse regularization outperforms l_21 norm, etc. However, we can't see it from the current experiments. ",
        "While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.\n \n (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)",
        "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.\n\nExperiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.\n\nI don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:\n\n- As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.\n\n- One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.",
        "Here's my understanding of what happened to the Roman military machine.\n\nEveryone knows about the famous Roman legions and how they operated. They were the the most professional, well-equipped, trained, led, and logistically supported force until the early modern era. Superior discipline, equipment and tactics made a competently led Roman legion, man-for-man, the most effective fighting force of the Classical era. With this in mind, Caesar's account of the Gallic wars makes sense, even without likely inflation of numbers.\n\nBut the Roman legions had some drawbacks.\n\n1. **The Roman discipline and tactics relies upon very strong *esprit de corps* and unit cohesion.** If you don't have a pool of culturally homogenous, motivated volunteer recruits, the combat effectiveness of the unit drops. What made Roman legions and cohorts so feared was the fact that they would never break and run unless shit reaaallly got ugly. Furthermore, in the Republic and Early Empire, the legions were still predominantly Italian, and facing foreign foes. It's no surprise that some of the worst performances of Roman legions occurred in civil wars. Furthermore, in the Imperial Era, to deal with the fact that legions were no longer citizen levies and instead standing professional forces, they adopted a proto-regimental system, where the history and culture of the Legion became the rankers' primary loyalty. \n\n2. **Equipping/maintaining a Roman legion was expensive**. Your average *miles* had a helmet, laminated or mail armor, a gladius and dagger, shield, and two *pilum*. That's a lot of metalworking for a pre-industrial society. One wonders what might have happened to Rome if they had developed steam power, or even mass utilization of coal. The main energy inputs for Roman heavy industry were either water power or charcoal. Producing charcoal in large quantities requires a massive amount of wood, and water power is ineffective for smelting and smithing. This means that churning out huge quantities of steel is very cost prohibitive. Furthermore, feeding a legion in the field requires a great deal of logistical support. These costs could all be borne when the Roman empire was economically firing on all cylinders and taking advantage of their ahead-of-their-time road network, but became problematic as the Empire transitioned into the later days. \n\n3. **A Roman legion functioned at its best when it was well led and well trained, particularly when facing predominantly cavalry armies.**[The battle of Carrhae](_URL_0_) very effectively demonstrated the systemic weaknesses of the Roman legion. Take a large army, give it indifferent leadership, get it in a foreign area, away from supply bases, and have it blunder into a large engagement against another professional army composed of both horse archers and heavy cavalry. The result? Ugly. The simple fact was this: Roman legions were the best infantry of their day, but this came at the cost of organic cavalry support. Which meant that facing large groups of cavalry, you better hope your legion has a very competent tactician and/or a strong defensive position.\n\nNow, we move to the time of Augustus. In his day, the Roman army was at one of it's peaks, with most of the traditional boundaries of the Empire in effect. However, Augustus was running into a new problem: the ability of the legions to expand and hold the boundaries of the Empire was diminishing. They were limited by the Parthians in the east, and the Germans across the Rhine and Danube. Augustus tried an adventure or two across the Rhine, but all that led to was  a massacre of three legions in the[ Teutoberg Forest](_URL_2_), after they blundered into a trap far from home.  So rather than risk expensive legions in foreign aggressions with a questionable stable of generals to draw upon, Augustus decided to switch to a defensive posture, and let the economic gains of the Empire come from developing the swathes of territory it had conquered in the last century, rather than wars of aggression. \n\nThis solved of logistical support and a reliance on good leadership to deal with less-than-optimal tactical situations. And this worked for a while. But this in turn exposed some new problems:\n\n1. The garrison troops became a power base of ambitious generals and governors to exploit in the event of weak central leadership. \n\n2. The pool of recruits began to dry up, as inflation reduced the legions' pay to nonexistence. As a result, they became more dependent on slaves, provincials, and eventually conscription for recruits. This exacerbated problem 1, as the Legions became more dependent on their local areas for fresh recruits and logistical support, making their loyalties more regional/cultural rather than to the Empire as a whole.  \n\n3. The institutional memory of the Legions, formed up the centurions, gradually dropped, as the Romans fought fewer battles against novel opponents.\n\n[Now we get to the Late Empire,](_URL_1_) and the now **six** major issues with the Roman Legions became very problematic. These formations were now very expensive, of reducing combat effectiveness, and a major internal security threat. So the late Emperors responded to this in several ways:\n\n1. They culled the best troops into central reserves. This started with the Praetorian Guard and became the *comitatus praesentales* of later Emperors. These formed both an internal security force and a second line of defense, if a barbarian invasion breached the line of frontier forts. \n\n2. They broke the legions up into smaller forces, and established a hierarchy, where legions were ranked on their combat effectiveness, and therefore support in terms of equipment and armament. \n\n   -The *palatini* were the top line troops, and found almost excusively with the reserves under direct control of the Emperors or their seconds.\n\n -The *comitatus* or regiional field armies, were the second tier.\n\n -And the *limitanei*, which were explictly garrison troops.\n\n3. They started using more cavalry to perform specific roles (specifically heavy and light cavalry- the *Scholae*, rather than smaller contingents of general-purpose cav). In addition, they also increased the overall headcount of the Roman army globally, to transition from a forward-defence strategy towards a defense-in-depth, as well as to compensate for reduced man-for-man effectiveness. \n\n\nNow as we're beginning to see, the Roman military machine that conquered and held the European world is rapidly beginning to become more trouble than its worth. Its size was becoming too burdensome to support economically, for diminishing effectiveness, and the internal security threats caused by ambitious generals and weakened central leadership was undermining the system.\n \nNow this is where the *feoderati* enter the picture. The Romans, both East and West, slowly realized that it was pointless to waste their best troops fighting off barbarians that would just keep coming, year after year, and instead, switch to buying off the barbarians with land and tribute, and use them as their troops to keep the other barbarians out. Furthermore, they became increasingly dependent on these barbarians to do the brunt of the fighting, to keep their high-grade troops intact. And well, we all know how well that worked out. \n\nBy this point, the legions as we think of them had pretty much ceased to exist, in the sense that all the things that defined them were no longer present. So at that point, why bother with the structure of the legion, which was dependent on a state to support, and required a great deal of support and cohesion to maintain.\n\nMercenaries and mercenary forces would be much more effective as irregular infantry fighting in open order, or cavalry forces, rather than a heavy infantry unit, fighting in close order.\n\nIt is worth pointing out however, that the East did retain the Legion system for a while after the West fell, but they also moved away from the Legions as they became dependent on foreign troops as well, and adopted the Theme system, which used field armies supported regionally by subdivisions of the Byzantine Empire, known as Themes.",
        "  This includes the airline, your flight's gate, the departure time, estimated arrival time (sometimes you won't hear this until you get on the flight) and your seat number.  Write this down on a card and stick it in a zipper pocket of a jacket or purse.  It's also a good idea to make a note of it on your phone, or write it on a slip of paper and place it inside your phone case.;\n,  Since you're traveling alone, you don't want to be weighed down with 50+ pounds of luggage.  Pack a small suitcase or medium-sized bag with:\n\n\nentertainment for the flight, such as books, colored pencils, a sketchpad or journal, headphones, etc.\nany medication you might need, such as ibuprofen for headaches or a stomachache relief tablet\nclothes and other important things (contacts, spare glasses, hearing aids, toothbrush) in case your luggage gets lost, delayed, or stolen\nsnacks (even if there will be a meal served)\ngum, lollipops and hard candy or mints to suck on and relieve pressure and ear popping\n\n, If you are blind, deaf, mute, in a wheelchair, etc., make sure a flight attendant on the plane is able to help you in case you need anything or would be able to assist you in the case of an evacuation.\n\n\nIf you are blind or physically limited in any way, make sure you can get to the lavatory easily.  If you can't, alert a flight attendant.  Some planes have flight-attendant alert cords to pull if you can't reach the alert button.\n\n,  Be sure you can receive a call, place a call, or turn the phone on and off.  This is important for communicating with your parents and the person picking you up at the end of the flight.\n\n,  Your bag may be the same as someone else's, but you will be able to tell yours from theirs if yours has a colorful ribbon tied around the handle, or a strip of polka-dot duct tape across the front.\n\n,  Put your name on the tag of your sweater and inside your shoes.  This may prevent you from losing them and from people walking off with them.\n\n,  Keep your ticket, cell phone, your passport, and any cash you're bringing in the purse.  Don't let it leave your side until you reach your destination.\n\n,  Don't wear anything that will be removed for security but takes a long time to be put back on.\n\n\nGirls, do not wear anything form-fitting.  You will be alone, and the airport is a very dangerous place for young girls.  Wear clothing that fits properly but does not show off any part of your body.\n\n,  Be sure that your suitcase(s) is/are fixed with labels and left with airport employees.\n\n,  If you were dropped off at the airport and are fending for yourself, make it look like you know where you are going and are determined to get there, even if you're not quite sure where to go.  Make a point of asking for help if you get lost.\n\n, When you reach security, remove your shoes and sweater before you get to the line.  If possible, load all of your belongings into a bin and carry the bin to the line, to help you move through faster.\n\n\nWhen you leave security, ensure that you have everything.\n\n,  If you realize you forgot something, like gum, you can buy it at one of the convenience stores.  This might be a good time to buy breakfast or lunch if you haven't already and it isn't being served on the flight, or go to the bathroom.\n\n,  Bring them into the bathroom stall with you and keep them right beside you in line for coffee or a magazine.\n\n,  Pay attention to what the flight attendant is saying.  You don't want to miss your category.\n\n, If someone on the flight is making you uncomfortable, don't hesitate to call a flight attendant.  You are, after all, an unaccompanied minor.\n\n,  In case of an emergency, you need to know what to do.\n\n,  Get a drink and a snack when the cart comes around.  If a meal is being served, eat it.  You don't want to be hungry or thirsty. That being said, listen to your body. If the food is gross and you get nauseous easily, pass on the meal and go for some pretzels and ginger ale instead.\n\n,  Breathe deeply and try to sleep.  Distract yourself with books and other things to do.  Eat some pretzels.  Drink some soda.\n\n,  Dispose of any garbage.  Make sure nothing is left behind.  Tie your shoes and put on your sweater even if you're feeling warm: you don't want to lose it.\n\n,  If items up above shifted during the flight, you'll be safe from falling objects.\n\n,  Inform them that you are off the plane and you will be at the baggage carousel.\n\n,  Before taking up a spot beside a carousel, make sure it has the luggage from your flight, otherwise you'll be standing there for a long time.\n\n, When you have your luggage, call the person meeting you again and ask them where they want to meet up with you.  Make your way there, or stay at the baggage claim and wait for them if they tell you to do so.\n\n\nDon't leave with anyone that you don't recognize.\n\n, While you are walking to meet them or waiting, call your parents and inform them that you landed safely and have all your luggage, and that you love them.  Parents really appreciate that last bit.\n\n",
        ";\n,,, The quantity and how much white and high fibre etc are up to you and your taste buds. For the first time, perhaps a kilo will be a good start.\n\n, Roughly, a bit more or less will not hurt and more will probably help it to rise faster.\n\n, For a kilo of flour, probably about 4 teaspoons is enough. You can't miss this stage because the salt helps the dough to rise. Don't use Lo-salt as it doesn't work. Really, it's up to you, according to your taste but you must have some.\n\n, It can be Lo cal if you like but it's more expensive. It's just like sprinkling sugar on cereal - according to taste. You will get the hang of it after two or three goes. Sugar, however, isn't an essential ingredient.\n\n, Do not use \"light\" because it's mainly water. Really it's up to your taste. It will help the bread to last longer but it's not absolutely necessary. Four or five knobs over the top for a kilo.\n\n, Make more than you need according to how the flour reacts. Use you fingers to test. When you get used to it you can forget the cups.\n\n, Add bits of water at a time until the flour absorbs it. This is the intuitive bit because you will get good at it after a few times.\n\n,, Press the centre with your right hand and fold over the dough from the left with your other hand. The idea is to trap the air. (Opposite hands for left handers.)Repeat for five to ten minutes.\n\n,,, Let it rise until doubled in size. If in a hot country, about 45 minutes or cold country, in a warm place for up to two hours.\n\n, Then knead again for 1 minute only.\n\n,,,, Put it near the top, if it's an old oven.\n\n,,,, Don't burn yourself. Tap the bottom of the loaf. Hollow sound means it is done.\n\n, Freeze if desired in half loaves in plastic bags so it doesn't go stale and enjoy.\n\n,,",
        "\n\n\nBefore you even look at that closet, hit the shower.;\n,,\n\n\nYour face should be clean-shaven and your sideburns clean cut, unless you can pull off the mop top.\nAcne should be kept under control, birth marks can't be helped but some Oxy pads or Clearasil will be more than enough.\n\n\nLean into the mirror, consider the probability that girls will be talking to you from less than two feet away.\nUse a nose hair trimmer even if you don't think you need to.\n\nYour unibrow should be kept in check (read NO UNIBROW).\n\n\nComb your hair. Your hair should be styled, or at the very least not messy or grungy. If you can pull off the bald look (not many can), go for it.\nSport facial hair at your own discretion, but mustaches aren't exactly what's hip right now (coming from a white guy).\n\n,,\n\n\nNot brushing, flossing, and using a little bit of mouthwash is a cardinal sin in her eyes, and unpardonable.\n\n,,,, Your clothes should be in good shape (stains and holes are a BIG turnoff), and not wrinkly. If you have to, iron your pants or shirt.\n\n\nTrying to go out in a button-down while attempting to hide a missing button, whether at the bottom of the shirt or on the cuffs, is a classic mistake.\nGirls are very keen on minuscule discrepancies like these and will be the first ones to notice.\n\n,, This also goes without saying, but consider where you're going tonight and remember Carmine Lupertazzi's dictum: A Don doesn't wear shorts!\n\n\nUnless you're going to a summer night barbecue, pants or jeans are the way to go. Dockers and slacks work best, but if you have the moxie to wear khaki's then have at it.\nFaded jeans with modestly sized holes is what's hot in the streets nowadays, but if you decide to go down this road, don't look like you're ready to go to a Grunge concert.\nYour pants should go down past your shoes and at the back of the heel stop before the rubber sole.\n\n,, You can't go wrong with a polo, button-down, or long-sleeve shirt, but wear a T-shirt at your own peril, and avoid shirt pockets!\n\n, If you must wear jewelry, try something like low keyed.\n\n,,,\n\n\nUnless you're over 35, a leather watch-strap is more often than not inferior to a metal band (non-expandable).\n\n, You're probably not going to a millionaire summit meeting, so try not to wear more than two or three rings, if any. Find a piece that is unique to you, it's a good icebreaker and many girls will compliment your trinket of individuality!\n\n,\n\n\nIs everything in place?\nAre your pants congruent with your shirt?\nDoes your jewelry clash with your outfit?\nAre your shoes stylish without being tacky?\nIs your collar shaped around your neck correctly?\n\n,,,,\n\n\nWhether at a casual dinner, bar, club, lounge, or hangout spot, your clothes are only as good looking as you carry them. Now go out there and do some work, son!\n\n,",
        "  We report the measurement of extremely slow hole spin relaxation dynamics in\nsmall ensembles of self-assembled InGaAs quantum dots. Individual spin\norientated holes are optically created in the lowest orbital state of each dot\nand read out after a defined storage time using spin memory devices. The\nresulting luminescence signal exhibits a pronounced polarization memory effect\nthat vanishes for long storage times. The hole spin relaxation dynamics are\nmeasured as a function of external magnetic field and lattice temperature. We\nshow that hole spin relaxation can occur over remarkably long timescales in\nstrongly confined quantum dots (up to ~270 us), as predicted by recent theory.\nOur findings are supported by calculations that reproduce both the observed\nmagnetic field and temperature dependencies. The results suggest that hole spin\nrelaxation in strongly confined quantum dots is due to spin orbit mediated\nphonon scattering between Zeeman levels, in marked contrast to higher\ndimensional nanostructures where it is limited by valence band mixing.\n",
        "1) Summary\n\nThis paper proposes an end-to-end hybrid architecture to predict the local linear trends of time series. A temporal convnet on raw data extracts short-term features. In parallel, long term representations are learned via a LSTM on piecewise linear approximations of the time series. Both representations are combined using a MLP with one hidden layer (in two parts, one for each stream), and the entire architecture is trained end-to-end by minimizing (using Adam) the (l2-regularized) euclidean loss w.r.t. ground truth local trend durations and slopes.\n \n2) Contributions\n\n+ Interesting end-to-end architecture decoupling short-term and long-term representation learning in two separate streams in the first part of the architecture.\n+ Comparison to deep and shallow baselines.\n\n3) Suggestions for improvement\n\nAdd a LRCN baseline and discussion:\nThe benefits of decoupling short-term and long-term representation learning need to be assessed by comparing to the popular \"long-term recurrent convolutional network\" (LRCN) of Donahue et al (",
        "While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. \nThere is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems. \n\nI think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. \nI have no significant objection to the presentation or methodology of the paper. \n",
        "  We review the possibility that the Supersymmetric Standard Model arises from\norbifold constructions of the E_8 x E_8 Heterotic Superstring, and the\nphenomenological properties that such a model should have. In particular,\ntrying to solve the discrepancy between the unification scale predicted by the\nHeterotic Superstring (g_{GUT}x5.27x10^{17} GeV) and the value deduced from LEP\nexperiments (2x10^{16} GeV), we will predict the presence at low energies of\nthree families of Higgses and vector-like colour triplets. Our approach relies\non the Fayet-Iliopoulos breaking, and this is also a crucial ingredient,\ntogether with having three Higgs families, to obtain in these models an\ninteresting pattern of fermion masses and mixing angles at the renormalizable\nlebel. Namely, after the gauge breaking some physical particles appear combined\nwith other states, and the Yukawa couplings are modified in a well controlled\nway. On the other hand, dangerous flavour-changing neutral currents may appear\nwhen fermions of a given charge receive their mass through couplings with\nseveral Higgs doublets. We will address this potential problem, finding that\nviable scenarios can be obtained for a reasonable light Higgs spectrum.\n",
        "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: “Measuring the reliability of MCMC inference with bidirectional Monte Carlo” is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p’(x)] <= p(x) but I think they mean: E[log p’(x)] <= log E[p’(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can’t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great. \n",
        " In purchasing adequate photographic equipment, read camera reviews and research photography trade periodicals.;\n, Bring some form of an extra lighting source as well. Relying on flash vs. a directional lighting unit spells the difference between amateur and professional.\n\n\nDepending on length of your vacation or travel plans, be sure to take along extra media memory cards > 1g. should provide ample space for your picture storage needs, depending on file sizes. Should you have plans to shoot video on the same camera then planning for extra digital memory cards are even more essential.\nRemember to pack up your camera's battery charger -- the same one that came with the camera at time of purchase. Without a way to charge your cameras battery you would have to settle for buying postcards, giving up your role as travel photographer.\nDuring your journey, it is prudent to carry an extra camera battery that is fully charged, in order to never miss those \"once in a lifetime\" shots, due to failed battery power.\n\n, In return you will be rewarded with years of pictures par excellence! Don't forget if you are using your camera on a daily basis, wipe your camera down nightly with a soft smooth cloth, using a small bottle of cleaning solution required for the maintenance of your photographic equipment.\n\n, Some modern digital cameras are so feature rich that it will take you time to learn all the benefits and nuances of your particular camera. Become an expert with its uses. Learn more photographic skills here, for free!\n\n, Shoot into a variety of lighting situations. After all, aren't those early morning dewy sunrises and shimmering golden afternoon sunsets capable of stirring our deepest being. These scenes powerfully affect us on many levels of which we are only beginning to understand. They can even remind us of our relationship to the infinite, our relationship to our creator. Learn more about photo lighting.\n\n, It is your center of interest that will make or break selling your travel photos.\n\n, Let's not forget: It is simple landscapes that often sell the best. People will buy these travel photos for several reasons, though for the most part these photos are purchased and subsequently framed and displayed, so they will eventually draw in our vision, while drawing out our fondest memories. The pictures can be divided into the categories of \"destination driven,\" \"activity driven,\" or simply \"people & life experience driven.\"\n\n, Destination travel photos will always be most popular. They remind people of having been there themselves or that this is somewhere they dream of going. Usually a destination is symbolic for a deeper value, such as a mental association with adventure, love & romance, perhaps some sweet friendships, experiences we have not remembered for years.\n\n\nSome feel that simple candid, non-posed people and portrait photographs remind them of their relationships with each other and the love that was shared. These travel photos should leave one with a sense of longing. Perhaps some sadness. Perhaps joy. As long as the picture moves us emotionally - perhaps we are reminded of our humanity, or our courage, our faith, or maybe they just remind us of loved ones we miss so dearly. Certain pictures have the power to transport us back into our youth, remembering good times.\n\n, Landscapes, seascapes, nature photos, landmarks, people and culture pictures, etc.\n\n, The photos to be marketable have to be sold \"professionally packaged\" so that the consumer can immediately take them home, display them or hang them up and enjoy them.\n\n",
        "Time to get some sense into this thread. I wrote a short bit about berserks [here](_URL_1_) (look further down in the comments to find [searocks collection of relevant links](_URL_0_)), so let that serve as a tl;dr on the background.\n\nNow, there are two ways to approach 'berserk studies': look at them like I did there, see them within an earlier pagan root of animal transformation (for which we have archaeological and pictoral evidence), which places them within a social tradition and basically makes them little more than a specialised group of regular warriors. By this interpretation, the 'berserk' would be pretty effective, as they are basically trained warriors.\n\nThe other approach is when you start from the Icelandic sagas, written after Christianisation. The berserks from the literature are a bit different, in that they are often just lone weirdos who go about Iceland making trouble and starting fights with random people. This is the kind of 'berserk-ness' that can be heriditary ('he is a berserk, and his father was too'), basically more of a way of life. Their role in the sagas often is that of a boogyman, a scary person, a half-monster. In that sense, they take over the literary trope of the half-man that Grendel has in the Beowulf, for example, who could also not be harmed by steel. These berserks have little to do with military fighting formations, and are mostly literary tropes (hence their later occurrence).\n\nA real, historical berserk that an anglo-saxon in the 10th century might encounter in battle is thus a bit difficult to place. The most realistic form would probably be the first type of totemic warrior. In this case, their animal transformation training, or their membership of a bear cult, might help them in battle. Both the Irish annals and the Byzantine accounts of the customs of the Varangians say that these warriors would 'sing' and dance into battle. This is actually something that armies of the early modern period also did, as did the armies of antiquity (though not the Romans who were known for their stoic silence in battle). It increases morale. Coordinated dance (as is common in shamanistic cults) also is good training for group movement in battle, which is fundamental for the kind of formation shield/spear fighting that was common at the time. So yes, in that sense the berserk/bear warrior would probably be a very effective warrior, but paradoxically, because of their relative discipline.",
        "This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. \n\nI would like to point out that a very similar attempt has been done in one of the past works in this area - ",
        ",\n\nIn General, set R1C1 to Off and select Show the 10 Most Recent Documents .\nIn Edit, set all the first options to checked except Automatically Convert Date System . Set Display number of decimal places to blank (as integers are preferred). Preserve the display of dates and set 30 for 21st century cutoff.\nIn View, click on show Formula Bar and Status Bar and hover for comments of all Objects . Check Show gridlines and set all boxes below that to auto or checked.\nIn Chart, allow Show chart names and set Data markers on hover and leave the rest unchecked for now.\nIn Calculation, Make sure Automatically and Calculate before save are checked. Set max change to .000,000,000,000,01 without commas as goal-seeking is done a lot. Check Save external link values and Use 1904 system\n\nIn Error checking, check all the options.\nIn Save, select Save preview picture with new files and Save Autorecover after 5 minutes\nIn Ribbon, keep all of them checked except Hide group titles and Developer .\n\n, Edit Go To cell range A1:H2912 and Format Cells Number Number Decimal Places 4, Font Size 9 or 10,, Insert New Comment \"This controls the ring size.\" Format cell Fill yellow, Border red bold outline.\nSelect cell range A1:E2 and Insert Name Create Names in Top Row, OK.\nF2: \"=2^6*3*PI()*103\" w/o quotes, which equates to Tip's value.\nA3:: Tip\nB3: Base\nC3: Spheroids\nD3: ShrinkExpand\nE3 103 and Format Cell Number Number Decimal Places 0. Format cell Fill yellow, border red bold outline. Insert New Comment \"Primes that work up to 107 are: (1),13,17,(29),(31),43,47,(59),(61),73,77, (89),(91),103,107 and -1 turns the other way … as does 107, with parentheticals having only half the turns or so ...823 and 827 are good, as are 1037, 4813.  This is the pattern: 13, 17 + 30 = 43,47 + 30 = 73, 77 + 30 = 103, 107 ... 13, 17 - 30 = -17, -13 which are both good!! So ±13±30n or ±17±30n, n={0,1,2,3,...}. Other primes and non-primes yield interesting results, but they're not spheres.\"\nF3: Prime\nSelect cell range E3:F3 and Insert Name Create Names in Right Column, OK.\nA4: \"=2^6*3*PI()*Prime\" w/o quotes\nB4: \"=2^4*Prime\" w/o quotes\nC4: 24 and Format Cell Fill Light sky blue.\nD4: .5 and Format Cell Fill yellow\nSelect cell range A3:D4 and Insert Name Create Names in Top Row, OK.\n\n,\nCommand + Select cell range A1:H1, A3:D3, A5:H5, E4:H4 and F3:H3 and Format Cell Fill White Font Electric Purple Bold. Select cell range A5LH5 and Format Cells underline. Select cell range C4, E2:E3, F2, E4:H5 and Format Cell Font Size 14.\n\n,\nEdit Go To cell range A7:A2886 and enter to A7 w/o quotes the formula \"=((A6+(-Tip*2)/(AjRows)))\" and Edit Fill Down. The bottom entry should match the top one, except that it's negative.\nB6: \"=(Base*24/Spheroids)\" w/o quotes and Format cell Fill light rose.\nEdit Go To cell range B7:B2886 and enter to B7 w/o quotes the formula \"=A6\" and Edit Fill Down.\nEdit Go To cell range C6:C2886 and enter to C6 w/o quotes the formula \"=Thickness*Spheroids/KEY*(COS((ROW()-6)*PI()/180*Factor1))\" and Edit Fill Down.\nEdit Go To cell range D6:D2886 and enter to D6 w/o quotes the formula \"=Thickness*Spheroids/KEY*(SIN((ROW()-6)*PI()/180*Factor1))\" and Edit Fill Down.\nEdit Go To cell range E6:E2886 and enter to E6 w/o quotes the formula \"=((SIN(A6/(B6*2))*GM*COS(A6)*GM*(COS(A6/(B6*2)))*GM)+C6)/ShrinkExpand\" and Edit Fill Down.\nEdit Go To cell range F6:F2886 and enter to F6 w/o quotes the formula \"=((SIN(A6/(B6*2))*GM*SIN(A6)*GM*(COS(A6/(B6*2)))*GM)+D6)/ShrinkExpand\" and Edit Fill Down.\nThe results in C2886:F2886 should match C6:F6.\n\n, Edit Go To cell range K6:K2180 and enter 1 into cell K6. Do Edit Fill Series Column Linear Step Value = 1, OK. Edit Go To cell range L6:L180 and enter .125 into L6 and Edit Fill Down. Edit Go To cell range M5:M180 and enter 0 to cell M5. Do Edit Fill Series Column Linear Step Value =   0.004166667, OK. Edit Go To cell range K6:M180 and Insert Name Define Name LOOKER to cell range $K$6:$M$180. Format Border red bold outline..\nEnter LOOKER2 into cell O5. Edit Go To cell range O6:O29 and enter 1 into cell O6. Do Edit Fill Series Column Linear Step Value = 1, OK. Edit Go To cell range P6:P29 and enter the following values from top to bottom: P6: 24, 16, 8, 4, =PI(), =PI(), =PI(), 2 from P13 to P21 and 1 from P22 to P29 via Edit Fill Down. Edit Go To cell range O6:P29 and Insert Name Define Name LOOKER2 to cell range $O$6:$P$29. Format Fill yellow and Border red bold outline.\n\n, Select All/Other, and scroll down to  Scatter Smooth Lined Scatter. If in Chart Wizard, a new Chart sheet will open for you, otherwise, a Chart will land atop the data to cut or copy and paste to the Chart worksheet's upper left hand corner. Hover over the lower right corner until the cursor become a double-headed arrow, then grab the corner and pull it down and to the right to form a square. Click in the Plot Area and select Chart Layout from the Ribbon and get rid of the axes, grid lines and legend. Resize the chart to be 5.55\" x 5.55\" and drag it to the center of the page. Select Chart Layout Series 1 and choose Line Color blue, Weight 1. Here's the chart:\n\n\n\n\n\n\n, Select All/Other, and scroll down to  Scatter Smooth Lined Scatter. If in Chart Wizard, a new Chart sheet will open for you, otherwise, a Chart will land atop the data. Move it to the right and format the scales as follows. Horizontal Minimum=.65, Maximum=1.35, Major Unit = .35, Vertical axis crosses at .65. Vertical: Minimum = -1.25, Maximum=.625, Major unit=.075, Horizontal Axis crosses at 0.0 I realize this cuts a shade off the top and that's pertinent. The spheroids should appear as fairly good spheres once you size the graph to be W= 4.92\" x L = 5.5\". Select Chart Layout Series 1 and choose Line Color blue, Weight 1. Here's the chart:\n\n\n\n\n\n\n, To create the Method2 spreadsheet, do menuitem Edit Move or Copy Sheet, with the Create a copy box checked, before sheet Chart and after sheet Data, to the same workbook, make a copy of the Data worksheet just done and title it Data (2) if not done so automatically. I may refer to this worksheet as the Method2 worksheet, instead of Data(2) -- both terms refer to the same sheet., Done away with are Thickness and KEY.\nSin: Edit Go To cell range D6:D2886 and enter to D6 w/o quotes the formula \"=(SIN((ROW()-6)*PI()/180*Factor1))\" and Edit Fill Down. Done away with are Thickness and KEY.\nThe results in C2886:F2886 should match C6:F6.\n\n, Here is a picture:\n\nB1: Change GM to GM=Other\nB2: Enter .75 (a lucky guess on my part but .8 works, and .1 does not).\nA4: Enter \"=Base*12*PI()\" w/o quotes\nB4: Enter \"=16*103\"w/o quotes\nF3: Delete the word Prime. Copy F3 and paste it to D1:E2, then to E3.\nCommand Select A1:F1, F2, A3:F3, E3:H3, A5:H5 and Format Font red. Select A5:H5 and Format underline. Command + Select F2, E4:H5 and do Font Size 14.\nG1: Enter 2880, the number of rows (approx).\nH1: Enter \"=Spheroids\" w/o quotes\nG2: Enter Rows/Sphere\nH2: Enter \"=G1/H1\" w/o quotes\n\n,\nCopy G6 and paste it to cell range G6:H2886.\nSelect cell G2887 and enter w/o quotes \"=SUM(G6:G2886)\" and copy it and paste it to H2887. My results are 0 and 0 and yours should be too, though there are plenty of individual differences in the cells above. However, a MAX() formula informs one that the largest difference is .0251 for x and .025 for y.\n\n, Select All/Other, and scroll down to  Scatter Smooth Lined Scatter. If in Chart Wizard, a new Chart sheet will open for you, otherwise, a Chart will land atop the data to cut or copy and paste to the Chart worksheet's upper left hand corner. Hover over the lower right corner until the cursor become a double-headed arrow, then grab the corner and pull it down and to the right to form a square. Click in the Plot Area and select Chart Layout from the Ribbon and get rid of the axes, grid lines and legend. Resize the chart to be 5.55\" x 5.55\" and drag it to the center of the page. Select Chart Layout Series 1 and choose Line Color red, Weight 1. Here's the chart:\n\n\n\n\n\n\n, Select All/Other, and scroll down to  Scatter Smooth Lined Scatter. If in Chart Wizard, a new Chart sheet will open for you, otherwise, a Chart will land atop the data. Move it to the right and format the scales as follows. Horizontal Minimum=.65, Maximum=1.35, Major Unit = .35, Vertical axis crosses at .65. Vertical: Minimum = -1.25, Maximum=.625, Major unit=.075, Horizontal Axis crosses at 0.0 I realize this cuts a shade off the top and that's pertinent. The spheroids should appear as fairly good spheres once you size the graph to be W= 4.92\" x L = 5.5\". Select Chart Layout Series 1 and choose Line Color red, Weight 1. Here's the chart:\n\n\n\n\n\n\n, In cell G45, enter \"=SUM(E6:E45)-SUM('Data (2)'!E6:E45)\". Copy it and paste it to cell H45 and select G45:H45 and Format Cell Border Border red outline.\nCopy E6:H45 and paste it to E46. Select E86 and paste it again. Select E126 and paste again.\nCommand+Select cell range E46:F85, G85:H85 and Format Fill yellow and Border red bold outline. Command+Select cell range E86:F125, G125:H125 and Format Fill light blue and border red bold outline.\nCopy E6:H125 and paste to E127 and repeat this process until you've reached down to cell H2885, grabbing larger and larger chunks done to copy and paste as your go, bearing in mind that at 1445, you're at the midpoint. Outline the final row in bold red.\nF2887, aligned right: TOTAL DIFFERENCE. Enter to G2887 \"=SUM(G6:G2886)\" and copy and paste to to H2887. Totals = 0 and 0.\nRe-split the window above Total Difference, with room for 24 row entries beneath it. Do not freeze panes.\nE2888: Sphere Number\nF2888: Sphere Subsection\nH2888, aligned right, bold red text: THE PATTERN:\nE2889: Enter 1 and Format Cell Number Number Custom \"00)\" with quotes.\nE2892 and every 3rd cell below it until Sphere 8 is reached: Enter \"=E2889+1\" and copy this to every third cell beneath it.\nF2889: Enter 1 and Format Cell Custom as above, \"00)\". F2890: Enter 2. F2891: Enter 3. These are the the 3 sections of each sphere. Copy F2889 and Paste Special Format to cell range F2890:F2891. Copy F2889:F2891 and paste it 7 times beneath itself so that the first 8 spheres have their beginning, middle and end separated.\nG2889: Enter \"=G45\" by entering = and then locating the subtotal in the upper window in cell G45 and clicking on it. Do this for each such subtotal, separated by 40 rows, so you cannot just fill down.\nObserve THE PATTERN that emerges and how the the spheres are directionally shifted in terms of x and y back and forth.\n\n,\nDo menu item Tools Goal seek Set Cell I2891 To Value 0 By changing cell, and then click over to worksheet Data (2) cell B2 and select it, so you have 'Data (2)'!$B$2 as the By changing cell response, and click OK.\nRelocate to THE PATTERN at worksheet Data cell H2888 and notice that ALL the Y's have gone to zero, and perhaps all the X's as well. I've seen both occur.\nCheck the all the charts and observe the differences in roundness, spread, etc.\nIt may just be that 0.778674031976484 is a better overall factor to use than the Golden Mean.\nSee below for the final charts.\n\n, So the result is a circle.,, Suffice it to say that it multiplies the sine by the cosine and again by a cosine, or the sine by the sine and then by the cosine, of various values -- be they constants or variables. Now the sine of an angle, given r=the hypotenuse, is y/r, and the cosine of an angle is x/r. If we let r=1, then sine = just the distance up the y axis and the cosine = just the distance along the horizontal x axis., At first, x is longer than y is tall. Then, at 45 degrees, both are equal. Then at 60 degrees, the roles are the exact reverse of what they were at 30 degrees and y is now taller than x is short by exactly the amount that x was longer than y at 30 degrees. So, therefore, in a circle, if the sine is long, the cosine is short and vice versa, or they're even. It's also possible that they measure 1 or 0, but those are the maximum and minimum values for a unit circle of radius r = the hypotenuse = 1.,, Then the sine times the cosine produces a rectangle, the diagonal of which is the radius! I get it! I'm a genius!\" ... it's not quite that simple, and yet it is also exactly that simple at the same time. It's just that the sine times the cosine do not define the endpoint of that diagonal when multiplied together, any more than 4*6 = 24 says anything about the point {4, 6} at the corner of the rectangle. Seems unfair, I know! I can sympathize, believe me. But you are a genius perhaps if you can agree that their multiplication as their values change produces the sequence (0, .25, .50, .25, 0, -.25, -.50, -.25, 0, .25, .50, .25, 0, -.25, -.50. -.25, 0)!!!, Because at 45 degrees, the sine and cosine = 1/2 the square root of .5, or .7071, and so there are 4 occurrences of .50 (2 positive and 2 negative), because there are 4 places on the circle where the sine and cosine are equidistant from the x and y axes. The .25's occur at each 15 degrees and there are 8 of those, two on either side of a .50. Then, there are 5 zeros: at 0 degrees, 90 degrees, 180, 270 and 360 degrees. Of course there are a lot of other values in between the 0 and the .25 and the .50, etc, etc., but that is the main way to understand it. This is then a \"half-curve\", relatively speaking., But if they are MULTIPLIED, they will equal at most .50 and at least, -.50,,\nFor more art charts and graphs, you might also want to click on Category:Microsoft Excel Imagery, Category:Mathematics, Category:Spreadsheets or Category:Graphics to view many Excel worksheets and charts where Trigonometry, Geometry and Calculus have been turned into Art, or simply click on the category as appears in the upper right white portion of this page, or at the bottom left of the page.\n\n",
        " A launch driver is responsible for the operation of a launch boat, usually a 24–30 foot (7.3–9.1 m) open-top, diesel-powered vessel. The launch driver handles the operation and maintenance of the vessel, in addition to providing passengers with friendly service and useful information. Duties include:\n\n\nStarting up the boat at the beginning of the day, making sure to check routine fluids including:\n\nThe level and condition of the engine oil (should be at the proper level as indicated on the dipstick and not too dirty).\nThe level of the coolant (should be visible when the cap is removed) (warning: never check coolant after running the engine, because hot coolant could spray out under pressure, causing injury).\nThe condition of the transmission fluid (should be dark red and translucent – if it is white and milky, salt has penetrated the transmission housing and must be flushed out before the transmission is engaged).\nThe amount of fuel in the tank – you don’t want to run out while untied in open water!\n\n\nTying the launch boat off while picking passengers up at the dock/pier.\nGreeting the passengers warmly as they enter the boat and providing up-to-date weather information and harbor information.\nAssisting passengers with stowing additional gear aboard the boat if they require help.\nNavigating the waterways safely and in accordance with local navigational rules.\nPlanning and executing an efficient drop-off and pickup route around the harbor.\nApproaching and landing safely on customers’ boats.\nHelping boaters who need battery jump starts, boat tows, and mooring assistance.;\n, In many ways, working as launch driver is a great occupation. Passing workdays under the sun and on the open water is relaxing and fun. Often, launch drivers receive benefits such as free boat services at the boat yard or yacht club they work for, discounts on marine products, and flexible schedules. Some launch drivers are fortunate enough to work where tipping is the norm and may make a handsome amount of money in cash.\n\n, On the other hand, the job can include long hours, physically demanding days, and periods of high stress. Working outdoors may sound nice at first, but after twelve hours it can be quite taxing. Additionally, it is often the sole responsibility of the driver(s) on duty to remember on the go which customers have radioed requesting pickups, while simultaneously completing pickups already on their route. This can lead to stress, as sometimes a pickup can slip your mind and anger a waiting customer.\n\n, Being a launch driver requires one to have organizational, logistical, and interpersonal skills.\n\n\nYou must be independent. Being a launch driver requires the confidence to take responsibility for the safety of passengers and respond to variable situations at a moment’s notice.\nYou must be friendly. Although your main duty is to pilot the boat, it is important to remember to interact with the passengers and make them feel at ease.\nYou must be responsible. Navigating varying weather, changing tides, and moving boats requires a risk-averse and conservative attitude.\n\n, Be at least seventeen years of age (eighteen to carry more than seven passengers), an English speaker, and a US citizen. It also helps to be familiar with the basics of boat operation.\n\n,, Sea service hours prove to the Coast Guard that the applicant is familiar with local waters, weather conditions, and traffic patterns. Log these hours on form CG-719S (Sea Service).\n\n\nFor a USCG Limited Masters license (allows seven or more passengers aboard), 120 days (4+ hours per day) of self-documented sea time with 90 days in the last year are required.\nFor a USCG Limited OUPV license (no more than six passengers aboard), 90 days (4+ hours per day) of self-documented sea time with 90 days in the last year are required.\n\n, The physical makes sure that you are able to perform the duties related to maritime work. It is very similar to a regular doctor’s office physical, but focuses more heavily on vision. Colorblindness, for example, is a disqualifying factor because most maritime markers and buoys are red or green. Have the physician fill out form CG-719K (Physical).\n\n\nOccupational centers such as Concentra are often fast and easy places to have a pre-employment physical done.\n\n, You must be able to pass a DOT-5 drug panel that tests for marijuana, cocaine, and opiates, among others. The results should be logged on form CG-719P (Drug Testing).\n\n, As the captain of the boat, you must be able to address basic medical situations if a passenger is injured.\n\n, You must complete this class ensuring that you are knowledgeable regarding basic navigation, operation, and boat safety.\n\n, After you pass the exam, you will be given certificate which you will include in your application packet.\n\n, A TWIC is a TSA license. To obtain one, you must visit an enrollment center, be photographed, provide biometric information, get a background check, and pay a fee.\n\n, The completed application should consist of forms CG-719B (Application), CG-719K (Physical), CG-719KE (Entry Level), CG-719P (Drug Testing), and CG-719S (Sea Service). Send them all to your local USCG office.\n\n\nIf your application is approved, you will receive a Merchant Mariner Credential that will allow you to start work as launch driver! Links for some of the aforementioned steps can be found at the bottom of the article.\n\n\n, Launch driving, and maritime work in general, consists of a very close-knit community. Chances are, if you live in a coastal community, you may know someone who was or is a launch driver. Ask them to recommend you to their employer. This personal introduction can work wonders for quick employment.\n\n, Look for opportunities to meet and interact with maritime professionals, recreational boaters, and dock-hands. These folks are always around the waterfront and will often be aware of job opportunities for the aspiring launch driver.\n\n, Since most launch driving positions are provided by private companies, ask businesses that have established launch services if they have openings.\n\n\nIt is often best to ask about available positions during the off-season, because that is when many of the previous season's drivers leave to work elsewhere.\n\n, Launch driving is a very niche profession. Since there are not many people qualified to drive launch boats, managers and business owners are often hard-pressed to find suitable employees. Approaching a business owner directly can seem daunting, but is often the quickest way to be hired as a launch driver.\n\n, Master the basics of boat operation, maintenance, and customer service during the training period. You will often work without direct supervision as a launch driver. This means that you will need to prove to your employer that you can handle the demands of the job without constant guidance.\n\n",
        "I like this paper in that it is a creative application of computer vision to Biology. Or, at least, that would be a good narrative but I'm not confident biologists would actually care about the \"Tree of Life\" built from this method. There's not really any biology in this paper, either in methodology or evaluation. It boils down to a hierarchical clustering of visual categories with ground truth assumed to be the WordNet hierarchy (which may or may not be the biological ground truth inheritance relationships between species, if that is even possible to define -- it probably isn't for dog species which interbreed and it definitely isn't for vehicles) or the actual biological inheritance tree or what humans would do in the same task. If we're just worried about visual relationships and not inheritance relationships then a graph is the right structure, not a tree. A tree is needlessly lossy and imposes weird relationships (e.g. ImageNet has a photo of a \"toy rabbit\" and by tree distance it is maximally distant from \"rabbit\" because the toy is in the devices top level hierarchy and the real rabbit is in the animal branch. Are those two images really as semantically unrelated as is possible?). Our visual world is not a hierarchy. Our biological world can reasonably be defined as one. One could define the task of trying to recover the biological inheritance tree from visual inputs, although we know that would be tough to do because of situations like convergent evolution. Still, one could evaluate how well various visual features can recover the hierarchical relationship of biological organisms. This paper doesn't quite do that. And even if it did, it would still feel like a bit of a solution in search of a problem. The paper says that this type of exercise can help us understand deep features, but I'm not sure sure how much it reveals. I guess it's a fair question to ask if a particular feature produces meaningful class-to-class distances, but it's not clear that the biological tree of life or the wordnet hierarchy is the right ground truth for that (I'd argue it's not).\n\nFinally, the paper mentions human baselines in a few places but I'm not really seeing it. \"Experiments show that the proposed method using deep representation is very competitive to human beings in building the tree of life based on the visual similarity of the species.\" and then later \"The reconstructed quality is as good as what human beings could reconstruct based on the visual similarity.\" That's the extent of the experiment? A qualitative result and the declaration that it's as good as humans could do? ",
        "This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the “related work” section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.",
        "This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.\n",
        "Mid-State Technical College (Mid-State) is a technical college in central Wisconsin with major campuses in the Marshfield, Stevens Point, and Wisconsin Rapids communities. There is also a learning center in Adams. It is a member of the 16 schools in the Wisconsin Technical College System.\n\nMid-State Technical College is accredited by the Higher Learning Commission of the North Central Association (NCA).\n\nReferences\n\nExternal links\nOfficial website\n\nWisconsin technical colleges\nStevens Point, Wisconsin\nEducation in Portage County, Wisconsin\nEducation in Marathon County, Wisconsin\nEducation in Wood County, Wisconsin\nEducation in Adams County, Wisconsin\nNJCAA athletics",
        "  We predict the nature (attractive or repulsive) and range (exponentially\nscreened or long-range power law) of the electrostatic interactions of\noppositely charged and planar plates as a function of the salt concentration\nand surface charge densities (whose absolute magnitudes are not necessarily\nequal). An analytical expression for the crossover between attractive and\nrepulsive pressure is obtained as a function of the salt concentration. This\ncondition reduces to the high-salt limit of Parsegian and Gingell where the\ninteraction is exponentially screened and to the zero salt limit of Lau and\nPincus in which the important length scales are the inter-plate separation and\nthe Gouy-Chapman length. In the regime of low salt and high surface charges we\npredict - for any ratio of the charges on the surfaces - that the attractive\npressure is long-ranged as a function of the spacing. The attractive pressure\nis related to the decrease in counter-ion concentration as the inter-plate\ndistance is decreased. Our theory predicts several scaling regimes with\ndifferent scaling expressions for the pressure as function of salinity and\nsurface charge densities. The pressure predictions can be related to surface\nforce experiments of oppositely charged surfaces that are prepared by coating\none of the mica surfaces with an oppositely charged polyelectrolyte.\n",
        "This paper proposes RNs, relational networks, for representing and reasoning about object relations. Experiments show interesting results such as the capability to disentangling scene descriptions. AR3 praises the idea and the authors for doing this nice and involved analysis. AR1 also liked the paper. Indeed, taking a step back and seeing whether we are able to learn meaningful relations is needed in order to build more complex systems.\n \n However, AR2 raised some important issues: 1) the paper is extremely toy; RN has a very simplistic structure, and is only shown to work on synthetic examples that to some extent fit the assumptions of the RN. 2) there is important literature that addresses relation representations that has been entirely overlooked by the authors. The reviewer implied missed citations from a field that is all about learning object relations. In its current form, the paper does not have a review of related work. The AC does not see any citations in a discussion nor in the final revision. This is a major letdown. The reviewer also mentioned the fact that showing results on real datasets would strengthen the paper, which was also brought up by AR3. This indeed would have added value to the paper, although it is not a deal breaker.\n \n The reviewer AR2 did not engage in discussions, which indeed is not appropriate. The AC does weigh this review less strongly.\n \n Give the above feedback, we recommend this paper for the workshop. The authors are advised to add a related work section with a thorough review over the relevant fields and literature.",
        "Distorsio minoruohnishii is a species of medium-sized sea snail, a marine gastropod mollusk in the family Personidae, the Distortio snails.\n\nDescription\n\nDistribution\n\nReferences\n\nPersonidae\nGastropods described in 1989",
        "To form a continuous line, you would have to assume that magma plumbing is continuous; that is that you apply magma at the base of the crust at position x and it comes out at y above, apply at x+1 and it will come out at y+1. That is not the case.\n\nMagma plumbing is highly discontinuous; magma will preferentially propagate along fractures and pre-existing weaknesses in a system. A pool of magma accumulating at the base of the crust will tend to propagate along the same route it first successfully found. That route will be used until the low-level magma suipply is cut off. At this point a new low level accumulation will form, until it too finds itself a new plumbing route.\n\nConsider too that the ocean floor is cut by numerous large faults which cut through pretty much the entire plate - these and their related structures can form preferentially weak loci for magma injection.\n\nThe vast majority of magma never even makes it out to the surface - most is simply crystallised at depth in magma chambers which dead-end several kilometres below the surface, or are injected as dykes or sills within the host strata.\n\nIt's a bit like considering the population living around a city, and thinking that you should be able to draw a straight line from their house to the centre, and that will be their route in, resulting in a perfectly radial distribution of travel. In reality, they move along main arterial roads because that is the *easiest* route, rather than climbing over fences, walking through other peoples houses, and navigating the *shortest* route.",
        "  We identify the class of f(R) dark energy models which have a viable\ncosmology, i.e. a matter dominated epoch followed by a late-time acceleration.\nThe deviation from a LambdaCDM model (f=R-Lambda) is quantified by the function\nm=Rf_{,RR}/f_{,R}. The matter epoch corresponds to m(r=-1) simeq +0 (where\nr=-Rf_{,R}/f) while the accelerated attractor exists in the region 0<m<1. We\nfind that the equation of state w_DE of all such ``viable'' f(R) models\nexhibits two features: w_DE diverges at some redshift z_{c} and crosses the\ncosmological constant boundary (``phantom crossing'') at a redshift z_{b}\nsmaller than z_{c}. Using the observational data of Supernova Ia and Cosmic\nMicrowave Background, we obtain the constraint m<O(0.1) and we find that the\nphantom crossing could occur at z_{b}>1, i.e. within reach of observations. If\nwe add local gravity constraints, the bound on m becomes very stringent, with m\nseveral orders of magnitude smaller than unity in the region whose density is\nmuch larger than the present cosmological density. The representative models\nthat satisfy both cosmological and local gravity constraints take the\nasymptotic form m(r)=C(-r-1)^p with p>1 as r approaches -1.\n",
        "edit:\n\nIf I recall correctly, there are at least four people that frequent /r/askscience who have completed medical school in the United States and then like, a few students getting ready for their USMLEs. \n\nI don't suspect the information I've provided is **terribly** wrong, but I would very much appreciate their attention and criticism.\n____________________________________________________________________\n\n\nLooks like the /r/askscience moderation team is asleep. Can't blame them.\n\n\nWith a rudimentary background in biology, I can tell you right now that \"growth\" in terms of healthy development is off the table.\n\nMuscle atrophy is a *huge* problem in coma patients that are 'under' for long periods of time, and it wouldn't be pretty in someone at that developmental stage.\n\nLook at the bodies of children with paralysis from a young age; that's what the coma victim would be in for, but even *more* compounded since they couldn't do any physical therapy whatsoever.\n\nAssuming they got proper nutrition from a very dedicated hospital staff/home nursing and round the clock care, there would be massive developmental deformity.\n\nAs regards the 'sexual' component of this discussion; yeah, hormones are going to be released as the body ages, and they'll undergo puberty and develop secondary sexual characteristics - mitigated again by the fact that their muscle tone is simply not there, they'll have awful deformities of the skeletal structure, etc.\n\nIf I had to be in a coma for ten years, I'd rather go from 69-79 or something.",
        "The reviewers were unanimous in their agreement about accepting this paper.\n Pros \n - novel formulation that don't require sample by sample prediction\n - interesting results\n \n Cons\n - lack of details / explanation in the mathematical formulation / motivations for the model.",
        "I am going to assume you are referring to classic Greek hoplite phalanx and not the later Macedonian phalanx.\n\nDeath on the battlefield before the mid-1800s almost always occured from the back. While a few soldiers did die in the melee, it was when one side turned and started to run that the real killing started. A hoplite would have a large shield, lower leg guards of bronze, a bronze or linen cuirass, a bronze helmet and possibly leather braces. Generally, it was hard to kill armoured people, especially in organised ranks. You did not get many oppurtunities for a clean hit.\n\nOnce one side started running, ranks broke down, the shield was no protection and back armour was usually weaker than front armour, not even speaking of lower leg armour being front only. You do not face any counterstrikes and do not need to worry about your enemy's comrades and can take your time to land a clean hit.\n\nGrossman theorises in his \"On killing\" that it is much easier for humans to kill another human when not seeing his face. It is also much easier for cavalry, which was usually comitted when the enemy started to rout, both because they were often nobility and thus professional soldiers who had killed other humans since young age, and the fact that they would ride past anyone they struck down, not having to see the result of their handiwork, making killing easier.\n\nStanding in the frontline was not an automatic death sentence, being the last one to run was, though (you would be caught up with first). the right-hand side of the phalanx was the position of honour for the best and most experienced soldiers, but it seems like the front ranks were usually made up of the best soldiers to ensure unit cohesion and ordered front ranks facing the enemy, as disorder could be devastating to the whole unit.",
        "This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.",
        "Alicinte Anveshanam () is a 1989 Malayalam-language film written and directed by T. V. Chandran.\n\nPlot\nSet in northern Kerala, the film narrates the journey of Alice in search of her missing husband, a college lecturer. During her quest, she slowly discovers disturbing aspects of her husband, including his descent from his earlier radicalism into bourgeois degeneracy. In the end she gives up her search and decides to take responsibility for her own life.\n\nCredits\n Direction & Screenplay: T. V. Chandran\n Cast: Jalaja, Ravindranath, Nedumudi Venu, C. V. Sriraman\n Cinematography: Sunny Joseph\n Music: Ouseppachan\n\nAwards\nIt was nominated for the Golden Leopard at Locarno International Film Festival in 1990. It also won the Kerala State Film Award for Second Best Film in 1990.\n\nReferences\n\nExternal links\n \n T. V. Chandran's profile at Cinemaofmalayalam.net\n\nIndian films\n1989 films\n1980s Malayalam-language films\nFilms directed by T. V. Chandran",
        " Unfortunately, fraud and identity theft are becoming more-and-more popular.In some cases, con artists will mail-out falsified tickets in order to obtain an individual's private information.\n\n\nMake sure all of the information provided on the ticket is accurate and complete.\nIs your name spelled correctly? Address correct? If any of this information is incorrect or missing it may be an indicator of fraud.;\n, Only the agency that issued the traffic violation can verify whether-or-not your ticket is indeed authentic.\n\n\nHave your citation number ready.\n\n, The time and day of your arraignment should be listed on your citation. This is also the last day to pay any fees or penalties if you choose to do so.\n\n\nThe ticket will also have the address and contact information for the courthouse you will be required to appear.\nIn some states, this is the latest date that you can pay or appear in court. In these states you may appear in court anytime before this date to be arraigned.\n\n, On this day, you will not be presenting your case or your evidence to the judge, but will instead, be entering a plea of guilty, guilty with explanation, or not-guilty.Those who enter a guilty with explanation plea, are admitting guilt but asking the court to consider the circumstances and conditions of their violation.\nThis can lead to reduced penalties.\n\n, When the judge announces your name and case you will be asked to identify yourself, at which time you will be able to enter a plea. Once you plead not-guilty, a date for your court-hearing will be set.\n\n\nBe sure to be on time.\n\n, The court clerk will help you with all of the appropriate paperwork, and will be able to provide you with further information regarding your personal defense.\n\n\nThis will include a time and date for your hearing, as well as a specific courtroom address and room number.\n\n, Properly filing a \"request for production\" is very important. It establishes a legal date of receipt.\n\n\nDepending on your state, the prosecutor will then have a pre-determined amount of time to respond.\nIf they fail to respond, you can ask to have your case dismissed.\nThis paperwork will be needed when you return to court.\n\n, Red light cameras take two pictures – one of the driver and one of the car's license plate.\n\n\nIf someone else is driving, and this is clear in the photograph, you can ask to have your case dismissed.\nYou can also ask to have your case dismissed if the photograph of the driver is blurred or unclear, to the point that the driver's identity remains in question.\nCheck the license plate. If it doesn't match the car you own, raise this as evidence for dismissal of the ticket.\n\n, This written request requires the prosecutor to provide you with a list of the evidence that is going to be used against you in court. If this request is not met within a set number of days--which is determined by your local state laws--it may be grounds to have your ticket dismissed.Consult your traffic court clerk for more details regarding the number of days your state allows the prosecutor to respond.\nEvidence can include photographs, lists of witnesses, etc., If you have already visited the court clerk, you will have all of the information needed to properly serve the prosecutor. When a prosecutor is served, he or she is legally acknowledging that they have indeed received your request. This establishes a legal date of receipt.If the prosecutor does not provide you with this; or does so beyond the legally allotted amount of time determined by your state, you can ask to have your case dismissed.\n\n, This includes copies of your written request, copies of the paperwork you filed to serve the prosecutor, and anything that you have received in return.\n\n\nAll of this is evidence.\nYou will need this in case you request to have your case dismissed for any reason.\n\n, Again, if the prosecutor has failed to present you with a \"request for production;\" or if it was not provided in the legally-allotted time, you can ask the judge to dismiss your case.\n\n, Specifically, review the photograph.\n\n\nIs it clearly you in the photograph? If not, it can be grounds for dismissal.\nIf you were not the individual driving the car, then the photographs should support that claim and you can have your case dismissed.\n\n, Visit the scene and take detailed notes. Time the traffic light as it transitions from yellow-to-red. If the timing is minimal, you might be able to stage an argument in court that there is not enough time to safely bring your vehicle to a stop at the particular intersection.\n\n\nA copy of the photograph should be included with your \"request for production.\"\n\n, You should have a clear line of defense and be able to explain exactly why it is your case should be dismissed. Is the timing of the light too short? Is it not you in the photograph? Are you hoping to have your case thrown out due to some other 'loop-hole'?\n\n\nKnow exactly what you are going to argue before you enter the courtroom.\nHave a clear reason for your case to be dismissed.\n\n, Arrange each piece of paperwork or evidence neatly in a folder and in the order that you wish to present it to the judge.\n\n\nIt might be helpful to have a checklist of points that you wish to make.\n\n, Specifically, the court address, hearing time, and court-room number. The courthouse that you will have your case heard may not be the same courthouse where you were first arraigned. Be sure you know where you are going and when you need to be there.\n\n, Your case will not be the only one that the judge will be hearing that day; rather, he or she will be hearing several cases. The order in which the cases are heard will be determined by the court.\n\n\nBe prepared to spend an entire day at the courthouse.\nYou may want to pack a lunch or bring snacks.\nYou might have to pay for parking, so be prepared.\n\n, The judge will then briefly review the details of your case before allowing the prosecutor to present the evidence against you.\n\n\nDo not interrupt.\n\n, There may be bits of information that are new to you, or that perhaps you were unprepared to address.\n\n, Specifically, pay attention to any discrepancies between the prosecutor's verbal argument and the hard-evidence being presented.\n\n, If any of these conditions have not been met than you may have grounds to have your case dismissed by the judge.\n\n\nA \"request for production\" was not received, or was not received in the proper amount of time.Your hearing has been delayed by the court beyond a responsible expectation to a \"fair and speedy trial.\" Your court clerk can will have more information regarding the number of days your state has determined as \"fair and speedy.\", When it is your time to present your case, begin by providing a detailed account of the event from your own perspective. Include relevant information such as exact time, day, intersection, weather conditions, etc.\n\n\nPoint out any discrepancies between your account and the prosecutor's.\nDoes the prosecutor's account match the hard evidence?\n\n, After you have described the conditions of the event, and after you have addressed any discrepancies, you should go ahead and re-assert to the judge why it is you wish to have your case dismissed.\n\n\nWere there any extenuating circumstances that you believe should be considered?\nIs the timing of the light flawed?\nWas it unsafe to stop?\n\n, Only provide evidence that fully-supports your claim, or that directly challenges the case as it was presented by the prosecutor.\n\n\nAfter you have presented your evidence, the judge will then move on to the closing statements.\nThe prosecutor will go first.\n\n, You do not necessarily need to respond to his or her closing statements as you have already made your case.\n\n, Reiterate and summarize your argument or the reason you are seeking dismissal. Be as clear and concise as possible. Speak as if answering the question, \"why should your case be dismissed?\"\n\n\nBe factual, not emotional.\n\n, If all went well, then you may have just successfully defended yourself against a red-light camera violation.\n\n",
        " Find a suit that is comfortable, fits well, and that you can move in. It shouldn’t fall off if you jump in the pool. Leave the embellished bikinis and baggy swim trunks for the beach or pool; you need something that is streamlined and easy to move in.\n\n\nBe extra cautious about the color white. Depending on the lining, it can be see through when wet.;\n, Not only will it protect your hair against the chlorine, but it will also make your body more streamlined and reduce water tension. If you have long hair, be sure to tie it up first, and then tuck it under the cap.\n\n\nSome swim caps contain latex. If you are allergic to latex, read the label, and make sure that you purchase a latex-free swim cap.\n\n, Nothing ruins a swim faster than water in the eyes. Get goggles that fit over your eyes, and make sure that they are comfortable. Never get ones that cover the nose and mouth.\n\n\nSome contain latex. If you are allergic to latex, be sure to check the packaging before you buy anything first; it will tell you whether or not it contains latex.\n\n, Many people find that items, like pool noodles, kickboards, and flippers help them with various aspects of swimming. If your swim teacher recommends these items, then you might want to consider buying them as well.\n\n\nYou can also purchase nose plugs and ear plugs to prevent water from getting into your nose and ear.\nIf you are swimming in an outdoor pool, be sure to get sunscreen.\n\n, Make sure your goggles are on. At this point, you may need to adjust the goggles by tugging on the straps so that they don't leak.\n\n\nIf you don't feel comfortable getting into the pool yet, you can practice this part in a bowl of warm water. The bowl should be twice the size of your face., Take a deep breath through your mouth first, then place your face into the water. Exhale slowly through your mouth, just enough to keep water from entering your mouth.\n\n\nSome swimmers like to exhale through both their nose and mouth. If you find this to be more comfortable for you, then you should do it.Some swimmers find that wearing a nose plug helps them exhale better underwater., You should take twice as much time to exhale as to inhale.If you find this difficult to keep track of, try timing your exhalation to the count of ten.\n\n, You will likely get water in your mouth at some point while you are underwater. While this may feel uncomfortable, it is not a near-death experience. It happens to a lot of people, especially when they are first learning how to swim.\n\n\nOne way to reduce swallowing water is to position your tongue as though saying \"Keh.\", Even though you are not swimming yet, this is a good practice to get into. It will help keep your body straight and aligned. If you keep your head out of the water, your body will be tilted upwards, and create more drag and resistance. It will make it harder to swim.If your pool has those black lanes, use those as a focal point.\n\n, You'll feel the pressure of the water resisting you, and you may even find it shifting your body around. Moving your arms sideways will cause your body to turn. Pushing downward will cause your body to move upward. moves your body up. Moving your arms backward will tilt your body forward.\n\n\nYou can do this standing or sitting, but you should be in the water up to your shoulders.\nThis is sometimes referred to as \"sculling.\"\n\n, Make sure that your head is out of the water.\n\n, Use your feet to push off of the pool's floor, and remember to breathe out through your mouth.\n\n, Push off the bottom with your feet to come up and grab the wall. Scull and kick while coming up.\n\n, If you want, you can even step away further from the pool's edge. Remember to go no deeper than where you can still stand. This way, if your confidence fails, all you have to do is stand up.\n\n, Become accustomed to having your face in the water and your body stretched out. Try to reduce your dependence on using a flotation device and being afraid of going underwater. You can even swim a little underwater before coming up. Your primary reaction in the water must be to stretch out on the surface, scull, kick, breathe and relax.\n\n\nDon't get discouraged if you accidentally swallow some water. It happens to everyone, even experienced swimmers.\n\n, If your hips are lower than your shoulders, your body will tip upwards, and you won't be able to stay afloat. You can practice this by trying to balance on a bed, bench, or chair.\n\n, Try to keep your body as straight as possible, with the back of your head between your shoulder blades. Move arms to your side and wave your hands, palms down, away from the hips. This will help you stay afloat and move around.\n\n\nBack floating is one of the easiest ways to learn how to float.\nIf you are having trouble with this, get someone who is experienced in swimming to help you get into position.\n\n, Face down to exhale, then turn over onto your chest and stomach. This is the body position for most swimming styles, including freestyle and breaststroke.\n\n, You can do this in the water, or on a bench/chair. Move your arm behind your head, over your head, and out in front of you in a circular motion.\n\n, Hold onto the side of the pool, a pool noodle, or a kickboard, and gently kick your legs in a soft, flutter-like motion. Try to keep your toes pointed, and your legs as straight as possible. Don't kick from your knees or kick too hard, as this will create too much drag and slow you down.This is the basic kick for swimming, be it on the back or face down.\nYour kicks should be low effort. Harder kicks won't necessarily make you go faster.\nYou can also practice your kicks while balancing upon a bench.\n\n, Go for 5 to 10 yards (4.6 to 9.1 m) while dipping face in the water to exhale. Do a few laps like this until you get comfortable. You can go the first length keeping your face out of the water, but try to work your way up to where you can swim with your face under the water. This way, you will be able to practice breathing. You may also find it easier to swim!Start practicing in shallower waters until you feel comfortable, then you can move on to deeper waters.\nOnce you feel confident, try it without the kickboard, and add the arm movements.\n\n, It is a great form of exercise to use after you learn to swim. This can be done while treading with the belt on.\n\n\nYou can also wear swimming fins while practicing your kicks. Don't wear them all the time, however, especially for warm-up and warm-down exercises., Learning how to swim is not a competition; save that for when you are more experienced. Don't force yourself to move into deeper waters if you don't feel comfortable at your current depth. If you feel tired, take a break, and get out of the deep end.\n\n\nEveryone started with the basics, so don't feel discouraged by the experienced swimmers around you. They will not think less of you or make fun of you. After all, they were in your position at one point too.\n\n",
        "  The Variable Star One-shot Project (VSOP) is aimed at (1) providing the\nvariability type and spectral type of all unstudied variable stars, (2)\nprocess, publish, and make the data available as automatically as possible, and\n(3) generate serendipitous discoveries. This first paper describes the project\nitself, the acquisition of the data, the dataflow, the spectroscopic analysis\nand the on-line availability of the fully calibrated and reduced data. We also\npresent the results on the 221 stars observed during the first semester of the\nproject. We used the high-resolution echelle spectrographs HARPS and FEROS in\nthe ESO La Silla Observatory (Chile) to survey known variable stars. Once\nreduced by the dedicated pipelines, the radial velocities are determined from\ncross correlation with synthetic template spectra, and the spectral types are\ndetermined by an automatic minimum distance matching to synthetic spectra, with\ntraditional manual spectral typing cross-checks. The variability types are\ndetermined by manually evaluating the available light curves and the\nspectroscopy. In the future, a new automatic classifier, currently being\ndeveloped by members of the VSOP team, based on these spectroscopic data and on\nthe photometric classifier developed for the COROT and Gaia space missions,\nwill be used. We confirm or revise spectral types of 221 variable stars from\nthe GCVS. We identify 26 previously unknown multiple systems, among them\nseveral visual binaries with spectroscopic binary individual components. We\npresent new individual results for the multiple systems V349 Vel and BC Gru,\nfor the composite spectrum star V4385 Sgr, for the T-Tauri star V1045 Sco, and\nfor DM Boo which we re-classify as a BY Draconis variable. The complete data\nrelease can be accessed via the VSOP web site.\n",
        "  Network tomography has been regarded as one of the most promising\nmethodologies for performance evaluation and diagnosis of the massive and\ndecentralized Internet. This paper proposes a new estimation approach for\nsolving a class of inverse problems in network tomography, based on marginal\ndistributions of a sequence of one-dimensional linear projections of the\nobserved data. We give a general identifiability result for the proposed method\nand study the design issue of these one dimensional projections in terms of\nstatistical efficiency. We show that for a simple Gaussian tomography model,\nthere is an optimal set of one-dimensional projections such that the estimator\nobtained from these projections is asymptotically as efficient as the maximum\nlikelihood estimator based on the joint distribution of the observed data. For\npractical applications, we carry out simulation studies of the proposed method\nfor two instances of network tomography. The first is for traffic demand\ntomography using a Gaussian Origin-Destination traffic model with a power\nrelation between its mean and variance, and the second is for network delay\ntomography where the link delays are to be estimated from the end-to-end path\ndelays. We compare estimators obtained from our method and that obtained from\nusing the joint distribution and other lower dimensional projections, and show\nthat in both cases, the proposed method yields satisfactory results.\n",
        " This will give you a thin sheet cake, perfect for making petit fours. If you do not have a lot of time on your hands, you can use a pre-baked sheet cake instead. Click here to learn how to assemble the cake, here to learn how to prepare the icing, and here to learn how to decorate the petit fours.\n\nA half-sheet cake pan is 12 by 16 by 3 inches (30.48 by 40.64 by 7.62 centimeters).;\n, Pour the flour into a mixing bowl, then add the baking powder and salt. Stir together until evenly combined, then set aside., Pour the sugar into a large mixing bowl, then add the butter and shortening. Beat together until smooth, using an electric mixer or hand-held beater., Keep mixing with your electric mixer until everything is smooth and fluffy., Add half of the flour mixture into the sugar mixture, and beat well until combined. Stir in half of the evaporated milk. Repeat with the remaining flour mixture and evaporated milk., Gently fold the sour cream into the batter using a rubber spatula. Be sure to scrape the bottom and sides of the bowl., Check the cake for doneness after 25 minutes be poking a toothpick into the center. If the toothpick comes out clean, the cake is ready. If the toothpick has crumbs on it, bake the cake for 5 to 10 more minutes., Allow the cake to cool in the pan for 10 to 15 minutes first, then carefully flip it over onto another sheet of parchment paper. Peel away the parchment paper from the bottom (if necessary), and discard it.\n\n, You can do this using a cake splitting tool, a long knife, or a piece of thread.\n\nIf you are using a pre-baked sheet cake, check the thickness; you may have to split it into thirds instead.\nIf you are using a pre-baked pound cake, cut it into ¼-inch (0.64-centimeter) thick slices.\n\n, Keep the top half of the cake on the cardboard, and set it aside for now. If you don't have any thin cardboard on hand, you can also use a thin cutting matt instead., You can also use a different type of jam, such as strawberry or blueberry. You could also use lemon curd. Try to use something tart or sour tasting; this will help balance out the overall sweetness.\n\nIf you split your pre-baked sheet cake into thirds, repeat this step for the middle layer.\nIf you are using a pound cake cut into thin slices, coat only half of the slices; reserve the other half for the top.\n\n, You don't absolutely have to do this, but there is nothing wrong with extra frosty goodness. Make sure that the frosting goes from edge to edge, corner to corner.\n\nDo not use your icing for this step; use buttercream frosting instead. Cream cheese buttercream or vanilla buttercream work especially well. You can use homemade buttercream or you can use store-bought buttercream.\nIf you split your pre-baked sheet cake into thirds, repeat this step for the middle layer.\n\n, Hold the top layer over the bottom one, and line up one of the edges. Carefully slide the thin sheet of cardboard out from under the top layer, letting the cake fall into place. Use your hands to nudge the top layer into place, if necessary.\n\nIf you are using a pound cake, start playing the bare slices on top of the jam-and-frosting covered ones.\n\n, Use an offset cake decorating spatula to spread a thin layer of frosting on the top layer of your cake. This will give you a smoother finish when you go to pour the icing on top. Do not cover the sides of the cake., Use a sharp knife to cut the edges off of your cake so that the corners are nice and sharp. Don't cut the cake into squares yet, however! Instead, use the parchment paper to lift the cake onto a baking sheet. Then, move the cake into your freezer. This will allow the cake to set up while you prepare the icing.\n, Fill a saucepan with 2 inches (5.08 centimeters) of water, then place a heat-safe glass bowl on top. Make sure that the bottom of the bowl is not touching the surface of the water. Also, make sure that the bowl is big enough to fit all of your icing ingredients., Make sure that the bowl is fitted, and that the stem can't escape. This will allow you to slowly melt your ingredients without fear of scorching them., Don't add the chocolate chips yet. If you aren't a fan of almond, you can use a different flavor instead, such as lemon or peppermint., At this point, you can turn down the heat on the burner; as long as there is steam, you will be fine. Keep stirring until the chocolate chips melt and everything turns smooth.Using white chocolate chips will allow you to change the color of the icing by adding in food coloring. If you don't care about the color and don't like white chocolate, you can use regular chocolate chips instead., Although not completely necessary, this can give your icing a lovely color! Keep in mind that a little bit goes a long way, so you won't need a lot of gel food coloring. Opt for light, pastel shades rather than bright or dark colors.\n\nThe food coloring will only work with white chocolate chips, not regular chocolate chips.\nDo not use liquid food coloring, as this may cause the icing to \"seize\" and become grainy.\n\n\n, You will be decorating your petit fours on this rack. The parchment paper will catch any excess icing. You can then melt the icing in your double boiler and reuse it.If you don't have any more parchment paper, you can use wax paper for this instead.\n\n, Something between 1 and 1¼ inch (2.54 and 3.18 centimeters) would be ideal.You can also cut the petit fours into other simple shapes as well, such as hearts, circles, or rectangles. Make sure that your shapes are around 1 inch (2.54 centimeters) however!, Make sure that you have some space between each square to allow the icing to drip through., Make sure that your pour the icing evenly, and that it completely covers the squares. You don't want to see any cake or filling at all. The icing should cover the sides of the petit fours in a smooth layer; you don't want any dribbles.\n\nDon't worry if the icing layer looks thin. You can fix that in a moment.\n\n, While you wait for the icing to set, take the excess icing off of the baking sheet below the cooling rack, and melt it in your double boiler. Pour the icing back over the petit fours as before. This will create a thicker layer.You only need to do this if your first layer is thin. If you can see bits of cake or filling, then your icing layer is too thin, and you should apply a second one.\n\n, At this point, your petit fours are essentially done, but you can decorate them even further to match the theme of your party or event. A quick and simple decoration would be to pipe little rosettes on top of each petit four. You can also use sugar candy shaped like flowers instead, or drizzle melted, colored chocolate on top. Other popular designs include polka dots, strips, and bows.",
        "  In recent years, there has been increasing interest in the specific heat $C$\nof insulators and semiconductors because of the availability of samples with\ndifferent isotopic masses and the possibility of performing \\textit{ab initio}\ncalculations of its temperature dependence $C(T)$ using as a starting point the\nelectronic band structure. Most of the crystals investigated are elemental\n(e.g., germanium) or binary (e.g., gallium nitride) semiconductors. The initial\nelectronic calculations were performed in the local density approximation and\ndid not include spin-orbit interaction. Agreement between experimental and\ncalculated results was usually found to be good, except for crystals containing\nheavy atoms (e.g., PbS) for which discrepancies of the order of 20% existed at\nthe low temperature maximum found for $C/T^3$. It has been conjectured that\nthis discrepancies result from the neglect of spin-orbit interaction which is\nlarge for heavy atoms ($\\Delta_0\\sim$1.3eV for the $p$ valence electrons of\natomic lead). Here we discuss measurements and \\textit{ab initio} calculations\nof $C(T)$ for crystalline bismuth ($\\Delta_0\\sim$1.7 eV), strictly speaking a\nsemimetal but in the temperature region accessible to us ($T >$ 2K) acting as a\nsemiconductor. We extend experimental data available in the literature and\nnotice that the \\textit{ab initio} calculations without spin-orbit interaction\nexhibit a maximum at $\\sim$8K, about 20% lower than the measured one. Inclusion\nof spin-orbit interaction decreases the discrepancy markedly: The maximum of\n$C(T)$ is now only 7% larger than the measured one. Exact agreement is obtained\nif the spin-orbit hamiltonian is reduced by a factor of $\\sim$0.8.\n",
        "This paper investigates three simple weight-pruning techniques for NMT, and\nshows that pruning weights based on magnitude works best, and that retraining\nafter pruning can recover original performance, even with fairly severe\npruning.\n\nThe main strength of paper is that the technique is very straightforward and\nthe results are good. Itâs also clearly written and does a nice job covering\nprevious work.\n\nA weakness is that the work isnât very novel, being just an application of a\nknown technique to a new kind of neural net and application (namely NMT), with\nresults that arenât very surprising. \n\nItâs not clear to me what practical significance these results have, since to\ntake advantage of them you would need sparse matrix representations, which are\ntrickier to get working fast on a GPU - and after all, speed is the main\nproblem with NMT, not space. (There may be new work that changes this picture,\nsince the field is evolving fast, but if so you need to describe it, and\ngenerally do a better job explaining why we should care about pruning.)\n\nA suggestion for dealing with the above weakness would be to use the pruning\nresults to inform architecture changes. For instance, figure 3 suggests that\nyou might be able to reduce the number of hidden layers to two, and also\npotentially reduce the dimension of source and target embeddings.\n\nAnother suggestion is that you try to make a link between pruning+retraining\nand dropout (eg âA Theoretically Grounded Application of Dropout in Recurrent\nNeural Networksâ, Gal, arXiv 2016).\n\nDetailed comments:\n\nLine 111: âsoftmax weightsâ - âoutput embeddingsâ may be a preferable\nterm\n\nS3.2: Itâs misleading to call n the âdimensionâ of the network, and\nspecify all parameter sizes as integer multiples of this number as if this were\na logical constraint.\n\nLine 319: You should cite Bahdanau et al here for the attention idea, rather\nthan Luong for their use of it.\n\nS3.3: Class-uniform and class-distribution seem very similar (and naturally get\nvery similar results); consider dropping one or the other.\n\nFigure 3 suggestion that you could hybridize pruning: use class-blind for most\nclasses, but class-uniform for the embeddings.\n\nFigure 4 should show perplexity too.\n\nWhat pruning is used in section 4.2 & figure 6?\n\nFigure 7: does loss pertain to training or test corpora?\n\nFigure 8: This seems to be missing softmax weights. I found this diagram\nsomewhat hard to interpret; it might be better to give relevant statistics,\nsuch as the proportion of each class that is removed by class-blind pruning at\nvarious levels.\n\nLine 762: You might want to cite Le et al, âA Simple Way to Initialize\nRecurrent Networks of Rectified Linear Unitsâ, arXiv 2015.",
        "The authors present methods to speed-up gradient descent by leveraging asynchronicity in a layer-wise manner.\n\nWhile they obtain up-to 1.7x speedup compared to synchronous training, their baseline is weak. More importantly, they dismiss parameter-server based methods, which are becoming standard, and so effectively just do not compare to the current state-of-the-art. They also do not present wall-time measurements. With these flaws, the paper is not ready for ICLR acceptance.",
        "Summary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.",
        "  We present a detailed study of the structural behavior and polarization\nreversal mechanism in phase III of KNO$_3$, an unusual ferroelectric material\nin which the nitrate groups rotate during polarization reversal. This material\nwas one of several studied in a previous work [O. Di\\'eguez and D. Vanderbilt,\nPhys. Rev. Lett. {\\bf 96}, 056401 (2006)] where methods were described for\ncomputing curves of energy versus electric polarization. In the present work we\nextend and systematize the previous first-principles calculations on KNO$_3$,\nand analyze in detail a two-parameter model in which the energy of the system\nis written as a low-order expansion in the polarization and the nitrate group\norientation. We confirm that this model reproduces the first-principles results\nfor KNO$_3$ very well and construct its parameter-space phase diagram,\ndescribing regions where unusual triple-well potentials appear. We also present\nfirst-principles calculations of KNO$_3$ under pressure, finding that its\nenergy-versus-polarization curves change character by developing a\nfirst-derivative discontinuity at zero polarization.\n",
        "\"The One with the Embryos\" is the twelfth episode of Friends fourth season. It first aired on the NBC network in the United States on January 15, 1998. In the episode, Phoebe (Lisa Kudrow) agrees to be the surrogate mother for her brother Frank Jr. (Giovanni Ribisi) and his older wife Alice Knight (Debra Jo Rupp). Meanwhile, a display by Chandler (Matthew Perry) and Joey (Matt LeBlanc) of how well they know Monica (Courteney Cox) and Rachel (Jennifer Aniston) by guessing the items in their shopping bag leads to a large-scale bet on a quiz, for which Ross (David Schwimmer) acts as the gamemaster.\n\nThe episode was directed by Kevin S. Bright and co-written by Jill Condon and Amy Toomin. The idea for Kudrow's character Phoebe becoming a surrogate mother coincided with the actress' real-time pregnancy. The producers wanted to find a way to use the pregnancy in a narrative for the fourth season and designated the task to the writers. Ribisi and Rupp reprised their recurring roles of Frank Jr. and Alice respectively which was initially difficult as both had filming commitments.\n\nIn its original broadcast on NBC, \"The One with the Embryos\" acquired a 17.3 Nielsen rating, finishing the week ranked fourth. The episode received critical acclaim, is generally considered one of the best of the entire series, and is a favorite amongst the cast members and producers. In 2009, \"The One with the Embryos\" was ranked #21 on TV Guide'''s list of \"TV's Top 100 Episodes of All Time.\"\n\nPlot\nRachel (Jennifer Aniston) and Monica (Courteney Cox) are woken up too early in the morning by Joey (Matt LeBlanc) and Chandler's (Matthew Perry) chick and duck, as the maturing chick has just begun crowing. Later, as Rachel returns with her shopping and complains to the others about the situation, Phoebe (Lisa Kudrow) urges the boys to get rid of their birds as they should not be living in an apartment.\n\nAs Phoebe leaves for her doctor's appointment to get her brother Frank (Giovanni Ribisi) and his older wife Alice's (Debra Jo Rupp) embryo transferred into her uterus, Monica and Joey enter having an argument after Joey boasts that he and Chandler know more about Rachel and her than vice versa. Chandler backs Joey up, and the two correctly identify the contents of Rachel's shopping bag. Monica suggests a trivia contest to see who knows more about whom: the men or the women. They place a $100 bet on the outcome and Ross (David Schwimmer) puts together some questions and plays as host.\n\nMeanwhile, Phoebe learns that the doctor will implant five of Frank and Alice's embryos into her uterus, which only has a 25% chance of success. She offers to do this as many times as possible for them, but is concerned when the two reveal that they are paying $16,000, which is all of their savings, for the single IVF procedure, and is helpless to influence the results.\n\nThe trivia game begins, with various facts about the characters being revealed such as Joey's space-cowboy imaginary friend (Maurice) and Rachel's actual favorite movie (Weekend at Bernie's). A nine-all score leads to a lightning round. Monica raises the stakes: If the women win, Joey and Chandler must give up their birds. Chandler rebuts by suggesting Rachel and Monica give up their apartment to them, which Monica immediately agrees to without consulting Rachel. The girls lose the lightning round because they cannot identify Chandler's job, and the boys win.\n\nAs the four pack up their respective apartments—Rachel, in particular, displeased about having to switch—Phoebe returns home and takes a pregnancy test, though it is too soon for a result, so she sits in the apartment for several days waiting for another result. Later with packing complete, Rachel finally refuses to move as Frank and Alice come by with another pregnancy test. The boys and the girls begin to argue along with Ross, which is cut short when Phoebe emerges from the bathroom and joyfully announces she is pregnant, the mood turning to one of celebration.\n\nThe tag scene shows Rachel and Monica horrified at having to deal with living in Chandler and Joey's cramped and dirty apartment, while the boys are content to live in the girls' large apartment.\n\nProduction\n\n\"The One with the Embryos\" was co-written by Jill Condon and Amy Toomin and directed by Kevin S. Bright. In October 1997, Lisa Kudrow announced she and her husband Michel Stern were expecting their first child. When Marta Kauffman first learned of Kudrow's pregnancy, she was overjoyed and wanted to find a solution of incorporating it into the show without choosing to cover up. At the time of filming \"The One with the Embryos\" in December, Kudrow was four months pregnant and the writers discussed ways of narrating the pregnancy on the show, settling with Kudrow's character carrying her brother's embryos.\n\nAccording to David Crane, the story arc with Phoebe carrying Frank and Alice's baby was considered \"risky\". When the plot was first discussed, the main concern was whether it was \"too crazy … where's the line with Phoebe?\". Crane felt if it were not for the actors, the storyline would not have been believable. The producers found it difficult to get Giovanni Ribisi to reprise his role as Frank Jr. on a longer term basis because the actor had continuous filming commitments. A similar situation occurred with Debra Jo Rupp, who was named as a cast member in the upcoming period sitcom, That '70s Show on the Fox network.\n\nThe chick and the duck, who first appeared in \"The One with a Chick and a Duck\" as Chandler and Joey's pets were used \"as a spark\" for the main plot. The animals were originally intended for one episode but because the producers believed they got \"so much mileage out of them\", they made recurring appearances. As many television shows used similar fictional pets, the producers settled on a chicken and a duck as they were different.\n\nThe idea for the trivia contest came up in the writers' room, partially based on a real game that writer Seth Kurland watched his friends play. The \"Miss Chanandler Bong\" joke was inspired by an incident from Kurland's childhood when his surname was misspelled on an address label.\n\nKauffman told TV Guide the writers felt it was important that the trivia contest reveal new information about the characters \"otherwise it's just exposition.\" The answer \"Viva Las Gay-gas\" in response to 'What is the name of Chandler's dad's show in Vegas?' changed \"about a million times\" in drafts according to Crane. On the night the show was being filmed, writers continued to pitch for different answers in order to receive a better response from the audience. The staff found it difficult coming up with different points of view for each character as all wanted to win the game.\n\nThe writers decided to go through with the apartment switch to avoid creating fake stakes, which they called \"schmuck bait\". \"The discussion was if we do it, we have to stick to it,\" Crane said. There was never discussion of changing the look of either apartment because \"the fun of it was that they were in spaces they 'shouldn't' be in,\" according to Bright.\n\nReception\nIn its original airing, \"The One with the Embryos\" finished fourth in ratings for the week of January 12–January 18, 1998, with a Nielsen rating of 17.3, equivalent to approximately 16.8 million viewing households. It was the fourth highest-rated show on NBC that week, following ER, Seinfeld and Veronica's Closet–all of which aired on the network's Thursday night Must See TV lineup.\n\n\"The One with the Embryos\" was Courteney Cox and Matt LeBlanc's favorite episode of the series. Cox liked the episode because she enjoys playing Monica at her most competitive, while LeBlanc spoke fondly of the pace of the episode and the information about the characters that came out. He identified scenes that featured just the six core cast as the best, \"because you don't have to introduce a character—you don't have to lay any pipeline—you just get right to the funny\". On the DVD audio commentary for the episode, Marta Kauffman cited the episode being \"so much fun to do\" and enjoyed the writing process. The scene involving Phoebe talking to the embryos was Kevin S. Bright's favorite in the show's history because of Kudrow's ability to \"draw you into the scene ... even though it's only her talking to the dish\". David Crane highlights how the episode explores generosity; doing a selfless act which pays off when Phoebe  gives birth in \"The One Hundredth\". Bright moreover felt the trivia contest was the catalyst that rejuvenated the entire fourth season and \"put Friends in a different place\".\n\nIn a 2001 review, Entertainment Weekly rated the episode A+, stating that \"Thanks to the trivia contest alone, Embryos is quite possibly Friends' finest moment\". The article singles out Rachel's line \"He's a transpon—transpondster!\" (in response to the question \"What is Chandler Bing's job?\") as the best line of the episode. The authors of Friends Like Us: The Unofficial Guide to Friends called it a \"sure-fire contender for the best episode of all time ... not one to be missed under any circumstances\". In 2004, Tara Ariano of MSNBC.com wrote that the character trivia is \"revealed in a manner completely organic to the plot. Beautifully written and acted, 'The One With The Embryos' encapsulates the whole series in a single episode\". The episode was ranked #21 on TV Guides list of \"TV's Top 100 Episodes of All Time\".\n\nIn a 2018 oral history marking the episode's 20th anniversary, TV Guide declared it the series' best episode and \"Friends'' at its peak, a lightning-in-a-bottle gem.\" Kauffman said she hopes \"the episode's legacy is what people would say about the series, which is it's really funny and real and sweet.\"\n\nMerchandise\nThe episode was released as part of Friends: The Complete Fourth Season in Regions 1, 2 and 4. As part of the DVD release, \"Who Knows Whom Best? – Ross's Ultimate Challenge\" an interactive game was included, based on the quiz in \"The One with the Embryos\". The game uses clips from the show to provide answers, allows viewers to choose a team (boys or girls) and call the coin toss.\n\nReferences\n\nExternal links\n\"The One with the Embryos\" at the Internet Movie Database\n\nFriends (season 4) episodes\n1998 American television episodes",
        "  This letter concentrates on the non-equilibrium evolution of magnetic field\nstructures at the onset of recombination, when the charged particle current\ndensities decay as neutrals are formed.\n  We consider the effect that a decaying magnetic flux has on the acceleration\nof particles via the transient induced electric field. Since the residual\ncharged-particle number density is small as a result of decoupling, we shall\nconsider the magnetic and electric fields essentially to be imposed, neglecting\nthe feedback from any minority accelerated population.\n  We find that the electromagnetic treatment of this phase transition can\nproduce energetic electrons scattered throughout the Universe. Such particles\ncould have a significant effect on cosmic evolution in several ways: (i) their\npresence could delay the effective end of the recombination era; (ii) they\ncould give rise to plasma concentrations that could enhance early gravitational\ncollapse of matter by opposing cosmic expansion to a greater degree than\nneutral matter could; (iii) they could continue to be accelerated, and become\nthe seed for reionisation at the later epoch $z \\approx 10$.\n",
        "The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn’t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n",
        "This paper studies knowledge transfer problem from small capacity network to bigger one. This is a follow-up work of Net2Net (ICLR 2015) and NetMorph(ICML 2016).  \nComments\n- 1) This paper studies macroscopic problem, with the morphing process composed by multiple atomic operations. While the atomic operations are proposed in Net2Net and NetMorph, there has not been study of the general modularized process principally. Thus this paper asks a novel question.\n- 2) The solution by composing multiple atomic transformations seems to be quite reasonable.\n- 3) In the “related work” section, it is better to change “network morphism” to “knowledge transfer” or in the subsection title, most of these works are known as knowledge transfer and it helps to connect to the existing works.\n- 4) The author shows experiments on variants of ResNet. While the experiment shows that initializing from ResNet gives better error rate than the ones trained from scratch, it is unclear what the source This paper studies knowledge transfer problem from small capacity network to bigger one. This is a follow-up work of Net2Net (ICLR 2015) and NetMorph(ICML 2016).  \nComments\n- 1) This paper studies macroscopic problem, with the morphing process composed by multiple atomic operations. While the atomic operations are proposed in Net2Net and NetMorph, there has not been study of the general modularized process principally. Thus this paper asks a novel question.\n- 2) The solution by composing multiple atomic transformations seems to be quite reasonable.\n- 3) In the “related work” section, it is better to change “network morphism” to “knowledge transfer” or in the subsection title, most of these works are known as knowledge transfer and it helps to connect to the existing works.\n- 4) The author shows experiments on variants of ResNet. While the experiment shows that initializing from ResNet gives better error rate than the ones trained from scratch, it is unclear what the source is.\n- 5) One major advantage of this type of knowledge transfer (Net2Net, NetMorph) is to speedup training and model exploration. There seems to be no experiments demonstrate such advantage (possibly due to the lose initialization of BatchNorm). This is the major drawback of this paper.\n-6)  The method proposed by the author can in principle do quite complicated transformation, e.g. transform  an entire resnet from a single conv layer, the experiment only consists of simple module transformations, which in some way can be covered by atomic operations. It would be more interesting to see what the results of more complicated transformations are (even if they are not as effective). \n\nIn summary, this paper studies a novel problem of knowledge transfer in a macroscopic level. The method could be of interest to the ICLR community. The experiments should be improved (comment 5) to make the results more convincing and practically useful and I strongly encourage the authors to do so.\n",
        "The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.\n\nAn interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.\n",
        "  Interrelation between the two-particle and mean-field problems is used to\ndescribe the strong pseudogap and superconducting states in cuprates. We\npresent strong pseudogap state as off-diagonal short-range order (ODSRO)\noriginating from quasi-stationary states of the pair of repulsing particles\nwith large total momentum (K - pair). Phase transition from the ODSRO state\ninto the off-diagonal long-range ordered (ODLRO) superconducting state is\nassociated with Bose-Einstein condensation of the K - pairs. A checkerboard\nspatial order observable in the superconducting state in the cuprates is\nexplained by a rise of the K - pair density wave. A competition between the\nODSRO and ODLRO states leads to the phase diagram typical of the cuprates.\nBiordered superconducting state of coexisting condensates of Cooper pairs with\nzero momentum and K - pairs explains some properties of the cuprates observed\nbelow Tc: Drude optical conductivity, unconventional isotope effect and two-gap\nquasiparticle spectrum with essentially different energy scales.\n",
        "  The paper is devoted to classification problem of finite dimensional complex\nnone Lie filiform Leibniz algebras. Actually, the observations show there are\ntwo resources to get classification of filiform Leibniz algebras. The first of\nthem is naturally graded none Lie filiform Leibniz algebras and the another one\nis naturally graded filiform Lie algebras. Using the first resource we get two\ndisjoint classes of filiform Leibniz algebras. The present paper deals with the\nsecond of the above two classes, the first class has been considered in our\nprevious paper. The algebraic classification here means to specify the\nrepresentatives of the orbits, whereas the geometric classification is the\nproblem of finding generic structural constants in the sense of algebraic\ngeometry. Our main effort in this paper is the algebraic classification. We\nsuggest here an algebraic method based on invariants. Utilizing this method for\nany given low dimensional case all filiform Leibniz algebras can be classified.\nMoreover, the results can be used for geometric classification of orbits of\nsuch algebras.\n",
        "I can give you a very basic overview. \n\nFirst off, you have the Dungeon Master. He's the person who creates the world and makes all the big decisions. Everybody else creates a character. \n\nThe Dungeon Master begins weaving the story, explaining to the characters where they are and what their goal is- a good DM will do this with lots of enthusiasm and flair!\n\nThe characters then make their own choices based on their religion, their background, and their skills. For example- a Human who hates Orcs would never say, \"I love Orcs! Let's be friends with this Orc!\" Instead, they'd be more likely to say, \"Kill the Orc!! I hate that guy!\"\n\nApart from the storytelling and role playing, there is combat. Whenever you come across a bad guy on your journey, the DM has created a character for him and will control him. If you decide to attack the bad guy, you must roll the dice a few times. \n\nThe first time is to figure out who gets to attack first- this can be a big factor in who wins a battle. \n\nThe second dice roll is to determine whether you hit him or not- if you roll below a certain number (which the DM knows but you do not), then you miss. If you roll a 1, that is a critical failure and commonly ends with you breaking your weapon or hurting yourself. Conversely, if you roll a 20, that's a critical hit and you get to do extra damage! \n\nDamage is the third roll. That's how you figure out how much health he loses when you attack him. \n\nNow, battles aren't the only place you get to show off your talents and skills! Often, throughout the story, you will have to do a skill check. These are used to figure out if you can complete a certain task and how well you do it. \n\nFor example- you need to lie to a bartender so he doesn't know you're looking for the treasure! If you roll a high speech skill check, you might say- \"We're not searching for treasure- we're just in town to sell our wares and buy new armor.\" If you roll low, you might say- \"We... uh... treasure? I never met the guy! Ahaha... we're definitely not treasure hunting. Nope. Not us. I don't even know what treasure is!\" \n\nThere are other skill checks, too- such as climbing, riding horses, swimming, and lots more!\n\nReally, the best way to learn is get a few friends and try it yourself! I highly recommend it!\n\nEDIT:\nIf you want to learn more, get help from current players, or join a group- check out r/dungeonsanddragons and r/lfg! \n\nANOTHER EDIT:\nAlso r/rpg! (Thank you, alienman911!) Also also r/dnd! (Thanks, DevilChicken!)",
        "We thank all the reviewers for the critical comments.\n\nAll the reviewers agree that our experimental results convincingly support our hypothesis. The disagreement is that whether our paper is suitable for ICLR and whether our contribution is important to know by the representation learning community.\n\nWe made the following changes in the pdf file revision addressing the reviewers’ concerns:\n\n1. We updated the abstract, emphasizing that the goal of our paper is not only about presenting another architecture for video captioning, but also about new video representations that are suitable for the video captioning task. Attention mechanisms are just technical means to achieve this goal.\n\n2. We updated the introduction, adding three technical challenges to overcome to use adaptive spatiotemporal feature representations with dynamic feature abstraction for the captioning task. We also explained why naïve approaches such as MLP/max(average)-pooling did not meet our requirements.\n\n3. We updated the related work including the hypercolumn representation.\n\n4. We put additional experimental results comparing our approach ASTAR to hypercolumns, MLP, and max/average-pooling in Table 1 on Page 8. Our results clearly demonstrate the significance of dynamically selecting a specific level. The hypercolumn representation without level selection has much worse performance than our method.\n\n5. We updated Figure 1 and moved Figure 2 below it to emphasize our contributions following the review comments.\n\n6. We added the reference of Yao et al., ICCV 2015.\n\nIn summary, we believe that our contributions are clear and important to know by the representation learning community. This line of thinking might influence other researchers to perform additional research on classification and other tasks. It also inspires us to design new deep architectures to efficiently learn and effectively utilize different levels of feature representations in a dynamic fashion.",
        "It takes a huge amount of mechanical pressure to kill bacteria. There is a food preservation process called [Pascalization](_URL_1_) that uses pressure to kill molds and spores. It is used as an alternative to [Pasteurization](_URL_0_), which uses heat. The pressures involved a pretty huge -- 50,000 PSI. That's giant hydraulic press kind pressure. \n\nPascalization is not as effective as Pasteurization. I've only ever seen it used for very acidic products, like grapefruit juice. Strong acidity excludes many of the most dangerous food-borne pathogens, and so pascalization is more to prevent spoilage than to protect humans. It does not render food completely sterile, but it does extend shelf life.\n\nWhen you step on something as small as a bacterial cell, you are also stepping on the goop the bacteria is living in (microbiologists call it a \"matrix,\" but I think \"goop\" is more illustrative). There is pretty much always goop, or the bacteria wouldn't be able to survive. Some bacteria do swim some of the time, but most of the time, they rely on diffusion to move compounds around their immediate vicinity. If you shrank yourself down to this scale, water would be like damp concrete or hard packed earth that is being vibrated (not a perfect analogy, but close). You can push things through it, but it takes a lot of work. Swimming is very, very hard work, and you'd want to avoid it if you can. Water with other stuff in it -- i.e., goop -- would be even stiffer. The cells are buried in that. Even if the layer of goop is completely invisible, from a cell's point of view, it could still be like sitting at the bottom of a lake.\n\nWhen you step on the goop, what will happen is the goop will squirt around the hard structures to which it is attached. This probably won't bother any cells that aren't already ruptured or breached. They have to be tough to move through the goop, and the kind of violent disturbance you would cause by stepping on the goop is probably not very different from other disturbances that the bacteria rely on for long-distance travel.\n\nAs your foot comes down on the floor, some of the goop will get trapped in small cavities, and in those places the local pressure could get quite high. Probably not 50,000 PSI, though, and so I doubt many cells would be physically harmed. [Unless you are the girl in the Paul Simon song, maybe.](_URL_2_)\n\nWhat will happen, though, is that some bacteria will be separated from the nutrient environment they were growing in, and that probably will kill them.",
        "Alright, so let’s start with WWII, because – let’s face it – you can never go wrong starting with a World War.  The entirety of the Korean peninsula was under military occupation by the Japanese Empire, and in fact had been since the early 1910’s.  The occupation was brutal, Koreans were treated as slaves (sexual and otherwise), and Japan did a helluva job in trying to extinguish Korean culture altogether.  But that’s for another post.\n\nAt the conclusion of the war, the Korean peninsula was divided – much like Germany – into two zones of control; the southern half would be put under the supervision of the nascent United Nation ~~(and de facto NATO control)~~, while the northern half was occupied by the Soviet Union.  This came about because of quite a few reasons that are complicated and don't really factor into what I'm talking about...  there's a more detailed explanation is in the below comments.  There was initially a hope of a peaceful reunification into a single Korean nation, but… y’know… then there was the Cold War, and that went out the window.  Enter Stage Left: Kim Il Sung.  With the backing of the Soviet general Terentii Shtykov, Kim was made the chairman of the Provisional People’s Committee for North Korea (*Bukjoseon Inmin Wiwonhoe*) in February 1946.\n\nUnder Soviet control, the feudal landlord system that had been “the norm” in the Joseon Kingdom as well as during the Japanese Occupation was ended, with most of the upper classes/landlord and people with ties to Japan fleeing south.  Under Shtykov’s direction, key industries were nationalized.  But from the outset, Kim Il-sung was positioning himself to be the ultimate power in North Korea once the Soviets inevitably departed back to Moscow (which would occur some 2 years later in 1948.)  From Cummings:\n\n >  Kim became the top leader when he chaired the North Korean Interim People’s Committee in February 1946, and he held on to maximum power until he died.  Within months of his accession to power, the evidence of a hagiographic and grandiose style was almost as palpable as it is today.  Agents making forays into North Korea in 1946 reported that pictures and posters of Kim festooned telephone poles and the like with tales of how “wise, clear-sighted, spirited, and wonderful” Kim Il-sung was.  At the same time, articles appeared describing him as “the Sun of the Nation,” “a beautiful new red star in the sky,” wisely guiding everything with his “brilliant, scientific” methods.\n\nA further example of this early deification can be found from Kim’s own personal biographer – which remains the “official state history” in North Korea even today.  The author, an unknown member of the guerrilla unit he had commanded during the resistance to Japanese occupation, wrote:\n\n >  This sort of person naturally has an extremely strong power of attraction to others […] And it goes without saying that a guerrilla organization with such a person at the center is incomparably strong.  The sublime good fortune of our guerrilla detachment was to have at our center *The Great Sun*.  Our general commander, great leader, sagacious teacher, and intimate friend was none other than General Kim Il Sung.  Our unit was an unshakeable one, following General Kim and having General Kim as the nucleus.  The General’s embrace and love are like the Sun’s, and when our fighters look up to and receive the General, their trust, self-sacrifice and devotion are such that they will gladly die for him.\n\nIt’s rather interesting the language that this author uses to describe the General.  It’s all highly moralistic – and all *very Confucian* in nature.  Kim embodies benevolence and love, and inspires in others trust, obedience, and respect.  This is a particularly resonant choice in a traditionally Confucian culture like Korea (and China, for that matter) because it is the very embodiment of the ideal relationship of not only Ruler and Ruled, but also of Father and Son; both are encapsulated in the most important of Confucian ethics – filial piety.\n\nCummings expands on this particular philosophy as it emerged in the swiftly-developing cult of Kim:\n\n >   And eleventh-century Song scholar, Fan Ziyu, said that “order and disorder in the world all depend on the heart-and-mind of the ruler.  If his heart-and-mind are correct, than the myriad affairs of the court will not be incorrect.”  The formula is virtuous-mind-become-master-of-the-body.  Now let the body be the body politic, and presto, the virtuous king becomes its master. […] Precisely by decades of reading the classics and tutoring by the philosophers […] the king then becomes a perfected being, “the supreme mind of the nation,” and woe to the person who challenges his authority or denies that he can walk on water.”\n\nCummings points out that, in spite of the Soviet trappings, the North Korean state that emerged from the wreckage of the Second World War was far closer to a Neo-Confucian kingdom like the Song or Ming Dynasties of Imperial China, than of a truly Stalinist-Soviet state.  “With its absurdly inflated hero worship and its nauseating repetition, the North Korean political rhetoric seems to know no bounds; to a person accustomed to a liberal political system it is instinctively repellent.  But it had been there from the beginning.”  And indeed, it was **the** dominant political ideology across East Asia for millennia.  The Chinese emperors were the literal Sons of Heaven, so too the Japanese Emperor being a literal embodiment of the Sun God, or even the Mongolian Khans claiming the mandate of Tengri – the Child of the Blue Sky.  Koreans had been wholehearted followers of this philosophy of governance for thousands of years, that its was natural and right that there would, in essence, be a divine Mandate of Heaven conferred onto a single and singular individual who would embody the entire collective wisdom, righteousness, and morality of the whole nation and people.  Once you understand where that idea comes from – and how long it dominated not just the Korean peninsula, but East Asia as a whole – its adoption by someone like Kim Il Sung, and its acceptance by the People of North Korea – doesn’t seem so far fetched.  If anything, its ultimately rejection by all of its neighbors in their own political systems – South Korea, China, and Japan – was something of a miracle in itself.  The PRC still operated under effectively the Great Man Personality Cult of Mao Zedong until his death in 1978, South Korea.  Japan had their Confucian-inspired “ruled by the Sun God” political system forcibly overthrown by an outside power in 1945, and even South Korea was under the military thumb of Park Chung-hee until his assassination in 1979… and even now his daughter, Park Geun-hye, is the *current president* of South Korea.  East Asia hasn’t divorced itself as completely from the Confucian-dynastic style of rule as one might initially think…\n\n________________\n#The Korean War\n\nIn September 1948, the Democratic People’s Republic of Korea was officially declared with Soviet backing (and tacit understanding that they held legal sovereignty over the entire peninsula, not just the northern half).  Kim was, of course, at the helm as Party Chairman.  Nevertheless, there’s quite a bit of evidence showing that both the USSR and China only actually backed Kim’s push toward “re”conquest of the South with extreme reluctance.  Mao, for instance, only approved of the invasion once Kim had assured him that he’d already gotten Ol’ Papa Joe’s approval.  Weatherby writes: \n\n >  In the spring of 1950 Stalin’s policy toward Korea took an abrupt turn. During meetings with Kim Il Sung in Moscow in April,66 Stalin approved Kim’s plan to reunify the country by military means and agreed to provide the necessary supplies and equipment for the operation. The plan to launch the assault on South Korea was Kim’s initiative, not Stalin’s. The Soviet leader finally agreed to support the undertaking only after repeated requests from Kim. Furthermore, Stalin’s purpose was not to test American resolve; on the contrary, he approved the plan only after having been assured that the United States would not intervene.\n\nThis, of course, didn’t exactly go as planned.  The US and ~~NATO~~ the UN did indeed intervene, and rather than rolling over the South, the North Koreans themselves got rolled all the way back up to their northern border before the PRC’s PLA at last stepped in and ground the whole conflict into a stalemate that – as each and every article on the Korean War and/or North Korea simply *must* reiterate (as so who am I to break tradition?) – technically is still ongoing.\n\n(Part 2 Below)",
        "The paper introduces a variation to the CNN-based texture synthesis procedure of Gatys et al. that matches correlations between spatially shifted feature responses in addition to the correlations between feature responses at the same position in the feature maps. \nThe paper claims that this \na) improves texture synthesis for textures with long-range regular structures, that are not preserved with the Gatys et al. method\nb) improves performance on texture inpainting tasks compared to the Gatys et al. method\nc) improves results in season transfer when combined with the style transfer method by Gatys et al. \nFurthermore the paper shows that\nd) by matching correlations between spatially flipped feature maps, symmetry properties around the flipping axis can be preserved.\n\nI agree with claim a). However, the generated textures still have some issues such as greyish regions so the problem is not solved. Additionally, the procedure proposed is very costly which makes an already slow texture synthesis method substantially slower. For example, in comparison, the concurrent work by Liu et al. (",
        "Chartered in 1886, the Canandaigua Street Railroad was a local streetcar line serving the lakeside city of Canandaigua, New York beginning in 1887. The railroad was sold to the Canandaigua Electric Light and Railroad which rebuilt and electrified the line in 1892. The Ontario Light and Traction Company purchased it in 1900, and leased the line to the Rochester and Eastern Rapid Railway in 1903. In 1905, the line came under the control of the Mohawk Valley Company, and in turn, New York State Railways in 1909. Operation was converted to bus operation some time in the 1920s, but this service ended when the Rochester and Eastern Rapid Railway shut down on July 31, 1930. The lease of the former Canandaiua lines was allowed to lapse.\n\nEarly History (1887-1905)\nThe line began operating as a horse car line on September 6, 1887, with a two-mile extension following a month later. Stretching from the steamship wharf at the north end of Canandaigua Lake, the line ran along Main Street to the western edge of town. A branch to the fairgrounds was opened in 1889. All cars met the steamships that brought passengers across Canandaigua Lake. The railroad closed in July 1892, and was sold to the Canandaigua Electric Light and Railroad Company which electrified and rebuilt the lines for streetcar operation in 1893. A hydro-electric plant in Littleville provided the power for the streetcar line.\n\nThe Ontario Light and Traction Company purchased the line on June 1, 1900, maintaining local service in the city. In 1901, the Rochester and Eastern Rapid Railway was chartered to build a new interurban line between Rochester and Geneva. Headquartered in Canandaigua, the Rochester and Eastern Rapid Railway leased the Ontario Light and Traction line in 1903 to gain access through the city limits. At this time the power plant in Littleville and the substation on Phoenix Street were abandoned because the Rochester and Eastern had built a larger power plant of its own in Canandaigua. In 1905 the branches serving the fairground and the steamship wharf were abandoned.\n\nControl by New York State Railways (1906-1930)\nThe New York Central Railroad began taking an interest in the streetcar and interurban railways springing up along their territory. The Mohawk Valley Company was formed in 1905 to take control of the Rochester Railway Company, the Rochester and Sodus Bay Railway, and the Rochester and Eastern Rapid Railway. In 1909 these properties were combined to form New York State Railways, and the Canandaigua route became part of the Rochester Lines. In 1917, the electric utility properties of the Ontario Light & Traction were sold to the Rochester Railway and Light Co., leaving the railway property in control of New York State Railways.\n\nThe Canandaigua line was never particularly busy, as two streetcars were assigned to the line to handle all of the traffic. Known locally as \"The Dinky,\" streetcars would run from the Orphans Asylum down to the lake and back. Sometime in the late 1920s, local city operation was converted to bus, but continued to be operated by New York State Railways. The Great Depression brought fewer customers and increased competition from better highways and more automobiles joining the roads. When the Rochester and Eastern Rapid Railway shut down for good on July 31, 1930, the lease of the Ontario Light and Traction was allowed to lapse. Ontario Light and Traction did not resume independent operation, and the streetcar era came to a close in Canandaigua.\n\nReferences\n\n \n\nStreetcars in New York (state)\nDefunct New York (state) railroads\nRailway companies established in 1886\nRailway companies disestablished in 1900\nAmerican companies disestablished in 1900",
        "  Using the matrix product formalism, we define a multi-parameter family of\nspin models on one dimensional chains, with nearest and next-nearest neighbor\nanti-ferromagnetic interaction for which exact analytical expressions can be\nfound for its doubly degenerate ground states. The family of Hamiltonians which\nwe define, depend on 5 continuous parameters and the Majumdar-Ghosh model is a\nparticular point in this parameter space. Like the Majumdar-Ghosh model, the\ndoubly degenerate ground states of our models have a very simple structure,\nthey are the product of entangled states on adjacent sites. In each of these\nstates there is a non-zero staggered magnetization, which vanishes when we take\ntheir translation-invariant combination as the new ground states. At the\nMajumdar-Ghosh point, these entangled states become the spin-singlets\npertaining to this model. We will also calculate in closed form the two point\ncorrelation functions, both for finite size of the chain and in the\nthermodynamic limit.\n",
        "  The study of the innermost circumstellar layers around AGB stars is crucial\nto understand how these envelopes are formed and evolve. The SiO maser emission\noccurs at a few stellar radii from the central star, providing direct\ninformation on the stellar pulsation and on the chemical and physical\nproperties of these regions. Our data also shed light on several aspects of the\nSiO maser pumping theory that are not well understood yet. We aim to determine}\nthe relative spatial distribution of the 43 GHz and 86 GHz SiO maser lines in\nthe oxygen-rich evolved star R Leo. We have imaged with milliarcsecond\nresolution, by means of Very Long Baseline Interferometry, the 43 GHz (28SiO\nv=1, 2 J=1-0 and 29SiO v=0 J=1-0) and 86 GHz (28SiO v=1 J=2-1 and 29SiO v=0\nJ=2-1) masing regions. We confirm previous results obtained in other\noxygen-rich envelopes. In particular, when comparing the 43 GHz emitting\nregions, the 28SiO v=2 transition is produced in an inner layer, closer to the\ncentral star. On the other hand, the 86 GHz line arises in a clearly farther\nshell. We have also mapped for the first time the 29SiO v=0 J=1-0 emission in R\nLeo. The already reported discrepancy between the observed distributions of the\ndifferent maser lines and the theoretical predictions is also found in R Leo.\n",
        "This paper is about using Bayesian neural networks to model learning curves (that arise from training ML algorithms). The application is hyper-parameter optimization: if we can model the learning curve, we can terminate bad runs early and save time. The paper builds on existing work that used parametric learning curves. Here, the parameters of these learning curves form the last layer of a Bayesian neural network. This seems like a totally sensible idea. \n\nI think the main strength of this paper is that it addresses an actual need. Based on my personal experience, there is high demand for a working system to do early termination in hyperparameter optimization. What I'd like to know, which I wish I'd asked during pre-review questions, is whether the authors plan to release their code. Do you? I sincerely hope so, because I think the code would be a significant part of the paper's contribution, since the nature of the paper is more practical than conceptual.\n\nThe experiments in the paper seem thorough but the results are a bit underwhelming. I'm less interested in the part about whether the learning curves are actually modeled well, and more interested in the impact on hyperparameter optimization. I was hoping to see BIG speedups as a result of using this method, but I am left feeling unsure how big the speedup really is. Instead of \"objective function vs. iterations\" I would be more interested in the inverse plot: number of iterations needed to get to a fixed objective function value. Since what I'm really interested in is how much time I can save. Ideally there would also be some mention of real time as sometimes these hyperparameter optimization methods are themselves so slow that they end up being unusable. \n\nFinally, one figure that I feel is missing is a histogram of termination times over different runs. This would provide me with more intuition than all the other figures. Because it would tell me, what fraction of runs are being terminated early. And, how early? Right now I have no sense of this, except that at least *some* runs are clearly being terminated early, since this is neccessary for the proposed method to outperform other methods.\n\nOverall, I think this paper merits acceptance because it is a solid effort on an interesting problem. The progress is fairly incremental but I can live with that.",
        "Careful, friend, we live in dangerous times. If you've got reasons to disappear, you might want to keep them to yourself! But if you're quick and quiet enough to get out of town before that neighbor woman's husband comes looking for you, there's some choices you can make. \n\nFirst of all, if you're thinking about hiding out in the woods, robbing passerby and such, then don't. It's better to face the gallows for whatever crime you've committed than go to the woods. Sure, it seems a merry time at first, with the robbing and the murder and the poaching. But sometime or another, you'll run out of folk you can rob, or the locals will drive you out at the point of a lance. You'll go somewhere to try and poach, because you're a thieving little shit now and forever, but you really don't want to do that. If you trespass in the lands of your betters for long enough, someday, you might run into me. I'm a forester, as you might have guessed from the bow and the green hood and all. My master entrusts me to watch his lands for him. In the fields and villages, you'd deal with the reeve and he'd get out the men to whip you like an ass that won't pull its cart. But in the woods, you deal with me. If I catch someone poaching or robbing in my master's forests, it'll be an arrow through the eye for you. Or worse. I put an arrow through a poacher's bollocks once by accident when I messed up my draw. He probably wishes I shot him dead. I know, that venison sounds tempting. But seriously, don't. I'm not the only one on the estate who's been in the wars, either, so don't go thinking you can slip by me and rob the place blind when I'm away. \n\nIf you want my advice, what you'd do is sign on with a garrison. Some remote place. Try the Scottish border, or Ireland, or around Calais. Supposedly, there'll be peace now because King Richard signed that treaty with the French at [Leulinghem](_URL_0_). I say bollocks to that. We've been fighting for decades now. I bet we'll be fighting for a hundred years! Ha, a hundred years war, can you imagine? Anyways, just wait a few months and I guarantee there'll be another war somewhere. Hell, try finding a company going to the continent. Maybe to Italy. I hear the plunder's nice this time of year. You can use a fake name to sign a contract, if you like. If the garrison captain needs an extra man and he sees that you know your way about a sword or a bow, he'll let you sign up even if you call yourself Robin Hood. If you make your fortune in a war, then you can come back to England and no one will care that you murdered or robbed someone in the next shire over. Or stay in London. Everyone's some kind of criminal in London, especially the shopkeepers. You'll fit right in! It might not be easy, but soldiering is the best way to get yourself a new life if you've made a mess of your old one. If there's enough of a need for soldiers, the king might give you a pardon for your crimes in exchange for fighting for him. Some kind of life, eh? But it's better than dying as a poacher in the woods or at the end of a noose.",
        "A summary of the reviews and discussion is as follows:\n \n Strengths\n Code for matrix library sushi2 and DL library sukiyaki2 are on Github, including live demos -- work is reproduceable (R2)\n Work/vision is exciting (R2)\n \n Weaknesses\n Projects preliminary (documentation, engineering of convolutions, speed, etc.) (R2)\n Perhaps not the right fit for ICLR? (R3) AC comment: ICLR specifically lists *implementation issues, parallelization, software platforms, hardware* as one of the topics of interest\n Doesn’t advance the state-of-the-art in performance (e.g. no new algorithm or UI/UX improvement) (R3)\n \n The authors responded to the pre-review questions and also the official reviews; they updated their demo and paper accordingly.\n \n Looking at the overall sentiment of the reviews, the extensive feedback from the authors, and the openness of the project I feel that it is a valuable contribution to the community. \n \n However, given that the paper doesn't clearly advance the state of the art, the PCs believe it would be more appropriate to present it as part of the Workshop Track.",
        "The paper is well motivated to explore the sparsity in RNNs after lots of works on sparse CNNs.\nIn general, methods in CNNs could be generalized to RNNs. It would be more comprehensive to compare the proposed method with those methods used in CNNs. For the speedup part, it would be more convictive to compare with structurally-sparse DNNs, which are recently proposed in CNNs:\n(1) ",
        ";\n, See the \"Things You'll Need\" list below.\n\n, It should be near the edge. A saber saw is best for this task.\n\n,,,, Level the drum. The top of the drum should be at least 4 inches (10.2 cm) below grade.\n\n,,, Cut another nipple about 2 1/2\" long and glue it in the other end.\n\n, The end with the short nipple should go into the upper drum. It should look like the photo in Step 9.\n\n, We will deal with sealing the connection to the upper drum later.\n\n, Align the \"Y\" to meet the incoming waste line, and glue it into the toilet flange.\n\n, The face of the two 45 degree bends should be perpendicular to the trench.\n\n,,,,, Pound the stake down until the level shows level when the block is on the second stake. The second stake is now 1\" lower than the first, or 1/4\" per foot.\n\n,,,, Slide one end into the 45 degree bend of the lower drum. Repeat on the other side.\n\n, Adjust by adding or removing gravel under the pipe.\n\n, Try a two part epoxy. You could also use silicon caulk. See the photo in step 6 for finished view of epoxy. Consider using flex pipe for this, so that if the ground shifts it will give a little.\n\n, Bury the trench to the top of the bottom drum with the remaining gravel.\n\n, This will prevent the soil from seeping into the gravel.\n\n,,",
        " These medications include acetaminophen, ibuprofen or naproxen, and they will help you to manage the body aches that you are going to feel during withdrawal. The small aches that you've been handling easily with narcotics are going to feel magnified as the drugs leave your system. Ibuprofen and naproxen are both NSAIDs which work very similarly and if you mix them, remember each contributes towards the maximum dose of any NSAID (dosing for acetaminophen would be independent).;\n, Sedating antihistamines such as diphenhydramine (Benadryl), dimenhydrinate (Dramamine) or meclizine (Dramamine II) will help you to conquer the nausea and sleep through a lot of the queasiness.\n\n, Your best choice is loperamide hydrochloride (Imodium AD). While this medication is structurally similar to the opioid meperidine (Demerol), loperamide doesn't cross the blood/brain barrier.\n\n\nBy acting on the opioid receptors in your intestines, loperamide will stop intestinal spasms and the diarrhea that comes with them. The medication moves food more slowly through your gut and increases water absorption. Many people report that this medication is the most important key to easing withdrawal symptoms.\nOnly take loperamide if you need it, but take double the suggested dose on the package. Remember, your intestines are used to a barrage of narcotics. A normal dose simply isn't going to be as effective.\nSkip loperamide if you are pregnant, if you have colitis or if you have a high fever of F 100.4 (38 C) or more. Also, don't take it if you start to notice blood or mucus in your stools., You definitely don't want to run out of medication and have to walk or drive yourself to the store in full withdrawal mode.\n\n, Clonidine a non-opiate, non-addictive blood pressure medication not to be confused with the addictive anti-anxiety medication Klonopin. Clonidine inhibits your body's sympathetic response and helps decrease the sweating, chills, anxiety and restlessness that you may feel during withdrawal.\n\n\nSide effects include dry mouth, sleepiness and, for some, insomnia. Your blood pressure will drop, so if you already have low blood pressure or fainting spells, you should definitely talk to your doctor. Also, be careful when you stand up quickly. Many people who take blood pressure medication \"see spots\" and feel dizzy when they suddenly change positions.\nClonidine does have the potential for physical addiction even though it doesn't remotely provide the euphoria you can get from narcotics. Try to maintain a low dosage: Take between 0.1 to 0.3 mg of Clonidine, 2 to 3 times daily.\n\n, Suboxone is a long-lasting partial opioid agonist that blocks withdrawal symptoms.\n\n\n\"'Advantages':\"., Suboxone is available by prescription, which means that you won't have to go to the methadone clinic., You may find it's easier to kick than methadone, and if you quit using other drugs, suboxone can help you feel normal pretty quickly. You can take it once in the morning, and you'll feel fine until the next morning.\n\nDisadvantages: You will have withdrawals from suboxone, although for some people they will be less severe and of shorter duration. Suboxone can also be expensive as doctors must have special training to prescribe it, and they can only have so many patients who take suboxone at one time. Also, no generic version of suboxone is available in the US\nBuprenorphine has both agonist, and antagonistic properties. Because of the antagonistic properties, as opposed to a pure agonist like methadone, approximately 10 percent of patients do not respond well to this medication.\nWait until the half-life of your opioid of choice has passed before you take suboxone. If you do not, you will have withdrawal symptoms which can be more severe and of much longer duration than opiate withdrawal, so be very careful with these.\n\n, These drugs can have physically addictive properties, and they can cause you to experience withdrawal.\n\n\nStick to small doses. Five mg of diazepam 2 to 3 times daily, or 0.5 to 1.0 mg of clonazepam 2 to 3 times daily, will get the job done. Don't take them long-term, and taper them off when you decide to stop using them.\nIf you abuse Valium or Klonopin and don't properly detox, you could experience symptoms including tonic clonic seizures and even death.\n\n,\n\n\nShare your narcotic addiction with your doctor\nYou can also chew some kratom leaves to take the edge off of your symptoms. However, don't make a \"kratom cocktail:\" boiled kratom leaves, ice, Coca-Cola and cough syrup. While the leaves themselves can really help to curb withdrawal, making the cocktail will only give you a new addiction and a new problem. Order kratom online in the US or Europe. It's illegal in Australia.Avoid alcohol. Alcohol is a depressant, and while the buzz may distract you for a while, alcohol can cause your mood to plummet and can interfere with your ability to sleep.\n\n, You may experience some or all of the following:\n\n\nAgitation and anxiety\nMuscle aches\nTearing\nSweating\nInsomnia\nRunny nose\nAbdominal cramping\nDiarrhea\nVomiting\nDilated pupils\nChill bumps\n\n,\n\n\nKeep your tablet or TV and DVD player nearby so that you can watch some lighthearted movies.\nMake sure that your room is at a comfortable temperature, and make sure that you have some soft blankets and maybe a fan. Prepare to change your sheets often because of sweating.\nWear loose and comfortable clothing. Again, you'll probably have to change clothes a lot because of the sweating.\n\n, If you don't plan to check yourself into a rehab facility, then stay with someone who can support you during the withdrawal period., Withdrawal may take up to 2 weeks, so try to take some time off of work. If you have a family, then check yourself into a rehab facility or go somewhere where your children won't have to see you going through withdrawals.\n\n, Reduce your doses of opioids or medications by about 20 to 25 percent every 2 or 3 days to minimize withdrawals.\n\n, Check out your local methadone clinic so that you can gradually wean yourself off of narcotics by taking gradually decreasing doses of methadone. Community detox will allow you to go on with your daily life without checking in to an in-patient facility.\n\n, Withdrawal can bring out these negative behaviors, which could put you in real danger. If you have a history of depression or other psychiatric problems, then do your detox under medical supervision.\n\n, Rehab will give you a variety of in-patient treatment options:\n\n\n\nDetox under anesthesia. With this kind of detox, you are placed under anesthesia and given substantial doses of opiate-blocking drugs. Use caution with this method because opiate withdrawal often causes vomiting, and you can aspirate or choke on vomit when you are under anesthesia.In-patient therapy and support groups. While you stay in a rehabilitation facility, you can talk to counselors about your addiction or you can spend time in support groups with other addicts.\n\n, Try some of these strategies:\n\n\nTell yourself that your withdrawal pains are like labor pains. You're giving birth to a new you.\nWrite a notice to yourself that says, \"I'm a fantastic person, and I'm doing something amazing.\" Post the notice where you can see it.\nGive yourself a non-drug reward for every day that you make it through withdrawal.\n\n, You may not feel like eating or drinking fluids, but your body needs nourishment and hydration. Eat saltines or yogurt or other foods that are easy on your stomach. Also, be sure to drink water or fruit juice to replace any fluids that you lose from vomiting or diarrhea.\n\n, Don't overdo it, but take a short walk around your neighborhood or do some light housework. Exercise will keep your spirits up and will help to distract you from your symptoms.\n\n, As corny as it sounds, you'll feel much better about the experience of quitting if you truly believe you can do it successfully. Tell yourself repeatedly that you can do this, that you're strong and motivated and capable. Try to be consistent in your positivity. Come up with a mantra or something similar if that helps you!\n\n, You won't maintain your resolve if you're trying to quit for your parents, your kids or even your spouse. Make up your mind that you're done getting high and you're done with the lifestyle.\n\n, Narcotics Anonymous follows a 12-step program similar to Alcoholics Anonymous to help you to navigate life after opiates. In addition to following the 12 steps, you can go to meetings to talk to people who share your experience, and you'll be connected with a sponsor who you can talk to at any time as you fight to beat your addiction.\n\n, Steer clear of people who will enable your lifestyle and friends from your druggie days. Also, avoid dealers or anyone else who used to give you access to drugs.\n\n, The acute withdrawal may take anywhere from 3 days to 2 weeks. However, you can also experience something called Post-Acute Withdrawal Syndrome, or PAWS.\n\n\nPAWS often mimics other psychological disorders like depression, anxiety disorder and psychosis. People often experience lethargy, fuzzy thinking, memory problems, sleep disturbances and emotional swings. At the far end of the spectrum, people have thoughts of suicide.\nGet some support if you find yourself going through the symptoms of PAWS. If left untreated, the symptoms can lead you directly into relapse.\n\n, You may find out that every part of your daily routine reminds you of the times that you took narcotics, but this will only be temporary. Also, it would be very beneficial to your sobriety to try to find new activities and new sober friends to add to your new life. These can help you down the right path. Of course, only start new things once you're starting to feel well enough and think you're up to it.",
        ": \"Imodium AD\"). Do not try that for more than 3 days, because that retains toxins from C-Diff. You might think the anti-diarrheal is helping but you may become drowsy, dizzy, nauseated and lose your appetite. Eventually the toxin can damage various systems (kidneys, liver) and your feet may swell and you may retain several liters of fluid in the body cavity (called \"third spacing\") because of toxin caused by this diarrhea not being excreted/being held in the body by your AD med.\n,, Antibiotics will have zero impact in treating a viral infection, so your doctor will not advise that you take antibiotics if you have a viral infection such as the flu.\n\n\nCaution: cases of Clostridium difficile almost always arise when you are already taking antibiotics for another illness. It is the taking of the antibiotics that predisposes your alimentary canal (gut) to \"bad bacteria\", making it susceptible to developing Clostridium difficile and C-dificille-colitis. \"Difficile\" is Latin for difficult (to cure).\nWhen you take antibiotics (for a prior illness), they are often effective at treating that illness; however, the antibiotics also kill off many of the good bacteria in your intestine, which normally have a protective effect. With many of the good bacteria gone, your gut is less protected and you become susceptible to a Clostridium difficile infection.\nIf you do have a serious bacterial infection requiring antibiotics, however, it is important that you follow through with treatment.\nUntreated bacterial infection can lead to blood poisoning (sepsis), and inflammation clogging small blood vessels, even gangrene (deadtissue). Do not stop antibiotic treatment in the hopes of preventing Clostridium difficile, because minor bacterial infections can become major and be life-threatening.\n\n, Although metronidazole is not designated by the FDA for C. difficile infection, it has been shown in use to be effective in mild to moderate infection. Side effects of metronidazole include nausea and a bitter taste in your mouth.For more severe and recurrent cases, vancomycin (Vancocin), also taken by mouth, may be prescribed. Another oral antibiotic, fidaxomicin (Dificid), has been approved to treat C. difficile. In one study, the recurrence rate of C. difficile in people who took fidaxomicin was lower than among those who took vancomycin. However, fidaxomicin costs much more than metronidazole (Flagyl) and vancomycin. Common side effects of vancomycin and fidaxomicin (Dificid) include abdominal pain and nausea. Your doctor will help to guide you as to when continuing antibiotics are beneficial for you to take, and when they are not/and need to stop., One of the highest risk areas are healthcare facilities, due to the larger number of cases of Clostridium difficile that occur in places such as hospitals, as well as the length of time that spores can survive on surfaces.\n\n\nEspecially if you are in a hospital or other healthcare setting, be sure to wash your hands regularly. Wash with soap and warm water for at least 20 seconds.\nAlcohol-based hand rubs/fluids are ineffective.Bleach wipes containing 0.55% sodium hypochlorite have been shown to kill the spores and prevent transmission between patients.Installing lidded toilets and closing the lid prior to flushing also reduces the risk of contamination., If a family member, a friend, or another person in a healthcare facility contracts diarrhea, it is key to avoid sharing the same space as them until the cause of their diarrhea is confirmed. Their diarrhea could be due to Clostridium difficile, which is highly contagious, or to other highly contagious gastrointestinal illnesses, none of which you want to catch.\n\n\nKeeping in your own separate space and avoiding shared objects can help you to prevent a Clostridium difficile infection or another unwanted illness.\n\n, See your doctor any time you have severe diarrhea, with fever, painful abdominal cramps, and possibly with mucus, blood or pus in your stool.\n\n",
        "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).",
        "I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. ",
        "The African Men's Olympic Qualifiers was held to determine the three African national teams for under 23 that will participate at the 1992 Summer Olympics football tournament held in Barcelona.\n\nFirst round\n\n|}\n\nSecond round\n\nGroup 1\n\n|}\n\nGroup 2\n\n|}\n\nThird round\n\nGroup 1\n\n|}\n\nGroup 2\n\n|}\n\nFourth round\n\n|}\n\nMorocco won 2–1 on aggregate and qualified for the 1992 Summer Olympics.\n\nGhana won 10–1 on aggregate and qualified for the 1992 Summer Olympics.\n\nEgypt won 4–1 on aggregate and qualified for the 1992 Summer Olympics.\n\nReferences \n\nFootball qualification for the 1992 Summer Olympics\nFootball at the Summer Olympics – Men's African Qualifiers",
        "  In spite of the large number of papers appeared in the past which are devoted\nto the lattice Boltzmann (LB) methods, basic aspects of the theory still remain\nunchallenged. An unsolved theoretical issue is related to the construction of a\ndiscrete kinetic theory which yields \\textit{exactly} the fluid equations,\ni.e., is non-asymptotic (here denoted as \\textit{LB inverse kinetic theory}).\nThe purpose of this paper is theoretical and aims at developing an inverse\nkinetic approach of this type. In principle infinite solutions exist to this\nproblem but the freedom can be exploited in order to meet important\nrequirements. In particular, the discrete kinetic theory can be defined so that\nit yields exactly the fluid equation also for arbitrary non-equilibrium (but\nsuitably smooth) kinetic distribution functions and arbitrarily close to the\nboundary of the fluid domain. Unlike previous entropic LB methods the theorem\ncan be obtained without functional constraints on the class of the initial\ndistribution functions. Possible realizations of the theory and asymptotic\napproximations are provided which permit to determine the fluid equations\n\\textit{with prescribed accuracy.} As a result, asymptotic accuracy estimates\nof customary LB approaches and comparisons with the Chorin artificial\ncompressibility method are discussed.\n",
        "Costas Papacostas (; 12 November 1939 – 21 September 2015) was the Cypriot Minister of Defence from 2008 to 2011. Following the Evangelos Florakis Naval Base explosion he resigned as Minister of Defence and was succeeded by Demetris Eliades, the Minister of Agriculture. In July 2013 he was found guilty of manslaughter, with the court saying he “closed his eyes to the danger” regarding the storage of explosives, for which he received a sentence of 5 years imprisonment.\n\nPapacostas was born in Ayia Trias in Cyprus. From 1955 to 1959 he took part in the EOKA armed resistance against the British. In 1966 he joined the Cypriot National Guard and was twice seconded to the Cypriot police.  Papacostas retired from the Army in 1984 having risen to the rank of colonel. He was elected to the House of Representatives representing Famagusta District for Progressive Party of Working People (AKEL) on 26 May 1996 and re-elected in 2001 and 2006.\n\nDeath \nPapacostas died on September 21, 2015 at the age of 75.\n\nReferences\n\nExternal links\nCypriot Ministry of Defence - Biographical Note Costas Papacostas\n\n1939 births\n2015 deaths\nProgressive Party of Working People politicians\nMembers of the House of Representatives (Cyprus)\nCyprus Ministers of Defence\nCypriot people of the EOKA\nPeople from Famagusta District",
        "Helkama Oy is a Finnish company and umbrella brand, operating several subsidiaries that focus on bicycles (Helkama Velox), cables for ships and communications (Helkama Bica), household appliances and refrigerators (Helkama Forste), accessories and imports for Škoda Auto automobiles (Helkama-Auto), and automobile maintenance and import (Uuttera Oy). Despite this broad variety of activities, Helkama is mostly known as a bicycle brand.\n\nHelkama has also made several mopeds until they shut down the moped production in the 1990s. Helkama-mopeds were very popular in Finland. The most popular moped model was Helkama Raisu, 1970s and 1980s a trial bike which won several trial races.\n\nNowadays Helkama has also electric bicycles on its productlist.\n\nExternal links \n Official website\n Helkama Velox – Bicycles\n\nManufacturing companies based in Helsinki\nCycle manufacturers of Finland\nFinnish brands",
        "Marko Nešić (Serbian Cyrillic: Марко Нешић) (March 2, 1873 – April 30, 1938) was a Serbian composer and tamburitza musician. He composed a number of folks songs for tambura, such as Neven Kolo, Žabaljka, Bogata sam imam svega & Đuvegije gde ste da ste.\n\nBiography and work \nMarko Nešić took part in a wide range of activities creating great cultural and social impacts in his time, but is still most remembered for his stylistic harmonization of folk songs from Vojvodina, which were published at his own expense and sent to tamburica groups and orchestras across Serbia. He played the tamburica (prim) and šargija and was the bandmaster of several tamburica orchestras including: Neven, Srbadija, Bratimstvo, Beli Orao and Excelsior.\nThe famous composer and tamburica player from Novi sad issued a \"School for the tamburica\", taught shoemaking, craft printing and carpentry, but since 1890 he was completely devoted to careers as a musician. He has written over 200 songs and instrumental compositions, many of which are considered folk songs. These include but are not limited to: \"Žabaljka\", \"Bogata sam, imam svega\" (I am rich, I have everything), \"Dones' mi vina krčmarice\" (Bring me the wine, bartender), \"Idem kući\" (I'm going home), \"Kukuruzi već se beru\" (The corn is already being harvested),\"Kad sam bio mlađan lovac ja\" (When I was a young hunter), \"Neven Kolo\", \"Đuvegije gde ste da ste\" (Grooms, wherever you are), \"Majka me psuje\" (My mother curses at me), \"Prolaze noći\" (The nights go by) and\n\"Biće skoro propast sveta\" (Soon the world will collapse), which served as the leitmotif for the cult film Žika Pavlović, and others ..\nWith the tamburica orchestra of Vasa Jovanovic, Nešić has appeared in most major cities of Europe and became a supporter of Esperanto. He is the founder of Esperanto Society in Novi Sad, which carries his name.\nNešić was a great supporter of the labor movement, when he died on 30 April 1938, just before May Day (International Workers' Day), at his funeral on Almaška cemetery in Novi Sad an imposing group of workers gathered with a large number of wreaths as a last salute from the comrades of various labor organisations.\n\nReferences \n\n \"Sa pesmom u narodu\" almanax,Tiski Cvet 2009, Novi Sad\n \"Vojvođanska tamburica\", Sava Vukosavljev, Matica Srpska 1990. Novi Sad\n \"Žice tamburice\", Savez muzičkih društava Vojvodine 1985. Novi Sad\n\nSee also \n NEVEN Serbian Craftsmen Singing Society\n Music of Serbia\n Serbian folk music\n\n1873 births\n1938 deaths\nSerbian composers\nMusicians from Novi Sad\nSerbian Esperantists\nCulture of Vojvodina",
        "  P2P IPTV applications arise on the Internet and will be massively used in the\nfuture. It is expected that P2P IPTV will contribute to increase the overall\nInternet traffic. In this context, it is important to measure the impact of P2P\nIPTV on the networks and to characterize this traffic. Dur- ing the 2006 FIFA\nWorld Cup, we performed an extensive measurement campaign. We measured network\ntraffic generated by broadcasting soc- cer games by the most popular P2P IPTV\napplications, namely PPLive, PPStream, SOPCast and TVAnts. From the collected\ndata, we charac- terized the P2P IPTV traffic structure at different time\nscales by using wavelet based transform method. To the best of our knowledge,\nthis is the first work, which presents a complete multiscale analysis of the\nP2P IPTV traffic. Our results show that the scaling properties of the TCP\ntraffic present periodic behavior whereas the UDP traffic is stationary and\nlead to long- range depedency characteristics. For all the applications, the\ndownload traffic has different characteristics than the upload traffic. The\nsignaling traffic has a significant impact on the download traffic but it has\nnegligible impact on the upload. Both sides of the traffic and its granularity\nhas to be taken into account to design accurate P2P IPTV traffic models.\n",
        "The London Philharmonic Choir (LPC) is one of the leading independent British choirs in the United Kingdom based in London. The patron is Princess Alexandra, The Hon Lady Ogilvy and Sir Mark Elder is president. The choir, comprising more than 200 members, holds charitable status and is governed by a committee of 6 elected directors. As a charity, its aims are to promote, improve, develop and maintain education in the appreciation of the art and science of music by the presentation of public concerts.\n\nThe LPC was formed in 1946 with Frederic Jackson as chorus master, for the London Philharmonic Orchestra (LPO). On 15 May 1947, the choir made its début with a performance of Beethoven's Ninth Symphony at the Royal Albert Hall under the baton of Victor de Sabata. Their first recording was of Stravinsky's Symphony of Psalms with the LPO in 1947 followed by the first radio broadcast of Vaughan Williams' Sancta Civitas and Verdi's Stabat Mater in March 1948 with the BBC Symphony Orchestra. Throughout Jackson's tenure (1947–1969), the choir worked closely with the LPO and with major conductors and soloists of the period including Sir Adrian Boult, Eduard van Beinum, Dame Janet Baker, Peter Pears and Kathleen Ferrier. Despite funding cuts to the LPO in the 1950s, the choir maintained work by being engaged by other orchestras. By the mid 1960s LPC's performance standards were slipping and Jackson was invited to retire. His successor, John Alldis improved the standards of the choir and also encouraged the performance of contemporary works such as David Bedford's Star clusters, Nebulae and Places in Devon. The choir worked with Bernard Haitink and Sir John Pritchard during their time as LPO Principal conductors in the 1970s. A noted LPC recording called Sounds of Glory in 1976, now marketed as Praise – 18 Choral Masterpieces, has become the best-selling recording for the choir to date. In 1979, LPC undertook its first overseas tour to Germany.\n\nIn 1982, Richard Cooke succeeded Alldis as Chorus Master and saw the choir through a productive decade. In 1984, the choir registered as a charity. The choir performed under Georg Solti and Klaus Tennstedt who were the two principal LPO conductors of that decade. The LPC also continued to enjoy touring overseas. A noted recording with Tennstedt of the Mahler eighth symphony won an award in 1987. However, the early 1990s was a period of turmoil for the LPO and LPC as financial recession and resignations at the LPO created a climate of uncertainty, while there was some press opposition to the appointment of Franz Welser-Möst as Principal Conductor. Cooke resigned in 1991 due in part to the strained working relationship with Welser-Möst and disputes between choir and LPO management. The LPO appointed Jeremy Jackman as the next Chorus Master in 1992. However, with the choir's difficulties being widely advertised, existing membership levels declined and recruitment of new members became a challenge. Jackman resigned in 1994 after only two seasons at the helm.\n\nNeville Creed became the next Chorus Master (1994–present). His enthusiasm helped to build back morale and membership. In 1996, at the end of the Welser-Möst tenure, the LPC became autonomous after being severed from the LPO's payroll. During this bleak period, the choir was able to secure concerts with other London orchestras and with arts promotion institutions such as IMG Artists and Raymond Gubbay for much needed financial aid. Over time, the choir's performance standard, visibility and reputation improved. Eventually, relations with the LPO settled into mutual respect and good will and the LPC was given the right of first refusal for most future choral projects with the LPO. In 1997, the choir celebrated its 50th anniversary with a concert at the Albert Hall attended by Princess Alexandra and Ursula Vaughan Williams. In 2002, the choir adopted a new constitution and became a registered charity with the legal protection of a limited company. For their 60th anniversary in 2007, the book Hallelujah! An informal history of the London Philharmonic Choir was published. The LPC continues to work closely with the LPO's Principal Conductor Vladimir Jurowski (2007–present) and Guest Principal Conductor Yannick Nézet-Séguin (2009–present).\n\nOrganisation\nThe LPC is an independent amateur mixed-voice choir holding charitable status. The choir, while being rooted in the British choral tradition, also performs a wide repertoire of different styles and languages. The choir's aim is to perform large choral works to professional standard whilst providing a friendly social network for its members. As a charity, its aims are to promote, improve, develop and maintain education in the appreciation of the art and science of music by the presentation of public concerts. The choir also aims to encourage and support for the public benefit all art forms, particularly but not exclusively those involving music, including other cultural and educational activities in order to make these more accessible to the public at large.\n\nChoir\nThe choir consists of a pool of more than 200 members ranging from college students, working age to retirees. There are four vocal sections; bass, tenor, alto and soprano. Each vocal section is divided into upper and lower voices. The choir also accepts female tenors and male altos as members. Each section has a voice representative who manages the section members, notes attendance and sits on the committee.\n\nAll members are volunteers and each member is auditioned prior to joining. Members who pass their audition pay a one-off £25 subscription. There is no annual membership fee. Existing members are re-auditioned every 1 or 3 years with the choir.\n\nThe choir rehearses on Monday and/or Wednesday nights depending on the current project and the rehearsals are normally based at Hinde Street Methodist Church.\n\nBoard of Directors and Trustees\nThe board is made up of members of the choir, and they are in charge of the running of the choir and liaising with the London Philharmonic Orchestra and other organisations. The board is made up of the Chairman, Secretary, Treasurer, Choir Manager, Membership Co-ordinator and Marketing Manager. Additional voluntary roles, including Voice Representatives and Librarian, support the board in the day-to-day running of the choir.\n\nRevenue is derived from initial subscription, donations and above all from concert engagements. The Artistic Director and the Accompanist are paid positions. The Artistic Director also holds an ex officio position on the committee.\n\nPatrons\n\nPresidents\n\nChorus Masters\n\nChairmen\n\nHistory\n\nJackson era (1947–1969)\nThe LPC was formed in December 1946 by former members of the Philharmonic Choir (founded in 1919 by Charles Kennedy Scott and disbanded in 1939 at the onset of World War II) and the London Philharmonic Orchestra. The appointed choir master was Professor Frederic Jackson as Charles Kennedy Scott was unable to resume conductorship. This alliance made the London Philharmonic Choir the first major London choir to be attached to one of the big independent London orchestras.\n\nIn the founding years, the choir was composed of amateur and professional singers, the latter being paid a sum of ten shillings and sixpence per rehearsal session. The amateur members paid the annual membership fee of one guinea. The choir also commenced a membership drive with the placement of an advertisement in the February 1947 issue of The Musical Times.. In March 1947, after recruiting over 300 members, rehearsals commenced on Wednesday evenings at the Westminster Cathedral Hall.\n\nThe choir made its début on 15 May 1947 with a performance of Beethoven's Ninth Symphony with the LPO conducted by Victor de Sabata at the Royal Albert Hall. The choir's first recording was Igor Stravinsky's Symphony of Psalms in 1947 under Ernest Ansermet. This was followed by their first radio broadcast of Vaughan Williams' Sancta Civitas and Verdi's Stabat Mater in March 1948 with the BBC Symphony Orchestra (BBCSO) under Sir Adrian Boult. Another first for the choir was the Proms performance in August 1952 of Stravinsky's Symphony of Psalms with the LPO conducted by Basil Cameron at the Royal Albert Hall.\n\nIn the early 1950s, the LPO was in financial difficulties as funding from the London County Council was severed. Despite the LPO's loyalty to the choir, the financial crisis resulted in the choir being used less during this period. The LPO board cited \"...because of the number of professional choristers, the cost of putting a concert with the Choir had become so great that it was difficult to maintain its interest.\" Jackson was now paid by engagement rather than a fixed salary. The LPO board also agreed \"that in the circumstances, no objection could be raised if the choir found work for themselves, provided reference was made to the LPO before any engagement was accepted\". By 1958, the choir's annual membership fee was raised to one pound ten shillings as a means to maintain administration funds. This was further raised in 1959 to £3 as the choir was now responsible for the remuneration of the Chorus Master.\n\nThe LPC continued its partnership with the LPO throughout the 1960s. In the spring of 1967, Bernard Haitink was appointed principal conductor of the LPO and in the first season under his reign, the LPC performed Britten's Spring Symphony, Bruckner's E Minor Mass and Mahler's Resurrection Symphony. In March 1968, the choir made its first television broadcast: a performance of Elgar's Dream of Gerontius at Canterbury Cathedral with the LPO conducted by Sir Adrian Boult with soloists Peter Pears, Dame Janet Baker and John Shirley-Quirk. The production was directed by Brian Large for the BBC and broadcast in colour.\n\nHowever, by the late 1960s the LPO board were dissatisfied with the dwindling performance quality of the LPC and by implication, with Jackson. Jackson's retirement as Chorus Master was announced in May 1969, \"... after 21 years owing to the pressure of other engagements...\". John Alldis, who was Founding Chorus Master of the London Symphony Chorus (LSC), succeeded as Chorus Master of the LPC that same year.\n\nFrederic Jackson died on 10 February 1972 while conducting Verdi's Requiem at the Royal Academy of Music. He was 67 years old.\n\nAlldis era (1969–1982)\nThe arrival of John Alldis fostered a new era for the LPC. The committee restructured with the addition of voice section representatives. The choir's annual membership fee was also abolished. Rehearsals were relocated to Bishopsgate Institute as Alldis favoured its acoustics. Recruitment of new members commenced almost immediately with advertisements and invitation by existing members. Alldis also re-auditioned existing LPC members to maintain standards. Some former LSC members loyal to Alldis followed him to the LPC. One new recruit who joined as a tenor in 1972 was David Temple. He is now the conductor and musical director of the Crouch End Festival Chorus. David had been invited to become the Music Director of Crouch End Arts Festival in 1984 by John Gregson, its Director and fellow LPC tenor. Together they founded Crouch End Festival Chorus in that year. It was at this time that Malcolm Hicks joined as accompanist and Deputy Chorus Master.\n\nAlong with maintaining a high performance level with standard choral repertoire, Alldis also encouraged the choir to undertake contemporary works such as David Bedford's Star Clusters, Nebulae and Places in Devon which was commissioned for the LPC and Brass of the LPO and was given its première on 7 March 1971 at the Royal Festival Hall.<ref name=\"star\">{{cite journal | jstor= 955458 | title=Music in London, (Choral; Bedford, Strauss) |journal=The Musical Times| volume=113| issue= 1551|page=475 | author= Ronald Crichton. |date=May 1972 }}</ref> Another performance of a contemporary work occurred in August 1972 when David Rowland's Cantate Laetantes Alleluia was featured at the International Carnival of Experimental Sound – ICES-72 – in the Roundhouse at Chalk Farm.Snowman, p. 46\n\nThe LPC performed with major classical soloists of the decade. These included, Kiri Te Kanawa, Heather Harper, Sheila Armstrong, Margaret Price, Norma Procter, Helen Watts, Peter Pears, Richard Lewis, Robert Tear, John Carol Case, John Shirley-Quirk, Norman Bailey and Raimund Herincx.\n\nIn 1976, the choir recorded Sounds of Glory which is a compilation of hymns and songs for choir and orchestra for use in television advertisements and the like. The recording is now marketed under the title Praise – 18 Choral Masterpieces and has become the best-selling album for the choir to date. In 1979, the choir undertook its first European tour, to Wilhelmshaven in Northern Germany, performing Bruckner's E Minor Mass with the local wind ensemble. This tour was arranged through contacts from a choir member as part of Wilhelmshaven's annual music festival Wochenende an der Jade.\n\nAfter 13 years as chorus master of the LPC, Alldis retired in 1982, the year of LPO's golden jubilee.\n\nJohn Alldis died on 20 December 2010. He was 81 years old.\n\nCooke era (1982–1991)\nAlldis' successor was Richard Cooke who took up the post on 10 March 1982. On 12 March 1984, the choir adopted the rules by the Charity Commission and a month later became a registered charity.\n\nDuring the 1980s, recordings became less frequent as most came to be offered to professional ensembles. However, the choir sang regularly under the baton of such conductors as Sir Georg Solti and Klaus Tennstedt. Opportunities for touring became more common; in 1985, for example, the choir visited Italy with Tennstedt, performing Beethoven's Ninth in Perugia and Pompeii. Tennstedt became the choir's first president when he commenced his tenure as the LPO's Principal Conductor and Artistic Director in 1983. It was with Tennstedt that the choir recorded the Mahler's Eighth Symphony together with the Tiffin School boys' choir and the LPO for EMI in 1987. This recording won the 1987 Gramophone Magazine's 'Orchestral Record of the Year Award'. Tennstedt stood down from the LPO in 1987 due to ill health, having nurtured good rapport with Cooke and the LPC during his tenure. In 1988, members of the choir wore monks' habits during their performance of the British concert première of Olivier Messiaen's 5-hour-long opera, Saint François d'Assise (Saint Francis of Assisi) at the Royal Festival Hall conducted by Kent Nagano, a performance they then took to Lyon.\n\nIn 1990, the LPO appointed Franz Welser-Möst to the post of Principal Conductor. That same year, the LPO became the first \"resident\" orchestra at the South Bank (the arts complex which includes the Royal Festival Hall). This enabled the LPO (and its choir) to have first choice in dates, rehearsals and repertoire. In 1991, Tennstedt conducted the LPC and LPO in three performances of Mahler's Eighth Symphony at the Royal Festival Hall one of which was attended by Mahler's granddaughter, Anna.\n\nThe economic recession of the 1990s was a turbulent period for the arts in Britain. High-level resignations at the LPO management fostered tension and uncertainty for the LPO and LPC. Furthermore, Welser-Möst was not enamoured with the choir, preferring what he called a more 'Continental sound'. Inevitably, the working relationship was strained between Cooke and Welser-Möst. In August 1991, after a performance at The Proms of Beethoven's Ninth Symphony conducted by Tennstedt, Cooke concluded his engagement with the LPC.\n\nRichard Cooke is now Music Director of the Royal Choral Society.Snowman, p. 69\n\nJackman era (1992–1994)\nThe LPO appointed Jeremy Jackman, a former member of the King's Singers, as the next Chorus Master in late 1991 to commence in 1992. The LPO did not programme any concerts involving the LPC in the 1992/93 season to allow the choir time to regroup. With the departure of Cooke, some LPC members, uncertain of the choir's future, defected to other choirs resulting in declining membership. Recruitment was made all the more challenging as the choir's difficulties were widely advertised. Despite this setback, Jackman and the fragmented choir worked hard to achieve decent results for Beethoven's Ninth with Tennstedt, Janáček's Glagolitic Mass with Jiří Bělohlávek and Haydn's Creation with Sir Roger Norrington. In March 1994, Jackman handed in his resignation after working with the choir for only two concert seasons. By late 1994, after months of searching and auditioning, the LPO eventually appointed Neville Creed as the next LPC Chorus Master.\n\nJeremy Jackman is now Musical Director of the English Baroque Choir, the Cecilian Singers in Leicester, and the Jay Singers in Norfolk. He also gives music masterclasses and workshops.\n\nCreed era (1994–present)\n\nNeville Creed was the former head of music at Tiffin School and conductor of the Bournemouth and Guildford choirs. He collaborated with the LPC by preparing the Tiffin Boys' Choir on the Mahler Eight recording in 1987. His brother, Marcus Creed, is also a noted English conductor, now based in Germany. Creed's enthusiasm and drive enabled the LPC to undertake a membership drive and to build up morale. The choir was able to give creditable performances with the LPO at the Royal Festival Hall in the 1994/95 LPO concert season of the Britten and Verdi Requiems under Welser-Möst, Berlioz concerts with Norrington, Beethoven's Ninth and Bruckner's Te Deum with Haitink, and two performances of Verdi's Aida with Zubin Mehta.\n\nHowever, the prevailing economic conditions in the arts in Britain meant orchestras were under ever increasing financial strains. By the time of Franz Welser-Möst departure in 1996, the LPC ceased to be on the payroll of the LPO and became autonomous. This meant that the choir needed to maintain some form of financial stability while recognising concerts with the LPO were no longer guaranteed. The main focus for the choir was to improve its standard of choral singing if it were to survive as reputation alone was not enough to garner any engagements. The choir began approaching and performing with other orchestras such as the Royal Philharmonic Orchestra, The London Symphony Orchestra and the Philharmonia Orchestra. The choir also actively pursued engagements from arts organisations through networks known by individual choir members, such as IMG Artists (Hampton Court Music Festival) and Raymond Gubbay (Classical Spectacular concerts). LPC members were also likely to be found augmenting other larger choirs and their respective orchestras, such as the Royal Choral Society or the London Symphony Chorus if a large force was required for a particular performance. Eventually, the relationship between the LPO and LPC settled into one of mutual respect and goodwill. The choir was now given the right of first refusal for future choral projects involving the LPO.\n\nIn 1996, Princess Alexandra, The Hon Lady Ogilvy accepted the choir's invitation to become its first Patron. In the same year, Sir Roger Norrington became the second President of the choir. In 1997, the LPC celebrated its 50th Anniversary with a performance of Vaughan Williams' Sea Symphony at the Royal Albert Hall conducted by Neville Creed and attended by Princess Alexandra and Ursula Vaughan Williams.\n\nIn 2002, the choir adopted a new constitution and became a registered charity with the legal protection of a limited company. In 2003, Neville Creed's role changed from Chorus Master to Artistic Director. This enabled him to have a say on the type of programming the choir was to undertake. However, Creed's increasing commitments as Director of Cultural Activities at St Edward's School in Oxford, resulted in the appointment of Matthew Rowe as Associate Chorus Director to work alongside Creed.\n\n2004 and 2005 saw an exceptional number of tours and high-prestige performances for members of the LPC. In January 2004, Rowe prepared and accompanied the LPC to perform Mahler's Resurrection symphony (and to première John Harbison's Abraham) before Pope John Paul II at the Vatican. For this \"Papal Concert of Reconciliation\", the LPC were joined by the Ankara State Polyphonic Choir, the Kraków Philharmonic Choir, members of the Mendelssohn Choir of Pittsburgh and the Pittsburgh Symphony Orchestra under Gilbert Levine. In April, the choir sang Haydn's Creation in Hong Kong, returning to perform La damnation de Faust with Mark Elder in London and the Mahler's Resurrection symphony with the Philharmonia under Esa-Pekka Salonen in Paris and at the Royal Festival Hall. Other performances that year included Glagolitic Mass (June), Janáček's The Eternal Gospel and Mahler's Third Symphony (July), La damnation de Faust and Carmina Burana (October), A Sea Symphony (November) and Raymond Gubbay's Christmas classics and Beethoven Ninth (December).www.charity-commission.gov.uk\n\nIn 2005 alone, the choir toured six countries beginning with Greece in January, Malaysia and Australia in June, Germany in July, Switzerland in September and finally Italy in November. In May 2005, the choir performed Britten's War Requiem with the LPO under Kurt Masur. This concert – the last before the Royal Festival Hall's closure for refurbishment – marked the 60th anniversary of the end of World War II in Europe and was recorded by the LPO for the orchestra's recently launched CD label. The LPC celebrated their 60th anniversary in May 2007, with a choral concert at the Queen Elizabeth Hall. This event also coincided with the book launch of Hallelujah! An informal history of the London Philharmonic Choir written by author and long standing member of the choir, Daniel Snowman.\n\nIn June 2007, the Royal Festival Hall was reopened following extensive refurbishment. The LPC participated in the gala opening concert, one highlight of which was a celebratory new composition, Alleluia, by the composer – and member of the LPC bass section – Julian Anderson. In September 2007, as part of its ongoing commitment as a charity, the LPC was involved with its first Mayor of London Open Rehearsal at the Bishopsgate Institute.\n\nIn July 2008, Rowe prepared the choir for the Doctor Who Prom with the BBC Philharmonic under conductors Stephen Bell and Ben Foster held at the Royal Albert Hall. Soloists were Melanie Pappenheim and Tim Phillips. He also undertook non-LPC engagements, such as mentor to Katie Derham in BBC 2's production of Maestro shown in August and September 2008. After 6 years, Rowe left the LPC at the end of 2008 to take up the position of symphony orchestra conductor for the San Diego State University School of Music and Dance in January 2009. Creed returned to full duties as Artistic Director and the role of Associate Chorus Director was made redundant.\n\nWith the success of the 2008 Doctor Who Prom, the choir was invited to perform in the \"Evolution!\" Prom in August 2009, performing Jón Leifs Hekla, Op. 52 and also the première of Goldie's composition Sine Tempore (Without Time) commissioned by the BBC. The creation of this work was featured in the two-part series Classic Goldie on BBC 2.\n\nIn September 2009, the choir, augmented by the London Chorus, recorded 50 greatest pieces of classical music'' with the LPO under David Parry at Henry Wood Hall. This \"download only\" recording released in December 2009, was the first for the LPC. This recording was ranked 4th on the Gramophone Magazine classical charts as of 30 October 2010.\n\nThe choir's first engagement under the LPO's Principal Guest Conductor Yannick Nézet-Séguin occurred in April 2009 with the performance of Brahms' Requiem with the LPO at the Royal Festival Hall. This performance was recorded for the LPO label and released 29 March 2010.\n\nAs part of the 115th BBC Prom season, the choir again participated in a Doctor Who Prom on 24 July which was reprised the following day. The prom also featured the BBC National Orchestra of Wales with conductors Ben Foster and Grant Llewellyn with music by Murray Gold.\n\nIn June 2014, Sir Mark Elder became the third President of the choir.\n\nNoted performances\n\nInternational tours\n\nReferences\n\nSources\n\nExternal links\n\nLondon Philharmonic Orchestra\n\nLaura Westcott, \"Life in a Successful choir\". The Times, 21 January 2010.\n\nLondon choirs\nMusical groups from London\nMusical groups established in 1947\n1947 establishments in England",
        "Pablo Burchard (November 4, 1875 – July 13, 1964) was a Chilean painter. His father was German architect Teodoro Burchard Haeberle, who arrived in Chile around 1855, and introduced the Gothic style, and his mother was María (Sofía) Luisa Eggeling Metzger. He taught in the University of Chile's School of Fine Arts from 1932 to 1959, and he won the National Prize of Art of Chile in 1944.\n\nReferences\n\nExternal links\n\n1875 births\n1964 deaths\nChilean people of German descent\nPeople from Santiago\nChilean male painters\n20th-century Chilean painters\nChilean male artists\n20th-century male artists\nMale painters",
        "This paper presents three improvements to the standard LSTM architecture used in many neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of the modifications is trivial to implement, so the paper is definitely of interest to any NLP researchers experimenting with deep learning. \n\nWith that said, I am concerned about the experiments and their results. The residual connections do not seem to consistently help performance; on SST the vertical residuals help but the lateral residuals hurt, and on IMDB it is the opposite. More fundamentally, there need to be more tasks than just sentiment analysis here. I'm not quite sure why the paper's focus is on text classification, as any NLP task using an LSTM encoder could conceivably benefit from these modifications. It would be great to see a huge variety of tasks like QA, MT, etc., which would really make the paper much stronger. \n\nAt this point, while the experiments that are included in the paper are very thorough and the analysis is interesting, there need to be more tasks to convince me that the modifications generalize, so I don't think the paper is ready for publication.",
        "  Ever since the first discoveries of the quantum-interference transport in\nmesoscopic systems, the electron dephasing times, $\\tau_\\phi$, in the\nconcentrated AuPd alloys have been extensively measured. The samples were made\nfrom different sources with different compositions, prepared by different\ndeposition methods, and various geometries (1D narrow wires, 2D thin films, and\n3D thickfilms) were studied. Surprisingly, the low-temperature behavior of\n$\\tau_\\phi$ inferred by different groups over two decades reveals a systematic\ncorrelation with the level of disorder of the sample. At low temperatures,\nwhere $\\tau_\\phi$ is (nearly) independent of temperature, a scaling\n$\\tau_\\phi^{\\rm max} \\propto D^{-\\alpha}$ is found, where $tau_\\phi^{\\rm max}$\nis the maximum value of $\\tau_\\phi$ measured in the experiment, $D$ is the\nelectron diffusion constant, and the exponent $\\alpha$ is close to or slightly\nlarger than 1. We address this nontrivial scaling behavior and suggest that the\nmost possible origin for this unusual dephasing is due to dynamical structure\ndefects, while other theoretical explanations may not be totally ruled out.\n",
        " Cotton and felts work best, but you can make this from nearly any material once you get the hang of it.\n, from your fabric. The shapes should be approximately 2 inches (5cm) long and one inch (2.5cm) wide.\n\nImportant: Make all of the football shapes exactly the same size, and allow an extra 1/4 inch (1/2 cm) for the seams.\n12 shapes from each of three different colors works very nicely for this project.\n\n, If sewing by hand, take care to make your stitches very small and even to prevent problems later.\n\nNote: Begin and end the seam a seam allowance width from the point to prevent bunching when sewing and turning.\n\n,, With right sides facing, sew one long edge together., This will create a three sided wedge which resembles a (somewhat fat) orange section.\n\nIf using three fabrics, use one fabric per side of your wedges so that you can \"change colors\" on the ball later.\n\n,,,,,,,,\n\nAlmost done... just one set of points left to be sewn.\n\n\n\n\n\n\n\n,\n\nAdd embellishments such as buttons or ribbon ties if you like, but be aware that you should not give a button encrusted ball to a very small child who may choke on the buttons.\n\n",
        "We thank all reviewers for their detailed feedback, and note that all reviewers recommend the paper for acceptance. Based on reviewer feedback about image resolution, we trained a 64x64 and 128x128 version of the model on the CUB dataset, results of which can be seen at sites.google.com/view/iclr2017figures. These and additional higher resolution results will be added to the revised paper. Since low-resolution was one of the main drawbacks to the paper according to the reviews, we hope that this can be reflected in an improved score.\n\nWe posted an updated version of the paper to OpenReview with an important correction to the caption of table 1: likelihoods are in *nats* per dim, not bits.\n\nBelow we respond to each review individually.\n\nAR1:\n\nThe time required for sampling is the main constraint on generating higher-resolution samples. However, we have been able to train some higher-resolution models in time for the rebuttal (see sites.google.com/view/iclr2017figures for some results). We agree that adding many more examples for comparison would help; these will be added in the upcoming version.  Please see the reply to AnonReviewer 3 for more precise details on timing and the experimental setup.\n\nPlease see other replies regarding comparison of GANs to pixelCNNs. In short, there are trade-offs between these two. We accentuate the trade-offs in the paper, in the hope that researchers will then know what are the key problems of each approach, and focus on developing solutions to those problems. A quantitative comparison is problematic because GANs don’t provide us with likelihoods. We can however include more samples. To this extent we will add more high-resolution samples, including the ones already provided via the link above.\n\nAR2:\n\nWe appreciate your suggestion of adding more results to the appendix for the final version, or even better create a website where users can explore the generated images by both approaches. The figures in the paper provide a reasonable depiction of the trade-offs between existing GANs and pixelCNNs, but we agree adding more comparisons will help.\n\nAR3:\n\nMatching GANs:\nIn the paper we demonstrate that autoregressive models can do text- and location-conditional image generation; although as the reviewer points out, the resolution is much lower so “match” is not the right word; GANs and auto-regressive models have complementary strengths and weaknesses. We are happy to add a more thorough discussion of these issues earlier in the paper, rather than at the end. Figure 9 queries were from a figure of positive results in the paper to which we compare - so presumably favourable to GAN - but we agree that many more comparisons are needed to study the different types of mistakes each method makes.\n\nReplaying training data:\nOne way to check for this is to compare likelihoods for the training set and held-out sets of data. In our case we did not observe significant overfitting, so copying seems unlikely. We also observe significant diversity of samples even with fixed text and structure constraints. However, as noted in the GAN paper to which we compare, even if the model had largely memorized the (text,location,image) training data, it could still produce novel images by conditioning on novel combinations of (text,location), or in general the combinatorial space of all its conditioning variables.\n\nMore implementation details:\nCurrently the paper says “We used RMSprop with a learning rate schedule starting at 1e-4 and decaying to 1e-5, trained for 200k steps with batch size of 128”. Additional details: The number of epochs varies by dataset - more for CUB because it is smaller, fewer for MS-COCO. Training took about 4 days and sampling at 32x32 resolution took about 2 minutes per image with batch size of 30. Sampling 64x64 took about 16 minutes per image, and 128x128 took about 2 hours. (However, note that sampling time is highly implementation dependent, and we used only the most naive approach in this paper).\n\nHigh-resolution comparisons:\nWe will add further hi-res comparisons in the revised paper.\n\nAutoregressive approach:\nI (first author) also have a bias toward GANs, having written several papers using them. However, I also think autoregressive approaches have complementary benefits compared to GANs - stable, easy to train, do not overfit to a few modes, best available image density estimators, etc - and are worth further developing. Also, autoregressive and adversarial approaches could be naturally combined; e.g. as likelihood and posterior models in PPGNs (",
        "The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it’s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn’t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I’m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size.",
        "Malanów  is a village in the administrative district of Gmina Sokolniki, within Wieruszów County, Łódź Voivodeship, in central Poland. It lies approximately  south-east of Sokolniki,  east of Wieruszów, and  south-west of the regional capital Łódź.\n\nReferences\n\nVillages in Wieruszów County",
        "This paper proposes Generative Adversarial Parallelization (GAP), one schedule to train N Generative Adversarial Networks (GANs) in parallel. GAP proceeds by shuffling the assignments between the N generators and the N discriminators at play every few epochs. Therefore, GAP forces each generator to compete with multiple discriminators at random. The authors claim that such randomization reduces undesired \"mode collapsing behaviour\", typical of GANs.\n\nI have three concerns with this submission.\n\n1) After training the N GANs for a sufficient amount of time, the authors propose to choose the best generator using the GAM metric. I oppose to this because of two reasons. First, a single GAN will most likely be unable to express the full richness of the true data begin modeled. Said differently, a single generator with limited power will either describe a mode well, or describe many modes poorly. Second, GAM relies on the scores given by the discriminators, which can be ill-posed (focus on artifacts). Since there is There is nothing wrong with mode collapsing when this happens under control. Thus, I believe that a better strategy would be to not choose and combine all generators into a mixture. Of course, this would require a way to decide on mixture weights. This can be done, for instance, using rejection sampling based on discriminator scores.\n\n2) The authors should provide a theoretical (or at least conceptual) comparison to dropout. In essence, this paper has a very similar flavour: every generator is competing against all N discriminators, but at each epoch we drop N-1 for every generator. Related to the previous point, after training dropout keeps all the neurons, effectively approximating a large ensemble of neural networks.\n\n3) The qualitative results are not convincing. Most of the figures show only results about GAP. How do the baseline samples look like? The GAN and LAPGAN papers show very similar samples. On the other hand, I do not find Figures 3 and 4 convincing: for instance, the generator in Figure 3 was most likely under-parametrized.\n\nAs a minor comment, I would remove Figure 2. This is because of three reasons: it may be protected by copyright, it occupies a lot of space, and it does not add much value to the explanation. Also, the indices (i_t) are undefined in Algorithm 1.\n\nOverall, this paper shows good ideas, but it needs further work in terms of conceptual development and experimental evaluation.",
        "The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.\n\nLSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it.",
        "  Using high-resolution UV spectra of 16 low-z QSOs, we study the physical\nconditions and statistics of O VI absorption in the IGM at z < 0.5. We identify\n51 intervening (z_{abs} << z_{QSO}) O VI systems comprised of 77 individual\ncomponents, and we find 14 \"proximate\" systems (z_{abs} ~ z_{QSO}) containing\n34 components. For intervening systems [components] with rest-frame equivalent\nwidth W_{r} > 30 mA, the number of O VI absorbers per unit redshift dN/dz =\n15.6(+2.9/-2.4) [21.0(+3.2/-2.8)], and this decreases to dN/dz = 0.9(+1.0/-0.5)\n[0.3(+0.7/-0.3)] for W_{r} > 300 mA. The number per redshift increases steeply\nas z_{abs} approaches z_{QSO}, and some proximate absorbers have substantially\nlower H I/O VI ratios. The lower proximate ratios could be partially due to\nionization effects but also require higher metallicities. We find that 37% of\nthe intervening O VI absorbers have velocity centroids that are well-aligned\nwith corresponding H I absorption. If the O VI and the H I trace the same gas,\nthe relatively small differences in line widths imply the absorbers are cool\nwith T < 10^{5} K. Most of these well-aligned absorbers have the\ncharacteristics of metal-enriched photoionized gas. However, the O VI in the\napparently simple and cold systems could be associated with a hot phase with T\n~ 10^{5.5} K if the metallicity is high enough to cause the associated broad Ly\nalpha absorption to be too weak to detect. We show that 53% of the intervening\nO VI systems are complex multiphase absorbers that can accommodate both lower\nmetallicity collisionally-ionized gas with T > 10^{5} K and cold photoionzed\ngas.\n",
        "The paper makes a solid technical contribution in establishing a universal approximation theorem for the boolean hypercube. They characterize the class of boolean functions that can be efficiently approximated by a two-layer network.\n \n We like the idea of noise stability, and it could explain why in practice, perturbation techniques such as dropout are effective. Moreover, humans can identify images, despite corruptions, and hence, intuitively the concepts we aim to learn should be robust.\n \n However, the framework of the paper deviated from the networks and data structures that are the norm in practice. In practice, we rarely have boolean functions. And it is well known that boolean functions can behave quite differently from continuous functions. \n \n We recommend that the authors widen the scope of their work, and attempt to connect their findings to practical networks and functions. Moreover, we recommend that they do a more thorough literature survey. For instance, the idea of robust concepts has appeared before\n ",
        "River Ryan  (population 238) is a community in the Canadian province of Nova Scotia, located in the Cape Breton Regional Municipality on Cape Breton Island.\n\nReferences\nRiver Ryan on Destination Nova Scotia\n\nCommunities in the Cape Breton Regional Municipality\nDesignated places in Nova Scotia\nGeneral Service Areas in Nova Scotia",
        "The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.\n",
        "  Mechanistic home range models are important tools in modeling animal dynamics\nin spatially-complex environments. We introduce a class of stochastic models\nfor animal movement in a habitat of varying preference. Such models interpolate\nbetween spatially-implicit resource selection analysis (RSA) and\nadvection-diffusion models, possessing these two models as limiting cases. We\nfind a closed-form solution for the steady-state (equilibrium) probability\ndistribution u* using a factorization of the redistribution operator into\nsymmetric and diagonal parts. How space use is controlled by the preference\nfunction w then depends on the characteristic width of the redistribution\nkernel: when w changes rapidly compared to this width, u* ~ w, whereas on\nglobal scales large compared to this width, u* ~ w^2. We analyse the behavior\nat discontinuities in w which occur at habitat type boundaries. We simulate the\ndynamics of space use given two-dimensional prey-availability data and explore\nthe effect of the redistribution kernel width. Our factorization allows such\nnumerical simulations to be done extremely fast; we expect this to aid the\ncomputationally-intensive task of model parameter fitting and inverse modeling.\n",
        "Quality, Clarity:\n \n The work is well motivated and clearly written -- no issues there.\n \n Originality, Significance:\n \n The idea is simple and well motivated, i.e., the learning of reward functions based on feature selection from identified subtasks in videos.\n \n pros:\n - the problem is difficult and relevant: good solutions would have impact\n \n cons:\n - the benefit with respect to other baselines for various choices, although the latest version does contain updated baselines\n - the influence of the initial controller on the results\n - the work may gain better appreciation at a robotics conference\n \n I am very much on the fence for this paper.\n It straddles a number of recent advances in video segmentation, robotics, and RL, which makes the specific technical contributions harder to identify. I do think that a robotics conference would be appreciative of the work, but better learning of reward functions is surely a bottleneck and therefore of interest to ICLR.\n Given the lukewarm support for this paper by reviewers, the PCs decided not to accept the paper, but invite the authors to present it in the workshop track.",
        "This paper was reviewed by three experts. While they all find merits in the paper (interesting new model class SSPN, new MAP inference algorithm), they all consistently point to deficiencies in the current manuscript (lack of parameter learning, emphasis on evaluation on energies, lack of improvements in accuracy). \n \n One problem that (I believe) is that manuscript as it stands makes neither a compelling impact on the chosen application (semantic segmentation) nor does it convincing establish the broad applicability of the proposed model (how do I run SSPNs on activity recognition or social network modeling). \n \n To be clear, we all agree that there is promising content here. However, I agree with the reviewers that the significance has not been established.",
        "Hello Authors,\n\nCongratulations on the acceptance of the paper.\n\nI've just reread parts of the revised paper and noticed a few things that you might want to consider and change before the camera-ready deadline.\n\n* You now include a reference to KLIEP after Eqn. (16), but this procedure is in fact known as least-squares importance estimation.\n* in turn, Eqn. (14) is actually more akin to KLIEP, the main difference being the use of the unnormalised form of the KL-divergence. So I think you meant to put the KLIEP reference here.\n\nFurther comment on making Eqn. (14) practical:\nIf you read the KLIEP paper, they formulate the procedure as a constrained optimisation problem:\nmaximise\n$$\nE_{p^*} log r_\\phi(x)\n$$\nsubject to the constraint that\n$$\nE_{q_\\theta} r_\\phi(x) = 1\n$$\n\nCompare this constrained optimisation to your solution, it is easy to make the connection: if you introduce a Lagrange multiplier to handle the constraint, one obtains the following unconstrained optimisation problem:\n\nseek stationary points of\n\n$$\n\\ell(\\phi, \\lambda) = E_{p^*} log r_\\phi(x) - \\lambda E_{q_\\theta} (r_\\phi(x) - 1)\n$$\n\nI do think that solving this unconstrained optimisation problem is actually possible, you can do that via stochastic gradient descent, and it does not include your nasty cross-entropy term.\n\nWhat am I missing?\n\nThanks,\n\nRev1",
        ", The tools and files you’ll need will depend on your device’s Android number. Android number 4.0.x is Ice Cream Sandwich, and Android number 4.1.x is Jelly Bean.\n\nFollow the steps outlined in method two if your device is running on Ice Cream Sandwich.\n\n, A full charge will help prevent your phone from powering down unexpectedly during the rooting process., Rooting will wipe and erase all personal data from your device.\n\n\nBack up your data to Google’s servers, your computer, or to a third-party cloud storage service to avoid personal data loss.\n\n,,, This software will install the necessary drivers for your Droid RAZR., Your computer will recognize your device immediately, and begin to install updated drivers.,, This file contains the toolkit needed to root your device.,,, This app must be installed on your device for rooting to work successfully.\n\nAlternately, you can download the app to your device using your computer at https://play.google.com/store/apps/details?id=com.motorola.contextual.smartrules&hl=en.\n\n,,,,,, Your RAZR Maxx HD will reboot when rooting is complete, and display “Superuser” in Apps., Your RAZR will now be rooted.",
        "This paper presents iterative PoWER, an off-policy variation on PoWER, a policy gradient algorithm in the reward-weighted family.\n\nI'm not familiar enough with this type lower bound scheme to comment on it. It looks like the end result is less conservative step sizes in policy parameter space. All expectation-based algorithms (and their KL-regularized cousins a-la TRPO) take smallish steps, and this might be a sensible way to accelerate them.\n\nThe description of the experiments in Section VI is insufficient for reproducibility. Is \"The cart moved right\" supposed to be \"a positive force is applied to the cart\"? How is negative force applied? What is the representation of the state? What is the distribution of initial states? A linear policy is insufficient for swing up and balance of a cart-pole. Are you only doing balancing? What is the noise magnitude of the policy? How was it chosen? How long were the episodes?\n\nThe footnote at the bottom of page 8 threw me off. If you're using Newton's method, where is the discussion of gradients and Hessians? I thought the argmax_theta operator was a stand-in for an EM-style step, which I how I read Eq (8) in the Kober paper. \n",
        "This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.\n\n\nPros:\n\nThe method is very simple and easy to understand and apply.\n\nThe experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.\n\nThe analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.\n\n\nCons:\n\nThere is little novelty -- the method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what’s done in the Inception BN results in Table 1-2?)\n\nThe analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.\n\nSection 3.3: it’s not clear to me what point is being made here.\n\n\nOverall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.",
        "  Internet-based personal digital belongings present different vulnerabilities\nthan locally stored materials. We use responses to a survey of people who have\nrecovered lost websites, in combination with supplementary interviews, to paint\na fuller picture of current curatorial strategies and practices. We examine the\ntypes of personal, topical, and commercial websites that respondents have lost\nand the reasons they have lost this potentially valuable material. We further\nexplore what they have tried to recover and how the loss influences their\nsubsequent practices. We found that curation of personal digital materials in\nonline stores bears some striking similarities to the curation of similar\nmaterials stored locally in that study participants continue to archive\npersonal assets by relying on a combination of benign neglect, sporadic\nbackups, and unsystematic file replication. However, we have also identified\nissues specific to Internet-based material: how risk is spread by distributing\nthe files among multiple servers and services; the circular reasoning\nparticipants use when they discuss the safety of their digital assets; and the\ntypes of online material that are particularly vulnerable to loss. The study\nreveals ways in which expectations of permanence and notification are violated\nand situations in which benign neglect has far greater consequences for the\nlong-term fate of important digital assets.\n",
        " If they are extremely tight, spray some lubricant on the nuts - a silicone lubricant or even vegetable oil will do the trick very well. (Many modern bikes will not have nuts. They have a quick-release which you can easily loosen and remove the wheel).\n\n\n\n\n\n\n;\n, Every brake setup is a little different but you should likely be able to slip the brake cable out of a socket on the brake arms to release them. Some brakes may require that you loosen the cable from a clamped position.\n\n\n\n\n\n\n\n\n, If it's the rear wheel, you will need to lift the chain clear of the gear cluster. To ease the removal of a rear wheel, shift the chain to the smallest gear on the wheel before loosening the skewer or nuts. If it's the front wheel, it will be easier.\n\n\n\n\n\n\n\n\n, With a presta valve you need to unscrew the top part of the stem to release the air in the tube. During this step you'll also want to remove the lock ring that screws onto the stem and sits flush on the rim if your bike has one.\n\n\n\n\n\n\n\n\n, With all the air out of the tube squeeze the tire together and you'll see the tire release from the inside of the rim, this will help when removing the tire.\n\n\n\n\n\n\n\n\n, You can use the handle of a spoon or similar object if you don't have tire levers but be very careful, as you risk scratching or damaging the rims of the wheel and/or puncturing the inner tube. Ease one lever in under the wheel rim and lever out the edge of the tire (taking great care not to puncture the inner tube) and pry it up over the wheel rim. Move around the rim about an eighth of the circumference and repeat the process again, leaving the first tool in place. Now zip the second lever around the wheel and the tire should come right off on one side.\n\n\n\n\n\n\n\n\n,, Best way to find an air leak is by inserting the tube with air in water. Where air bubbles come out , there is a hole in your tube.\n\n\n\n\n\n\n\n\n, Use caution when feeling inside the tire as a nail, or glass can cut you. Be sure to remove any thorns, glass, or nails with pliers if you find them. Adjust rim tape to cover protruding spoke.\n\n\n\n\n\n\n\n\n, Unwrap your new tube and remove the plastic dust cap and the lock ring.\n\n\n\n\n\n\n\n\n, Putting a few pumps of air in the tube before will help to avoid pinching the tube when putting the tire on the rim.\n\n\n\n\n\n\n\n\n, This process is difficult but try not to use tire levers, a screwdriver, or other similar object as you vastly increase your chance of puncturing your new tube. Check the tire wall for an arrow or similar to indicate the direction of rotation - some tires have a \"direction specific\" tread pattern. Put one side in first, then ease the partially inflated tube into the tire and put the other side on.\n\n\n\n\n\n\n\n\n,,,,,",
        "I'll try and answer, but it's been many years (10+) since I've done this study. \n    \nOne of the most predominant reasons is gluconeogensis, when you do not eat for a period of time your body detects the fall in serum (bloodstream) glucose (This and your stomach being physically empty will trigger a hunger mechanism) and eventually it begins to increase the gluconeogenesis pathway.       \n\nThis is a process by which your body begins to increase glucagon production (Hormone that tells your body to raise glucose levels) and maintain your blood glucose level, conversely it will decrease insulin production (Most easily explained as: insulin is necessary to transport glucose for use at the cellular level and therefore decreases serum glucose levels).     \n\nAs you enter gluconeogenesis, your body is now breaking down predominantly proteins and fats rather than utilising the glucose as it is entering your bloodstream from the food absorbed in your (predominantly small) intestines.\nInitially these proteins and fats will be from intramuscular/lipid/cellular stores and then finally, provided by directly catabolising your own body tissue and organs.  \nThroughout this process, essential for life neurotransmitters: phosphates, magnesium and potassium continue to be utilized to maintain fluid balance, electroconduction and intracellular movements (which includes the glucose we're 'producing' and using).\n \nOnce a stasis level is achieved (stomach has collapsed, intestinal peristaltic motion has slowed considerably, serum glucose is stable) you no longer feel hungry as there are no rapid changes occurring, essentially your body is being supplied the necessary fuel it currently requires to keep going. \n(And from a perhaps historic standpoint: There was probably no point tormenting you that you're hungry while you're trying to hunt for food).  \n\nIf you maintain this fasting for long enough, there is a risk of potential serious medical compromise (Cardiac Arrest, Seizure etc) predominantly due to neurotransmitter depletion and in some cases due to direct catabolism of the vital organs to provide glucose (such as the heart).   \n   \nThere is a further risk if you enter in to this state and that's resuming eating, this is called 'refeeding syndrome'.   \nIn refeeding syndrome, where the body is already in an elevated state of glucagon production, the sudden increase in blood glucose results in a sudden increase in insulin and thus consequently your body's overall metabolism spikes which continues to include proteins and fats (Elevated Glucgaon).  With this, neuotransmitters (Potassium and Phosphate predominantly) which the bodies stores have already depleted to maintain serum levels, have a sudden intracellular shift and serum levels can decrease to fatal levels.",
        "The authors propose a method to investigate the predictiveness of intermediate layer activations. To do so, they propose training linear classifiers and evaluate the error on the test set.\n\nThe paper is well motivated and aims to shed some light onto the progress of model training and hopes to provide insights into deep learning architecture design.\n\nThe two main reasons for why the authors decided to use linear probes seem to be:\n- convexity\n- The last layer in the network is (usually) linear\n\nIn the second to last paragraph of page 4 the authors point out that it could happen that the intermediate features are useless for a linear classifier. This is correct and what I consider the main flaw of the paper. I am missing any motivation as to the usefulness of the suggested analysis to architecture design. In fact, the example with the skip connection (Figure 8) seems to suggest that skip connections shouldn't be used. Doesn't that contradict the recent successes of ResNet?\n\nWhile the results are interesting, they aren't particularly surprising and I am failing to see direct applicability to understanding deep models as the authors suggest.",
        "This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. \n\nThis paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of $h$ and $w$. $h$ is a hidden unit, but what is $w$? I could not find any definition. Furthermore, I could not know how $h$ is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. \n\nTotally, I am not sure that this paper is suitable for publication. \n\nProns:\nEmpirical performance is good.\n\nCons:\nNovelty of the proposed method\nSome description in the paper is unclear.\n\n",
        ";\n,, If ball is caught or secured by goalie, the ball is dead and goalie switches to offense. If ball misses target but does not strike vehicle, ball is also dead and positions switch. If ball strikes car and bounces away, it is a live ball and either player may pursue. If goalie secures, ball is dead and positions switch. If offensive player secures, they may shoot at goal from any location. In this scenario, the goalie may seek to block the shot or swipe ball free and secure.\n\n, A single-hand hold may be legally stolen.\n\n, However, bumping, pushing, and hand/arm waving are allowed.\n\n, A ball that enters trunk and bounces out is no goal (as it cannot necessarily be verified by the defensive player).\n\n, If the other player agrees, this scenario is played out. The other player may, however, offer an alternative number (usually higher if that player is behind in the game), and a negotiation ensues until an agreement is made. If no agreement, play continues until another announcement is made. If no agreement is made a second time, an automatic ten shot each rule is imposed. Whoever is leading at the end of all remaining shots used is the winner.\n\n, Players determine who shoots first by a best of three rock-paper-scissors game. Game ends when first player scores.\n\n,",
        "**Disclaimer:** I am a molecular biochemist. Was a spider-butt scientist for years. Materials scientists and engineers please feel free to correct me.\n\nSpider silks, like silk worm silk, are made of mostly protein. Spiders can produce many different kinds of silk proteins, which are mixed and matched into different kinds of silk that they use for different purposes, such as egg sacs to protect their eggs (tubularform silk), prey capture (acinoform silk), web capture spiral (flaggeliform silk), and structural silk (dragline/major ampullate silk). Due to the specialized mixture of protein in each of these types of silks, they display very unique properties.\n\nThe most well known/studied silk type is the dragline silk, which is used by the spider for many purposes, such as the structural struts of their web (not the spiral on the web) and the silk they put down when they walk around in case they fall, hence the term \"dragline\" (sometimes also called lifeline silk).\n\nDragline silk is for the most part composed of two kinds of proteins, MaSp1 (major ampullate spidroin 1) and MaSp2. The specific ratio of these two proteins vary widely from species to species, and even within an individual based on health/diet. As mentioned earlier, the different proteins contribute to the overall silk's properties.\n\nAt this time, there is something that needs to be clarified. The term \"strength\" is often just used as an umbrella term. The term \"strength\" technically refers to the amount of force required before the material \"fails\", or breaks. The term \"extensibility\" ~~or \"yield\"~~ or \"strain\" refers to the material's ability to stretch before failing. If you plot these two properties on a graph, the area under the curve is the \"toughness\", which roughly equates the amount of energy the material can overall dissipate before failing.\n\nA \"strong but stiff\" material is something like kevlar. It can withstand a lot of force before it breaks, but it doesn't stretch very well. A \"weak but stretchy\" material would be something like rubber, which is the opposite. However, if you look at the toughness of these materials, they cannot absorb much energy before breaking relative to a material that is both \"strong and extensible\". The unfortunate thing here is that most materials, due to our understanding, cannot be both strong and extensible. Spider (dragline) silk is unique in that it has a relatively high strength (still much weaker than kevlar), and relatively high extensibility (but not as stretchy as say a rubber band). This results in a much higher overall toughness that is rarely observed.\n\nBack to your original question. Spider silk is mostly protein. These proteins self-assemble (with help from the spider) into fibers. On a molecular scale, these proteins assemble into two types of structures: unstructured \"amorphous\" domains and structured \"crystalline\" domains. The amorphous domains - imagine very tangled strands of headphone wires. If you pull on it, the overall fiber stretches to a certain point. This is thought to contribute to the extensibility of the fiber. The crystalline domains - imagine layers of double sided velcro. It is very hard to pull out a single layer of velcro from this stack. These highly structured domains are thought to contribute to the strength of the overall fiber. It is the finely tuned interplay of these domains that result in the unique balance of strength and extensibility - resulting in high toughness.\n\n It is very hard to reproduce these material properties synthetically. We have gotten very good at engineering the extremes. Very molecularly structured materials end up very strong, but stiff (think kevlar or carbon fiber). This is due to high \"bond density\" - there are a lot of molecular forces packed together holding the molecules together. These bonds are so well organized that when it fails, all the bonds let go at the same time. Very molecularly unstructured materials are stretchy but weak. This is because there are less bonds per volume  (or weaker bonds) that hold the material together, but these materials tend to be able to rearrange their bonds as the material is stressed, thus not instantly failing when deformed.\n\nWe are actually able to make many materials that are pretty strong, and also pretty stretchy. Plastics are a great example of this. However, we are just not able to control the molecular organization of plastics to get both high strength and extensibility. edit: As /u/PhaedrusBE points out, steel is a great comparison as well.\n\n**tl;dr** - Spider silks are made of proteins which assemble into domains that contribute strength and extensibility. It is the unique ratio of these two properties that grant spider silk it's high \"toughness\".\n\n**Edit: ELI5?** -  Spider silks are made of long entangled strings. Some parts of the strings are stiff and strong, some parts of the strings are stretchy. The combination of the two properties allow the silks to absorb a lot of energy and not break easily when you pull on them.\n\nPS. The next time you meet a shady merchant, you should never buy spider silk armor. That stuff ain't gonna protect you much even if it doesn't break if it deforms up to 30% into your body. Buy kevlar if you can't afford mithril.",
        "I am a career paid Fireman. The best way to simplify it is to say that we do not ALWAYS enter a burning structure. There are quite a few common sense reasons that are a definite no go for us (such as flames through the roof, indicating a high probability for collapse on most residential fires). If there is a possibility that we can enter the structure, the incident commander has to make a decision which comes down to \"risk a little to save a little, risk a lot to save a lot\". If there is any chance that a VIABLE human life is inside, you can guarantee that every firefighter on the scene would risk their life without question to save the person inside. The big word there being viable. Most of the time the toxic chemicals produced by modern manufacturing give off super heated and immediately deadly fumes that can kill you after only a few breaths. We have to take that into account and will not risk as much to recover someone who has without a doubt perished. Our next goal is to save property that can be saved. Family photos and heirlooms are irreplaceable and we make every effort to minimize your loss. If I can read the conditions of the fire and make a direct attack inside the house right at the seat of the fire, it extinguishes the fire quicker, preventing extension and also limiting damage from water being shot in through Windows. Most of our nozzles put out at least 200 gallons per minute. That is a lot if water to put inside your house blindly through a window. Most kitchen fires could probably be extinguished with less than 30 gallons if there is no extension into the walls/roof. Also as others have said, there are a lot of times you can't be certain that the building is unoccupied. A lot of accidental fires happen because squatters light \"camp fires\" in abandoned buildings to keep warm. They have a right to be saved and us not completing a search of the building (conditions permitting) would be negligent. In residential buildings we keep our eye out for indicators of people that would occupy the structure. For instance if there is a car in the driveway then someone was likely home. Or if there are small children's toys in the yard or house then we automatically assume the possibility that we need to account for children. Sorry for any typos, I'm on mobile. Hope this helps answer your question. If you take anything away from this, just know that if there is a possibility for me or any of the firefighters I know to save a life, we will fight until our last breath to get you out safely.\n\nEdit: Thank you all for the kind words and thank you for the gold! Reading through all of your comments really brightened my day!",
        "  An exhaustive classification of certain class of static solutions for the\nfive-dimensional Einstein-Gauss-Bonnet theory in vacuum is presented. The class\nof metrics under consideration is such that the spacelike section is a warped\nproduct of the real line with a nontrivial base manifold. It is shown that for\ngeneric values of the coupling constants the base manifold must be necessarily\nof constant curvature, and the solution reduces to the topological extension of\nthe Boulware-Deser metric. It is also shown that the base manifold admits a\nwider class of geometries for the special case when the Gauss-Bonnet coupling\nis properly tuned in terms of the cosmological and Newton constants. This\nfreedom in the metric at the boundary, which determines the base manifold,\nallows the existence of three main branches of geometries in the bulk. For\nnegative cosmological constant, if the boundary metric is such that the base\nmanifold is arbitrary, but fixed, the solution describes black holes whose\nhorizon geometry inherits the metric of the base manifold. If the base manifold\npossesses a negative constant Ricci scalar, two different kinds of wormholes in\nvacuum are obtained. For base manifolds with vanishing Ricci scalar, a\ndifferent class of solutions appears resembling \"spacetime horns\". There is\nalso a special case for which, if the base manifold is of constant curvature,\ndue to certain class of degeneration of the field equations, the metric admits\nan arbitrary redshift function. For wormholes and spacetime horns, there are\nregions for which the gravitational and centrifugal forces point towards the\nsame direction. All these solutions have finite Euclidean action, which reduces\nto the free energy in the case of black holes, and vanishes in the other cases.\nTheir mass is also obtained from a surface integral.\n",
        "  A game-theoretic framework is used to study the effect of constellation size\non the energy efficiency of wireless networks for M-QAM modulation. A\nnon-cooperative game is proposed in which each user seeks to choose its\ntransmit power (and possibly transmit symbol rate) as well as the constellation\nsize in order to maximize its own utility while satisfying its delay\nquality-of-service (QoS) constraint. The utility function used here measures\nthe number of reliable bits transmitted per joule of energy consumed, and is\nparticularly suitable for energy-constrained networks. The best-response\nstrategies and Nash equilibrium solution for the proposed game are derived. It\nis shown that in order to maximize its utility (in bits per joule), a user must\nchoose the lowest constellation size that can accommodate the user's delay\nconstraint. Using this framework, the tradeoffs among energy efficiency, delay,\nthroughput and constellation size are also studied and quantified. The effect\nof trellis-coded modulation on energy efficiency is also discussed.\n",
        " He is the first in game character you meet on the game; he will guide you through the tutorial. You start off the game with a hut and the town hall, which will be explained later.;\n, Once you start the game, you are in the Stone age, there are many ages/eras in the game, from this one to the Bronze age and so on. You now need to open up the building menu by clicking on the hand holding a hammer symbol at the bottom of the screen then click on the hut and place it in the required space.\n\n,, Click on it to see your next quest.\n\n, Click to build the hunter, you can place it anywhere, as long as it is connected to road.\n\n, You need to click on the button with the glass science flasks on it and open up the tech tree. This is where you can research technologies to advance in the game, click on 'stilt houses' and use your forge points to research it. You start with 10 forge points; they gradually replenish over time.\n\n, Then click 'back to city'. Your town hall will now change because you have entered a new age! This will happen every time you do, Ragu's appearance will also change but not every time you enter a new age., Now that you are comfortable in the Bronze age, it is time to build 2 new houses to increase your population. Your population is shown at the top of the screen, with a symbol of 2 people in the bar next to the face that shows happiness. Build the 2 houses anywhere that is connected by road, of course then go on to see your next quest!\n\n, As said earlier, the face at the top of the screen shows this, if it is smiling, it means that your people are happy, build decorations to increase happiness.\n\n\nTo build decorations, open the building menu again and click on the tree in a pot symbol, that is where all the decorations. Build one, decorations don't need road so they can be put anywhere! This will increase your happiness and make your people happy!\n\n, You get 2 spear fighters, military units as a reward from the previous quest and this is when they can be put into good hands! You need to click the button with a compass like symbol on it, this will open up the continent map. This is where you and your troops can go into battle and conquer the world!\n\n, Click on attack and it will appear with the preparations menu, click attack again and it will go into battle!\n\n, Click on the space to move your troop there; there are several terrain types on the battlefield. Different ones give you different boosts for your troops; you can't go over water until later in the game so don't click on a water area.\n\n\nThis is a turn-taking game, once you move your first unit, the opponent will move one of his units. Next, move your unit close to the enemy in order to attack it!\n\n,, Now you are free from the main tutorial but it is not over yet!\n\n, Supply production buildings like the hunter have 6 products you can make them produce, they range from 5 minute production to 1 day production times, click on the 5 minute one to start you off. Your town hall produces coins once every day anyway., Once done, click on 'back to city' and build one, like before it needs to be connected to road. The pottery will be in on the menu where the hunter is. Once it is built, put it into production as well, you don't have to but it is always helpful.\n\n, Continue to do Ragu's quests to advance in the game, later in the game, new faces will appear! They are mostly side quests, you don't need to do these but it is always good to help out your friends!",
        " Be careful not to disturb or damage them while working around them.\n\nBarely loosen (break loose) each of the 11 bolts (10mm) connecting the OHC (overhead cam) valve cover to the engine. Loosen the 9 bolts around the edge of the cover that connect it to the block (one in on the very end). Two of those 10mm bolts are upon the cover with one about 3 times longer and (it was covered up by that power steering hose) and the one in the center is about 4 times longer than the ones around the edges. Once all are just loosened then finish removing them.,, The engine should be easy to turn with a socket wrench on the crankshaft bolt since the spark plugs are out and all pressure is released., aligned with the top of the engine head while at the same time you line up the timing mark on the crankshaft harmonic pulley pointing straight up at the same time as the cams are aligned (or use the crankshaft key and slot later in the process when the harmonic pulley is off).\n\n\n\n\n\n\nCaution: the TDC (top dead center) could be 180 degrees off of the correct TDC even when the crankshaft mark or crankshaft key is correctly pointing straight up (\"It would then be at the top of the exhaust stroke.\"); so then align it as explained next.\nCaution: check timing is at the correct TDC (top dead center of the compression stroke) and find the marks are on the back of the sprockets and align them with the edge of the engine head. The older models have a distributor and you can remove the distributor cap to see if it is pointing at spark plug cable terminal number one (for cylinder number 1) when the mark on crankshaft pulley shows it is at TDC. If it points to the opposite side of the distributor then turn the engine slowly 360 degrees on the crankshaft to be back around to TDC (\"top of the compression stroke\"). Newer engines do not have a distributor and so the correct TDC can be found while the harmonic pulley timing mark is approaching TDC. This is done using the compression stroke air pressure measured in the \"number one spark plug hole\" by a \"compression gauge\" to know when the piston is on the compression stroke and then turning slowly and carefully to exactly the TDC as shown by the crankshaft and the cam sprockets timing marks. Compression pressure means that the piston in cylinder one was on the compression stroke not the exhaust stroke which also comes to TDC.\nRequired: securely place a \"plastic/rubber\" stop-block (you may use a bottle stopper from a wine bottle preferably \"not real cork\" which might crumble) to put it in place to jam the cam sprockets with the \"rubber stop-block.\" Use the stop-block before releasing the tensioner pulley to remove the timing belt, or else when the timing belt is removed the cams will \"turn\" (in opposite directions) about two notches--pushed by the valve springs pressure off TDC of the cam lobes. Later before the timing belt is re-installed use two \"open-end\" wrenches or adjustable spanners for 1 inch (25.5 ~ 26mm) on the hex part of the camshaft (not on the sprocket bolt) to slightly nudge the sprockets back and forth (small wiggling) to get the stop block to hold both cam marks correctly aligned while the stop-block is between the two cam sprockets. So it holds both cams at the same time in TDC position (You may purchase a special stop-block made for this purpose if you prefer.).\n\n, Put on the parking brake.,, Then use a jack-stand to support the car. Do not depend on the jack to support it. You will be removing the wheel adjacent to (next to) the pulleys only.,, Watch your head., It should work under the air conditioner bracket which is massive on the older models. The engine being supported is because you will remove (not yet) the motor mount in steps when you get to it in a later step; that mount is between the top and lower halves of the timing-cover and is very much in the way of working on the accessory belts, timing belt and water pump., The two toward the firewall of the auto are difficult to reach and even to see. Use a short, small ratchet drive extension., You probably can wiggle and jiggle the pulley while start gently prying and pulling it off the crankshaft. If not, then you may need to use a steering wheel puller or similar puller., This was necessary to be able to remove the lower timing cover., One screw is hard to see in the middle of the cover, right below the motor mount.,",
        "  This paper investigates the use of evolutionary optimisation techniques to\nregister a template with a scene image. An error function is created to measure\nthe correspondence of the template to the image. The problem presented here is\nto optimise the horizontal, vertical and scaling parameters that register the\ntemplate with the scene. The Genetic Algorithm, Simulated Annealing and\nParticle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation\nwith starting points chosen in a pre-processing stage. The paper investigates\nthe precision and accuracy of each method and shows that all four methods\nperform favourably for image registration. SA is the most precise, GA is the\nmost accurate. PSO is a good mix of both and the Simplex method returns local\nminima the most. A pre-processing stage should be investigated for the\nevolutionary methods in order to improve performance. Discrete versions of the\noptimisation methods should be investigated to further improve computational\nperformance.\n",
        "  We have used data from the Sloan Digital Sky Survey (SDSS) Data Release 5 to\nexplore the overall structure and substructure of the stellar halo of the Milky\nWay using about 4 million color-selected main sequence turn-off stars. We fit\noblate and triaxial broken power-law models to the data, and found a `best-fit'\noblateness of the stellar halo 0.5<c/a<0.8, and halo stellar masses between\nGalactocentric radii of 1 and 40kpc of (3.7+/-1.2)x10^8 M_sun. The density\nprofile of the stellar halo is approximately r^{-3}; it is possible that the\npower law slope is shallower inside 20kpc and steeper outside that radius. Yet,\nwe found that all smooth and symmetric models were very poor fits to the\ndistribution of stellar halo stars because the data exhibit a great deal of\nspatial substructure. We quantified deviations from a smooth oblate/triaxial\nmodel using the RMS of the data around the model profile on scales >~100pc,\nafter accounting for the (known) contribution of Poisson uncertainties. The\nfractional RMS deviation of the actual stellar distribution from any smooth,\nparameterized halo model is >~40%: hence, the stellar halo is highly\nstructured. We compared the observations with simulations of galactic stellar\nhalos formed entirely from the accretion of satellites in a cosmological\ncontext by analysing the simulations in the same way as the data. While the\nmasses, overall profiles, and degree of substructure in the simulated stellar\nhalos show considerable scatter, the properties and degree of substructure in\nthe Milky Way's halo match well the properties of a `typical' stellar halo\nbuilt exclusively out of the debris from disrupted satellite galaxies. Our\nresults therefore point towards a picture in which an important fraction of the\nMilky Way's stellar halo has been accreted from satellite galaxies.\n",
        " There is no doubt that it is absolutely simple. You will need:\n\n\n1 Rubber Band\n1 Sheet of Paper\nScissors or Ruler (Optional);\n, Take your piece of paper. Cut or tear the piece of paper into smaller pieces with your hands, scissors, ruler, etc. The small papers should be roughly near 7 cm by 1 cm. Scrunch up the small pieces of paper into a good-sized, tight and firm paper ball. This will be called a mini-paper bullet. Make a few more bullets to add to your ammo stock.\n\n, Wrap it around your thumb and index finger. Make sure to put the rubber band on your default hand (hand that you usually use).\n\n, Pinch it with your thumb and index finger of your other hand. Don't let go of the bullet.\n\n, If you are pulling the closest band towards you, you are doing it correctly.\n\n,,, Pull the band that is further away from you towards you. This will overlap the band that is closest towards you. Do not pull the band closest towards you.\n\n,,,, This time, pull both bands.\n\n,, The tricky part is coming up. Although it is easy to do practically, describing it in words is a different matter.\n\n, Pull it under the band farthest from you.\n\n, Hold the bullet by pinching it with your thumb and index finger of your other hand.\n\n,,,, That is, wrapped around your thumb and index finger.\n\n, Twist the band you are holding and place it in the base of your middle finger. Your middle finger should be resting near the middle of your palm.\n\n, Pull back the rubber band with the bullet.\n\n,, Whether in school, home, or work, you've got to hide your masterpiece of fun. In school, you might shoot your bullets across the room but you should hide your weapon so that people aren't suspicious of you. You certainly don't want your enemy or other people using your weapon. The best hiding spot for your bullets is your pocket. Shorts pocket, jacket pocket, all are fine. Hiding it in a pocket is another good thing because your hand could reach it easily.\n\n, Pretend to be writing or pretend to be doing something with your hands. The person you shot might think that your working hands means that it is too busy for games.\n\n, The rubber band makes a twanging sound after firing the bullet so it is a good idea to hide it quickly. If you have a watch, you could use it to cover up your rubber band. Or else you put it inside a pocket or put it next to other bracelets or things on your arm to make it look like everyday stuff.\n\nYou might not get it to shoot well at first, but remember to place the bullet in the right place so the power will go into the bullet. Don't forget to read the tips below to learn more\n\n\n\n\n\n\n\n",
        "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.",
        "Nazi-era Germany wasn't necessarily destination number one for foreigners looking to travel abroad, but that isn't to say it wasn't seeing any visitors, and during the 1930s, Germany wanted to encourage people to see it as a travel destination for both economic and propagandist reasons. the push came both from private, industry related organizations such as well as government related groups, one of the big ones being the Reichsbahnzentrale für den Deutschen Reiseverkehr (German Railroads Information Office, or RDV), which was one of the biggest initiatives for promotion of Germany as a vacation destination, which took up the vast majority of its advertisement focus. They ran 31 offices in 26 countries by 1938, and coordinated from its headquarters in Berlin. The aim of the RDV in its promotions was to serve state needs, not only bringing in foreign currency from visitors, but also attempting to create more positive images of Hitler's Germany for people, even if they were unable to follow through with the journeys the RDV was selling.\n\nThe image that the RDV projected was one heavily laced with propaganda. They billed Germany as a modern, attractive, cultural destination, but heavily played up \"*how Germany is going ahead: no unemployment, production at peak levels, social security, gigantic projects for industrial development, economic planning, organized efficiency, a dynamic will of pulling together – a happy, energetic people who gladly share their achievements with you*\" to quote one ad. The low cost of Germany as a destination was also a popular draw that the RDV played up, a favorable exchange rate for most foreign visitors allowing for the country to be billed as a good destination for those on a budget.\n\nTo focus on the USA, the RDV had an annual budget of 470,000 RM which were spent on initiatives such as newspaper advertisements, promotional films, and informational handouts for travel agencies, highlighting and promoting different destinations of interest in Germany and various events being held through the year that might draw people. They also did 'cultural' promotions with museums and schools. Prospective travelers could reach out to the RDV office and receive sample itineraries to help them plan their trip, as well as informational packets to guide them on various things they would need to be aware of. Although it of course doesn't mean every single person ended up going through with a trip, the RDV was receiving anywhere from 65,000 to 150,000 such inquiries a year during the 1930s, which can help give some idea of the level of interest for travel to Germany from the USA during the decade. The campaign was evidently successful too, and authorities recorded that from 1934 to 1937, numbers of American tourists had doubled. Interestingly, even after war broke out in 1939, the RDV continued to operate in the US, working to signal German confidence in a speedy victory and resumption of travel in the near future, nothing more than thinly veiled propaganda at that point in time - to the ire of many - but it wouldn't be closed down by the US until June of 1941 (offices in neutral and Axis countries would remain open beyond that point).\n\nAll in all, the push for foreign tourists seems to have been a successful one. from 1933 to 1935, German authorities claimed that foreign tourism increased 260 percent. Although as with any numbers from the Nazi government, it must be taken with a grain of salt as to its precise accuracy, the numbers certainly were going up. To quote from Semmens' \"Seeing Hitler's Germany\", from which I've been drawing on here, she provides a brief overview of the numbers for the middle of the decade:\n\n > The Olympic Games marked a banner year in international tourism and Berlin was not the only city to benefit. That summer, 1.2 million foreigners, about 15 per cent of all registered visitors, arrived in Germany, an increase of 55 per cent over the summer of 1935. Thuringia alone witnessed a 61.3 per cent rise in visitors from abroad.77 But the Olympic year merely presaged what was to come. Berlin welcomed almost 56,000 more foreigners in 1937 than it had in 1936.\n\nLooking at the entire initiative, it seems to have been on the whole successful in the goals Germany wanted. While the rising amount of tourist traffic to Germany should be understood as part of the larger international picture which \"saw a continual increase in leisure travel after the Great War\", the initiatives by the German government specifically played an undoubted part in seeing the country, specifically, chosen as a destination out of other options. And although not every visitor of course was swayed and instead left with sour impressions of the Nazi movement and the changes it had wrought, most visitors seem to have left the country for home with thoroughly positive impressions of what they had seen, extolling the \"pleasant normality\", a small victory for the propagandist aims of the tourist initiative as a whole, although of course hardly enough to sway international opinion in the end.\n\nAll cited from \"Seeing Hitler's Germany: Tourism in the Third Reich\" by Kristin Semmens. Specifically see Ch. 6 \"International Tourism\" pp. 129-153\n\nEdit: I like run on sentences...",
        "Warren Harding *is* usually considered one of the worst US Presidents in the surveys of history professors that /u/itsallfolklore discusses [here](_URL_1_). Though, of course, there are historians whose take on Harding diverges from this consensus. \n\nThere are a variety of reasons why Harding's reputation is so poor - namely, the perception that he was something of a puppet, the personal scandals associated with Harding that emerged after his death, the political scandal of the Teapot Dome being the first time a Cabinet member had gone to prison, and Harding not really having signature policy achievements beyond a somewhat nebulous 'return to normalcy'.\n\nFirstly, the perception of Harding for a long time, until the 1960s, was that he was a small-town newspaperman plucked from obscurity by men in the archetypal smoke-filled room for his good looks, his genial manner, and his basic lack of interest in actually running the country. His public perception was perhaps similar to the public perception of George W. Bush in the 2000s, when people assumed he was something of a puppet of Dick Cheney without much depth. The Harding family kept the Hardings' letters and correspondence private until the 1960s, which meant that this was the dominant perception of Harding for close to half a century (the private papers made it clearer that Harding had a lot more influence on policy direction than had previously been thought, and that he was more articulate on policy matters in private).\n\nSecondly, a number of lurid details were published about Harding relatively soon after his death in 1923, which further fixed public views about him for some time. Firstly, in 1927, Nan Britton released a book called *The President's Daughter* in which she claimed that her illegitimate daughter's father was in fact Harding. In a more socially conservative age, this claim did a lot of damage to Harding's reputation amongst conservatives, though its veracity [was in dispute until 2015](_URL_0_), when DNA testing showed that it was true. Secondly, a 1930 book by Gaston Means, who had worked at the Bureau of Investigation (but who Coffey calls a 'charlatan' and 'swindler') claimed not only that this was true but that Means had even confirmed this to Florence Harding (Harding's wife), who had promptly poisoned her husband. Florence had died in 1924, and was not around to deny such things, and the Harding family keeping their papers private until the 1960s meant that they could not shed light on these rumours one way or the other.\n\nThirdly, a lot of the public was inclined to believe such claims because of the Teapot Dome scandal, which painted Harding in a poor light - his Secretary of the Interior Albert Bacon Fall was the first ever Cabinet member to go to prison, for his part in taking bribes to lease government-controlled oil reserves to oilmen (including at Teapot Dome in Wyoming, thus the name of the scandal) at low rates. Even a basically positive biography of Harding like 1969's *The Harding Era* by Robert K. Murray finds that there were several character flaws of Harding's which directly led to the Teapot Dome scandal (Murray pushes hard against the small-town newspaperman plucked to power by the smoke-filled room thing, but even he ultimately sees Harding as unfit for the Presidency, temperament-wise).\n\nAs to Harding's policies, the defining 'brand' of Harding's presidency was the 'return to normalcy'. Phillip G. Payne claims that\n\n > it was not the case that Harding was a polarizing figure while president whose policies were vigorously debated, indeed it was just the opposite: there are few policy debates from the Harding\nyears.\n \nIt's difficult to put modern political labels of progressive and conservative on the political parties of the time, but there were certainly progressive movements in the early 20th century, and Harding was decidedly in contrast to the progressive, statist values of Woodrow Wilson that, by 1920, were seen poorly by the public. Harding's big brand was the 'return to normalcy', and his stance was a sort of return to the 'small government' values of the Gilded Age. These kinds of policies continued throughout the 1920s and early 1930s under Coolidge and Hoover, and they seemed to be unable to deal with the Great Depression, especially once Franklin Delano Roosevelt's Keynesian New Deal policies became the basis of essentially bipartisan policy until Reagan. Insofar as Harding's policies were perceived as playing a role in leading to the Great Depression, perhaps unsurprisingly they were received poorly by historians, especially left-leaning ones. And insofar as there were few policy debates from the relatively-shortlived Harding years, and so there was little in the way of big ideas and strong stances for historians to discuss, Harding's lack of policy prowess means that there aren't a huge amount of strong defenders of his legacy. It's perhaps indicative that the most positive portrayal of Harding comes from a 2004 biography by John Dean, who, well, also happens to be a person went to prison for his role in Watergate.\n\nSources:\n\n* 'Progressivism in an Age of Normalcy: Women’s Rights, Civil Service, Veterans’ Benefits, and Child Welfare' by John F. Fox, Jr in Katherine Sibley's (ed.) *A Companion to Warren G. Harding, Calvin Coolidge, and Herbert Hoover*\n\n* 'Harding Biographies' by Justin P. Coffey in Katherine Sibley's (ed.) *A Companion to Warren G. Harding, Calvin Coolidge, and Herbert Hoover*\n\n* 'The Harding Presidency: Scandals, Legacy, and Memory' by Phillip G. Payne in Katherine Sibley's (ed.) *A Companion to Warren G. Harding, Calvin Coolidge, and Herbert Hoover*",
        "The paper reports that \"[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\"\n\nIs it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with \"Recurrent Neural Network Regularization\" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.\n\nThis will likely also be desired for the other experiments, such as character LM.",
        " This should be located at the top of the breaker panel.;\n,,\n\n\nConnected to this screw should be a red, black, or blue covered wire.\nIf you have properly turned off the power, there should be no signs of voltage.\n\n, You may need a small, pointy tool, such as a screwdriver, to get them out.\n\n, The cable clamp will keep the four-conductor cable in place after you run it through the hole.\n\n\nRemove the lock-nut from the cable clamp.\nPush the cable clamp through the hole. The side that comes through the bottom should have spiral edges for screwing it in.\nTighten the lock-nut back around the cable clamp by placing it at the underside of the clamp where you see the spiral edges and screwing it in with Channel-Lock pliers.\n\n, You may need a professional to look at your breaker box and tell you which kind of four-conductor cable you need.\n\n, Inside the covering, you'll find a copper wire (the ground wire), a white covered wire (the neutral wire), a black wire (the hot wire), and a red wire (another hot wire). In the sub-panel box, the neutral and ground wires will connect the same as they do in the main breaker box, but the red and black wires will connect to the hot bar instead of a circuit breaker.\n\n, It is a metal strip with a row of screws running down it. Using a flathead screwdriver, unscrew one of them slightly and push your ground wire through. Screw it back in tightly after you insert it.\n\n, Like the ground bus bar, this will be a metal strip with a row of screws, but the neutral bus bar is typically white.\n\n\nTake a neutral wire and cut away about a centimeter of the covering at the end of the wire.\nUse your flathead screwdriver to unscrew one of the screws and then push the wire through.\nScrew it back in after you have inserted the neutral wire.\n\n,\n\n\nYou should find a list of acceptable circuit breakers you can use.\nMake sure that your circuit breaker is of acceptable size and voltage.\n\n, It will be clearly indicated where this should go. Depending on the kind of circuit breaker you must use, you may have to attach the red hot wire as well.\n\n, One set will be on the left side, and the other on the right side.\n\n,,,,",
        "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",
        "  For many real spin-glass materials, the Edwards-Anderson model with\ncontinuous-symmetry spins is more realistic than the rather better understood\nIsing variant. In principle, the nature of an occurring spin-glass phase in\nsuch systems might be inferred from an analysis of the zero-temperature\nproperties. Unfortunately, with few exceptions, the problem of finding\nground-state configurations is a non-polynomial problem computationally, such\nthat efficient approximation algorithms are called for. Here, we employ the\nrecently developed genetic embedded matching (GEM) heuristic to investigate the\nnature of the zero-temperature phase of the bimodal XY spin glass in two\ndimensions. We analyze bulk properties such as the asymptotic ground-state\nenergy and the phase diagram of disorder strength vs. disorder concentration.\nFor the case of a symmetric distribution of ferromagnetic and antiferromagnetic\nbonds, we find that the ground state of the model is unique up to a global O(2)\nrotation of the spins. In particular, there are no extensive degeneracies in\nthis model. The main focus of this work is on an investigation of the\nexcitation spectrum as probed by changing the boundary conditions. Using\nappropriate finite-size scaling techniques, we consistently determine the\nstiffness of spin and chiral domain walls and the corresponding fractal\ndimensions. Most noteworthy, we find that the spin and chiral channels are\ncharacterized by two distinct stiffness exponents and, consequently, the system\ndisplays spin-chirality decoupling at large length scales. Results for the\noverlap distribution do not support the possibility of a multitude of\nthermodynamic pure states.\n",
        "A combination of word2vec and LDA could be potentially interesting. The main\nproblem with the current paper is that the technical details are\nincomprehensible. Section 2 needs a complete rewrite so that a reader familiar\nwith word2vec and LDA could relatively easily get a high-level picture of how\nthe models are being combined. The current presentation doesn't achieve that.\n\nMore detailed comments:\n\nThe third paragraph of the introduction makes no sense to me. \"requires\nderiving a new approximation\" - approximation of what? why is it time consuming\nto develop prototypes? Why is it easier to evaluate features?\n\nWhy use the same word vectors for pivot and target (unlike in word2vec)? What's\nthe motivation for that decision?\n\nwhat does it mean to separate words from a marginal distribution?\n\nwhat's co-adaptation?\n\n\"If we only included structure up to this point\" - what kind of structure?\n\n\"it's similarity\" -> its\n\nFootnote 1 breaks anonymity.\n\nThere doesn't appear to be any evaluation. The days when it was ok to just give\nsome example clusters are long gone in NLP. Figure 2 looks like it might be a\nquantitative evaluation, but it's only described in the overly long caption.\n\nThe statement in the conclusion that the model solves word analogies is\noverstating what was shown, which was just a few cherry-picked examples of king\n+ queen etc. sort.\n\nThe Chang ref has the conference/journal name as \"Advances in ...\" You'd like\nme to guess the venue?",
        "The paper aims to consolidate some recent literature in simple types of \"reading comprehension\" tasks involving matching questions to answers to be found in a passage, and then to explore the types of structure learned by these models and propose modifications. These reading comprehension datasets such as CNN/Daily Mail are on the simpler side because they do not generally involve chains of reasoning over multiple pieces of supporting evidence as can be found in datasets like MCTest. Many models have been proposed for this task, and the paper breaks down these models into \"aggregation readers\" and \"explicit reference readers.\" The authors show that the aggregation readers organize their hidden states into a predicate structure which allows them to mimic the explicit reference readers. The authors then experiment with adding linguistic features, including reference features, to the existing models to improve performance.\n\nI appreciate the re-naming and re-writing of the paper to make it more clear that the aggregation readers are specifically learning a predicate structure, as well as the inclusion of results about dimensionality of the symbol space. Further, I think the effort to organize and categorize several different reading comprehension models into broader classes is useful, as the field has been producing many such models and the landscape is unclear. \n\nThe concerns with this paper are that the predicate structure demonstrated is fairly simple, and it is not clear that it provides insight towards the development of better models in the future, since the \"explicit reference readers\" need not learn it, and the CNN/Daily Mail dataset has very little headroom left as demonstrated by Chen et al. 2016. The desire for \"dramatic improvements in performance\" mentioned in the discussion section probably cannot be achieved on these datasets. More complex datasets would probably involve multi-hop inference which this paper does not discuss. Further, the message of the paper is a bit scattered and hard to parse, and could benefit from a bit more focus.\n\nI think that with the explosion of various competing neural network models for NLP tasks, contributions like this one which attempt to organize and analyze the landscape are valuable, but that this paper might be better suited for an NLP conference or journal such as TACL.\n",
        "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?",
        " Cortisol is a very important steroid hormone which has many physiological functions in the body that are related to homeostasis of body tissues. This hormone is secreted by the adrenal gland in its cortical area in response to stimulation by the adrenocorticotropic hormone which is secreted by the anterior pituitary gland in response to stimulation from the hormone corticotropin releasing hormone which is secreted by the hypothalamus.;\n, Cortisol has many functions in the body so that its disorders are manifested clinically by many symptoms.\n\n, Cortisol is an anti-inflammatory compound which usually functions by suppressing cells of the immune system. It is usually prescribed by doctors to treat inflammatory reactions such as allergies. Also cortisol has an important effect on glucose level in the blood. Its excess in the blood can cause a state of hyperglycemia with the development of diabetic symptoms in the affected individuals.\n\n, Also cortisol can have an effect on blood pressure in the sense that its excess can increase the blood pressure due to the increased metabolic rate in the body that cortisol enhances.\n\n, There are two main disorders which involve the hormone cortisol. These are: cushing syndrome and Addison's disease. Cushing syndrome is by definition a medical condition in which cortisol is excessive in the blood. This can have several causes. One of them is the result of administration of cortisol as a supplement for various reasons such as to treat inflammatory conditions of the skin for example. This is called iatrogenic cushing syndrome.\n\n, The first cause that is related to increased secretion of ACTH by the pituitary gland can be caused for example due to an adenoma of the gland which oversecretes this hormone.\n\n, The net result in both of these two conditions is the excessive stimulation of the adrenal cortex to synthesize and secrete cortisol to the blood circulation with its concomitant symptoms of obesity and other symptoms.\n\n,, This occurs in up to 20% of all patients with cushing syndrome. Adenoma is more common in females more so than in males.\n\n, This is primarily a disorder of the adrenal cortex in which it occurs due to an autoimmune destruction of the adrenal cortex by circulating antibodies. Also tuberculosis is another possible cause to this disorder. The autoimmune cause is however the main culprit to this disorder.\n\n, Also one of the signs of addison's disease is hyperpigmentation of the skin, especially in areas where the skin creases.\n\n, Also this condition can sometimes cause hypoglycemia and decreased blood pressure due to the usual effect that cortisol makes.\n\n",
        "The paper proposes a new exploration scheme for reinforcement learning using locality-sensitive hashing states to build a table of visit counts which are then used to encourage exploration in the style of MBIE-EB of Strehl and Littman.\n\nSeveral points are appealing about this approach: first, it is quite simple compared to the current alternatives (e.g. VIME, density estimation and pseudo-counts). Second, the paper presents results across several domains, including classic benchmarks, continuous control domains, and Atari 2600 games. In addition, there are results for comparison from several other algorithms (DQN variants), many of which are quite recent. The results indicate that the approach clearly improves over the baseline. The results against other exploration algorithms are not as clear (more dependent on the individual domain/game), but I think this is fine as the appeal of the technique is its simplicity. Third, the paper presents results on the sensitivity to the granularity of the abstraction.\n\nI have only one main complaint, which is it seems there was some engineering involved to get this to work, and I do not have much confidence in the robustness of the conclusions. I am left uncertain as to how the story changes given slight perturbations over hyper-parameter values or enabling/disabling of certain choices. For example, how critical was using PixelCNN (or tying the weights?) or noisifying the output in the autoencoder, or what happens if you remove the custom additions to BASS? The granularity results show that the choice of resolution is sensitive, and even across games the story is not consistent.\n\nThe authors decide to use state-based counts instead of state-action based counts, deviating from the theory, which is odd because the reason to used LSH in the first place is to get closer to what MBIE-EB would advise via tabular counts. There are several explanations as to why state-based versus state-action based counts perform similarly in Atari; the authors do not offer any. Why?\n\nIt seems like the technique could be easily used in DQN as well, and many of the variants the authors compare to are DQN-based, so omitting DQN here again seems strange. The authors justify their choice of TRPO by saying it ensures safe policy improvement, though it is not clear that this is still true when adding these exploration bonuses.\n\nThe case study on Montezuma's revenge, while interesting, involves using domain knowledge and so does not really fit well with the rest of the paper.\n\nSo, in the end, simple and elegant idea to help with exploration tested in many domains, though I am not certain which of the many pieces are critical for the story to hold versus just slightly helpful, which could hurt the long-term impact of the paper.\n\n--- After response:\n\nThank you for the thorough response, and again my apologies for the late reply.\n\nI appreciate the follow-up version on the robustness of SimHash and state counting vs. state-action counting.\n\nThe paper addresses an important problem (exploration), suggesting a \"simple\" (compared to density estimation) counting method via hashing. It is a nice alternative approach to the one offered by Bellemare et al. If discussion among reviewers were possible, I would now try to assemble an argument to accept the paper. Specifically, I am not as concerned about beating the state of the art in Montezuma's as Reviewer3 as the merit of the current paper is one the simplicity of the hashing and on the wide comparison of domains vs. the baseline TRPO. This paper shows that we should not give up on simple hashing. There still seems to be a bunch of fiddly bits to get this to work, and I am still not confident that these results are easily reproducible. Nonetheless, it is an interesting new contrasting approach to exploration which deserves attention.\n\nNot important for the decision: The argument in the rebuttal concerning DQN & A3C is a bit of a straw man. I did not mention anything at all about A3C, I strictly referred to DQN, which is less sensitive to parameter-tuning than A3C. Also, Bellemare 2016 main result on Montezuma used DQN. Hence the omission of these techniques applied to DQN still seems a bit strange (for the Atari experiments). The figure S9 from Mnih et al. points to instances of asynchronous one-step Sarsa with varied thread counts.. of course this will be sensitive to parameters: it is both asynchronous online algorithms *and* the parameter varied is the thread count! This is hardly indicative of DQN's sensitivity to parameters, since DQN is (a) single-threaded (b) uses experience replay, leading to slower policy changes. Another source of stability, DQN uses a target network that changes infrequently. Perhaps the authors made a mistake in the reference graph in the figure? (I see no Figure 9 in ",
        "Doctor Zhivago is the title of a novel by Boris Pasternak and its various adaptations.\n\nDescription\nThe story, in all of its forms, describes the life of the fictional Russian physician and poet Yuri Zhivago and deals with love and loss during the turmoil of the Russian Revolution and war.\n\nAdaptations\nMedia using the name Doctor Zhivago includes the following:\n\nDoctor Zhivago (novel), a 1957 novel by Boris Pasternak\nDoctor Zhivago (film), a 1965 film adaptation by David Lean\nDoctor Zhivago (TV series), a 2002 TV drama serial by Giacomo Campiotti, starring Hans Matheson\nDoctor Zhivago (musical), a 2006 musical, composed by Lucy Simon\n\nFormer disambiguation pages converted to set index articles",
        "  We report the results of our first-principles investigation on the\ninteraction of the nucleobases adenine (A), cytosine (C), guanine (G), thymine\n(T), and uracil (U) with graphene, carried out within the density functional\ntheory framework, with additional calculations utilizing Hartree--Fock plus\nsecond-order Moeller-Plesset perturbation theory. The calculated binding energy\nof the nucleobases shows the following hierarchy: G > T ~ C ~ A > U, with the\nequilibrium configuration being very similar for all five of them. Our results\nclearly demonstrate that the nucleobases exhibit significantly different\ninteraction strengths when physisorbed on graphene. The stabilizing factor in\nthe interaction between the base molecule and graphene sheet is dominated by\nthe molecular polarizability that allows a weakly attractive dispersion force\nto be induced between them. The present study represents a significant step\ntowards a first-principles understanding of how the base sequence of DNA can\naffect its interaction with carbon nanotubes, as observed experimentally.\n",
        "  We present the results of a deep wide-field near-infrared survey of 12 square\ndegrees of the Pleiades conducted as part of the UKIDSS Deep Infrared Sky\nSurvey (UKIDSS) Galactic Cluster Survey (GCS). We have extracted over 340 high\nprobability proper motion members down to 0.03 solar masses using a combination\nof UKIDSS photometry and proper motion measurements obtained by\ncross-correlating the GCS with data from the Two Micron All Sky Survey (2MASS),\nthe Isaac Newton (INT) and the Canada-France-Hawai'i (CFHT) telescopes.\nAdditionally, we have unearthed 73 new candidate brown dwarf members on the\nbasis of five band UKIDSS photometry alone. We have identified 23 substellar\nmultiple system candidates out of 63 candidate brown dwarfs from the (Y-K,Y)\nand (J-K,J) colour-magnitude diagrams, yielding a binary frequency of 28-44% in\nthe 0.075-0.030 Msun mass range. Our estimate is three times larger than the\nbinary fractions reported from high-resolution imaging surveys of field\nultracool dwarfs and Pleiades brown dwarfs. However, it is marginally\nconsistent with our earlier ``peculiar'' photometric binary fraction of\n50+/-10% presented in Pinfield et al. (2003), in good agreement with the 32-45%\nbinary fraction derived from the recent Monte-Carlo simulations of Maxted &\nJeffries (2005) and compatible with the 26+/-10% frequency recently estimated\nby Basri & Reiners (2006). A tentative estimate of the mass ratios from\nphotometry alone seems to support the hypothesis that binary brown dwarfs tend\nto reside in near equal-mass ratio systems. (abridged)\n",
        "It is weird that the paper describe LFW dataset but do not provides the results on it.",
        "The reviewers mostly agree that this paper offers valuable insights about a family of problems in automatic \"reading\" of text, as well as current solutions. The paper seems to fail to generate excitement because it doesn't really point the way forward. I disagree with some reviewers about the work's suitability for ICLR (as opposed to an ACL venue) since ML researchers are also thinking about these tasks now. The consensus is that the paper will have greater impact (wherever it is published) with a clearer message.",
        " You can use 4 to 6 ramekins or an 8-cup (2 liters) casserole dish Whatever you choose to use, make sure that it is made out of glass or ceramic, and is safe to put into the oven. Set the dish aside when you are done.;\n, Separate the yolks from the whites. Place the yolks into a small mixing bowl and set the whites aside. Beat the sugar into the yolks by hand or using an electric mixer until light and creamy.\n\n\nYou will be using the remaining 1 tablespoon of sugar later in the recipe.\n\n, Sift the flour first, so as to avoid any lumps or clumps. This will give you a smoother, silkier pudding in the end.\n\n\nWhole milk is best. For a lighter option, use reduced-fat milk., Beat the egg whites using an electric mixer until stiff peaks form. Add the sugar gradually; this will help the egg whites remain stiff.\n\n, Stir the mixture gently until everything is evenly combined. There should be no streaks or swirls.\n\n, If you are pouring the pudding into ramekins, make sure that you distribute it evenly.\n\n, The water level should come halfway up the pudding dish.If there is too much water, pour some out.\n\n, If you are baking the pudding in ramekins, test the pudding after 25 minutes. The pudding is ready if it springs back when you touch it lightly.Another way to test whether or not the pudding is done is by inserting a fork into the middle; if the fork comes out clean, it is done.\n\n, Pull the casserole dish or ramekins out of the water-filled pan using potholders, and set it down on the table. You can serve it as is, or add some whipped cream or ice cream.A few grated pieces of lemon rind over the cream or ice cream make for a nice final touch.\n\n, You can use six 1-cup (240 milliliters) ramekins or an 8-cup (2 liters) casserole dish.Whatever you choose to use, make sure that it is made out of oven-safe glass or ceramic.\n\n, Pour everything into an electric mixer, and beat together using a high or medium speed. If you don't have one, you can use a food processor fitted with whisks instead.\n\n, Separate the egg yolks from the whites first, and set the whites aside. Add the egg yolks into the butter mixture one at a time. Beat the mixture well after each egg that you add.\n\n, When you add the lemon juice, the mixture will start to curdle. Don't worry; it will turn out fine in the end. Add the flour and milk next, and combine using a lower speed setting on your mixer. The mixture will start to look a little like thin batter.\n\n, Make sure that the bowl is clean and dry, and that you are using a whisk attachment or a balloon whisk. Use a medium or high speed setting.\n\n, Use a spatula to fold half of the whisked egg whites into the butter mixture. This will loosen the mixture up a bit, and make it easier to blend. Next, fold the rest of the beaten egg whites until everything is just combined., If you are using ramekins, make sure that you distribute the mixture evenly between them.\n\n, The water should come up halfway up the sides of your casserole dish or ramekins. If there is too much water, pour some out.\n\n, Another way to test it is by inserting a fork into the middle; if the fork comes out clean, it's done.\n\n\nPudding baked in ramekins will be finished sooner than pudding baked in a casserole dish.\nIf you are using a glass dish, you will notice a creamy sauce under the sponge topping. This indicates that the pudding is done., Use potholders to take the casserole dish or ramekins out of the water, and set it down on a table. You can serve it as is, or garnish it with powdered sugar, whipped cream, or ice cream.\n\n",
        "The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images “counterfactuals” which seems like quite an unnecessarily grandiose name for literally, a scaled image. \n\nThe authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.\n\nWhile the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more “importance” on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.\n\nUltimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.\n\nConsidering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. “Gradients of Coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.",
        "This paper presents an multi-view learning algorithm which projects the inputs of different views (linearly) such that the neighborhood relationship (transition probabilities) agree across views.\n\nThis paper has good motivation--to study multi-view learning from a more information retrieval perspective. Some concerns:\n-- The time complexity of the algorithm in its current form is high (see last paragraph of page 4). This might be the reason why the authors have conducted experiments on small datasets, and using linear projections.\n-- The proposed method does have some nice properties, e.g., it does not require the projections to have the same dimension across views (I like this). While it more directly models neighborhood relationship than CCA based approaches, it is still not directly optimizing typical retrieval (e.g., ranking-based) criteria. On the other hand, the contrastive loss in \nHermann and Blunsom. Multilingual Distributed Representations without Word Alignment. ICLR 2014. \nis certainly a relevant \"information retrieval\" approach, and shall be discussed and compared with.\n\nMy major concern about this paper is the experiments. As I mentioned in my previous comments, there are limited cases where linear mapping is more desirable than nonlinear mappings for dimension reduction. While the authors have argued that linear projection may provide better interpretability, I have not found empirical justification in this paper. Moreover, one could achieve interpretability by visualizing the projections and see what variations of the input is reflected along certain dimensions; this is commonly done for nonlinear dimension reduction methods. \n\nI agree that the general approach here generalizes to nonlinear projections easily, but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of CCA and other multi-view learning algorithms limits the significance of the current paper. ",
        " Whether the problem is cleaning your room, doing your homework, or applying a raise to your allowance, listen to what they have to say and think about it carefully. Never assume you know what they want; they may have changed what they expect of you as you’ve grown. Ask questions if you’re confused about their reasoning or need some clarification.\n\n\nIf your parents are having a hard time expressing themselves to you, or you need a direct answer about their expectations, encourage them to share their views with you. Try a conversation prompt such as “I’m very interested in hearing what you think about ,” or “Do you think it would be alright if I ?”;\n, Do you want to buy a pair of hundred-dollar jeans but your parents will only spring for fifty-dollar jeans? Think about what your folks are proposing and ask yourself if there’s an alternative to their decision that would make you happier. If there is, suggest it to your parents and explain why.\n\n\nYour counteroffer is probably what you originally intended to do before your parents curtailed your decision.\nFor instance, perhaps your plan was to go out with your friends until midnight. Before you could express this to your parents, they informed you that you were to be home by 10:00. When they inform you that you’re to be home at 10:00, gently and patiently respond, “Oh, I was planning to be out until midnight. Is that okay?” With luck, they will accept your counteroffer, and there will be no need to compromise. If they do not accept it, however, you’ll need to think about developing a compromise.\nDo not develop a counteroffer long after your parents express what they want. If you wait, they might feel that their initial decision has been “set in stone” and be unwilling to waver on it.\n\n, When dealing with an issue or rule which is long-term or permanent, your parents will be unwilling to make compromises.For instance, where there are hard rules such as no drinking, no drugs, and no driving the Lamborghini, you should not bother trying to compromise. Where compromise is not an option, you should comply accordingly.Doing so will build trust with your parents and make them more disposed toward compromising on other, more debatable topics.\n\n, For instance, if you want to go out until midnight but your folks want you home at 9:00, you’ll have greater luck getting them to compromise for a curfew of 10:00 than 11:00, and more likely to get them to agree to an 11:00 than to a midnight curfew.\n\n\nThe specific conditions of the compromise determine its efficacy. If you’re out on a school night, for instance, you will be less likely to get an extended curfew than if it’s a Friday or Saturday. Think about the specific conditions under which you are compromising in order to develop a logical argument with which to make your case.\n\n, A perfect, down-the-middle compromise would be for you to go half a day. But if you can convince your parents to let you take the first two periods off, call it a win and don’t feel obligated to keep pushing for a half day off.\n\n, The best time to have a serious talk is after dinner.Asking for your parents to consider a compromise in the morning is folly. Around breakfast time, everyone -- including you, probably -- is scrambling to get out the door to work or school. Asking for a compromise, laying out your reasoning, and conducting the back-and-forth conversation which compromise entails is difficult or impossible when you ask for one in the morning. For this reason, choose a time in the late evening when everyone is winding down and has time for contemplation.\n\n, Choose a quiet, well-lit room so as to focus attention on the conversation itself. Do not try to talk to your parents while they are working on the computer or talking on their phone. Ensure you have their undivided attention before asking for a compromise. Arrange some chairs in a tight circle, enough for everyone who is part of the compromise.\n\n\nFor instance, if your compromise is for you and your sister to do something together, your parents, your sister, and you all need to be seated.\nTry to always include everyone who the compromise affects in the conversation. If, for some reason such as time constraints, you cannot have all affected parties present when you discuss the compromise, keep an empty chair in the circle in order for each of you to think about what the absent individual would think or say about the proposed compromise.\nFor instance, if your sister can’t make it to the compromise talk but you and your parents can, keep an empty chair in the circle in order to represent her and her potential wishes. As you discuss the various facets of the compromise, ask of each, “What would think about that?”It can also be good to seek a compromise on something while in the car with your parents. The car is a relatively quiet, enclosed space where you’re ensured direct access to them. Ensure the car ride is long enough for you to lay out your vision for a compromise., Walk away from the conversation for awhile and think about how you can approach the situation from another angle.\n\n\nFor instance, if you feel your face start to flush with rage when discussing when you need to be home at night, take a few deep breaths and retreat to your room, or go for a walk. After clearing your head, try to develop an alternative compromise or word the compromise differently.\n\n, This will make them indisposed to compromising with you. Stay calm and demonstrate that you understand their point of view.\n\n\nOpen the negotiations to compromise with the following: “Thank you for allowing me this audience. I am interested in proposing a compromise on the matter of . While I respect and understand your initial decision, I believe that if we can reach a compromise, we can arrive at a solution in which we both get what we want.”At that point, you can launch into the specific details of your compromise.\nWhen your parents accept a compromise, thank them sincerely for their flexibility and understanding. Parents love feeling appreciated.\nFailing to be respectful -- for instance, by telling your parents they’re being dumb or annoying -- may scuttle any hopes of your compromise being accepted or even considered.\n\n, If your initial compromise doesn’t work, try to think of a way to turn the situation more favorably in your direction. For instance, if you want to stay out until one and your parents want you home at 10, they might refuse your initial compromise to stay out until 11:30. Instead, offer to come home at 10 every night the rest of the week if you can stay out until one just this one night.\n\n, Think of this person as a mediator on your behalf. You might get help from another family member, a teacher, a therapist or conflict mediator, or a clergy member who is close to your family.\n\n\nBefore enlisting help, be sure that your parents’ rules are overly strict. If they are reasonable and you seek help in dealing with them, you’ll look immature or insincere.\n\n, If you feel kind of tired one night and your parents ask you to be home at 10 instead of 11 like you planned, accept their request and head home at ten. It’s only an hour, and you’ll probably be ready to hit the hay by then anyway since you’re already tired.\n\n, Comparative reasoning involves comparing your case to a similar case in order to justify a reasonable compromise.For instance, if your parents insist you come home at 10 each night but your brother, who is of a similar age, is allowed to come home at one or two, you could compare your own situation to his to demonstrate how unfairly you’re being treated. Recognizing that your brother is being treated differently than you are might sway their decision in your favor.\n\n, Once a compromise has been reached, you’ll need to hold up your end of the bargain.Having a roadmap with which to get you through the compromise with your parents is a big step toward making it happen.\n\n\nFor instance, if you decided to be home at 9:00 instead of 11:00, write it in marker on your calendar (“PARTY TONIGHT UNTIL 9:00!”). Set your phone and/or watch with an alarm for 8:30 or a point in time from which you can make it home by 9:00. Ask a friend to remind you when the clock strikes 8:30 so that you’ll know you need to go.\n\n, Doing so will make your parents more likely to compromise with you again in the future. On top of that, you’ll feel good when you keep your word and meet your parents’ expectations.\n\n\nBreaking your promise to your mom and dad will make them doubt your maturity and ability to compromise on other issues. Your parents will likely be angrier than they might normally be since from their point of view, they were showing leniency in their decision to compromise\n\n, If you are going to a ball game that starts at 8:30 and the compromise you worked out with the folks designates a curfew of 9:00, you probably won’t be coming home on time. A broken compromise won’t make your parents happy. For that reason, only make compromises that you will try your best to fulfill.\n\n, If your situation changes or you doubt you can fulfill your end of the bargain, let your parents know. If you come to them in good faith asking for a modification of the compromise, they will probably understand.For instance, perhaps you’re going to a party with a friend. You compromised with your parents to let you stay out until 11:00 instead of your usual bedtime, 9:00. But you find out that your friend who was supposed to give you a ride home is drunk, so you won’t be able to get home until your other friend leaves closer to 11:30.\nCall your parents as soon as you find out the new information and ask them to let you stay out later. As good parents, they will surely not insist that you get a ride home with a drunk friend just so you fulfill the terms of the compromise.\n\n",
        "The Director of a film (or a play, or a television show episode) is the person responsible for the creative vision of the piece. They create a concept from the script (which may or may not be something concretely found in the script, it may be metaphorical or tangential) and from the concept lead the design and production team towards a collaborative vision. Once rehearsals/filming have begin, the director blocks the piece (i.e. tells actors where to move), provides objective and subtextual support to the actors (i.e. tells them why they are saying the things the writer wrote) and ensures that the visual style and setting are within the original vision or concept parameters.\n\nIn film, they also work closely with the DP, first story-boarding the script, and then, once on set, making sure that each shoot is framed, blocked and shot per their vision. Including ALL design aspects, from the color of the walls to the type of purse a character might wear.\n\nIn essence they are the Captain of the ship. A lot of my notes below can also be laid at the feet of bad writing, but in film (less so TV and theatre) directors have a great deal of oversight on the writing, so they are typically held accountable if the writing is terrible.\n\nA film which has been directed badly will usually (but not always, the problem with a collaborative art form, which is what film is, is that there are many, many chefs in the kitchen. However, since the director tends to get the credit when everything works, they also tend to get the blame when it doesn't)--usually show the following flaws:\n\n1. Incoherent story telling. You don't know what is happening. Or why it is happening. Or who it is happening to. Sometimes things are just blatantly implausible.\n\n2. Cliche or trope ridden dialogue/shots/events. You feel like you've seen all of these things before. All the characters are stereotypes, all the plot points unfailingly predictable. Note: cliches, tropes and stereotypes can all be used well. But bad directors tend not to.\n\n3. Bad dialogue. Dialogue that is forced and unnatural. Dialogue that is too on-the-nose. People telling other people things instead of doing things. People explaining how they feel ad nauseam. Dialogue spoken only to allow for the plot to push forward, leading us to:\n\n4. Coincidental plotting, or plots hole you could drive a freight train through (not the small inconsistencies that almost every movie has, but HUGE giant massive oh-my-god-this-movie-is-broken plot holes). Coincidental plotting is when everything that has to happen for the plot to move forward does, without any effort on the part of the hero (or the bad guy).\n\n5. Bad acting. Directors are responsible for getting a performance out of their actors, so even if the actor can't act (one reason why casting is important) the director is still the one people are going to hold responsible for any painful moments on screen (this is less true in TV and theatre). \n\n6. Over or under designed. Over designed is when the concept/vision of the piece becomes more important then any other element. Think 300: Rise of an Empire or Sin City: A Dame to Kill For (not-at-all-oddly, both Frank Miller graphic novel adaptations, where the look was where the design team started with). Tim Burton is also a well-known director who can go to far with his vision/design to the point of over balancing the movie. Under designing is when there is a lack of design and the production feels (usually) cheap or not-thought-through. Good design elevates the narrative, supports the characters and provides visual clues to the audience about what is happening--excellent design can comment on and complement the action, enhancing the entire experience.\n\n7. Movies/TV only: bad editing. Either because there were technical difficulties during filming and the needed shots weren't gotten (or a director wasn't prepared and didn't get the shots they needed), and therefore the editor is attempting to make up for missing and/or bad shots; or because the editing itself is just bad. Odd cuts, odd shots going back to back, odd audio issues. Various other things.  While most early directors at a studio on a movie won't have any say over the final cut, most editing issues are from a lack of footage (which is the director's issue), not bad editing. OR a director who does have final cut approval and shouldn't, which is where you got a three-hour movie that should have been 2 hours and 10 minutes max.\n\n8. Poor production value. An overall feel that the movie wasn't cared for (this isn't about money, this is about time and support). Usually shows in bad lighting, bad audio, bad set dressing, bad costumes--just an overall sense that these things weren't considered important or there wasn't time to pay attention to them.\n\nA film, tv show or theatrical play is an immense, multi-part beast, and the Director is the one that tries to tame it. To varying degrees of success. Every director probably has one (or many) bad movies to their name, as its how we all learn. The more telling test is not if they directed a bad movie, but if people wanted to work with them again. And, sometimes, the love of the thing they are creating can shine through the worst movie and make it, somehow, good (think Sam Raimi's original Evil Dead).\n\nHope that helped!",
        "Before one more person comes here to say \"water\":\n\n* Medieval people drank water. Coroners' reports from 14th century England tell of young children who drowned when they fell into creeks while trying to scoop water into a cup to drink. \"Bread *and water*\" was the standard penitential and perpetual-imprisonment diet.  & c.\n\n* Watered-down (*watered*-down) wine, ale, and beer were popular drinks for socio-economic and taste reasons, not because water was inherently unsafe. Water was the \"beggars' drink\" because it was free. Drinking ale meant you could afford to drink ale/afford the time to make it for yourselves.\n\n* Medieval people understood that some water was dirty. In Viterbo, a giant riot that resulted in the death of a local woman exploded after a visiting cardinal's assistant bathed his puppy in the local fountain from which drinking water was drawn.\n\n* Medieval people understood that boiling dirty water could help. A 14th century letter from a man to his sons at university in Toulouse reminds them that one of the local rivers has dirty water, and they must not drink it unless they \"cook\" it.\n\nOh! I completely forgot earlier: I've previously discussed [the difference between diets in England, ca. 1500, and modern America](_URL_0_), if anyone is interested!",
        "Your work is closely related to batch selection: \"Online batch selection for faster training of neural networks\" from ICLR Workshops 2016 where the authors provided \"an initial study of possible online batch selection strategies\". In your work, batch selection is denoted as \"batch construction\" and you study a broader set of selection strategies. \n",
        " These can be obtained over-the-counter at your local pharmacy or drugstore. They can also be received in stronger formulations by asking your doctor for a prescription, if the over-the-counter (OTC) versions are insufficient to relieve your pain.\n\n\nAn example of an NSAID medication is ibuprofen (Advil, or Motrin). The typical dose is 400 – 600mg every four to six hours as needed. Follow the dosing instructions on the bottle.\nAnother NSAID option is naproxen (Aleve). This is available over-the-counter, or as a stronger version that is available via a prescription from your physician.;\n, The first-line narcotic used in the treatment of ovarian cyst pain is morphine, an opiate.\n\n\nNarcotics are the last option when it comes to pain relief. In light of the national epidemic of narcotic abuse/misuse in the United States, opiate medication should be only taken in an emergency room setting or for the shortest possible amount of time.\nAdditionally, if you have a substance abuse history and previous addiction to illicit or prescription medication, the decision to utilize this should be weighed against the risks of harm and/or relapse.\nMorphine for ovarian cyst pain is most often given via an IV, and in a hospital setting.\nThis is because, to warrant this strength of medication, the pain is usually very severe, resulting in a visit to the Emergency Room.\nInitially, a small dose of Morphine is administered via IV; the dose is increased incrementally until the pain gets under control.\nMorphine is also a very safe option within the hospital setting. It can readily be reversed with Naloxone should any adverse reactions occur.\n\n, If you are experiencing abdominal or pelvic pain, it is important to see your doctor and to receive a physical examination, an ultrasound, and any other necessary tests to confirm that the source of your pain is indeed an ovarian cyst(s).Many other conditions may cause pain similar to ovarian cyst(s), so it is important to see your doctor to confirm that is indeed an ovarian cyst(s) that is the source of your pain.\n\n\nYour doctor may perform a pelvic ultrasound in which a wand-like device is inserted into your vagina and uses high-frequency sound waves to create a picture of your ovaries on a video screen. This imaging test can help your doctor confirm the presence of a cyst, identify its location and determine whether it's solid, filled with fluid or mixed., If the pain persists for two to three menstrual cycles, if it is large, doesn't look like a functional cyst, or is growing, you will likely be advised to proceed with surgery to have your ovarian cyst(s) removed.\n\n\nThe vast majority of ovarian cysts are benign.Either just the cyst can be removed from the ovary, or the entire ovary can be removed. The extent of the surgery will depend upon the number of cysts present on the ovary, as well as the age of the patient and reproductive considerations. (The surgery is generally more extensive following menopause.)\nFortunately, if the entire ovary needs to be removed surgically, there is still the ovary on the other side, so the woman does not need to lose her fertility by having a surgical procedure performed.\n\n, If there is the risk of them developing into a cancer, your doctor will advise that surgery be performed to prevent this from occurring.\n\n\nWhen an ovarian cyst(s) is removed due to the possibility of it becoming cancerous, it is advised to have the fallopian tubes and uterus removed in addition to both ovaries.\nThis, of course, has reproductive implications that need to be discussed with your doctor, because having all of these structures removed will make you infertile.\n\n, This consists of using pain medications as needed to relieve your ovarian cyst pain, while hoping that the cyst ultimately resolves itself without the need for medical intervention. You must, however, commit to thorough follow-up with serial ultrasounds.This will ensure the cysts don't get worse.\n\n\nIf the cysts do not improve with time, surgery may be advised.\n\n, As such, doctors advise patients with ovarian cyst(s) to begin hormonal birth control pills as a method to prevent the problem from getting any worse than it currently is.\n\n\nYou can receive a prescription for birth control pills from your family doctor.\nThe pills are taken once a day for three weeks, followed by one week off (or one week of \"sugar pills\"). This cycle repeats monthly.\nThe hormones in the birth control pills replace the hormones that are normally produced by your ovaries.\nThe ovaries then temporarily \"shut down\" hormone production while you are on the pill, and this greatly reduces the risk of any new cysts forming.\n\n, If you are a current smoker and older than 35 years old, you will also be advised against taking the pills due to a heightened risk of developing blood clots. Similarly, if you have another bleeding disorders (such as a hereditary bleeding disorder), you should not take the pill due to the risk of blood clots.\n\n\nYour doctor will go over your medical history with you to determine the safety of taking hormonal birth control pills.\nFor the majority of people, it is completely safe to take the pills.\n\n, Taking birth control pills will reduce the chance of developing new cysts in the future. In addition, it can reduce your chances of developing ovarian cancer. In fact, the longer you take birth control pills, the more your risk is reduced.\n\n, Examples include ginger and turmeric. Both ginger and turmeric can be added as spices to foods, and you can make them part of your home recipes to decrease the painful inflammation that can be associated with ovarian cysts.\n\n, Applying heat over your abdominal/pelvic area (over the source of the pain) can help to relax your muscles and to ease the pain. Try using a hot water bottle or a heating pad for 15 minutes at a time, as needed, to relieve ovarian cyst pain.\n\n\nYou may also opt for a hot bath as a means to apply heat to the area.\nAn Epsom salt bath is even better, as the magnesium in the salts helps with muscle relaxation, which can reduce the pain even further., Although acupuncture and hypnosis are not generally recommended by traditional western medical doctors, some people find them helpful with pain management (such as helping with the pain of ovarian cysts). Acupuncturists work with needles to correct the flow of energy through the body, with the ultimate goal of relieving (or diminishing) pain. Hypnotists work to decrease your mind's perception of pain., Find an activity you can do that will distract you from the pain from your cyst. Reading a good book, using guided imagery, playing a video game, doing something crafty, or doing anything that will take your mind off the pain can help.\n\nTry deep breathing or meditation to cope with your pain.\n\n",
        "There is a series of somewhat-related papers on learning to optimize for sparse coding and other L1/L2 optimization problems.\nInference in sparse coding is often performed with the Iterative Shrinkage and Thresholding Algorithm (ISTA). \nISTA can be viewed as a kind of recurrent net in which the matrices and non-linearities are specified by the reconstruction matrix.\nThe main idea is to train the matrices in this recurrent net to produce an approximate solution faster than ISTA.\n\nHere are two examples:\n[Gregor & LeCun, ICML 2010] ",
        "To make a long story short, they are just not good enough (yet?) for most applications. More to the point, they are not as good as the current king of the PV market, namely crystalline-Si solar cells. To see better what I mean, we can look at three key areas of PV performance. \n\n1. **Efficiency.** Take a look at [this chart](_URL_0_) showing the efficiency of various PV technologies. Organic solar cells are shown in solid red circles. As you can see over the past decade, the power conversion efficiency of these cells has slowly crept up, topping up just over 10%. If you tried to make commercial modules out of these cells, the efficiency would be below 10%. Now compare that to silicon, where lab-grade cells reach 25% and even commercial modules hover around 20%. It's simply not clear at this moment whether organic solar cell will ever get much better than 10%, giving Si cells a strong leg up. \n\n2. **Stability.** The ideal solar cell is one that you could stick on your roof (or in a solar park) and it will keep cranking out power for decades. Silicon solar cells are pretty damn solid in this regard, often showing little degradation even after years and years of use. Organics on the other hand tend to be quite a bit more flimsy, with the efficiency slowly drooping as the organic compounds degrade. The problem can be solved in part by using more stable active materials and encapsulating the cells, but it's not clear what the best case scenario is.\n\n3. **Cost.** One of the key reasons people started investigating organic solar cells is because crystalline silicon was so expensive. That logic has changed drastically in recent years. As the production volume of crystalline silicon rose, the price plummeted [as shown in this graph](_URL_2_). In some sense, that shouldn't have been too shocking. Silicon is one of the most abundant elements on Earth and [economies of scale](_URL_1_) are very much real. As a result, the cost of the silicon is becoming an increasingly smaller share of the total price of a solar cell. Moreover, the best organic cells use rather complex active materials that require many synthetic steps that drive the price up. As a result, it's becoming less and less clear whether organics can potentially offer a significant reduction in cost compared to silicon cells.",
        "In Rodney Cline's 1960 paper, \"The Conservatism of Philosophical and Educational Liberals,\" published in the *Peabody Journal of Education*, he declares that **\"Philosophical liberalism and Educational liberalism have strongly held sway in American Education for the greater part of the twentieth century.\"**\n\nEdmund Fawcett's book *Liberalism: The Life of an Idea*, traces modern liberalism (we'll define it here as preferring new ideas to the tried-and-true, an alternative to don't-fix-it-unless-its-broke) to the 1830s and the ideas of Wilhelm von Humbolt, a German scholar and diplomat who encouraged universal education.\n\nThere's an argument to be made that **higher education became liberal once the notion of universal education became popular.** In the 19th century, universal education was a radical concept in Western Europe. It had its strongest supporters in Germany, and many of those supporters migrated to the United States in the 1840s and 1850s, part of a flood of immigration about the time of the failed 1848 general European revolution.\n\nBecause conservative factions won that revolution almost universally across Europe, it was the liberals who emigrated to the United States, carrying their ideas with them. They believed in things like kindergarten (a German word, mind you), universal education, the elimination of slavery, and other radical concepts along those lines. German and Austrian immigrants made up an even larger portion of the Union armies in the Civil War than did the famed Irish regiments.\n\nWith conservative Democrats out of Congress because of the war, a liberal Republican-led Congress in 1862 passed the Land-Grant College Act of 1862, which provided grants of land to states across the United States to finance universities specializing in \"agriculture and the mechanic arts.\"\n\nThis was a huge innovation, and it's a key sign that higher education was seen as politically liberal even in 1862. Indeed, President Lincoln was already leaning upon the politically liberal ideas of college professors when he approved guidelines for the treatment of prisoners of war and general rules of war to be followed by the Union armies in combat.\n\nThe Land-Grant College Act was a key milestone in the advancement of higher education in the United States, because it democratized it, allowing it to be accessible to many more Americans than before. Until 1862, colleges tended to cater toward the elite. Institutions like Yale and Harvard, among the oldest in the United States, produced theologians and lawyers, people among the upper strata of society. \n\nThey were exactly the people most inclined to want to keep society the way it was, because they had the most to lose if it changed.\n\nThe establishment of West Point was a step toward the democratization of higher education in the United States, because it didn't distinguish based on the ability to pay for the education. Furthermore, it taught engineering, which at the time was considered a \"lesser\" study than the law. West Point (and Annapolis, later) still had a high bar for entry, but it was a *political* bar rather than a financial bar. One had to be recommended to the institution, and while that was easier, it wasn't *easy.*\n\nOnce the Land-Grant College Act arrived, there were a wave of institutions to choose from, and competition begat lower prices in a classic case of supply and demand. Once World War II arrived, the Montgomery GI Bill ensured that anyone (Note: blacks were excluded) could attend these colleges, which by 1944 had 80 years to become established. That was the last step needed for the democratization of higher education.\n\nNow, it should be noted that **the whole idea of public education, enshrined in the United States from its earliest days** (I'm thinking in particular of the Land Ordinance of 1785) **is at its heart a liberal idea.** The notion of giving a schoolroom education to everyone was a departure from the previous process of education through apprenticeships and journeymen in a trade. Previously, in Western Europe, general education was usually restricted to the elite, or religious institutions. Overton Taylor's \"Liberal Education and Liberalism,\" published in the Jan. 1945 issue of *Ethics*, is a good walkthrough of how the idea of widespread *lower* education was also a liberal idea, but that's outside the scope of your question. I think it's worth mentioning, because it plays into the subsequent idea of *higher* education being a liberal force. \n\n*Note: This is a badly worded question, and trying to answer it is like trying to eat soup from a cardboard box.*",
        " Do some research and look around on whom has the best deals for your specific needs. Things to watch for are: Speed, Usage, and most importantly price!;\n, After you set up your ISP, they will usually give you a modem. Modems vary in shapes and sizes, as well as functionality. There are some modems that have wireless routers built in which will make it easy to set up a network because all you will need to do is plug it in. Others will simply have one output jack, which you will then need to connect to your router. Once you get a router from your ISP, the first thing you need to do is figure out what kind you have, I will be using a standard rogers modem for my examples but they will vary depending on your ISP.\n\n, Usually you want a place that is close to the center of your house so you will get the most range covered by the network. You don't want to be walking up to your room with your laptop then realize that you have a weak signal there because your network station is on the other side of your house and in the basement.\n\n\nA standard Rogers modem is simple, it will have a power jack, a cable jack, and a RJ45 Ethernet jack. Common sense would tell you to connect the power cable to the power jack, a TV cable to the cable jack, and a standard Ethernet cable to the Ethernet jack. (HINT: Connect the modems power cable directly to a wall socket for best performance)\n\n,, As expected the higher quality routers will have a higher price. To find a router you will need to go to some technology store such as Future Shop, Best Buy, or Staples. There are many more and you can even find them in the Electronics department of most department stores. Best advice is to shop around, see if you can find any sales because routers can get pricey!\n\n, Much like the modem routers are all different, they will have different amounts of antennas, different amounts of RJ45 output jacks, and different power/input options. You will need to connect your Ethernet cable from the modem output jack, into the input jack of your router. Then connect the power cable to the router after that. (NOTE: You may need to reset your modem after connecting it to the router, simply unplug and replug the power cable to the modem.)\n\n, You will often see a wireless network show up on your device named after the brand of device you decided to get.\n\n, Use the CD in you computer and follow the on-screen instruction. Usually, the device you get will say the CD is needed on every computer you use with the network, but i have found this is not the case.\n\n, To do this write the address (usually looks like an IP address which will be in the format 0.000.0.0.00 or something like that) that was given in the instructions for your router into the address bar of your browser. This will open your routers home page, you will need to login. To do this, use the default login information given with your router.\n\n, Once you have completed this, you are almost done!\n\n, Simply try to connect to the network and then enter your created password.\n\n,",
        " Popular types of encyclopedias include the Encyclopedia Britannica, the World Book Encyclopedia and the Columbia Encyclopedia. Wikipedia is an online encyclopedia that is often used in place of books in a library.\n\n\nPublished encyclopedias are more likely to be extensively fact-checked than online encyclopedias; however, the volumes must be published recently in order to provide accurate information.\nOnline encyclopedias, such as Wikipedia, are updated regularly. The reliability of the sources differs greatly from subject to subject.;\n, If you are starting with little knowledge of the subject, choose a general term, like “gardening,” “Russia” or “linguistics.”\n\n, For example, if you are researching “Russia,” find the volume for the letter “R.” Go to the section in the library that houses the volumes and move down the volumes in alphabetical order until you find “R.”, Follow the bolded subjects alphabetically until you find the word you are looking for.\n\n, Most encyclopedias can’t be checked out. Return the volume after you have a copy of the information.\n\n\nIf you are using an online encyclopedia, you can print out your selection to take it with you for continued review.\n\n, Annotate by writing important information in the margins of your photocopy.\n\n, Write down one to five names or subjects that will help you continue your research. For example, if you are researching Russia, you may write down “Vladimir Lenin,” “Bolsheviks” or “Kremlin” to look up in the encyclopedia.\n\n\nIf you are using an online encyclopedia, click on the underlined words to follow links to other subjects.\n\n, Take down the letters that correspond with those subjects. For example, to research “Bolsheviks” you will need the letter “B” and to research “Vladimir Lenin,” you will need the letter “L.”\n\n\nEncyclopedia entries usually correspond with the last name of the person you are researching.\n\n, Replace the volumes.\n\n,, Check out those books to learn more on a given subject. For example, if you are researching Vladimir Lenin, you may want to check out his “April Theses” to continue your research after you have read the encyclopedia entries.\n\n,, Write down the author, encyclopedia name, city of publication, publisher and year of publication. Also, write down the subjects and pages that you referenced.\n\n\nSome encyclopedias do not list authors. If any of the above information is not available, you can skip it.\n\n, “Article Title” Encyclopedia Name (in italics). City of publication: Publisher, Year of Publication. Page numbers. Print.”\n\n\nFor example, “Murphy, Karen. “Russia” Encyclopedia Britannica. London: Encyclopedia Britannica, 2009. 504-509. Print.\nIf there are multiple authors, list the first author with the last name, then first name. List subsequent authors with the first name and then the last name.\n\n, “Article Title” Encyclopedia Name (in italics). City of Publication: Publisher, Year Published. Website Title. Web. Date Month Year Accessed.\n\n\nFor example, Murphy, Karen. “Russia” Encyclopedia Britannica. London: Encyclopedia Britannica, 2009. EncyclopediaBritannica.com. Web. 24 Mar. 2014.\nTry to find all the information listed. If it is unavailable, leave it out. Online encyclopedias rarely list authors.\n\n, Encyclopedia Name (in italics), Edition Number. “Article Title.” Publication City: Publisher Name, Year Published.\n\n\nFor example, Murphy, Karen. Encyclopedia Britannica, ed. 208. “Russia.” London: Encyclopedia Britannica, 2009.\n\n, Encyclopedia Name (in italics), Edition Number. “Article Name.” City of Publication: Publisher Name, Year of Publication. URL (accessed Month Day, Year).For example, Murphy, Karen. Encyclopedia Britannica, ed. 208. “Russia.” London: Encyclopedia Britannica, 2009. http://www.encyclopediabritannica.com/russia (accessed March 24, 2014).\n\n",
        "  IRAM 30m observations reveal that the deeply obscured IR-luminous galaxy\nNGC4418 has a rich molecular chemistry - including unusually luminous HC3N line\nemission. We furthermore detect: ortho-H2CO 2-1, 3-2; CN 1-0, 2-1; HCO+, 1-0.\n3-2, HCN 3-2, HNC 1-0, 3-2 (and tentatively OCS 12-11). The HCN, HCO+, H2CO and\nCN line emission can be fitted to densities of n=5 x 10E4 - 10E5 cm-3 and gas\ntemperatures Tk=80-150 K. Both HNC and HC3N are, however, significantly more\nexcited than the other species which requires higher gas densities - or\nradiative excitation through e.g. mid-IR pumping. The HCN line intensity is\nfainter than that of HCO+ and HNC for the 3-2 transition, in contrast to\nprevious findings for the 1-0 lines where the HCN emission is the most\nluminous. We tentatively suggest that the observed molecular line emission is\nconsistent with a young starburst, where the emission can be understood as\nemerging from dense, warm gas with an additional PDR component. We find that\nX-ray chemistry is not required to explain the observed mm line emission,\nincluding the HCN/HCO+ 1-0 and 3-2 line ratios. The luminous HC3N line emission\nis an expected signature of dense, starforming gas. A deeply buried AGN can not\nbe excluded, but its impact on the surrounding molecular medium is then\nsuggested to be limited. However, detailed modelling of HC3N abundances in\nX-ray dominated regions (XDRs) should be carried out. The possibility of\nradiative excitation should also be further investigated\n",
        "  This is a sequel to [Ca01]=math.AG/0110051. We define the bimeromorphic {\\it\ncategory} of geometric orbifolds. These interpolate between (compact K\\\" ahler)\nmanifolds and such manifolds with logarithmic structure. These geometric\norbifolds are considered from the point of view of their geometry, and thus\nequipped with the usual invariants of varieties: morphisms and bimeromorphic\nmaps, differential forms, fundamental groups and universal covers, fields of\ndefinition and rational points. The most elementary properties, directly\nadapted from the case of varieties without orbifold structure, are established\nhere. The arguments of [Ca01] can then be directly adapted to extend the main\nstructure results to this orbifold category. We hope to come back to deeper\naspects later. The motivation is that the natural frame for the theory of\nclassification of compact K\\\" ahler (and complex projective) manifolds includes\nat least the category of orbifolds, as shown in [Ca01] by the fonctorial\ndecomposition of {\\it special} manifolds as tower of orbifolds with either\n$\\kappa_+=-\\infty$ or $\\kappa=0$, and also, seemingly, by the minimal model\nprogram, in which most proofs work only after the adjunction of a \"boundary\".\n  Also, fibrations enjoy in the bimeromorphic category of geometric orbifolds\nextension properties not satisfied in the category of varieties without\norbifold structure, permitting to express invariants of the total space from\nthose of the generic fibre and of the base. For example, the natural sequence\nof fundamental groups is exact there; also the total space is special if so are\nthe generic fibre and the base. This makes this category suitable to lift\nproperties from orbifolds having either $\\kappa_+=-\\infty$ or $\\kappa=0$ to\nthose which are special.\n",
        "Abida ateni is a species of air-breathing land snail, a terrestrial pulmonate gastropod mollusc in the family Chondrinidae.\n\nGeographic distribution\nA. ateni is endemic to France, where it occurs only at three locations in the Aspe valley, in the Western Pyrenees (Pyrénées-Atlantiques).\n\nEcology \nA. ateni is a rock-dwelling species of land snail. It lives on limestone in valley habitats at 300–400 m altitude.\n\nConservation \nA. ateni is listed as vulnerable (VU) in the IUCN Red List of Threatened Species. The species is protected under the French law; however, there is no specific conservation plan for it.\n\nSee also \nList of non-marine molluscs of Metropolitan France\n\nReferences\n\nChondrinidae\nEndemic molluscs of Metropolitan France\nGastropods described in 1973",
        " Medieval can be anything from 900 (kilts and loose gowns) to 1600 (neck high corsets and hoop skirts). If you show up in an Elizabethan gown and everyone else is sporting basic tunics and pants, it will look silly.;\n, Find out if it will be cheaper to make it (if you have the skills for that) or cheaper to buy it online.\n\n, Then you can buy in bulk, cheaper. Also, you'll be able to spot your family easily in the crowd despite strange clothing you're not used to.\n\n, A man who hates the notion of wearing tights will be embarrassed and uncomfortable for the whole wedding. A woman who would never display her cleavage shouldn't wear a corset.\n\n, Offer to put them in touch with anyone else going to the wedding - often for a large order, they'll let you get things at a lower rate.\n\n, If you mention to a vendor that another place is willing to make a bulk deal, you could get better rates - but do *not* lie to them. A lot of those people work together at the same fairs, and have good friendly relationships.\n\n, The more people dressed by the same company, the more classy the wedding will look - and the less you'll stick out.\n\n, Men: Don't tuck in your shirt, strap a belt across it. And wear the belt under your belly, if you have a belly. It just looks silly when someone tries to girdle in their gut with a leather strap. It should go where your pants belt goes. Ask someone who does medieval re-enactment or a friend who is a history buff to help you try on the clothing. Some things that look like they go on easily don't. Corsets lace in the back, belts wrap around you twice, and men look very silly if they tuck in their shirt, since the shirt has a billowy loose bottom.\n\n, Find out if the venue has somewhere you can change, or at worst, wear some of the costume and put on the parts you can't wear in the car (corset, belt, etc) once you get there.\n\n, One of the best parts of medieval clothing is that you can wear loose pants or skirts that hide your shoes entirely, and medieval clothing never, ever had high heels until well after the Victorian era. Do not, under any circumstances, wear a traditional gown with heels. You'll be very off balance, more likely to fall, and look ridiculous.\n\n, By the end of the day you might be relieved to put on a pair of jeans and a sweater. Being stuck in something you're not used to wearing might be frustrating or even embarrassing if you're pulled over or have to stop for food. People who wear that kind of clothing all the time for fairs or re-enactments are used to being looked at funny in the 7-Eleven, but it might be pretty humiliating if you aren't.\n\n, Otherwise, sell it on Etsy or eBay. You can make a decent penny back on your investment that way, and if you have a set of family costumes, you might even help another family going to a wedding!\n\n",
        ";\n, Open the drop-down list button next to the \"Browse\" link at the top of the page and click the \"Giveaways\" option near the top of the list., Do this to ensure that you can use this service properly, should you be the winner. This link is located in the upper right corner of the Giveaway page.\n\n, Goodreads states, at the top of the Giveaways page, that authors will tend to give away books to select members on a random basis (as sort of a raffle).\n\n, Goodreads has several books that they have giveaways for, but for those who don't read that much from certain authors, you probably won't find much of the genre you'd like to read.\n\n\nAlthough Goodreads will start you on the \"Ending soon\" page, they have other pages you can access by clicking the appropriate button.\n\n,, Oftentimes, this number will be one copy.\n\n,, The country information can be found underneath the number of copies available and amount of other people entering.\n\n\nObviously, US is denoting the United States of America, CA is Canada, and GB is Great Britain/Northern Ireland. These are just a few of the possibilities that this raffle could be entered from. Look on the next page for the remainder of the countries.\n\n,,,,\n\n,",
        "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability.\n\nOverall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs.\n\nThe experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case.\n\nOverall, the paper bears great potential. However, I do see some points.\n\n1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available.\n\nI want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here.\nZoneout does not seem to improve that much in the other tasks.\n\n2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K’ at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal’s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis.\n\n3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away.\n\n\nAn extreme amount of “tricks” is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2)\n\nConsequently, the paper reduces to a “epsilon improvement, great text, mediocre experimental evaluation, little theoretical insight”.\n",
        "Out of curiosity: did you compute MNIST results? I was just wondering, since the Ladder paper presented those as well.",
        "Averaged over the entire earth, not really. The energy that was converted to electricity doesn't disappear; it'll eventually be released, and almost all of it will end up as thermal energy again. For instance, maybe it turns into chemical potential energy in an electric car's battery, then into kinetic energy when the car accelerates, and then finally into heat in the air as the car does work against air resistance. \n\nUltimately it has to end up either as heat or as some form of potential energy, and since we have a finite capacity to store potential energy, eventually the energy input and the heat output balance out.\n\nIf anything, solar panels would very very slightly *raise* the global temperature, because solar panels are dark, and we'd be absorbing slightly more solar energy that would otherwise have been reflected into space. (Of course, I'd expect this to be overwhelmed by the fact that we'd be pumping less CO2 into the atmosphere.) It's also conceivable that a gigantic solar farm could alter climate patterns on a local basis.",
        "Modified from [an earlier answer of mine](_URL_0_) \n\n >  Enjoy the war, for the peace is going to be terrible- popular German joke in the last year of the war\n\nAs the fortunes of war turned against Germany after the Battle of Stalingrad, German propaganda found an imperative need to readjust to this new reality. Prior to the military reversals of 1942, German propaganda had operated on the principle of presenting an \"ersatz reality,\" wherein the state-dominated media maximized Germany's victories and ignored the salient truth that Germany's war was not a short one and her enemies persisted in fighting Germany. The scale of defeats like Stalingrad, the growing Allied bomber raids, and the surrender of German forces in North Africa pricked this media bubble and German propaganda organs responded accordingly. \n\nThis retooling of the Third Reich's propaganda apparatus in light of defeat pursued several seemingly counter-intuitive strategies. For one thing, despite the fact that the Third Reich was a personalist dictatorship *par excellence*, the figure of Hitler disappeared from German propaganda. In contrast to propaganda from the earlier years of victory, post-Stalingrad news of German military operations seldom invoked Hitler's name or connected him too heavily to military operations. This was part of a deliberate strategy on Goebbels's part as he recognized connecting Hitler too intimately to Germany's military fortunes made him, and by extension, the legitimacy of the entire regime, culpable when these operations did not bear fruit. Rather than present images of the Führer, Hitler was  invoked in late war propaganda as an abstract figure that stood for all Germans. This could just be from invoking his title, or oblique historical analogies such as films that made apparent the connection between Hitler and historical personages like Frederick the Great. Hitler, whose visage was omnipresent in state propaganda between 1933-1941, became an abstraction. By the same token, German propaganda also emphasized the severity and violence of German military setbacks, but with a unique spin. Allied bombing, the Soviet massacres of Polish officers at Katyn, and other actions of the Allies became staples of German propaganda after the tide had turned as it showed that Germany's enemies were merciless. The idea behind this emphasis upon the Allies' purported barbarity was to bind the Germans together through a policy of \"strength through terror.\" This dehumanization of the Allies' military underscored that no compromise was possible and this was a war in which there was to be no quarter given and none expected.  \n\nThese new strategies often dovetailed with established propaganda discourses that had been present within the Third Reich since 1933. The regime's castigation of the so-called \"November Criminals\" of 1918 also found new currency in this environment. Interrogations of German troops captured after 20 July 1944 often reported back that one key motivation for fighting on was to prevent a repeat of Germany's humiliating defeat at the end of the First World War.  One important component of the demonization of the Allied military was that German retribution was in the making. Since 1933, one of the central legitimizing planks of the NSDAP was that it had enabled German technology and genius to reach its full potential. The vaunted V-weapons tapped into this established narrative that German technical expertise brooked no rivals. But beyond rockets and other *Wunderwaffen*, National Socialism had always stressed the ability of the will to transcend any material obstacles. This propaganda's emphasis upon collective action in the face of numerical superiority fed into this notion that the will is superior to rational logic. Similarly, the destruction of German landmarks and the seemingly indiscriminate nature of Allied bombing heightened the sense that this was a cultural war and that the Germanic culture constantly trumpeted by the Third Reich was in existential danger. \n\nOne sinister aspect of the late war propaganda was its turn to a heightened antisemitism. Goebbels used the solidarity of Allied coalition of both the imperial Britain, hypercapitalist United States, and the Bolshevik USSR as evidence of grand global Jewish conspiracy against Germany. Victor Klemperer, a German Jew who by fortune escaped deportation and murder, would note in his diaries the increasingly shrill antisemitism in propaganda as Germany's fortunes waned. The widespread knowledge about the Holocaust amongst the German public imparted a weight to this propaganda that it might not have otherwise possessed. Although they might not have known the specifics of the Holocaust, most Germans were aware that something quite terrible had happened to the Jews in the East. Even though the antisemitism was troweled on so thick to strain credibility in this propaganda, it encouraged the expectation that the Allies would hold Germany collectively responsible  for the mass murder of the Jews. This does not mean that the German public accepted the NSDAP and Propaganda Ministry's antisemitism wholesale, but in some cases interpreted antisemitism quite differently than the state. One popular rumor among German civilians in 1943/4 was that Hungary had not been the target of any Allied  bombings was because the Hungarian government had spared its Jews. The SD recorded a number of complaints that because the Horthy government has ghettoized Jews in Budapest the Allies would not attack this human shield, and there was grumbling within the German populace that Hitler did not do the same for cities like Berlin or Hamburg. And some of this disgruntlement was not clandestine, but took the form of direct petitions to Goebbels. There were a string of letters to the Propaganda Ministry after the mass operations to clear Hungary's Jews in 1944 demanding that they be used as human hostages against Allied bombing. But the general acceptance of some of the antisemitism produced by Goebbels's machine precluded any thought or possibility of a negotiated peace for much of the German public. News of the Morgenthau Plan, which would have deindustrialized Germany, the expansion of Allied bombing, and the scale of German reverses fostered the expectation of a Carthaginian peace.   \n\nThe effectiveness of this late-war propaganda is open to interpretation. While it could not rekindle hope in final victory, it did strengthen the resolve of some Germans to see the war to its bitter conclusion. Yet, even as propaganda turned to negative integration (uniting around a threat), it could not arrest the gradual estrangement of much of the German public to the National Socialist state. Goebbels himself appreciated this sentiment and his famous  February 1943 *Sportpalast* speech had veiled threats against the \"Golden Pheasants\" of the NSDAP who were thus far still enjoying a prewar lifestyle.  This late-war propaganda often worked in conjunction with greater arbitrary state violence directed against Germans, especially after the 20 July plot. Extralegal state violence had been embedded in the DNA of the Third Reich since 1933, but outside of political enemies and German Jews, most Germans' interaction with arbitrary state violence was the *threat* of it until around Stalingrad, when the security services began a much more thorough crackdown against shirkers and potential fifth-columnists. The 20 July plot helped to further this turn towards extralegal violence and other forms of domestic terror. \n\nMost Germans would have been obviously aware that the war had been going poorly by 1943. Analysis of letters sent back home as well as SD reports show a drop in expectations of victory. There were simply too many salient reverses to ignore. The Allied bomber campaign against the Ruhr and the sound of flak would have been something impossible to ignore. The government itself publicized some of the Allied victories as an example that Germany's back was against the wall. Goebbels and other media arms would try and manage news of defeats such as framing retreats as tactical withdrawals. Other mass surrenders like Stalingrad and Tunisia were framed as modern day sacrifice in the vein of Leonidas and the 300 Spartans. This spin-doctoring of defeat became so ubiquitous that one grim homefront joke was \"what sound does a clock make when it goes forwards? *Tick-~~tock~~tack*. When it goes backwards? *Tak-tik*.\" By the the winter of 1944, the Red Army had begun to occupy German territory in the East (both in the Reich and those annexed in 1939) and Goebbels's propaganda ministry published lurid atrocity tales. Moreover, the advance of Anglo-American arms across France had eliminated what had been Hitler's greatest strategic victory in 1940. The retreat of German troops as well as the fall of the German city of Aachen in October 1944 would have been a reality that would have been difficult to ignore. \n\nThe mid- and late-war propaganda drive for mass action and a collective response to Allied aggression worked in often counterintuitive ways. While it stiffened resolve to not have a repeat of November 1918, propaganda along with the deteriorating war effort engendered a kind of grim fatalism for the future. Both rhetoric and reality heightened the sense of social anomie and the breakdown of society that came as bombing and wartime pressures destroyed the German infrastructure and stretched the civilian domestic economy well past its breaking point. The final agonies of the last few months of the war, as well as the violence meted out to Germans that shirked in their duties, helped to cement the postwar myth that Germans were double victims of the war- who were both subject to extreme violence from their military enemies, but also brutalized by a hypocritical criminal regime.",
        "This paper is relatively difficult to parse. Much of the exposition of the proposed algorithm could be better presented using pseudo-code describing the compute flow, or a diagram describing exactly how the updates take place. As it stands, I'm not sure I understand everything. I would also have liked to see exactly described what the various labels in Fig 1 correspond to (\"SGD task-wise, 1 comm\"? Did you mean layer-wise?).\nThere are a couple of major issues with the evaluation: first, no comparison is reported against baseline async methods such as using a parameter server. Second, using AlexNet as a benchmark is not informative at all. AlexNet looks very different from any SOTA image recognition model, and in particular it has many fewer layers, which is especially relevant to the discussion in 6.3. It also uses lots of fully-connected layers which affect the compute/communication ratios in ways that are not relevant to most interesting architectures today.\n",
        "The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.",
        "  In this talk we discuss at hand of two examples the crucial role played by\nself consistency in hadron physics. The first example concerns the quark-mass\ndependence of the baryon octet and decuplet masses. It is shown that within a\nself consistent one-loop approach based on the chiral Lagrangian the\n'mysterious' quark-mass dependence of the $\\Xi$ mass predicted by the MILC\ncollaboration may be recovered in terms of a discontinuous chiral\nextrapolation. This is a consequence of self consistency imposed on the partial\nsummation, i.e. the masses used in the loop functions are identical to those\nobtained from the baryon self energies. In the second example we discuss recent\nstudies on the properties of D mesons in cold nuclear matter as they are\npredicted by coupled-channel dynamics. Here a self consistent many-body\napproach reveals the close interlink of the properties of D meson and\nopen-charm baryon resonances in nuclear matter. The relevance of exotic baryon\nresonances for the spectral distortion of the $D_s^\\pm$ in nuclear matter is\npointed out.\n",
        "Roman Polačik (born 24 August 1963) is a Slovak water polo player. He competed in the men's tournament at the 1992 Summer Olympics.\n\nReferences\n\n1963 births\nLiving people\nSlovak male water polo players\nOlympic water polo players of Czechoslovakia\nWater polo players at the 1992 Summer Olympics\nPeople from Piešťany",
        "One of the most (in)famous verboten everyday topics of the Victorians was, of course, sex and sexuality, and the prohibition of discussion of those topics in public. This is something that holds true to the stereotypical 'victorian' imagery. I want to be very clear, however, and notes that this does not mean that Victorians themselves were prudes or particularly hypocritical about sex and sexuality. Those ideas and caricatures (hypocrisy, prudery, etc) are caricatures created by the later victorians (1870s+) of their parents, the early Victorians (early 1800s). The most famous of these, and indeed, probably the work that did more to create the idea of the 'Victorian' is Samuel Butler's *Way of All Flesh,* which is a semi-autobiographical inveighment against his own parents. '\n\n\nIndeed, with all of the problems and critiques of Foucault, he is definitely right when he remarked that the nineteenth and twentieth centuries are more united than divided by their great interest in sex and sexuality, something that doesn't feature as much of a concern, I would argue, in earlier centuries. Michael Mason notes that \n\n > It is correct to think that the later twentieth century, in the West, is unusual in its permissiveness about sex. By the same token, it is wrong to single out the Victorians as exponents of the opposite philosophy...Attacks on masturbation may seem absurd to twentieth-century Western readers, but we cannot take heard from the notion that only the demented Victorians thought otherwise; our culture is hopelessly outvoted on this question as on many others relating to human sexuality. Male and female masturbation, unconstrained liaisons for both sexes and all kinds of non-marital intercourse for women...[etc etc] have been deplored, repressed, and even punished in the majority of known human societies.\n\nMason goes on to argue that much of our stereotypes of Victorians as Victorians originate from their own reactions to eighteenth-century developments that tend to strike us as very progressive and positive, but to them were seen as negative. I am frequently reminded of Lawrence Stone's famous idea of 'the pendulum' in *Family, Sex and Marriage in England*:\n\n > Historical change is not a one-way street, and even in the West over the last five hundred years, continuous linear development has only occurred in the one field of technology. The trend towards the isolated nuclear family, greater personal autonomy, and emphasis on affective ties has not run a steady course from the sixteenth century to the twentieth. In terms of both sexual attitudes and power relationships, one can dimly begin to discern huge, mysterious, secular swings from repression to permissiveness and back again.\n\nSo yes, you would be frowned upon very strongly by the Victorians by trying to discuss sex and sexuality. Indeed, when one such person tried to do that in *The Maiden Tribute of Modern Babylon* he was roundly denounced for bringing pornography before the eyes of the public--even though he was trying for an expose on child prostitution. I will let /u/prehensilefoot elaborate a bit more here.",
        "I didn’t work with glue or caulking but I was a machine operator at a sex toy factory so we worked with the silicone or rubber material (still don’t know what it is.) It came in buckets, texture was similar to sand or mud. The sand materiel made “harder” toys and the mud material made more “jelly like” toys. The machines were set at certain temperatures to melt the material and molds would be filled. We would mix the material with the “rejects” as well so no material was wasted.\n\nBut to answer your question, last 30 minutes of work we would turn off the machines and have to basically unclog the machine by poking it with a metal stick or else it would dry up and get clogged up. \n\nHere’s 2 vids of the actual factory I use to work at\n_URL_1_\n_URL_0_\n\nEdit: should’ve mention that it was very important that the material was still hot before unclogging cause once it dried up it was a pain to unclog. The next morning we would just start the machine, set it at a high temperature, feed it and let it run for a bit to get rid of any dried excess material that was there the day before. Willing to bet money they do this with glue as well.",
        "In this paper, a referential game is proposed between two agents. Both agents observe two images. The first agent, called the sender, receive a binary target variable (t) and must send a symbol (message) to the second agent, called the receiver, such that this agent can recover the target. The agents both get a reward, if the receiver agent can predict the target. The paper proposes to parametrize the agents as neural networks - with pretrained representations of the images as feature vectors - and train them using REINFORCE. In this setting, it is shown that the agents converge to  optimal policies and that their learned communications (e.g. the symbolic code transmitted from the sender to the receiver) have some meaningful concepts. In addition to this, the paper presents experiments on a variant of the game grounded on different image classes. In this setting, the agents appear to learn even more meaningful concepts. Finally, multi-game setup is proposed, where the sender agent is alternating between playing the game before and playing a supervised learning task (classifying images). Not surprisingly, when anchored to the supervised learning task, the symbolic communications have even more meaningful concepts.\n\nLearning shared representations for communication in a multi-agent setup is an interesting research direction to explore. This is a much harder task compared to standard supervised learning or single-agent reinforcement learning tasks, which justifies starting with a relatively simple task. To the best of my knowledge, the approach of first learning communication between two agents and then grounding this communication in human language is novel. As the authors remark, this may be an alternative paradigm to standard sequence-to-sequence models which tend to focus on statistical properties of language rather than their functional aspects. I believe the contributions of the proposed task and framework, and the analysis and visualization of what the communicated tokens represent is a useful stepping stone for future work. For this reason, I think the paper should be accepted.\n\n\n\nOther comments:\n- How is the target (t) incorporated into the sender networks? Please clarify this.\n- Table 1 and Table 2 use percentage (%) values differently. In the first, percentages seem to be written in the interval [0, 100], and in the second in the interval [0, 1]. Please correct this. Perhaps related to this, in Table 1, the column \"obs-chance purity\" seems to have extremely small values. I assume this was mistake?\n- \"assest\" -> \"assess\"\n- \"usufal\" -> \"usual\"",
        "  Classical population genetics a priori assigns fitness to alleles without\nconsidering molecular or functional properties of proteins that these alleles\nencode. Here we study population dynamics in a model where fitness can be\ninferred from physical properties of proteins under a physiological assumption\nthat loss of stability of any protein encoded by an essential gene confers a\nlethal phenotype. Accumulation of mutations in organisms containing Gamma genes\ncan then be represented as diffusion within the Gamma dimensional hypercube\nwith adsorbing boundaries which are determined, in each dimension, by loss of a\nprotein stability and, at higher stability, by lack of protein sequences.\nSolving the diffusion equation whose parameters are derived from the data on\npoint mutations in proteins, we determine a universal distribution of protein\nstabilities, in agreement with existing data. The theory provides a fundamental\nrelation between mutation rate, maximal genome size and thermodynamic response\nof proteins to point mutations. It establishes a universal speed limit on rate\nof molecular evolution by predicting that populations go extinct (via lethal\nmutagenesis) when mutation rate exceeds approximately 6 mutations per essential\npart of genome per replication for mesophilic organisms and 1 to 2 mutations\nper genome per replication for thermophilic ones. Further, our results suggest\nthat in absence of error correction, modern RNA viruses and primordial genomes\nmust necessarily be very short. Several RNA viruses function close to the\nevolutionary speed limit while error correction mechanisms used by DNA viruses\nand non-mutant strains of bacteria featuring various genome lengths and\nmutation rates have brought these organisms universally about 1000 fold below\nthe natural speed limit.\n",
        " Don't get book bags that fall apart easily. Get a book bag that has enough zippers and compartments so that everything has a place, but not so many compartments that you're constantly looking for things. It's better to get a backpack if you will be carrying a lot of heavy books or binders, because the sling bags can ruin your shoulders. Recommendations for carry weight is 10% of your total weight.If your school allows it, it's a good idea to have a medium sized book bag or tote to carry around class to class also, though some schools don't allow any bags, many will say no backpacks but totes or book bags are fine. This really helps as you will not have to carry around a huge load of books, as most will fit in your bag.;\n, A key to having everything organized in middle school is having all the supplies you need, nice and clean and organized. Some schools give you out the supplies you'll need, but you can generally buy supplies cheap at stationary stores, especially at the Back-To-School times. Here are some purchases you should make:\n\n\nCheck each teacher's supply list to make sure you have everything you need. Have your parents or guardians get everything on the lists so you're not the last student to have supplies. At the very least, you should have notebook paper, pencils, and folders for each class. Remember, you will be changing classes and taking your things with you to your next class so there is no need to purchase multiple packs of things like colored pencils or highlighters.\n\n\n\n\n\n\nBuy, make, or reuse a pencil case. In the pencil case put things in like pencils, pens, erasers, sticky notes, etc.\n\n\n\n\n\n\nKeep your planner with you. With your planner, your whole homework schedule and after school activities can be organized. Write things in it like homework, events, meetings, etc.\n\n\n\n\n\n\n\n, Math papers don't belong in your English notebook or your pocket. Be consistent. Keep papers in their corresponding notebook at all times. It will make things so much easier. You won't have to be worried on the way home about which binder something is in (and you forgot one of your binders). Many schools or teachers will require you to have a binder for their class and even schedule \"notebook checks\" where they make sure that you have all your papers in the right places in your binder.\n\n, A sample schedule might be: Wake up and shower at 6:00, get dressed at 6:20, eat breakfast at 6:45, do your hair and makeup at 7:00, and leave at 7:15. It also helps to pack everything you can the night before, like putting all your homework and books in your bag, so all you have to add is your lunch. With the schedule, make sure you allow extra time in case something goes wrong (ex: you wake up late and don't have time to do something).When you have LOADS of homework: Take a sheet of paper write down all the subjects you have, then write the time you think you will need for each subject. Then try to put that time for homework in your weekly schedule.\n\n, You want to have backup supplies just in case you lose any of it, or you forget it somewhere. Make sure to have a couple of extra packs of paper and pens/pencils in your locker.\n\n, This really helps as you know where everything you're not using is. It is also a good idea if you plan to buy bulk packs of things, which is very cheap, but can be hard to keep track of. If you have a \"School Supply Drawer\", you will always have a place to put extras, and know where those things are when you eventually need them.\n\n, If you are in 8th grade, you probably won't need to buy lots of glue sticks and crayons, while a 5th grader won't need a graphing calculator. If you only buy what's on your supply list, and anything else you KNOW that you will use, you'll have a lot less junk to keep track of.\n\n, Labeling your notebooks and folders makes them easier to find. If you can make the folder and notebook for a class the same color, that helps too.\n\n,,,, All of your papers will also get crumpled and ripped, and you don't want to hand them in like that!\n\n, If you do not pay attention and one of your teachers calls on you, how will you know the right answer? You could use your study guide if you have one and know where it is.\n\n, Otherwise, do your best to divide it into several sections. Example: One for textbooks, one for notebooks, one for binder, and another for a backpack.\n\n\nKeep a magnetic pencil holder and a whiteboard on your locker door. The magnetic pencil holder is for spare pens, pencils, highlighters, erasers, and supplies. The whiteboard is where you will write your assignments, classes, and memos on.\n\n\n\n\n\n\n\n",
        "SUMMARY \nThis paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. \n\nPROS \nInteresting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. \n\nCONS \nThe paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). \n\nCOMMENTS \nIt would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. \nAlso, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. \n\nMINOR COMMENTS \n- Figure 1 could be referenced first in the text.  \n- ``Color coded'' where the color codes what? \n- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. \n- On page 5, mention how the orthogonal projection on S_k is realized in the network. \n- On page 6 ``divided into segments'' here `segments' is maybe not the best word. \n- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean?",
        " Before you begin on this project, set yourself up in a great workspace. Locate a space with lots of natural light or strong artificial lighting. Make sure your workspace has a large flat surface and enough room from all of the necessary materials and tools for this project., To create a knife sheath, you will need to collect the following materials:\n\n\n8 or 9 ounce leather\nGraph paper\nCardstock or cardboard\nMasking tape\nLeather dye\nWaxed thread\nNeatsfoot oil\n\n, To create a knife sheath, you will need to collect the following toos:\n\n\nPencil\nRuler\nScissors\nThin bladed utility knife\nCompass\nAwl (a sharp pointed tool used to puncture leather)Overstitch wheel (a wheel with sharp points used to mark even stitching holes)Drill and 1/16 inch bit\nWood rasp (a file used to shape and eliminate wood)Belt sander\nTwo needles\nSpray bottle filled with water\nContact cement glue\nDauber (a stick with a cotton ball attached to one end that is used to apply dye)\n220 grit sandpaper\nfine sandpaper\nClean rags\n\n, Create your custom sheath pattern for your knife on a piece of graph paper. Use a ruler to locate the center of the paper. Draw a line down the center of the page from top to bottom to divide the paper in half. This line represents the sheath’s fold., Place the knife on the prepared graph paper to the left of the centerline. Position the top of the knife blade approximately ⅛” from the centerline—the sharp side will face away from the centerline.\n\n\nUse a pencil to trace the sharp side of the knife. Begin at the centerline approximately ⅛” down from the tip of the blade. Following the blade’s curve, draw a line up and around the sharp side of the knife. Continue the line ⅔ of the way up the handle. Do not trace directly along the handle. Since the handle is thicker than the blade, extra material is required in order to enclose it.Measure and draw the welt. The welt is an additional piece of leather placed inside of the sheath. It runs along the sharp side of the blade to prevent the knife from cutting through the sheath. Create a second curved line that is 9/16” to ⅝” away from the first curved line.Design a sleek top line to connect the pattern’s curved outer-line to its straight centerline. Locate and mark a point on the centerline that is approximately 1 inch above the top of the curved outer-line. Connect this point with the top of the second curved line—this connecting line should gradually rises from the lower horizontal line up towards the top horizontal line.Create the belt loop. Shift the centerline to the left ¼ inch and extend it up from the top of the handle 4 ½ inches. Connect the original centerline to the shifted centerline with a slight curve. Draw a ¾ inch straight horizontal line from the shifted centerline to the left. Connect the sheath’s second curved line with the belt loop’s top horizontal line at a 45° angle., Fold the paper along the original centerline. Unfold the paper and cut out the left side of the pattern—make sure to stop at the centerline. Refold the pattern and carefully cut along edges of the sheath pattern—the right side of the sheath pattern will not include the belt loop pattern., When creating a knife sheath, it is ideal to use 8 to 9 ounce leather.\n\n\nLay the leather on your work surface so that the soft side is facing up.\nDetermine if you will wear your sheath on the right or the left. If you are right handed, you may wish to wear your belt sheath on the right. Flip the pattern over so that the belt loop is on the right side of the pattern. If you are left handed, you may wish to wear your belt sheath on the left. Flip the pattern so that the belt loop is on the left side of the pattern.Place your pattern onto the leather and secure the pattern in place with masking tape.\nUse a pencil to trace the pattern onto the leather—trace directly over the masking tape.\nUse a sharp thin bladed utility knife to cut out the pattern., The area between the two curved lines on the original pattern represents the welt.\n\n\nTo create a welt pattern, cut along the inner curved line of the original pattern. Continue cutting ¼ inch beyond the centerline.\nPlace the welt on the leather and secure it in place with masking tape.\nTrace the welt pattern onto the leather with a pencil and cut it out with the utility knife., In order for the welt to fit properly, it is necessary to slit the bottom of the sheath along the centerline. Punch a hole in the leather along the centerline 9/16” to ⅝” from the bottom. Use your utility knife to slit the leather along the centerline from the base of the circle to the bottom of the sheath., A stitch line allows you to keep your stitches straight. To create the stitch lines you will need a pencil and a ruler.\n\n\nUse a ruler to draw a straight horizontal line 1 ¾ inch from the top of the belt loop—begin and end the line ¼ inch from each side. This will be the bottom stitch line.\nPlace your pencil on the right end of the horizontal line. Moving your pencil towards the top of the belt loop, draw a 1 ½ inch straight vertical line. Repeat on the left side.\nConnect to the vertical lines with a straight horizontal line., To ensure even stitches, it is beneficial to mark the stitches before puncturing the leather.\n\n\nRun a damp rag over the stitch line to wet the leather slightly.\nUse an awl, a sharp pointed tool, to mark the first stitch in the lower left corner of the stitch line.Place one spoke of an overstitch wheel, a wheel with sharp points used to mark even stitching holes, in the hole you created with the awl.Rotate the overstitch wheel along the stitch line to create a series of evenly spaced dots along the stitch lines., Lay the sheath on a piece of wood—the wood prevents the drill or awl from puncturing your work surface. Individually puncture each hole with a drill bit or an awl by inserting the tool through the leather and into the wood.\n\n\nWhen using a drill, create the holes with a 1/16 inch drill bit., The belt loop attaches directly to the soft side of the leather. You will attach the belt loop with glue and stitches. In order for glue to adhere to the leather, you must roughen up the leather at the points of attachment.\n\n\nHold the belt 2 inches from the top of the leather strap and fold the belt loop ½ inch above its base towards the soft side of the leather.\nUse a pencil to trace the top 2 inches of the leather strap to indicate where the belt loop will attach to the sheath.\nRoughen up the attachment site with sandpaper, a utility knife, or a wood rasp. Using your tool of choice, carefully score the top 2 inches of the belt loop and the area inside the pencil markings on the sheath., Coat the attachment sites with a fine layer of contact cement glue. Put the sheath aside and allow the glue to cure. When it feels dry to the touch, you are ready to proceed. Dampen the leather at the fold of the belt loop with a spray bottle. Fold the belt loop and align the attachment sites. Press down on the attachment site firmly., Lay the unfolded sheath on top of a piece of wood. Use a drill or an awl to individually re-puncture the belt loop’s existing stitch holes. Make sure the drill or awl goes through both layers of leather and into the wood., To attach the belt loop to the sheath, you will need waxed thread and two needles.\n\n\nCut a length of waxed thread that is 5 times longer than the stitch line. Thread one needle on each end of the thread. To prevent the thread from slipping out, pull one inch of thread through the eye and bend it.\nInsert one needle (needle A) into the stitch hole in the top left corner. Pull the needle through the hole until you an even amount of thread on each side of the sheath. The other needle (needle B) will remain on the opposite side.\nInsert needle A through the next stitch hole (move to right) and pull it through. Needle A is now on the same side as needle B. Insert needle B into the same hole and pull it through. Needles A and B are now on opposite sides. Pull tightly on the thread to create a tight stitch. Repeat this process, until you return to the first stitch hole.Create 1 more forward stitch. Individually insert the needles into opposite sides of the first stitch hole and pull through.\nCreate 2 backwards stitches. Reverse the direction of the stitches. Individually insert the needles into opposite sides of the last stitch, followed by the second to last stitch hole.\nInsert the needle on the inside of the sheath (the flesh side) through the third stitch hole. Needles A and B are both on the outside of the sheath. Trim the ends close to the leather. Use a lighter to melt the threads in the the second and third to last stitch holes., Lay the sheath on a flat work space with the flesh side facing up. Align the welt along the right side of the sheath and trace the inner line with a pencil. Repeat on the left side of the sheath., Coat the area of the sheath between the edge and the pencil marks you just created with a thin, even layer of contact cement. Coat each side of the welt with a thin, even layer of contact cement as well. Set the pieces aside and allow the glue to cure. When it feels dry to the touch, you are ready to proceed., Place the top edge of the welt along the top edge of the sheath. Working from top to bottom, carefully position the welt along the edge of the sheath. When you reach the bottom, force the welt, which was intentionally cut long, through the slot., Precision is the key to folding the sheath and aligning the edges.\n\n\nDampen the leather. When leather is dry, it is difficult to mold and its risk of splitting increases. Use a spray bottle to dampen the edges of the sheath. Also, wet the sheath’s centerline. Wipe away excess water with a damp rag.\nFold the sheath along the centerline. Align the top corners of the sheath together. Clamp down on the edges with your fingers to press the corners together.\nAfter aligning a small section of the sheath’s edges, press down firmly with your fingers to secure it in place. Repeat until you reach the bottom., Cutting the welt to the correct size will provide a straight, clean look to the fold along the centerline. Retrieve a pair of sharp scissor or a utility knife. Cut the welt so that it is flush with the sheath., The stitch line will run down the center of the welt. This will help ensure that the welt remains in place. Set your compass to ¼ inch. Begin at the top of the sheath. Place the compass along the edge of the sheath and the pencil on the leather. Slide the compass down the edge. Continue around the bottom curve until you reach the centerline., A straight, even stitch line will give your sheath a professional finish. Run a damp rag over the stitch line to wet the leather slightly. Use an awl to mark the first stitch at the top of the stitch line. Place one of spokes of the overstitch wheel into the first hole you created with the awl. Rotate the overstitch wheel along the stitch line., To make marks left behind by the over-stitching wheel more visible, widen and deepen each mark with an awl.Place the sheath on top of a piece of wood. Use a 1/16 inch drill bit to individually puncture the stitch holes. Make sure the drill or awl goes through both layers of leather and into the wood., The main seam is long and thick—the needle must travel through three layers of leather. Cut your thread extra long and have a pair of needle nose pliers on hand to pull the needles through the holes.\n\n\nCut a length of waxed thread 6 to 7 times longer than the stitch line. Thread each end of the waxed thread through separate needles.\nInsert one needle (needle A) into the stitch hole at the top of the sheath. Pull the needle through the hole until you there is an even amount of thread on each side of the sheath. The other needle (needle B) will remain on the opposite side.\nInsert needle A through the next stitch hole and pull it through. Needle A is now on the same side as needle B. Insert needle B into the same hole and pull it through. Needles A and B are now on opposite sides. Pull on the thread to create a tight stitch. Repeat this process, until you reach the last stitch hole.Backstitch. Reverse the direction of the stitches. Individually insert the needles into opposite sides of the last stitch, followed by the second and then third to last stitch holes. Insert one needle through the fourth stitch hole so that both needles are on the same side of the sheath.\nTrim the ends close to the leather.\nUse a lighter to melt the threads in the the second and third to last stitch holes., To ensure that your knife fits securely in the sheath, you must shape, or form, the leather to your knife.\n\n\nDampen the leather. In order to form the leather to the shape of your knife, it must be damp. Use a spray bottle to wet both sides of the sheath. Allow the water to soak into the leather. Continue to wet the leather until it is easy to mold. Wipe away any excess water with a damp cloth.Form the leather around your knife. Insert your knife into the damp sheath—make sure the tip of the blade reaches the bottom. Use your fingers to press the leather around the blade and handle. Pull the knife out and reinsert it 5 to 6 times to ensure that it is fitting properly. Once the leather is formed, remove the knife and hang up your sheath to dry., After cutting out the sheath from the leather, you have the option to dye the knife holder.\n\n\nPrepare your workspace. Cover your work space with 2 to 3 layers of newspaper. Lay the sheath on top of the newspaper—make sure the soft side of the leather is facing up. Put on a pair of disposable rubber gloves.Apply an even coat of dye to the soft side. Pour your dye into a small disposable cup. Dip a dauber—a stick with a ball of fluff on the end—into the dye. Carefully tap off any excess dye before applying a thin, even coat of dye to the leather. Re-saturate the dauber as needed. As the dye dries, use a dry rag to polish the leather. This will remove any excess dye and residue., Flip the sheath over so that the rough flesh side is facing up. When applying the dye to the flesh side, take care to leave a 9/16” to ⅝” strip of undyed leather around the edge of the sheath. You will attach the welt to this area with glue.As the dye dries, polish the leather with a dry rag., After hand stitching the sheath together, you have the option to sand and dye the edges.\n\n\nUse a spray bottle to mist the edges of the sheath with water. Wipe down the edge with a damp cloth.Shape the edges with a belt sander or wood rasp. Then use a belt sander to even out the three layers of leather with a belt sander and an 80 grit belt. Once the layers are even, use a 220 grit belt to smooth the edge.Touch up any rough spots with a piece of fine sandpaper.Dye the edge to match the sheath. Cover your work space with 2 to 3 layers of newspaper and put on a pair of disposable rubber gloves.Pour your dye into a small disposable cup. Dip a dauber into the dye. Apply a thin, even coat of dye to the leather with the dauber.Re-saturate the dauber as needed. While the dye dries, remove any excess dye and residue with a dry rag., To increase the lifetime of your sheath, polish and waterproof the leather. Shine up your leather sheath by rubbing it with a dry, clean cloth. Pour neatsfoot oil into a shallow basin. The oil seals the leather and protects it from water damage. Submerge your leather sheath into the oil and then hang it up to dry.",
        "Very good paper, I hope it will be accepted. I keep my original evaluation.",
        "The paper proposes an interesting approach, in that (unlike many second-order methods) SR1 updates can potentially take advantage of negative curvature in the Hessian. However, all reviewers had some significant concerns about the utility of the method. In particular, reviewers were concerned that the method does not show a significant gain over the Adam algorithm (which is simpler/cheaper and easier to implement). The public reviewer also points out that there are many existing quasi-Newton methods designed for DL, so it is up to the authors to compared to at least one of these. For these reasons I'm recommending rejection at this time.",
        "My english is not the best but i'll try to provide some elements ... because i've read atrocious things \n\n\n\nSo why would the french do that ?\n\n 2 reasons : - To administer  &   exploit its new possession. \n\n\n\nSince the political situation in France was not very stable ( 2nd Republic, 2nd Empire, 3rd Republic... ) and french politicians were aware of that, the 1848 Constitution stated :\n\nArt.109 :  \"Algeria has been declared a french territory - and will be ruled by decrees - until a special bill is voted in order to place it under the current Constitution.\"\n\nThis special bill has never been voted.\n\n One of the main objective was to perpetuate  the colonization process despite the political situation / troubles in the French Metropole. It was also an experiment on what was at the time the largest colony of France, \"an administrative experiment\" , which if sucessful, could have been imitated in all of the colonies. By exporting French legal system in Algeria, France could now fully exploit/control its territory and protect the huge population of French migrants who emigrated to Algeria ( and collect taxes ! ) : land spoliation was helped by decrees in favour of europeans, coercive mesures against natives, etc..\n\nNow think of Algeria at the time as what we see today in Afghanistan. There was absolutely no \"unity\", Algeria was composed of hundreds of different Ethnic groups, some of them ennemies since the dawn of time. One could argue that France \"created\" Algeria, because the idea of Algeria as a nation emerged during the 50\"s with the Jabhat at-Tahrīr al-Waţanī group... Anyway, those Tribal leaders would give the french a hard time , attacking the french convoys, controling some areas/villages, they would collect taxes, etc... By turning Algeria into a departement, France tried \"to break-up\" the tribe system, they would reorganize the territory and its population, turning the tribes into \"*[douars](_URL_1_)*. A *douar* was an  \" artificial \" village created by the french, in which they would regroup people from different tribes/areas to bring them some \" enlightenment\" ( or freedom as our american friends would call it). On this matter , /u/DerProfessor explained very well the ideological differences between French/English in the colonization process.\n\nAnother reason to turn Algeria into a french territory was because of the fierce competition between the military power and the civilian authority at the time.\nUnder  Military governorship , the economy was a complete disaster ( big crisis in 1866-1870 ). \n\nMilitary governorship was not working at all, that's when the Emperor decided to change the administration  for a civilian authority - which proved to be much harsh towards natives- . \nAlgeria was now composed of 3 \"departements\", under the rule of a \"civilian governor\", Algerian jews were declared french citizens ( because $$$ / jewish communautarism ), and the creation of a \" special native code\". \nSo yeah, even though Algeria was technically part of France, its people were never treated as french citizens ( I will need to check but I think they only got the right to vote after WWII in 1947 ).\n\n\n\n\nI will add further information later on the rise of imperialism at the time, the critics of colonialism in France  , etc.... I'm late for work :)\n\n\n\nSource : This is a very very very rough summary of a +200pages chapter in this book : \" La France de 1848 à nos jours\", M. Agulhon.  \n\nfurther reading : Tocqueville wanted Algeria to become for France what the americans colonies had become for  England\n_URL_0_",
        " This is very important. Keep a neutral exterior at first, in prefer to prevent people from prejudging you.;\n, Have good body hygiene, be fresh, and stay clean. Nobody will want to talk to you after a whiff of body odor. Brush your teeth at least twice a day, wash your hair everyday and take a shower at least four times in a week.\n\n\nShower regularly. Take a shower every one or two days, depending on the weather and the level of activity you've undertaken. Always shower after a heavy workout, though.\n\nSome girls have to wash their hair everyday because it's naturally greasy, and some girls only every other day. Find out what's right for you.\nTuck your hair up under a shower cap. Do so if you wish to take a shower but feel your hair doesn't need a wash.\n\n\nSmell sweet. Use scented hair or body products in the shower, and use some body lotion in order to keep your skin soft. Spray yourself lightly with perfume or body mists. You may wish to buy coordinated scents to maximize the effect. Whilst a faint smell is lovely, beware of overdoing it, especially at school. Less is more.\n\nNo amount of 'smellies' can successfully mask the smell of sweat. Use a good antiperspirant to prevent sweating. You may wish to top off with a deodorant which will mask the smell of your sweat.\n\n\n\n, If you wear a uniform, you may not need to wash your jumper, trousers, or skirt more than once a week. However, you should remember to change your shirt everyday or every other day.\n\n, Get rid of acne or pimples if you have any. Use a daily facial cleanser to reduce or prevent acne. Choose one which complements your natural skin type: normal, dry, sensitive, oily or combined. Follow up with a moisturizer as facial cleansers tend to be drying.\n\n\nExperiment with different products to find what best suits you. Talk to a pharmacist about the products on offer. However, give each product a fortnight or so before you expect to see any real effect.\nIf you find that you have a lot of problems with spots, visit your doctor. They may prescribe certain medications to help clear it up.\n\n, Use light, natural tones for school. In addition, save the bright colors for parties.\n\n\nUse a close to your complexion foundation or for a lighter feel, a tinted moisturizer. Apply concealer under the eyes and on pimples. Pat it on, and don't rub it in. Afterwards, lightly dab on face powder to cover your face, and blend in your foundation with your concealer.\nRemember to remove all makeup every night. Otherwise, it will clog your pores and cause acne.\n\n, This showcases your lovely eyes and suited for your eye color. Choose what works for you best. If you prefer only to wear lip gloss and concealer, then do that.\n\n\nIf you have blue eyes, there are several shadow shades that will work for you. Shades in the brown, light pink and violet families work best generally. Also, try rich browns, golds, warm taupe, soft peaches, and pretty lilacs.\n\nFor eyeliner, go with a brown shade or dark brown to keep it subtle and soft. You want to look as if you woke up that way. With mascara, use brown or brown/black mascara, or even clear mascara to look the most natural.\n\n\n\"Decorate\" your green eyes. Violet and plum eye shadow shades are best for green eyes. Warmer colors like deep purples, mauve, lilacs, and medium pinks are also good choices. Browns, bronzes, golds, coppers, and deep greens will also compliment your eye color.\n\nFor eyeliner, try a dark brown, hunter green, or plum shade. Black eyeliner can often be too overwhelming. However, if you think it can look nice, then use it. Your mascara should be brownish black too. Use black mascara sparingly.\n\n\nComplement your brown eyes. Brown eyes are some of the easiest to complement. This eye color can get away with greens, golds, browns, pinks, blues, grays and purples. Earthy colors tend to look the best for neutral looks.\n\nBlack eyeliner will work nicely with brown eyes. Use some black mascara to open and brighten the eyes.\n\n\nComplement hazel eyes. Shades of pale pink, light purple and baby blush look great with hazel eyes. Use some shimmer on your lids to bring out the twinkle in your eyes. Hazel eyes stand out with deep greens, lavenders and shades with soft yellow undertones. To bring out the gold in your eyes, choose golden based eye shadows.\n\nTo bring out the green specks in your eyes, choose a brighter green shade like emerald and apply in three different intensities.\nTo bring out the blue specks in your eyes, go with purple in three different shades. To highlight the brown base of your hazel eyes stick with natural shades of brown. This will give you that \"do-like\" appearance.\nTo bring out the green and blue specks with a more natural look, go with soft natural shades paired with dark green or blue liner and black mascara.\n\n\n\n, This way, you won't need to lick them all day. Exfoliate your lips once every 2 weeks with a homemade scrub or one bought from the store.\n\n\nUse a medicated lip balm or chap-stick of your choice. Do this in order to keep them smooth and supple. Apply your favorite lip gloss/lip tint on top, in order to get sheer and shiny lips. Lip glosses come in many sweet flavors. Do not lick them off!\n\n, If you wish to remove hair from your arms or legs, try shaving, waxing or using a hair removal cream.\n\n\nTry shaving if you haven't already. Use a shaving cream with a razor to remove hair. Buy a woman's razor to avoid nicking yourself. Sensitive razors are available for shavings your underarms and bikini area. If you are afraid of cutting yourself, try a depilatory cream or lotion.\n\n, Waxing is the longest term solution available to most teenage girls. If you wish to start waxing, find a reputable practitioner or try it yourself at home. Follow the instructions carefully to ensure the best experience and results.\n\n, They cause pimples and bumps.\n\n, Any stray hairs should be plucked away. Straight eyebrows tend to look more youthful, arched eyebrows look more mature and grown up.\n\n\nIf your eyebrows are thin and undefined, use an eyebrow pencil, or matte eyeshadow. Use it close to your eyebrow color to color them in. Use a pencil/shadow at least 2 shades lighter then your hair. Never use black; it looks far too harsh, even on black eyebrows.\n\n, Dry hair is more prone to damage and breakage.\n\n\nUse a high quality shampoo and conditioner, then apply your styling products.\nIf your hair gets very oily and you feel like you have to shampoo your hair everyday, invest in a dry shampoo from a local beauty supply store. If your hair is more textured (curly or wavy) shampoo your hair once to twice a week, and use a conditioner on your no-shampoo days. Every hair type deserves personalized care; curly hair has different needs to straight or wavy hair.\nIf you want to add something interesting, try coloring your hair; do not use anything too bold or different. Don't go from black to platinum blonde, you may be labeled as unoriginal or fake.\n\nDo soft, subtle shades about one to two shades lighter for a complete dye. For highlights or a soft ombre, you can temporarily try different colors of your choice. Nothing too out of the ordinary, especially at private schools.\n\n\n\n, Once in a while heat manipulation isn't too bad.\n\n\nAlways use a heat protestant to avoid damaging your hair too much. If you prefer using no heat, there are tons of heat-less ways to differently style your hair on YouTube. You can even get new hairstyle tutorials to try.\n\nFor example, use a french braid, fishtail, waterfall, heat-less waves, creative ponytails and buns, etc. Style it differently everyday. A little accessory makes a difference.\n\n\n\n, Try curling your hair sometimes for loose waves, or scrunching it for a wet beach look. You can also straighten it for a sleek look, and try different ponytail and bun styles with varied headbands.\n\n, If your new school allows you to wear whatever you please, make it your own. Wear plenty of cute tops with skirts and jeans. Simple cardigans, sweaters, and jackets can dress up or down an outfit.\n\n, Never pick style over comfort. Cute wedges, ballerina flats, sandals (if your school allows it), and sneakers (if you're the sporty type) can always make your outfit look great.\n\n\nWear what your school allows. If you have a skirt that you feel is a bit too long, Roll it up a bit, but no more than two to four inches above the knee. The school will know and the people will confront you in front of the whole class. How embarrassing!\nTry untucking your school shirt, and unbuttoning the first two buttons, which is usually above your cleavage. But, if you can't do anything to alter the uniform due to strict teachers, leave the uniform alone.\n\n, Try jewelry, a pretty headband, and a cute tote bag. Wear the expected socks or shoes. Most private schools have a designated color, such as brown or black.\n\n, For black shoes, wear ballerina flats, low heels, Toms, Sperrys, or Converses/Vans. Many of those shoes come in different color options.\n\n, If you aren't allowed to wear nail polish, just put on a clear polish. If you are then experiment, and try fun and funky colors ranging from red to white, green to blue. Try anything! If you want to look simple, and a bit more matured, you can also try a pretty french manicure on your natural nails or acrylic/gel.\n\n, The nails can snag, and make your natural nail brittle and weak. Weigh out the pros and cons.\n\n, At every class, try to sit and talk to someone new, don't be rude to anyone it can come back and nip you in the bottom.\n\n\nOn the first week of school, just tag along. Don't try to be friends with anyone just yet. Find a person to help you adapt to class. She will be a good friend to you, so don't stop talking to her when you get popular.\n\n, That's the easiest way to become popular, or if they are mean. You don't want to turn into that then stay with the kids you met in classes.\n\n\nIf you gain more friends and are confident enough, try joining the school council, or even president. Don't be fake and try to like everything they like, you still want to be yourself right? And what fun would it be anyway if you weren't yourself?\n\n, Try Instagram, Twitter, Facebook, Tumblr, etc. You can add friends on those websites. Try hanging out in places they would normally be at. ex: Starbucks/Caribou, school gym, mall, etc. Ask to sit with them, Be prepared for a solid \"No\". They might not exactly trust you yet. If they say \"yes\" then your right on track.\n\n\nSoak up their trends, gossip, and everything else. Do not distribute it, though.\n\nThey might not even care for your opinion right now, as they might not even consider you a friend.\nYou can get into conflicts with others for gossiping about them.\nIf you tell a secret of theirs to someone else they won't trust you anymore. You'll be at square one again, so don't get too overconfident and forget who you are.\n\n\n\n, You'll know when you did everything correctly when they have a party/sleepover that they invite you to, That means they like you. They won't just invite anyone to their house, right? If they have any plans and choose to invite you, that will seal the deal.\n\n\nIf you aren't invited, don't be too disappointed, maybe you'll have to initiate interactions. You can invite her (leader of clique), and her only to your house. She might accept.\nKeep the conversations light and flowing. Do fun things like prank call people, watch movies, bake things, do nails, makeup, facials, and hair. Don't feel restricted to only be at your place. If your old enough you could plan it on a night that has a party, you could get ready at your place, (Hair, makeup, cute outfits) and have a blast at the party. Afterwards, you could come home and do something else interesting.\nThe next time you can invite the others, but if you feel like you have to always initiate conversations and she never tries to talk to you either, then maybe it isn't even worth it. She may also be hard to convince or trust people as a close friend.\nDon't share your beliefs and deep secrets yet, focus a little on her for now so she can build your trust as well as you, so you can be certain she doesn't distribute your secrets and beliefs either.\n\n, To have a clique find a few people you feel the most connection with and make a group with them.\n\n, Be yourself, and honest. Don't lie or brag, Nobody likes a liar!\n\n",
        "Attempts to use chatbots for every form of human-computer interaction has been a major trend in 2016, with claims that they could solve many forms of dialogs beyond simple chit-chat. This paper represents a serious reality check. While it is mostly relevant for Dialog/Natural Language venues (to educate software engineer about the limitations of current chatbots), it can also be published at Machine Learning venues (to educate researchers about the need for more realistic validation of ML applied to dialogs), so I would consider this work of  high significance.\n\nTwo important conjectures are underlying this paper and likely to open to more research. While they are not in writing, Antoine Bordes clearly stated them during a NIPS workshop presentation that covered this work. Considering the metrics chosen in this paper:\n1)\tThe performance of end2end ML approaches is still insufficient for goal oriented dialogs.\n2)\tWhen comparing algorithms, relative performance on synthetic data is a good predictor of performance on natural data. This would be quite a departure from previous observations, but the authors made a strong effort to match the synthetic and natural conditions.\n\nWhile its original algorithmic contribution consists in one rather simple addition to memory networks (match type), it is the first time these are deployed and tested on a goal-oriented dialog, and the experimental protocol is excellent. The overall paper clarity is excellent and accessible to a readership beyond ML and dialog researchers. I was in particular impressed by how the short appendix on memory networks summarized them so well, followed by the tables that explained the influence of the number of hops.\n\nWhile this paper represents the state-of-the-art in the exploration of more rigorous metrics for dialog modeling, it also reminds us how brittle and somewhat arbitrary these remain. Note this is more a recommendation for future research than  for revision.\n\nFirst they use the per-response accuracy (basically the next utterance classification among a fixed list of responses). Looking at table 3 clearly shows how absurd this can be in practice: all that matters is a correct API call and a reasonably short dialog, though this would only give us a 1/7 accuracy, as the 6 bot responses needed to reach the API call also have to be exact.\n\nWould the per-dialog accuracy, where all responses must be correct, be better? Table 2 shows how sensitive it is to the experimental protocol. I was initially puzzled that the accuracy for subtask T3 (0.0) was much lower that the accuracy for the full dialog T5 (19.7), until the authors pointed me to the tasks definitions (3.1.1) where T3 requires displaying 3 options while T5 only requires displaying one.\n\nFor the concierge data, what would happen if ‘correct’ meant being the best, not among the 5-best? \n\nWhile I cannot fault the authors for using standard dialog metrics, and coming up with new ones that are actually too pessimistic, I can think of one way to represent dialogs that could result in more meaningful metrics in goal oriented dialogs. Suppose I sell Virtual Assistants as a service, being paid upon successful completion of a dialog. What is the metric that would maximize my revenue? In this restaurant problem, the loss would probably be some weighted sum of the number of errors in the API call, the number of turns to reach that API call and the number of rejected options by the user. However, such as loss cannot be measured on canned dialogs and would either require a real human user or an realistic simulator\n\nAnother issue closely related to representation learning that this paper fails to address or explain properly is what happens if the vocabulary used by the user does not match exactly the vocabulary in the knowledge base. In particular, for the match type algorithm to code ‘Indian’ as ‘type of cuisine’, this word would have to occur exactly in the KB. I can imagine situations where the KB uses some obfuscated terminology, and we would like ML to learn the associations rather than humans to hand-describe them.\n",
        "  Optical/near-infrared (optical/NIR; OIR) light from low-mass neutron star\nX-ray binaries (NSXBs) in outburst is traditionally thought to be thermal\nemission from the accretion disc. Here we present a comprehensive collection of\nquasi-simultaneous OIR and X-ray data from 19 low-magnetic field NSXBs,\nincluding new observations of three sources: 4U 0614+09, LMC X-2 and GX 349+2.\nThe average radio-OIR spectrum for NSXBs is alpha ~ +0.2 (where L_nu propto\nnu^alpha) at least at high luminosities when the radio jet is detected. This is\ncomparable to, but slightly more inverted than the alpha ~ 0.0 found for black\nhole X-ray binaries. The OIR spectra and relations between OIR and X-ray fluxes\nare compared to those expected if the OIR emission is dominated by thermal\nemission from an X-ray or viscously heated disc, or synchrotron emission from\nthe inner regions of the jets. We find that thermal emission due to X-ray\nreprocessing can explain all the data except at high luminosities for some\nNSXBs, namely the atolls and millisecond X-ray pulsars (MSXPs). Optically thin\nsynchrotron emission from the jets (with an observed OIR spectral index of\nalpha_thin < 0) dominate the NIR light above L_x ~ 10^36 erg/s and the optical\nabove L_x ~ 10^37 erg/s in these systems. For NSXB Z-sources, the OIR\nobservations can be explained by X-ray reprocessing alone, although synchrotron\nemission may make a low level contribution to the NIR, and could dominate the\nOIR in one or two cases.\n",
        "Gynaecology was largely used a method of controlling women's bodies, and participated in wider assumptions about gender. This began with classical scholars such as Aristotle, Galen and Hippocrates, who argued that females were fundamentally different in terms of structure than men, and a departure from the norm. By around the thirteenth century, this works had been translated into Latin and introduced to the learned elite. Because these scholars were so appreciated by humanists, a lot of their medical writing went unchallenged and so this filtered into medieval and Renaissance thought.\n\nDuring the fifteenth, sixteenth and seventeenth centuries there was heightened interest in anatomy and dissection, and for those interested in studying female anatomy the central focus of this was the womb, largely because of the primogeniture system of society. Paternity was fragile, and female fidelity was crucial to this, so medicine began to be used in a way to justify women's roles as mothers above all else.\n\nAlthough human dissection was rare (particularly of women) the most ludicrous theories about female anatomy were dispelled in this way, but beliefs about the womb were simply adapted to fit new evidence, rather than dismiss them completely. The entire idea around analysing women was to understand how exactly women were these imperfect versions of men, and uncover the \"secrets of women\", a popular phrase used then to refer to their sexual reproductive organs. The De Secretis Mulierum argued that the womb cured ailments, and in turn men could expel sicknesses in the womb. Another writer at the time, Joubert, argued that the womb's temperature was important - baby girls needed to be kept warmer than baby boys as they needed longer to reach maturity. These texts were therefore being used as tools to justify femininity as weak whilst claiming 'real' medical evidence to back it up, in an attempt to gain some control back over the uncertainty of paternity. These medical texts emphasised male strength in sperm, whilst the woman's womb was weak, bolstering the importance of the father and diminishing the mother's influence.\n\nIt was entirely the womb that dominated female anatomy - showing the male almost obsession with it. This was because in their eyes, the womb was a woman's only purpose: being a mother. The other side to this was menstruation, which was seen as punishment from Eve's downfall, and a way of punishing women for their sexual desires. Monstrous birth stories in the sixteenth century were always blamed on a woman being too sexual. However, it was acknowledged to be essential and many medical texts offered remedies to provoke it rather than suppress it because it was a sign of fertility; women were just expected to feel guilty about it. It was also used as a way to justify women not be suitable for learning, because it was said to deaden their brain, and instead it was suited for motherhood because in being with a man they receive heat and this warms up their \"frigid natures\"! \n\nPregnancy obviously also dominated gynaecological thought. Bastard children were blamed solely on mothers and their sexual depravity, and it was believed that women controlled which gender a child was. The ideal case was a son who resembled their father as this meant the female aspect was completely sacrificed.\n\nIn conceiving a child a woman could not show herself to be sexual, because it was believed that a woman who had lots of sexual itnercourse produced weak foetuses, or none at all. Their looks were judged upon also and only one person (Savonarola) acknowledged the idea that a woman should enjoy the sex to conceive, advising rubbing her clitoris and caressing her nipples! Although some others believed women orgasming was necessary, it wasn't so that she could enjoy it, but more to speed the process up.\n\nOverall, women were viewed as sexual, destructive and dangerous people who needed to be controlled, and the field of gynaecology contributed to this by having a medical justification and a biological basis to societal concerns. They were able to rationalise a lot of thought at the time, and through the development of this field, established a male authority on \"women's secrets\" that took over from wise women and midwives. There was an overall distrust of female control of sexuality and motherhood was the way to control this by spouting ideas that it helped their health, and they were inherently weak and depended upon men.\n\nGynaecology was a way of asserting power over women,\nand sex differences in general were used to define and shape society. Gender roles were a powerful tool for submission, and it was this representation of all that a “Woman” should be, through the trusted authority of medicine, that allowed them to separate men and women further.\n\n\n\n\nReferences (and recommended reading if you're interested in this topic):\n\nMaryanne Cline Horowitz, 'Aristotle and Woman' Journal of the History of Biology, vol.9, no.2 (1976)\nLesley Ann Dean-Jones, Women's Bodies in Classical Greek Science (Oxford, 1996)\n\nIan Maclean, The Renaissance Notion of Woman: A Study in the Fortunes of Scholasticism and Medical Science in Europe (Cambridge, 1983)\n\nCadden, Joan, Meanings of Sex Difference in the Middle Ages: Medicine, Science, and\nCulture (Cambridge, 1993)\n\nCanning, Kathleen, Gender History in Practice: Historical Perspectives on Bodies, Class,\nand Citizenship (Ithaca, 2006)\n\nGreen, Monica, ‘Gendering the History of Women’s Healthcare’, Gender and History,\nvol.20, no.3 (2008), pp.487-518.\n\nGreen, Monica, Making Women’s Medicine Masculine: The Rise of Male Authority in Pre-\nModern Gynaecology (Oxford, 2008)\n\nPark, Katharine, ‘Dissecting the Female Body: From Women’s Secrets to the Secrets of\nNature’, in Seeff, Adele, and Donawerth, Jane, (eds.), Attending to Early Modern Women\n(London, 2000), pp.29-47.\n\nPark, Katharine, Secrets of women: gender, generation, and the origins of human dissection\n(New York, 2006)\n\nI did my dissertation on this exact topic, so feel free to PM me if you have any other questions :)",
        "My apologies if this is not a sufficient answer, first time commenter here, hopefully this can be a placeholder for someone more knowledgeable on the topic :)\n\nHepatitis was a major issue for **both** Axis and Allied troops, and we were largely unprepared for it. Although we cannot definitively say if it was or was not a deciding factor in military victory or loss in the African theatre, there are some clues we can pick up on to make an educated guess based off the number of infected prisoners of war that the Allies captured and the incidence rate amongst Allied troops in that locality (bolded text below). For American troops, the hepatitis epidemic *started* in North Africa, while for the British it started in the Middle East. The German population, however, was suffering from it for a longer period of time: in Germany itself there were *one to two million cases* in 1941, with estimates of 60% of German units being infected.^3 \n\nMy background is clinical research in hepatology (study of the liver), specifically treatment of Hepatitis C. Some basic information, so others may know, is that Hepatitis A thrives in areas of poor sanitation, typically involving water and feces which would, logically, be major factors to consider for mass military operations. Symptoms are pretty much the same between Hepatitis A, B, and C, where you will see flu like symptoms: fever, fatigue, nausea/vomiting, body aches, and later jaundice due to the beginnings of liver dysfunction. Although war will give you that adrenaline rush, if your body is decompensated enough you will be operating at a much lower level of performance - sure, hepatitis didn't kill you (very low mortality rate of 0.2%), but it put you out of action for long periods of time.\n\n > Relatively small numbers of U.S. troops went into Egypt in the early autumn before the subsequent major landings in North Africa in November 1942. It became known very shortly that French, British Commonwealth, and Axis troops had had a bitter experience with epidemic hepatitis at various times during the period from 1939 to 1942 in the North African desert. The French stationed large bodies of troops in southern Tunisia during the spring and summer of 1939 to handle any threat the Italians might make from Libya. Hepatitis appeared among them in September 1939, reaching its peak in December, and declining in January 1940.^1 Jaundice first developed among the German troops near Sirte and Benghazi during the fall of 1940, rapidly became a serious problem among them, and later spread to the Italian ground troops and air force. Thousands of cases occurred but the Germans appeared more vulnerable than the Italians, with a ratio of three to one cases. **In May 1943, the closing of the Tunisian campaign revealed that large numbers of German and Italian prisoners were jaundiced.**^2 Australian troops were involved late in 1941 while attempting to hold Tobruk, and in the following year they, the New Zealanders, and the British Eighth Army were caught up in a great epidemic during and after the campaign for El Alamein.\n\n > In 1942, the epidemic was well on its way in August and reached a peak in November. In U.S. troops in Egypt during that autumn and winter, the disease first occurred in the fliers and ground troops of the Ninth Air Force, as these men operated between Alexandria, Benghazi, and Tobruk. **Despite the fact the U.S. troops in Egypt had an appreciable amount of hepatitis among the small numbers of men there, the incidence among U.S. forces in North Africa (Algeria and Tunisia) was low during the autumn and winter of 1942-43 and remained so into the summer of 1943.**\n\n > This security was not to last for long, however, and in the late summer of 1943 began the volcanic experience that was to erupt into the most expensive and devastating problem that we were to encounter in relation to infectious hepatitis. It came to be, during 1943-45, the most important cause of man-days lost due to illness among U.S. forces in the Mediterranean-North African theater. As a cause of death due to medical disease it ranked high, although the case fatality was low (about 1.8 per 1,000). That this should have occurred coincident with the invasion of Sicily and should involve British Commonwealth and American troops in Egypt, in North Africa, and subsequently in Sicily and Italy was a hindrance of inestimable magnitude to military activity and strained the medical facilities almost beyond their capabilities. It is no exaggeration to say that the hospitals from Cairo to Palermo were filled with patients with hepatitis, with beds occupying the corridors and every other available space. The task of caring for all these patients in addition to the mounting numbers of battle casualties was indeed a formidable one.\n\n\n\n**References:**\n\n- Essential Technical Medical Data, North African Theater of Operations, U.S. Army, for September 1944, 1 Oct. 1944.\n\n- Final Report on Infectious Hepatitis in MTOUSA, Col. Marion H. Barker, MC, to Surgeon, Mediterranean Theater of Operations, APO 512, 23 July 1945.\n\n- Baudot, Marcel. The Historical Encyclopedia of World War II. London: Macmillan, 1981. Print.\n\nMost of the information was taken from [here](_URL_0_) which contains a great wealth of information with proper references.",
        "They're made up of things your body doesn't absorb (other than the water), basically they just pass through you. As far as I'm aware, there's no proven negative health effects as a result of drinking them. \n\nThough, if there was a negative health effect, it wouldn't be due to the lack of calories, water also has no calories.\n\n**Edit:**\n\nThis got way more popular than I would have guessed. To clarify a couple of things:\n\nYes, it's true that depending on the exact ingredients, some of it can be absorbed by your body, and the way that works is it's a small enough amount to be considered negligible calorie wise.\n\nAs far as being detrimental to health: Yes, there is *some* conflicting information, but as per the rules of ELI5\n\n > Only give explanations from a brutally unbiased standpoint. Full stop.\n\nThe fact is, despite there being a lot of extensive research in this area, there is no adequate evidence that they have a negative impact on your health in reasonable amounts. Remember you can also die from too much water. \n\n   \n\n^(**Edit 2:** Thank you kindly for the gold, *anonymous redditor*.)",
        "Sergei Ivanovich Shmatkó (26 September 1966 – 7 November 2021) was a Russian businessman and politician specializing in the energy industry. He was Russia's Minister of Energy from May 2008 until May 2012.\n\nEarly life and education\nShmatko was born in Stavropol (South-West of Russia). He earned degrees from both Urals State University in Yekaterinburg (USSR) and the University of Marburg in West Germany.\n\nBusiness career\nShmatko's business career from 1992 involved stints consulting in Germany and Russia, and working at the All-Russia Bank of Regional Development. He was appointed head of economic strategy at Rosenergoatom in 1997 and in 2005 became president of Russia's nuclear-power export monopoly, Atomstroyexport.\n\nMinister of Energy\nShmatko was appointed to the newly established position of Minister of Energy in May 2008, during the incoming administration of President Dmitry Medvedev.\n\nDeath\nHe died from COVID-19 on 7 November 2021, during the COVID-19 pandemic in Russia.\n\nReferences\n\n1966 births\n2021 deaths\nRussian politicians\nRussian businesspeople\nUral State University alumni\nUniversity of Marburg alumni\nEnergy ministers of Russia\nUnited Russia politicians\nPeople from Stavropol\nDeaths from the COVID-19 pandemic in Russia",
        "J. carnea may refer to:\n\n Janaesia carnea, a South American moth\n Justicia carnea, a flowering plant",
        "  Certain integrable models are described by pairs (X,Y) of ADET Dynkin\ndiagrams. At high energy these models are expected to have a conformally\ninvariant limit. The S-matrix of the model determines algebraic equations,\nwhose solutions are mapped to the central charge and scaling dimensions of the\ncorresponding conformal field theory. We study the equations of the (D_m,A_n)\nmodel and find all solutions explicitly using the representation theory of Lie\nalgebras and related Yangians. These mathematically rigorous results are in\nagreement with the expectations arising from physics. We also investigate the\noverlap between certain q-hypergeometric series and modular functions. We study\na particular class of 2-fold q-hypergeometric series, denoted f_{A,B,C}. Here A\nis a positive definite, symmetric, 2x2 matrix, B is a vector of length 2, and C\nis a scalar, all three with rational entries. It turns out that for certain\nchoices of the matrix A, the function f_{A,B,C} can be made modular. We\ncalculate the corresponding values of B and C. It is expected that functions\nf_{A,B,C} arising in this way are characters of some rational conformal field\ntheory. We show that this is true in at least one case.\n",
        "Do you mean slaves of Arab descent or slaves taken by Arab traders?  If you mean people of African origin, then the answer is, there are most definitely groups descended from slaves.  If you mean within North Africa and the Middle East, that is less an area of study for me, but I'll see what I can do at the end of this post.\n\nIn India, there are the Siddhis, who are the descendants or partial descendants of sub-Saharan Africans traded from the Arab Peninsula all the way to India.  Shihan de Silva in \"African Identity in Asia: The Cultural Effects of Forced Migration\" has charted different rituals and folk traditions, linguistic markers, and dance and musical traditions to show Siddhi influence from Yemen to the Deccan Peninsula.  \n\nPortuguese and Dutch sources in the Indian Ocean often remarked on the military and seafaring prowess of Siddhis.  Indian kings and Shahs used Africans as cavalrymen, bodyguards, generals, and soldiers, citing their lack of familial ties as a true loyal bond.\n\nSome, like Malik Ambar, ended up being powerful enough to control kings and depose one that slighted him by not wanting to marry his daughter.  The states of Sachin and Janjira actually survived Mughal invasion somewhat intact, and lasted through British colonial control, and both of those states had descendants of the Siddhis.  \n\nNote, in different source text, you may also see the word \"Habshi\" or \"Habash\" instead of Siddhi.  Most of my area of study here is on the African Diaspora, so if your question is more on slave enclaves in North Africa in the Middle East instead of African descendants moved around from the diaspora and slave trades, then let me know.\n\nWithin North Africa, Mauritania may be closer to what you are looking for.  Khaled Esseissah in \"Paradise is Under the Feet of Your Master: The Construction of the Religious Basis of Racial Slavery in the Mauritanian Arab-Berber Community,\" asserts that slavery still exists in a racialized form in Mauritania and the issue is downplayed by the current government.  Esseissah points out that modern race relations, economic and political institutions and rights even today show that racial splits passed down and inherited from the earlier slave trade.  This may be more your question, and answers with an affirmative.\n\nIf you have other questions let me know, this may have been disjointed, but these two examples sprang to mind.\n\nSources: African Identity in Asia: Cultural Effects of Forced Migrations by Shihan de Silva Jayasuriya (a great primer how the African Diaspora affected Asia, with a lot of interesting linguistic and anthropological data mixed in)\n\nIslam's Black Slaves: The Other Black Diaspora by Ronald Segal (background material and reference)\n\nKhaled Esseissah - \"Paradise is Under the Feet of Your Master: The Construction of the Religious Basis of Racial Slavery in the Mauritanian Arab-Berber Community\" - This may be more your jam and shows that these ethnic groups and their modern descendants definitely experience racial differences and discrimination in the modern day.",
        " You’ll need measuring tape, a pencil, and a book or a ruler (or some other flat surface)., Their head should be facing straight ahead, and not tilted upwards or downwards, so you do not get a faulty measurement of their height.\n\n\nMake sure their feet are side by side, with both feet firmly on the floor and their heels touching the wall.Allow your child to keep their shoes on, as they will be wearing them when they ride the bike.\n\n, Place the ruler, book or other flat surface on your child’s head. The ruler should be positioned against the wall, parallel with the floor. Mark where the ruler touches the wall with a pencil., Once you have marked the point on the wall, have your child move aside so you can measure their height. Extend the measuring tape from the point you have marked to the floor and write down the measurement. This dimension is the height of your toddler.Keep in mind that kid’s bikes are measured by the size of the wheels, not the frame height. Bikes for toddlers usually range from 12 and 16 inches.\nIn general, 12-inch wheels are designed for children 36-40 inches tall, 16-inch wheels are for children 41-49 inches tall, and 20-inch wheels are for children 50-56 inches tall., Measuring their inseam is important because you need to be sure your child can place their entire foot on the ground while on the bike. If the bike is too big and your child cannot put their feet down, it will be hard for them to balance and they may get frustrated while trying to learn how to ride.\n\n\nYou can easily determine your child’s inseam measurement if they try the bike before you purchase it. But if you are buying a bike without your toddler, you will need to measure their inseam.\n\n, Write down the measurement on a piece of paper and take it with you when you go bike shopping for your toddler.\n\n, Place the string on the floor, between the child's feet, and stretch it to the child's crotch. Then, put your finger on the place on the string to indicate the height of their crotch. Move the string and cut with it with scissors at the place where your finger is. Measure the length of tape using a ruler.\n\n\nKeep in mind that toddlers with an inseam up to 18 inches and a height up to 40 inches need a bicycle with 12-inch wheels. Toddlers who have an inseam up to 18 inches and are taller than 40 inches need 16-inch wheels., The diameter of the bike’s wheels should be clearly labelled on the bike’s packaging or the box. Ask the sales associate at the store if you are not sure where to find the diameter of the bike’s wheels.\n\n\nIf you are purchasing a used bike, use a yardstick or measuring tape to measure the diameter of the tire. Start on the outside of the tire and pull the measuring tape straight across until you get to the other side of the tire--the number on your measuring tape with be the tire’s diameter.\nUse your child's height and inseam measurements to find the right diameter size for the wheels. You can also access a size chart onlineto determine the right wheel diameter for your child, based on their age.\n\n, If you have a two or three year-old who is tall, it is better to provide him with a smaller bike that has 12-inch wheels because they will sit lower in the seat. It will also make it easier to for them to reach the bike’s handlebars.\n\n\nIf your child is older, but short for their age, choose a bicycle with 16-inch wheels. Older children have longer legs and arms, so a smaller bike will be awkward for them to use., Loosen the screw under the bike seat, located where the seat goes into the frame. When you loosen it, you will then be able to move the seat up on or down to suit your child.\n\n, To ride the bike easily and comfortably, your child’s knee will be slightly bent when the pedal is in the down position, as this allows a stronger stroke on the pedal than when their leg is stretched out.\n\n\nWhen the pedals are in the up position, your child’s thighs should be horizontal and not stick out.\n\n, Correct handlebar placement is especially important when your child is first learning how to balance and steer a bike.\n\n\nYour child should be able to sit upright and be close to the handlebars but their knees should not hit the handlebars when they pedal.Your child should also be able to comfortably place both hands on the grip pads and have enough movement to turn easily., Press them with your little finger, as children do not have the same pressure in their fingers as adults. If you can press the brake levers with your little finger without much effort, your toddler will be able to press them too., Most childrens’ bikes come with adjustable handlebars and seats because children grow quickly, so it shouldn’t be hard to find an adjustable bike.\n\n\nSome bikes come with quick-release adjustments, so adjustments can be made without needing any tools, but this also makes it easier for bike parts to be removed and stolen.\nOther bikes come with a traditional adjustment system, which can require basic tools or special parts that are often included with the bike.\nCheck to see what type of adjustment system is on your child’s bike and look for one that matches your adjustment preferences.\n\n, Always bring your child with you to test a used bike before purchasing it, and make sure the brakes work and that all the other pieces are in order.\n\n\nDon’t be afraid to ask the seller questions about how old the bike is and if it has been in any major crashes. Used bikes are best if they are on their newer side (only a few years old) and haven’t been in any major crashes.\nIf the bike doesn’t fit or isn’t in a safe condition, walk away and look for a better option. Used bikes are sometimes stolen merchandise being resold. If you are suspicious that the used bike might be stolen, look for the serial number, which is usually located underneath the main support post. Contact the police department or search the internet with the serial number and description of the bike to see if any stolen bike reports matching the number appear.\n\n, Employees at specialty stores tend to receive better training so they can recommend the correct bike size and features for your child.\n\n, When sitting on the seat with their hands on the handlebars, a child should be able to place the balls of both feet on the ground at the same time.Your child should be able to straddle the tallest bar on the bike while still having their feet flat on the ground on both sides of the bike, which helps them quickly stop the bike or get off if they start to fall.If your child can’t fulfill these requirements, try a smaller size bike or adjust the seat so it is on a lower setting.\n\n, This bike style is a popular option and comes in very small sizes. This bike looks like a typical toddler bike, but does not have pedals, so your toddler can straddle the bike and walk with it. Balance bikes are a good starting point for young children because they give them the feel of a bike and will teach them to steer and control the bike without having to use pedals.\n\n\nThey can be used by children as young as 18 months old.\n\n, Once they are ready for a real bike, you can add training wheels to this style.\n\n\nTraining wheels should be attached next to the rear wheel and should start flush with the ground. As your child gets more comfortable balancing on the bike, you can raise the training wheels, which will force them to balance on their own.Often, toddlers that start with balance bikes can skip the training wheel phase or go through it quickly.Keep an eye on your child to monitor their progress and comfort levels.\n\n, This way, your child can pedal backwards to brake. Young children don’t have the hand strength or coordination to use hand brakes.In fact, children younger than six should only use coaster brakes.\n\n\nMost bikes come with hand brakes when they are have 16 inch wheels and bigger, which the average child will fit into when they are 6-7 years old.Make sure the brakes can be activated quickly and have the power to stop the bike. Have your child test the brakes, and then test them yourself with a hand or foot to see how quickly they react.\n\n, These styles aren’t ideal for children until they have a firm grasp on riding a bike.\n\n\nOnce your child becomes a more confident rider, you can experiment with bikes for different types of terrain.\nAll children should learn to ride a bike on a flat, even surface, like cement. If your child will be riding on dirt roads or cobblestone paths, look for a bike with larger shocks for a more comfortable ride.\nFor best results and personal recommendations, consult a biking professional at a sporting goods or biking store.\n\n, In general, the less expensive bikes will be heavier because they are made of steel instead of the more expensive alloy metals.Steel bikes are still safe, but can be slightly more difficult to maneuver and are heavier if your child crashes and gets trapped under their bike.\n\n, Helmets are required by law in most states when riding in public places, and they are always a good idea for young children learning to ride a bike. Helmets also reduce the risk of bicycle-related head injury by 88%.A helmet is only effective if it fits your child’s head properly. The right helmet will rest just above your child’s eyebrows and sit level around their entire head. It shouldn’t move around on their head when shaken, and the strap should be snug but still allow your child room to move their mouth and talk.\nHelmets that are designed specifically for biking are best, but your child can also wear a helmet designed for another activity, like rollerblading or skateboarding, as long as it fits them properly.\n\n, Though these are not required, it may be beneficial to protect your child from the typical falls and stumbles that may happen when they first learn how to ride.\n\n\nMost pad sets are sold according to the height, age, and weight of a child. Use the same measurements you took for correct bike sizing to choose the correct pad size.\nHave your child try the pads on to ensure they are comfortable. You also ask a store employee to help you with sizing and material recommendations for safety gear.\n\n",
        "The authors introduce a variant of the variational autoencoder (VAE) that models dataset-level latent variables. The idea is clearly motivated and well described. In my mind the greatest contribution of this paper is the movement beyond the relatively simple graphical model structure of the traditional VAEs and the introduction of more interesting structures to the deep learning community. \n\nComments:\n\n- It's not clear to me why this should be called a \"statistician\". Learning an approximate posterior over summary statistics is not the only imaginable way to summarize a dataset with a neural network. One could consider a maximum likelihood approach, etc. In general it felt like the paper could be more clear, if it avoided coining new terms like \"statistic network\" and stuck to the more accurate \"approximate posterior\".\n\n- The experiments are nice, and I appreciate the response to my question regarding \"one shot generation\". I still think that language needs to be clarified, specifically at the end of page 6. My understanding of Figure 5 is the following: Take an input set, compute the approximate posterior over the context vector, then generate from the forward model given samples from the approximate posterior. I would like clarification on the following: \n\n(a) Are the data point dependent vectors z generated from the forward model or taken from the approximate posterior? \n\n(b) I agree that the samples are of high-quality, but that is not a quantified statement. The advantage of VAEs over GANs is that we have natural ways of computing log-probabilities. To that end, one \"proper\" way of computing the \"one shot generation\" performance is to report log p(x | c) (where c is sampled from the approximate posterior) or log p(x) for held-out datasets. I suspect that log probability performance of these networks relative to a vanilla VAE without the context latent variable will be impressive. I still don't see a reason not to include that.",
        "  We present the properties of 11 $\\mu$m selected sources detected in the early\ndata of the North Ecliptic Pole Deep (NEP-Deep) Survey of AKARI. The data set\ncovers 6 wavelength bands from 2.5 to 11 $\\mu$m, with the exposure time of 10 ~\n20 minutes. This field lies within the CFHT survey with four filter bands ($g',\nr', i',z'), enabling us to establish nearly continuous spectral energy\ndistributions (SEDs) for wavelengths ranging from 0.4 to 11 $\\mu$m. The main\nsample studied here consists of 71 sources whose 11 $\\mu$m AB magnitudes are\nequal to or brighter than 18.5 (251 $\\mu$Jy), which is complete to more than\n90%. The 11 $\\mu$m band has an advantage of sampling star forming galaxies with\nlow to medium redshifts since the prominent PAH feature shifts into this band.\nAs expected, we find that the majority (~68%) of 11 $\\mu$m bright sources are\nstar forming galaxies at 0.2 < z < 0.7 with $L_{IR} ~ 10^{10}$ -- $10^{12}\nL_{\\odot}$ based on the detailed modelling of SEDs. We also find four AGNs\nlying at various redshifts in the main sample. In addition, we discuss a few\nsources which have non-typical SEDs of the main sample, including a brown dwarf\ncandidate, a steep power-law source, flat spectrum object, and an early-type\ngalaxy at moderate redshift.\n",
        "  The motion of a black hole about the centre of gravity of its host galaxy\ninduces a strong response from the surrounding stellar population. We treat the\ncase of a harmonic potential analytically and show that half of the stars on\ncircular orbits in that potential shift to an orbit of lower energy, while the\nother half receive a positive boost and recede to a larger radius. The black\nhole itself remains on an orbit of fixed amplitude and merely acts as a\ncatalyst for the evolution of the stellar energy distribution function f(E). We\nshow that this effect is operative out to a radius of approx 3 to 4 times the\nhole's influence radius, R_bh. We use numerical integration to explore more\nfully the response of a stellar distribution to black hole motion. We consider\norbits in a logarithmic potential and compare the response of stars on circular\norbits, to the situation of a `warm' and `hot' (isotropic) stellar velocity\nfield. While features seen in density maps are now wiped out, the kinematic\nsignature of black hole motion still imprints the stellar line-of-sight mean\nvelocity to a magnitude ~18% the local root mean-square velocity dispersion\nsigma.\n",
        "  Capacity improvement from transmitter and receiver cooperation is\ninvestigated in a two-transmitter, two-receiver network with phase fading and\nfull channel state information available at all terminals. The transmitters\ncooperate by first exchanging messages over an orthogonal transmitter\ncooperation channel, then encoding jointly with dirty paper coding. The\nreceivers cooperate by using Wyner-Ziv compress-and-forward over an analogous\northogonal receiver cooperation channel. To account for the cost of\ncooperation, the allocation of network power and bandwidth among the data and\ncooperation channels is studied. It is shown that transmitter cooperation\noutperforms receiver cooperation and improves capacity over non-cooperative\ntransmission under most operating conditions when the cooperation channel is\nstrong. However, a weak cooperation channel limits the transmitter cooperation\nrate; in this case receiver cooperation is more advantageous.\nTransmitter-and-receiver cooperation offers sizable additional capacity gain\nover transmitter-only cooperation at low SNR, whereas at high SNR transmitter\ncooperation alone captures most of the cooperative capacity improvement.\n",
        "  We report the detection of five Jovian mass planets orbiting high metallicity\nstars. Four of these stars were first observed as part of the N2K program and\nexhibited low RMS velocity scatter after three consecutive observations.\nHowever, follow-up observations over the last three years now reveal the\npresence of longer period planets with orbital periods ranging from 21 days to\na few years. HD 11506 is a G0V star with a planet of \\msini = 4.74 \\mjup in a\n3.85 year orbit. HD 17156 is a G0V star with a 3.12 \\mjup planet in a 21.2 day\norbit. The eccentricity of this orbit is 0.67, one of the highest known for a\nplanet with a relatively short period. The orbital period for this planet\nplaces it in a region of parameter space where relatively few planets have been\ndetected. HD 125612 is a G3V star with a planet of \\msini = 3.5 \\mjup in a 1.4\nyear orbit. HD 170469 is a G5IV star with a planet of \\msini = 0.67 \\mjup in a\n3.13 year orbit. HD 231701 is an F8V star with planet of 1.08 \\mjup in a 142\nday orbit. All of these stars have supersolar metallicity. Three of the five\nstars were observed photometrically but showed no evidence of brightness\nvariability. A transit search conducted for HD 17156 was negative but covered\nonly 25% of the search space and so is not conclusive.\n",
        "The distinction of the liver as the only organ that can repair/regrow is a bit misleading.  It is commonly cited as the prototypical self-repairing organ, but in actuality many parts of the body can constantly repair and renew itself.  The basal layer of the epidermis is a great example of a cell type that retains some stem-ness and is able to regenerate skin and provide compensatory responses to injury.  Similarly, adult stem cells are found in many other sites, such as the crypts of Lieberkuhn in the intestines, hematopoietic stem cells in the bone marrow, and even neural precursor cells in the brain.  Admittedly, we don't yet have a great understanding of the roles each of these cell types play in the regeneration/homeostatic process, but we do know that they are much more prevalent than previously thought.\n\nWith regards to the liver, it's important to note that true 'regeneration' is not occurring - rather, there is compensatory hyperplasia as the remaining liver tissues receive various chemical signals to proliferate and restore lost hepatic function.  Many other organs, including kidney, pancreas, adrenal glands, thyroid, and the lungs of very young animals, also demonstrate compensatory hyperplasia, but just to a less noticeable extent.  With this compensatory hyperplasia, the surgically resected lobes are not regrown - rather, the remaining lobes enlarge, such that function is restored without restoration of original form.  This is mediated by signalling molecules such as HGF, TGFa, EGF, TNF, Il-6, as well as hormones and neurotransmitters like norepinephrine, serotonin, insulin, thyroid and growth hormone.",
        "In this paper, the author proposed an approach for feature combination of two embeddings v1 and v2. This is done by first computing the pairwise combinations of the elements of v1 and v2 (with complicated nonlinearity), and then pick the K-Max as the output vector. For triple (or higher-order) combinations, two (or more) consecutive pairwise combinations are performed to yield the final representations. It seems that the approach is not directly related to categorical data, and can be applied to any embeddings (even if they are not one-hot). So is there any motivation that brings about this particular approach? What is the connection? \n\nThere are many papers with similar ideas. CCPM (A convolutional click prediction model) that the authors have compared against, also proposes very similar network structure (conv + K-max + conv + K-max). In the paper, the author does not mention their conceptual similarity and difference versus CCPM. Compact Bilinear Pooling,",
        "Soluble transferrin receptor conventionally refers to the cleaved extracellular portion of transferrin receptor 1 that is released into serum. This receptor is a protein dimer of two identical subunits, linked together by two pairs of disulfide bonds.\nIts molecular mass 190,000 Dalton.\n\nBlood testing of the soluble transferrin receptor (sTfR) is used as a measure of functional iron status and the investigation of iron deficiency anemia. Ferritin, a routine investigation for anemia, is an acute-phase reactant, and may be elevated in states of inflammation, thereby falsely indicating that iron stores are adequate. Because sTfR is insensitive to inflammation, it can detect anemia in patients with preexisting inflammatory states, and is particularly useful in distinguishing between the anemia of chronic disease and anemias caused by lack of iron intake.\n\nTo date, the conventionally identified soluble transferrin receptor has not been itself implicated in intracellular delivery of transferrin and associated iron stores.\n\nA soluble receptor for any ligand could also refer to a molecule present is solution (for example a secretory protein) which would bind with the target ligand and then  effect cellular delivery. In this context the multifunctional glycolytic enzyme glyceraldehyde-3-phosphate dehydrogenase (GAPDH) has been reported as a soluble receptor for transferrin. It has been demonstrated to deliver more transferrin as compared to the receptors anchored on the cells surface in numerous cell types.\n\nSee also\nFerritin\nTransferrin\nTotal iron binding capacity\n\nCitations\n\nProteins",
        ", If your horse has this but makes it past the first 14 days since infection he should be okay.\nChronic: Chronic EIA horses go through the same severity of acute EIA horses but seem to recover after a few weeks. The symptoms then go away for weeks or even months until the horse has a relapse of worsened intensity, usually as result of being worked hard. Chronic EIA horses also can have rough hair coats, lack stamina, and be anemic.\nCarrier: Carriers of the EIA virus will seem to completely recover from the initial attack, yet still carry and can transmit the disease., A healthy adult horse's temperature while resting varies from to 99-101°F (37.2 - 38.3°C), while fever associated with EIA can range from 105-108°F (40.5 - 42.2°C).\n\n\nThe fever of an EIA-infected horse can either be highly intermittent or continuous. It may help to take your horse's temperature several times throughout the day.;\n, This can be checked by placing the hand under the horse's left elbow. Count the beats for 10 seconds then multiply the beats by 6 to get the heartbeat number per minute. A normal heart rate is 35-40 beats per minute.\n\n\nIrregularity may be a sign of infection with EIA.\n\n, A healthy horse likes his food and while eat it nearly as soon as you offer it. A horse that refuses food or has a reduced appetite, however, may have EIA.\n\n, Don't be concerned unless there has been no change of diet and your horse has had rapid or chronic weight loss.\n\n, Tiredness and depression are also signs of EIA.\n\n, Swelling is another sign of the horse having EIA.\n\n, Similar diseases include Piroplasmosis, African Horse Sickness, Anthrax, Dourine, Equine Viral Arteritis, Japanese Encephalitis, Equine Influenza, Equine Herpes Virus, and Babesiosis.\n\n, He should be able to give you advice and confirm your horse's symptoms.\n\n, This test is fairly inexpensive and only requires a certified vet sending a sample of your horse's blood to a lab. Usually the process takes 3-5 days.\n\n\nA negative Coggins/EIA test result means your horse is not a carrier of the disease. If your horse continues having symptoms, tell your vet; your horse may be sick with something else.\nA positive Coggins/EIA test result means your horse is an EIA carrier. If you feel the test was a false positive you can ask your vet to retake the Coggins, though you will need to pay for the test again.\n\n,, This will reduce the potential of EIA spreading throughout the area.\n\n\nSome areas say area of exposure is within proximity of 200 yards (180 meters) of the infected horse, other areas say within proximity of 3 miles (4.8 kilometers).\n\n, Legally you only have three choices: Keeping your horse at a quarantine facility for the rest of its life, having the horse euthanized, or selling the horse for immediate slaughter.\n\n\nThere aren't many quarantine facilities, but one of the largest ones, Florida Research Institute for Equine Nurturing, Development and Safety, is located in Southern Florida.\n\n, Large-mouthed flies such as deer-flies and horseflies will quickly spread the EIA virus by mechanically sharing an infected horse's blood with other horses.\n\n\nRegularly clean and muck stalls/pens around the barn, depositing the manure a good distance away.\nEliminate any standing water; flies use these as breeding grounds.\nUse fly-repelling sprays and fly traps around the barn.\n\n, Infected mouth germs will share the EIA disease just like blood.\n\n, This will prevent an infected horse's blood from getting into other horses.\n\n, While this is a good practice anyway, the threat of EIA makes it an even better one.\n\n, While diseases can definitely be fatal, a horse is much less prone to becoming sick if he's happy and well cared for.\n\n",
        "The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.",
        "To elaborate a bit on what /u/TheBaris said: Turkey was in a weird position. The Ottoman Empire was a world power during its heyday and when the empire collapsed, it collapsed *hard.* In the chaos that followed, and the ill-advised attempts to partition the old Ottoman heartlands, the Turks found themselves fighting half their neighbors: Greeks in the west, French in the south, Armenians in the east, and various skirmishes with the Brits. Constantinople, the Ottoman capital since Mehmed II captured in 1453, was occupied. It was pretty much chaos. And Turkey was somewhat fortunate that the Russians were too busy fighting their own civil war to meddle in it. \n\nTurkey eventually won, obviously. But the important thing isn't the battles or the casualty figures: it's the fact that Turkey was fighting the Entente. The men who came to the forefront of Turkish politics in this period -- Ataturk, Karabekir, Cebesoy, Inonu -- were fighters. Many of them fought in the Balkan Wars, most of them fought in WWI (Yazici, who would go on to command in Korea, fought at Gallipoli), and many of the men who would go on to hold high posts in Turkey (President, Prime Minister, Speaker) served some military role. \n\nAnd then, while Turkey is still trying to find its place in the world (especially after the chaos that was WWII), it holds open elections and for the first time ever the government is dominated by men who didn't fight in one war or another. Bayar was elected in 1950 and the closest thing he had to military experience was as a young political radical (he would remain in office until the 1960 coup). His Prime Minister (who shared his long tenure), Menderes, similarly had no military experience. Koraltan, Speaker of the National Assembly over the same period, was in the same boat. As you can probably imagine, shifting from a government that was led primarily by former military officers or the politicians who worked with them to a government comprised almost exclusively of men who had never seen combat represented quite a shift in Turkish politics. \n\nWhile all of this was happening on the domestic front, Turkish foreign policy was changing, too. Early in the republics history, it had committed to a policy of neutrality: it wanted no part of any alliances. Turkey and (most of) the great powers sat down and figured out what to do with the Bosporus and Dardanelles, leading to the [Montreux Convention](_URL_1_), which imposed all sorts of military restrictions on transiting those waters. Everything seemed all hunky-dory and then WWII happened. When the dust settled and the Soviets were triumphant, they suddenly had an enormous military advantage over Turkey. In 1945, the treaty of friendship between the two countries expired and was not renewed. In 1946, the Soviets sent two letters suggesting Turkey agree to \"alter\" the Montreux Convention by making the Soviets a guarantor of the treaty (which, in diplomacy-speak, means a permanent Soviet military presence in the area) and, while they were at it, the Soviets [they wanted land, too](_URL_0_). You can read a bit more about it [here](_URL_4_), which the relevant parts being around page 44 on (in the pdf reader; the pdf pages say it's page 27). \n\nIn the face of Soviet demands, the Turks really three options: capitulate and effectively become a satellite state, stand alone and [risk invasion](_URL_2_), or go to America. So the Turks went to America. As it happened, America was in the process of forming NATO, which would come into existence in 1949. To America, Turkey wanting in was a wonderful boon. Control over the straits, even if through an ally, was effectively a lever against the Soviets. \n\nAs an aside, it's probably worth noting that Turkey also hosted nuclear-tipped missiles from 1959 until 1963. If those dates seem odd to anyone, they may be interested in knowing that the Cuban Missile Crisis was in 1962. If that seems an odd coincidence, it's because it wasn't. The Soviets pulled their missiles out of Cuba [because America agreed to pull their missiles out of Turkey](_URL_3_), not because they were intimidated. It makes you wonder if the Soviets sent missiles to Cuba solely to get them out of Turkey. But I digress.\n\nSo when the Korean War broke out, Turkey had an opportunity: by sending troops there, it could achieve two objectives: it gave Turkey the opportunity to prove its commitment to America (and thus effectively guaranteeing NATO membership) and simultaneously align itself very publicly against the Soviets. It effectively sent a message that Turkey wasn't going to be bullied into accepting an unfair treaty by the Soviets and that, if the Soviets forced the issue, that Turkey would have friends that would help them. It would take until 1952 to finalize, but Turkey was effectively under NATO's aegis from 1950 on. 15,000 men sent to Korea was a small price to pay for that security. \n\nEdit: gold? wat? :D",
        "1) Evolution does not eliminate everything that is not beneficial for reproduction. There are plenty of things about us that could potentially function better, or have no use at all, but which persist because they are not unforgivably detrimental to the species as a whole. \n\n2) A trait that evolves and appears to be non-useful for reproduction on its face, may simply be more subtly useful. I.e. non-parenting organisms can provide child-rearing assistance for parenting organisms, thus improving the level of care an offspring receives.\n\n3) An apparently non-beneficial trait may simply be a side effect of a trait which actually is beneficial for reproduction. For example, some studies have suggested that female relatives of homosexuals have increased fecundity. The lack of reproduction from one party may be insufficient to offset the benefit of greater reproductive capacity in the other.\n\nBasically, not everything is necessarily \"progressive for evolution.' If an organism works enough of the time, it persists.",
        ",,,, Note that the number you choose is only for the skin and face mold–you can personally choose the eye color later on.\n, This number is: 800-628-5145.\n, It's best to use the official term. If the employee on the phone transfers you to someone else or is unsure about your request, don't worry–not all employees are even aware they offer the bald dolls.\n\n,\nIf you'd like to add a hearing aid or pierce your doll's ears, they probably won't ask, so be prepared to tell them.\nIf you're ordering anything else, be sure to tell them. You can add anything–even another doll–to this order; you don't have to wait and do that online or anything.\n\n, If you have an online account through American Girl already, inform them of this so you can skip giving them your address and other ordering info., This is so you can track your package.\n, You do not have to go online to confirm. Tracking is available online, and often dolls without hair come faster than a typical American Girl doll, though don't expect it. Typical shipping takes around a week to a week and a half.\n",
        "WARNING LONG! But, it's my field of work (computational biology), so I'll give you the detailed answer.\n\nThis is like my exact field of study, so I can comment on this.  You hear \"epigenetics\" and this is kind of the surface answer to explain *heritable* information from cell to cell that causes varied gene expression, even though one cell from the other has the exact same DNA. However, there is more to it than that. There is also something called \"stochastic noise\" that can actually affect development too, of which I will get to later. I also think it is prudent to point out that there may also be other unknown factors yet to be identified. \n\nSo, let me give you a broader explanation that hopefully makes sense. Your DNA is identical in every single cell of your body, whether it is the DNA in your lungs, or your skin, your heart cells, your liver cells and so on (typically, some exceptions). What makes each cell different? What makes each cell different is something called \"gene expression,\" or, more simply, what makes each cell different is the spectrum of proteins that are produced in each different cell.\n\nHow does that work though? How does one cell that has the same DNA as another cell \"know\" what proteins to produce? The answer is a word we use quite often, and that is, by \"regulation.\" You see, at the end of the day, cells don't \"know\" anything. They don't think. They are just proteins and various molecules floating around and bumping into each other. And, if they happen to bump into each other, sometimes interesting things can happen. You see, it is all just about the \"probability\" of bumping into each other though. This is what we call cellular \"noise.\"\n\nThink of it like this. If there is a large concentration of a certain protein, there is a much higher probability that it will bump into something which can cause a reaction, which can cause some kind of event downstream. If there is a low concentration, sometimes there may be just a random chance of things bumping into each other, but it will be more rare. This is a form of cellular regulation.\n\nSo, this brings me back to the question, well why does one cell with the same DNA produce different proteins than the another cell? Simple, it has to do with the way DNA is packaged up. Unlike the common picture of the double helix you often see in TV, DNA in reality is covered in proteins. I won't go into the amazing amount of variability and functions of it all, so simply, in one cell, the DNA gets packaged up differently than others. For example, in one cell, the DNA will be \"wound up\" extremely tight in certain areas, whilst open in others. Where it is wound up tight, proteins are unable to bind to the DNA to activate the process(transcription) of which proteins are derived from the DNA (Transcription  >  Translation). You can kind of say that certain genes are hidden from collisions with other proteins in these tightly wound regions. Where it is loose, they will bind. Ultimately, this is the core way one cell differs from the other, the way the DNA is packaged up. A liver cell will only have its cocktail of proteins able to be translated, whilst the others packed up, and other cells might have a very different regime of proteins to produce.\n\nI should note that this is still poorly understood. We know a lot more now than we did 10 years ago, but a lot of thesis work is going on in this department cause it is incredibly complex. But that is just a side note, so I wouldn't worry about it here.\n\nNow that you have this big picture (albeit simplified), we go back to your question, how do identical twins look different even though they have the same DNA? Well, one answer is that we have a field called epigenetics. What this is is that through environmental factors, modifications happen to the proteins that package the DNA (methylations), and for one person, through their own life choices, experiences, and environmental factors, the DNA may get packaged slightly different. These will not be extreme differences, as most cellular function must remain the same, but lets use an extreme example: The twins are separated at birth, and one goes to a very poor home and another to a well-fed home. Let's say several times in this child's life he was malnourished, or just didn't eat as healthy or something. Maybe that body went into a more \"survival\" mode and he just grew less tall, or more puny than his twin that was well-fed.  The DNA modified its packaging to stop transcription of certain proteins or activate others that another person may not have, like their twin.  Sometimes it might be as simple as lowering the expression of a protein by half in one person compared to the other. These modifications that happen due to environmental factors will carry on in each replication. Hence, it's not in the DNA code itself, but there is stored information that copies how the DNA is packaged as well, thus it's not true genetics, it is epigenetics. So, the twins end up looking different because some of these proteins affect the phenotype, or the outward, visible expression of a protein. For example, brown hair verse red hair are phenotypes. Short vs tall, etc. Some genes are related to various phenotypes, and these just may end up being expressed different from one twin to the next due to epigenetic modifications of their DNA.\n\nAlso, DNA methylations are heritable. If you want to read up on the exact term, we call it \"genomic imprinting.\" There was another post here that already covered it nicely, but let's say in very early cell-differentiation you had half the methylated DNA on one side, half on the other, if the cell splits to form twins, one of the twins will receive some portion of the heritable methylations from the mother and the other twin will get the rest. This will lead to some different gene expression between the two of you as well.\n\nThere is another issue though. What if I told you that in a theoretical world, if both twins had 100% exactly the same environmental factors to face them through their whole life, that they still will most-likely not be perfect matches of each other? WHAT!? How is that possible? Remember I talked about how in the cell things are just floating around and bumping into each other? Well, this noise is \"stochastic,\" meaning random. In early cellular development, even if 2 cells side-by-side have the same cocktail of proteins and is the same scale, there is still only the \"probability\" of things bumping into each other enough to hit certain thresholds for certain signaling pathways to be activated. Early stochastic noise in the cell can actually cause fairly significant downstream differences. Maybe some early signaling for a growth factor ended up being 25% stronger just through \"stochastic\" noise. Now, through probability, most things will be the same, but also through probability, there is going to be some differences no matter what. This is also why if you took an animal and cloned it, the DNA may be the same, but it becomes nothing more than a \"twin\" who is not a perfect match due to the stochastic changes that occurred in early cellular development.\n\nAnyway, there is a bit more to it than that, but I thought I'd hopefully explain a bigger, more understandable picture to anyone that stumbled upon this, also because I am procrastinating my own work right now and thought, why not, I'll give a big answer. Hope it helped.\n\n**EDIT** Wow, this has gotten a lot more traction that I had even imagined! I probably would have given some finer details if I knew it would explode like this, as now I am re-reading it and thinking to myself, \"Oh great, now where did I go wrong and am about to be corrected?\" Thanks everyone. It's an extremely fascinating world we live in, and imo, there is nothing more fascinating than the incredible workings of the cell. Keep learning!",
        "Tampei-Kukuo is a community in Tamale Metropolitan District in the Northern Region of Ghana.\n\nSee also\nSuburbs of Tamale (Ghana) metropolis\n\nReferences \n\nCommunities in Ghana\nSuburbs of Tamale, Ghana",
        " Prior to cutting any wood, make sure you correctly measure the width of your window to ensure that you create a backplate of the appropriate dimensions.\n\n\nDon’t measure the entire open space of the window, rather, measure from the first inner lip on the interior frame of the window.\nDo this to ensure that your built box overlaps the window frame, rather than falls through it.\nThis flush (even edges) construction also makes sure that the window remains weather-sealed.\nMake sure your measurements account for any abutting vinyl ridges or channels within the window frame.;\n, Prior to using any power saws, ensure that your measurements are exact and that you are safe from any debris or wood chips.\n\n\nIf your measurements are not exact, your entire box will be a waste of effort.\nFollow the old adage: measure twice, cut once.\nMark your measurements on your plank of wood, drawing out lines that indicate the height and width of your backplate.\n\n, Following the lines you marked, use your saw to cut your plank of wood down to a size that will fit perfectly in your window.\n\n\nChoose a track saw to ensure your line is as clean and straight as possible.\nIf you do not have a track saw available, you can use a regular circular saw.\nBe aware that using a circular saw will be more difficult and result in a more uneven cut.\nSave the excess wood, because it is the same length as your backplate and can be used later.\nWear safety goggles to protect your eyes from any flying wood particles.\n\n, Measure the width and the height of the air conditioning unit to know how large of a hole you will need to cut in the backplate.\n\n\nMeasure the width at its widest consistent point, not where the sides of the unit begin to curve in towards its face.\nTake any pieces jutting off of the unit into consideration to provide enough clearance.\nThis will make sure the unit can pass through the opening of the box you will create.\nTake measurements of the flanges on the bottom and top, because your wood panels will need to incorporate two indented portions to accommodate the flanges and secure them in place.\nPencil these measurement marks onto your backplate with a carpenter's square and straightedge to make sure you have lines to follow when cutting.\n\n, Using the measurements you took from your AC unit and the widths of your boards, find the proper cutting dimensions for your wood.\n\n\nFor example leave 3-1/4 inches at the top to catch the window and clear the flange, 11-3/8 for the height of the unit, and then 1-1/2 at the bottom for the lip. This makes a total of 16-1/8 inches for the weight of the cut hole.\nAdd a little to ensure clearance, bumping up the cutting measurement to 16-1/2.\nMark your wood with a pencil at 16-1/2 on both sides of the wood to align the track for the saw.\n\n, Trace the lines where you will cut, along the straight edge, to ensure that you will cut at the proper places.\n\n\nMake sure your measurements are accurate, using a straight edge and carpenter’s square to align the measurements on both sides of the wood.\nIf you ever realize that you made a mistake in the measurements, be sure to erase your old lines to avoid following them with the saw.\n\n, Drilling holes in the corners of where you want to cut out the center of your backplate allows the jigsaw blade to get started without cutting in from the side.\n\n\nYou can’t cut in from the side, because that would cut an entire piece off of the wood that you need.\n\n, Insert your jigsaw into the first pilot hole and follow your traced line to cut out the box from your backplate.\n\n\nHold the jigsaw steady to ensure a clean line.\nJigsaws have a tendency to “walk”, meaning they can start moving in other directions if you aren’t careful.\nIf you have a track saw, use that for a cleaner, more stable cut.\n\n, To determine whether or not you need to make measurement adjustments, put your backplate over your AC unit and see if it bumps anything.\n\n\nIf anything sticks, or if the AC unit doesn’t fit at all, you know you need to cut a bit more from the backplate's interior edges.\n\n, The horizontal window catch is a narrow strip of wood that will abut the lower edge of the window's sliding portion, to lock the box in place.\n\n\nUse your carpenter’s square and straightedge to measure out a strip that will be attached the backplate to provide a ridge accommodating the flange on the top of the AC unit.\nIf you have a table saw, set it to a fenced 1-1/2 inches and cut the strip.\nThen, use a miter saw to cut the strip down to the length of your frame.\nThis strip will be sandwiched between the backplate of your box and the vertical window catch, to provide an area for the window to lodge in.\n\n, Using the previously drawn line that represents the flange, align the strip you just cut with its proper location on the backplate.\n\n\nMake sure the strip you cut for the flange and the box’s backplate are even.\nClamp the strip to the backplate to ensure easy attachment.\nOn the backplate's edges, mark the location where the screws will go.\nUse a countersink bit to create a clean pilot hole for the screws and add a taper to let head of the screws fit flush against the wood.\nOnce you’ve drilled the pilot holes, drive screws into the wood to attach the strip to the backplate.\n\n, The vertical window catch will ensure that the box cannot simply fall backwards and out of the window.\n\n\nCut a piece of wood, equal in length to the horizontal window catch, but wider, to act as the back of the channel.\nDrill pilot holes through the vertical window catch, to the horizontal window catch you just attached.\nSpace the pilot holes roughly every 6 inches (15.2 cm), and leave an inch or so at the end to provide clearance space.\nLook on the opposite side to make sure you won’t hit any of your other screws.\nUse screws to attach the vertical window catch to the horizontal window catch.\n\n, These pieces will make sure that that the bottom of the box fits snugly and securely in the bottom of the window.\n\n\nMeasure and cut a piece of wood that will act as a gap bridger (spacer), between the backplate and the sash filler.\n\nThis piece should provide just enough clearance for the sash filler to lock into the bottom channel of the window.\n\n\nThen measure and cut a sash filler (a small piece) that will hook inside the bottom plastic lip of the windowsill.\nUse your track saw for a clean cut.\nMake sure the piece of wood is thick enough to catch the lip or nub on the inside of the bottom of the windowsill.\n\n, Use screws to attach this pieces to the bottom.\n\n\nAttach the gap bridger directly to the bottom of the backplate, and then attach the sash filler on top of that.\nMake sure that the screws have at least a half-inch of bite, which will ensure that they stick properly.\nNow, your box should be complete.\n\n, When placing the box into your window, make sure that it fits flush and does not wiggle or fall out.\n\n\nRemove the screen before placing your box into the window frame.\nThe sash filler you built into the bottom your box should accommodate the ridges of the windowsill, fitting together like two pieces of a puzzle.\nThe catch on the top of your box (created by the vertical and horizontal window catches) should provide a snug place for the bottom edge of the window to slide down into.\n\n, If the box fits well in the window, you can now secure your AC unit to the box.\n\n\nOn the top flange of the AC unit, there should be a single hole meant for one screw.\nThis is usually used to attach the AC unit to the window frame, but in this case you should put a screw through the hole into your box.\n\n, Using a putty knife, smooth some Spackle over the screw holes to make them look natural and flush.\n\n\nMake the holes smooth with a putty knife.\nLet it dry.\nSand it down to make it smooth.\n\n, Fill the edges with caulking to provide a seal against drafts and to keep your cold air in your house.\n\n\nCaulking the edges makes sure that the AC unit does not leak air.\nMake sure you want to keep the AC in the box before you caulk, because this can make it a semi-permanent fixture.\n\n, Priming and painting the box will make sure that it can withstand the test of time.\n\n\nLet the primer dry for an hour if there is low air humidity.\nThis will make sure that rain and humidity do not rot your wood box.\n\n",
        "The paper presents a general approach to modeling for natural language understanding problems with two distinct textual inputs (such as a question and a source text) that can be aligned in some way. In the approach, soft attention is first used to derive alignments between the tokens of the two texts, then a comparison function uses the resulting alignments (represented as pairs of attention queries and attention results) to derive a representations that are aggregated by CNN into a single vector from which an output can be computed. The paper both presents this as an overall modeling strategy that can be made to work quite well, and offers a detailed empirical analysis of the comparison component of the model.\n\nThis work is timely. Language understanding problems of this kind are a major open issue in NLP, and are just at the threshold of being addressable with representation learning methods. The work presents a general approach which is straightforward and reasonable, and shows that it can yield good results. The work borders on incremental (relative to their earlier work or that of Parikh et al.), but it contributes in enough substantial ways that I'd strongly recommend acceptance.\n\nDetail: \n- The model, at least as implemented for the problems with longer sequences (everything but SNLI), is not sensitive to word order. It is empirically competitive, but this insensitivity places a strong upper bound on its performance. The paper does make this clear, but it seems salient enough to warrant a brief mention in the introduction or discussion sections.\n- If I understand correctly, your attention strategy is based more closely on the general/bilinear strategy of Luong et al. '15 than it is on the earlier Bahdanau work. You should probably cite the former (or some other more directly relevant reference for that strategy).\n- Since the NTN risks overfitting because of its large number of parameters, did you try using a version with input dimension l and a smaller output dimension m (so an l*l*m tensor)?\n- You should probably note that SubMultNN looks a lot like the strategy for *sentence*-level matching in the Lili Mou paper you cite.\n- Is there a reason you use the same parameters for preprocessing the question and answer in (1)? These could require different things to be weighted highly.\n",
        "  Barchans are dunes of high mobility which have a crescent shape and propagate\nunder conditions of unidirectional wind. However, sand dunes only appear above\na critical size, which scales with the saturation distance of the sand flux [P.\nHersen, S. Douady, and B. Andreotti, Phys. Rev. Lett. {\\bf{89,}} 264301 (2002);\nB. Andreotti, P. Claudin, and S. Douady, Eur. Phys. J. B {\\bf{28,}} 321 (2002);\nG. Sauermann, K. Kroy, and H. J. Herrmann, Phys. Rev. E {\\bf{64,}} 31305\n(2001)]. It has been suggested by P. Hersen, S. Douady, and B. Andreotti, Phys.\nRev. Lett. {\\bf{89,}} 264301 (2002) that this flux fetch distance is itself\nconstant. Indeed, this could not explain the proto size of barchan dunes, which\noften occur in coastal areas of high litoral drift, and the scale of dunes on\nMars. In the present work, we show from three dimensional calculations of sand\ntransport that the size and the shape of the minimal barchan dune depend on the\nwind friction speed and the sand flux on the area between dunes in a field. Our\nresults explain the common appearance of barchans a few tens of centimeter high\nwhich are observed along coasts. Furthermore, we find that the rate at which\ngrains enter saltation on Mars is one order of magnitude higher than on Earth,\nand is relevant to correctly obtain the minimal dune size on Mars.\n",
        " Learn about at-risk communities in both developed nations as as well as in under-developed countries. Take a class or doing some research or discussion.;\n,, Volunteering often involves mundane and simple activities, but these mundane and simple activities are the very things that are vital and missing from that community and contribute to it being at-risk. Take the task with an element of humility and a vision of the larger purpose and plan. Cherish the individuals that you are affecting through your direct efforts.\n\n,, Push your comfort zones.\n\n, (Introduce them to wikiHow!) Adjust those lessons to make sense and work in their specific situation. Be their biggest fan in both their own goals and in their implementing of your advice.\n\n, Each individual will have their own outlook and set of circumstances. The learning and exchange process may even be very mutual. But eventually you may be able to coach one or more these persons to help and then co-mentor with you. Your efforts will begin to multiply!\n\n, Introduce a Habitat for Humanity project into the neighborhood.\n\n, Develop their social network. Start a community watch from this, if there isn't one already.\n\n, Provide them with the incentives that you gained (progress, excitement, appreciation, etc) in order to encourage them to join or support your efforts with regular time or financial support, or with their work expertise. Time and money are both necessary resources to develop individuals, families, and communities.\n\n",
        "  We present maps of 7.78 square degrees of the Lupus molecular cloud complex\nat 24, 70, and $160\\:\\mu$m. They were made with the Spitzer Space Telescope's\nMultiband Imaging Photometer for Spitzer (MIPS) instrument as part of the\nSpitzer Legacy Program, ``From Molecular Cores to Planet-Forming Disks'' (c2d).\nThe maps cover three separate regions in Lupus, denoted I, III, and IV. We\ndiscuss the c2d pipeline and how our data processing differs from it. We\ncompare source counts in the three regions with two other data sets and\npredicted star counts from the Wainscoat model. This comparison shows the\ncontribution from background galaxies in Lupus I. We also create two color\nmagnitude diagrams using the 2MASS and MIPS data. From these results, we can\nidentify background galaxies and distinguish them from probable young stellar\nobjects. The sources in our catalogs are classified based on their spectral\nenergy distribution (SED) from 2MASS and Spitzer wavelengths to create a sample\nof young stellar object candidates. From 2MASS data, we create extinction maps\nfor each region and note a strong corresponence between the extinction and the\n$160\\:\\mu$m emission. The masses we derived in each Lupus cloud from our\nextinction maps are compared to masses estimated from $^{13}$CO and C$^{18}$O\nand found to be similar to our extinction masses in some regions, but\nsignificantly different in others. Finally, based on our color-magnitude\ndiagrams, we selected 12 of our reddest candidate young stellar objects for\nindividual discussion. Five of the 12 appear to be newly-discovered YSOs.\n",
        "Actually, right at the time of his death, there were strong indications from the Khmer Rouge that they were considering turning him over to the international community for a tribunal. Unfortunately he died before this could happen, so from that regard it's really a timing thing.\n\nNow, if you're asking as to why he was never brought to justice earlier, I'm a little unsure of how that might be done. When these atrocities were happening, it was when Pol Pot was in control of an entire nation. Even after he lost power, he didn't retire to some townhouse but instead fled with the Khmer Rouge to establish a military resistance. So I mean in those regards, it's really a logistics type thing. In terms of modern comparisons, Kim Jong Il was never \"brought to justice\" for his alleged crimes against humanity, and even Kony, who was indicted by the ICC, is still running around in the jungles. It's not as if they could send police officers to his door; it would involve an entire military campaign (which was in fact being conducted against him when he was exile, all the way to his death).\n\nSo I mean to me, it seems much easier said than done to suggest that we \"bring him to justice.\" I suppose there is also the implication of why the international community didn't send a task force or more aggressively oppose his regime or hunt him after his fall, but there are obviously a ton of geopolitical and diplomatic issues at play there which I don't think I should speculate on. I hope this helps answer at least part of your question; to be honest, I wasn't entirely sure what you were trying to get at.",
        "This paper builds on the work of Weston (2016), using End-to-end memory network models for a limited form of dialogue with teacher feedback. As the authors state in the comments, it is closely related to the question answering problem with the exception that a teacher provides a response after the model’s answer, which does not always come with a positive reward. Thus, the model must learn to use the teacher’s feedback to significantly improve performance.\n\nOverall, the paper is written clearly, and several interesting models are tested. It is certainly only a limited form of dialogue that is considered (closer to question answering, since the questions do not require the agent to look further back into the context), but investigating in this direction could prove fruitful once the tasks are scaled up to be more difficult.\n\nMy main concern is with the paper`s novelty. In the words of the authors, this paper has two primary differences with the work of Weston:\n\n“(i) That earlier work did not use the natural reinforcement learning/online setting, but “cheated” with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language.”\n\nPoint (ii) is very much appreciated, but adding additional human testing data is not sufficient for a conference paper. Thus, the main point of the paper is that “the model also works if we collect the data online (i.e. the agent’s policy is used to collect data rather than a fixed policy beforehand)”. While this is a step in the right direction, I’m not sure if it’s significant enough for an ICLR paper. Little model novelty is required to solve this additional requirement on these tasks beyond using epsilon greedy exploration. Thus, the paper is borderline accept/reject. \n\nEDIT: I have updated my score slightly in light of the author's response, where they make a good point that real-world implementation should be more strongly considered as part of the contribution.\n",
        "  We show that dynamical relaxation in the aftermath of a galactic merger and\nthe ensuing formation and decay of a binary massive black hole (MBH), are\ndominated by massive perturbers (MPs) such as giant molecular clouds or\nclusters. MPs accelerate relaxation by orders of magnitude relative to 2-body\nstellar relaxation alone, and efficiently scatter stars into the binary MBH's\norbit. The 3-body star-binary MBH interactions shrink the binary MBH to the\npoint where energy losses from the emission of gravitational waves (GW) lead to\nrapid coalescence. We model this process based on observed and simulated MP\ndistributions and take into account the decreased efficiency of the star-binary\nMBH interaction due to acceleration in the galactic potential. We show that\nmergers of gas-rich galactic nuclei lead to binary MBH coalescence well within\nthe Hubble time. Moreover, lower-mass binary MBHs (<10^8 Msun) require only a\nfew percent of the typical gas mass in a post-merger nucleus to coalesce in a\nHubble time. The fate of a binary MBH in a gas poor galactic merger is less\ncertain, although massive stellar structures (e.g. clusters, stellar rings)\ncould likewise lead to efficient coalescence. These coalescence events are\nobservable by their strong GW emission. MPs thus increase the cosmic rate of\nsuch GW events, lead to a higher mass deficit in the merged galactic core and\nsuppress the formation of triple MBH systems and the resulting ejection of MBHs\ninto intergalactic space.\n",
        "The way a solid object stays together as one solid object is because of the forces between the atoms and molecules inside the object. If you push or pull one end of it, that pushes or pulls the bit next to it, which pushes or pulls the bit next to that, and so on and so on until the whole object appears to move together.\n\nHowever, it actually takes some time for a push or pull on one side of an object to reach the other side. This is actually much slower than the speed of light - it's the speed of sound. This means there's no need for the entire object to get pulled in at once - one part could cross the event horizon before the other end even knows what's happening.\n\nAlso, this strength is of course not infinite - if the strength of the bonds in the solid object are weaker than the tidal forces of the black hole, the object will get ripped apart.\n\nSo the short answer is that it would break down whatever is falling into it.\n\nThe black hole would also expand a bit, just because it's gaining extra mass. But it's not like it's opening its mouth to suck in something bigger than its diameter.",
        "The short answer is yes, they did think all that.  Or most of it, outright conquering most of China wasn't really part of the plan.\n\nThe key word here is autarky.  At the end of WWI, the leaders of the Japanese Empire was stunned by the defeat of the Prussian forces and the Central Powers in general.  Analyzing things they reached the conclusion that one of the critical, and new, factors was the fact that WWI was a **WORLD** war and that unlike in prior wars where the various neutral powers had made loans and sold supplies to various sides, in WWI the Central Powers had been choked off from the necessary war making supplies due to the fact that most other powers had either picked a side or had remained neutral in a new way that included not making loans or selling supplies.\n\nThere was, therefore, a large faction in the Japanese leadership strongly advocating for autarky, the expansion of Japan's sphere of influence and control until it directly commanded all of the necessary war making materials and was not dependent on foreign trade to import the necessary goods to continue a war.  Petroleum was high on that list, but making war then also needed rubber, tungsten, manganese, and a wide variety of somewhat scarce materials that simply did not exist in any quantity in the Japanese islands. [1]\n\nJapan was also looking at the world through the lens of colonialism, and saw their natural sphere of influence as being East Asia and the Asian Pacific.  The European colonial powers were viewed as interlopers who needed to be evicted so that Japan could take their place.  Their reasoning was that Europe should colonize Africa, and America colonize South America, and leave East Asia for Japan to colonize. \n\nJapan already had most of Korea and large sections of North East China.  Their goal was not so much to actually own outright the South Pacific and the rest of China, but to take over the colonial quasi-ownership that the European powers had put in place there.  Korea and Manchuko were planned to be directly colonized by ethnic Japanese moving in and remaking the native cultures into something more Japanese, not so much the rest of the Empire.\n\nAs for the US, Japan's plan was influenced both by cultural attitudes and their own history.\n\nFor this we need to go back to the Russo/Japanese war.  Russia had taken Port Arthur (now  Lüshun City), then as now an essential railhead and deep water port.  Japan easily defeated the small Russian naval force and besieged the city in what was really a precursor to WWI complete with godawful static trench warfare, massive casualties for minuscule gains, and all the other things that made WWI so horrible.\n\nThe Russian Empire decided to send the Baltic Fleet to break the siege, a decision complicated by the fact that the Russian Empire was then fighting the British Empire and couldn't take the Suez Canal.  It took nearly a year for the Baltic Fleet to get all the way to Japan, and the plan was to run the strait of Tsushima up to Vladivostok and get in some maintenance, repair, resupply, and R & R for the sailors before heading back out to  engage the Japanese near Port Arthur.\n\nInstead the Japanese Navy (mostly by incredible good luck) caught the Russian navy in the strait of Tsushima and defeated them in the single most one sided naval victory that has ever taken place.  Japan lot three torpedo boats while sinking six Russian battleships and fourteen other ships, capturing Russian seven ships, and seeing a tiny handful flee, broken, to Vladivostok.\n\nShortly afterward, deciding basically that Port Arthur wasn't worth the trouble, the Tsar made peace on favorable terms with the Japanese.\n\n***********************\n\nThat was what Japan was looking to duplicate with America.\n\nThey didn't want to, or plan to, fight an extended war with the USA.  \n\nJapan's leadership knew that America's main interests and sympathies were in Europe, they knew that FDR was desperate to get into the war on the UK's side and fight against Hitler, so they hoped that a show of strength, a quick decisive battle that devastated the US navy as they had devastated the Russian Baltic Fleet, would convince America to do as the Russians had done and basically pull out of East Asia and leave Japan to solidify and expand its colonial possessions there, thus achieving autarky and finally being safe from foreign attack.\n\nWith the US out of the picture and the other European powers fighting among themselves, Japan intended to evict the Dutch, French, Germans, and so on from their various East Asian colonial possessions, take over the administration of those possessions, and thus gain the materials produced there.  \n\n[1] Rubber was not only necessary for tubing and tires, but also consumed in various industrial processes (today we use synthetic products and not so much natural rubber for the same processes), over 100 pounds of rubber were simply used up in making the parts needed for a tank, for example.\n\nManganese is critical for producing steel in any quantity.  It only takes a kilo or so per tonne, but without that kilo of manganese you get a spongy mess of useless crap, not useful steel.\n\nTungsten is necessary for hardening steel to make metal cutting tools, armor, armor piercing shells, and more.\n\nAll those materials came from either China, or the South Pacific.  There are no useful sources of any in either Japan, Europe, or the USA.  There's also sources in South America and Africa, but they weren't very developed when WWII started, and one of the big projects that doesn't get a lot of attention was the US development of those sources during WWII so they could keep the factories going.\n\nEDIT: Sources\n\n*Japan Prepares for Total War*, Michael Barnhart\n\n*War and National Reinvention - Japan in the Great War*, Frederick Dickinson\n\n*Japanese Imperialism*, W.G. Beasley",
        " A green top is the cornerstone of any leprechaun costume. The simplest way to create this base is to wear a solid or checkered green button-down shirt.Avoid wearing a T-shirt. Leprechauns are old world creatures, and the look of a button-down shirt creates a classy, antiquated ambiance more effectively than a T-shirt would.\nAlternatively, consider a green jacket and white shirt. Even though a green button-down shirt is the easiest way to start making your costume, a more traditional version of leprechaun attire is a white button-down shirt paired with a solid green suit jacket.;\n, The idea is to be green from head to toe. Green trouser pants are the perfect article of clothing to pair with your green top, but you could also opt for green shorts made from trouser material if you want to wear your costume when the weather is on the warmer side.\n\n\nIf you have trouble finding green pants, you can purchase white pants and use fabric dye to dye the garment a bold shade of green.\n\n, While girls and women can pull off a green top and green slacks, they also have the option to slip on a green dress, or to pair a green top with a green skirt.If working with a green skirt, you can still pair it with a green button-down shirt or white button-down shirt with a green suit jacket.\nDepending on the print of the jacket and the print of the dress, you could even shrug a green suit jacket over a green dress. Try to keep the jacket a solid color rather than a print, though, and make sure that the shade of green on the dress is a little different from the green on the jacket.\n\n, If your legs are exposed or partially exposed, wear a pair of white knee-high socks to cover your calves.This is important if you are wearing shorts, a skirt, or a dress. If you are wearing long pants, you should still wear white socks, but they do not necessarily need to be knee-high since any portion of the sock covering your calf will not be visible, anyway.\n\n, Green top hats and pilgrim-style hats are usually fairly easy to find around St. Patrick's Day, but you might even be able to find a hat that can work during another time of year by searching online stores, thrift stores, or costume stores.You could make a leprechaun hat out of paper or fabric if you do not want to buy one or are having trouble finding a pre-made hat you like.\nOne of the easiest ways to make a green hat if you cannot find one is simply to purchase a top hot or bowler hat from a thrift store and spray paint it green.\n\n, Make sure that the belt buckle is gold. If it is not already gold, use a bit of acrylic paint to paint the buckle gold.\n\nIf you go with a dark green belt, make sure that the green is darker than the green of your trousers.\n\n, Find a green necktie or green bow-tie that matches your pants or jacket. Tie this around your neck for a classy leprechaun look.You could use a solid green tie, or you could liven things up with a green checkered pattern. A pattern works especially well if you are wearing a solid green shirt or jacket and solid green bottoms.\n\n, For the gold, you can use gold-wrapped candy coins or plastic gold tokens. Carry the fake gold in a black plastic kettle, black bowl, or satchel style pouch.You could also make fake nuggets of gold by spray painting small, smooth stones with gold spray paint.\n\n, Look for black slip-on shoes without laces. Spray paint the shoes using acrylic craft paint in green glitter or metallic green.\n\n\nIt is an option to leave the shoes black. Black shoes with a buckle can have a traditional leprechaun appearance, but painting the shoes green can add a bit of flair to your costume.\nIf you do not have an old pair of shoes you are willing to paint, check out some nearby thrift stores for a pair that might work.\nAlternatively, apply green glitter with Mod Podge. If you cannot find green spray paint, apply Mod Podge or another type of craft adhesive to the shoes and sprinkle a coating of green glitter over it., Trace and cut two smaller rectangles out of this larger rectangle to form the shape of a buckle.\n\n\nThe final shape of your buckle should be a rectangular frame with one rectangle cut out of the top and a second cut out of the bottom. There should be a cardboard bar separating the two final rectangular cut-outs.\nCut out two of these buckle-shaped pieces of cardboard. You will need one for each shoe.\nEach buckle should be about 3 inches (7.6 cm) wide and 4 inches (10 cm) tall.\n\n, Use acrylic craft paint or spray paint to color this piece of cardboard gold.\n\n, The strip should be skinny enough to fit through the rectangular cut-outs of your cardboard buckle.\n\n\nThe length of each strip should be about 6 or 7 inches (15.24 or 17.78 cm). You will need two strips (one for each buckle and each shoe).\n\n, The construction paper strip should go under the outer frame, over the middle bar, and under the outer frame again.Center the cardboard buckle in the middle of your strip.\n\n, The strip of paper should run vertically, and the buckle should be taped to the shoe so that the buckle lies at the front top with the paper strip standing just above it, as though it were the tongue of the shoe.Use heavy, durable tape so that the paper buckle stays securely in place.\n\n, Trace the shape of a beard or beard and mustache combination on a letter-size sheet of orange or brown construction paper. The length of the beard outline should span nearly the entire length of the paper, and the width of the beard should take up 2/3 to the full width of the paper.To get a more accurate perspective, hold the piece of paper up to your face and mark the distance from ear to ear. This is the width you will need for your paper beard. The length does not need to be quite as precise, though.\n\n, Cut width-wise strips of orange construction paper, each with a width of roughly 1 inch (2.5 cm)., Take each strip of orange paper and wrap it around a pencil or pen to create paper curls.Wrap the paper tightly as you work. If you wrap the paper too loose, the curls will not hold well.\n\n, Apply a line of glue to the bottom of the beard. Press orange curls onto the glue, following the outline, until the entire bottom row is filled with bushy orange curls.Continue attaching the orange curls in this manner, working in rows from the bottom to the top of the beard base.\nWhen done, none of the base should be visible, and the entire thing should be covered with bushy orange curls.\nMake sure you allow time for the glue to dry.\n\n, Punch holes near the top of the beard, one on each cheek end. Tie a long piece of yarn to one hole and another piece to the other hole.The pieces of yarn should be long enough so that you can wrap them around the back of your head and be tied together into a shoelace bow.\nAlternatively, buy a beard instead of making one. You can buy orange costume beard around St. Patrick's Day or around Halloween. If you want to buy one during any other time of the year, look in a costume store, party store, or online store.\n\n, The ball of clay should be about 3 inches (7.6 cm) in diameter.Make sure that the clay is well kneaded, first. Kneading the clay makes it soft and easy to work with. If you do not knead the clay, it will be more likely to crack and break as you work with it.\nForm the ball into the shape of the pipe bowl as you attach it to the dowel. Use your thumb to create an imprint in the ball, making “walls” around your thumb and creating a bucket inside the center.\n\n, Cut the wire to your desired stem length. , This layer of clay should be about 1/2 inch (1.25 cm) thick. Make sure to create a mouthpiece for the stem., Direct the wire through the opening of the bowl. Connect the stem and opening of the bowl.Avoid making the clay look too smooth as you layer it on. Use your thumb or fingertips to leave shallow impressions along the clay, giving it a lumpy and rustic look. Or, use a sculpting tool (like a dull blade) to make ornamental designs in the clay., Place the pipe baking parchment paper., The clay needs to be baked in an oven preheated between 210 to 220 degrees Fahrenheit (98.9 to 104.4 degrees Celsius) for 45 minutes to an hour.Check the clay frequently as it bakes to make sure that it is not burning. The exact amount of time you will need to bake the clay varies by brand, but it usually falls out to 15 or 30 minutes per 1/4 inch (0.625 cm) of thickness.\nPlace the clay pipe on a baking sheet lined with aluminum foil, wax paper, or parchment paper when you bake it.\n\n, Apply a coat of light to medium brown acrylic paint to the clay pipe after it cools.The color should be brown like wood, but you can use colors like green or orange for extra embellishment.\n\n, Use a soft cloth to rub a coating of clay varnish onto the paint after it dries. Make sure you allow the varnish to dry completely., While a homemade leprechaun pipe can add whimsical charm to your costume, if you are short on time or crafting supplies, you can usually find an old-fashioned pipe in a costume store, thrift store, or online.\n\n",
        "I researched this event extensively for my first novel and I'm happy to share what I found.  \n  \nThe program you're talking about was *The Adventures of Superman* which aired from 1940 through 1951 and was sponsored by Kellogg's Pep Cereal (that's going to be important in this story later). Keen comic book historians will note that this show began airing a mere two years after Superman's first comic book appearance in 1938 (Action Comics #1). This shows just how quickly Superman, and the whole concept of superheroes, had risen in popularity (in the wake of Superman, you had Batman, the Human Torch, Namor, and many lesser known debuts before the '30s ended). By the mid-40s, the entire nation was obsessed with superheroes and, of that group, Superman was king. \n  \nOf course, one of the biggest reasons for Superman's rapid rise of popularity (and superheroes in general) was WWII. In every iteration (comic book, newspaper strip, radio show), Superman battled Nazis and Axis powers and bolstered the morale of American boys and girls nervous about the fates of their fathers and brothers overseas. It was the perfect combination, a villain so powerful and evil it could only be met by a hero with superpowers.  \n  \nAnd then the War ended in 1945 and the creators of these superhero stories were faced with a dilemma. Whom would the most powerful beings in the world face now that Hitler was gone?  \n  \nEnter human rights activist and author Stetson Kennedy. Kennedy wanted to expose the inner workings of the Ku Klux Klan to the world and so infiltrated the Klan and recorded everything he witnessed. (It should be noted that the Klan during the '40s is considered the Second Klan, and was different from the Klan of the '60s or the Klan of the 1800s. The nation was greatly divided in their opinions about the Klan and many associated the Klan with fascism) Kennedy took all of his notes exposing illegal activity on the part of the Klan to the police, however the local police refused to do anything about it, reportedly because the Klan had infiltrated the police force.  \n  \nAnd so you had a group of writers looking for a new villain comparable to the Nazis and an activist wanting to expose a group many saw as fascists in America. It was a match made in heaven.  \n  \n\"The Clan of the Fiery Cross\" was a fifteen part series that aired in  1946. In the story, an Asian-american kid named Tommy Lee needs Superman's help as he and his family are facing opposition from a mysterious group that wears hoods and terrorizes the minority community. In the series, the writers used the information given to them by Kennedy, including code words, strategies, etc. and built a villain that was recognizable to their audience. And the audience loved it.  \n  \nAnd the Klan did not. The Klan petitioned Kellogg's to order the writers to stop the series or drop the show all together. But the reaction of the public was so unbelievably positive, Kellogg's denied the claim. This would prove to be a fatal blow to the Second Klan. The entire organization fell apart on a national level and, from the '50s through the '60s, any Klans that rose up were local groups with no true national leadership. And when the Klan rose up, they faced a reality that America now viewed them as comic book status villains instead of the freedom fighters they had hoped to portray themselves as.  \n  \nOne trivia item of note, you talked about how Superman had as association with \"all-American\" values, and you probably think of the phrase \"Truth, Justice, and the American way.\" The tag on, \"The American way,\" wasn't included in the radio show and only became part of the Superman vernacular until much later.  \n  \nSources:\n[Listen to the radio series on Youtube](_URL_2_)  \n[Read a summary of the story on Superman Homepage](_URL_1_)  \n[A brief article on Mental Floss](_URL_0_)  \n*The Klan Unmasked* by Stetson Kennedy  \n*Superman: The Complete History: The Life and Times of the Man of Steel* by Les Daniels",
        "  The electric-field-induced resonant spin polarization of a two-dimensional\nhole gas described by Luttinger Hamiltonian with structural inversion asymmetry\nand Zeeman splitting in a perpendicular magnetic field was studied. The spin\npolarization arising from splitting between the light and the heavy hole bands\nshows a resonant peak at a certain magnetic field. Especially, the competition\nbetween the Luttinger term and the structural inversion asymmetry leads to a\nrich resonant peaks structure, and the required magnetic field for the\nresonance may be effectively reduced by enlarging the effective width of the\nquantum well. Furthermore, the Zeeman splitting tends to move the resonant spin\npolarization to a relative high magnetic field and destroy these rich resonant\nspin phenomena. Finally, both the height and the weight of the resonant peak\nincrease as the temperature decreases. It is believed that such resonant spin\nphenomena can be verified in the sample of a two-dimensional hole gas, and it\nmay provide an efficient way to control spin polarization by an external\nelectric field.\n",
        "Parafovea or the parafoveal belt is a region in the retina that circumscribes the fovea and is part of the macula lutea. It is circumscribed by the perifovea.\n\nEffect on reading\nIn reading, information within 1° (approximately 6–8 characters) of the point of fixation is processed in foveal vision, while information up to 6° of visual angle benefits from parafoveal preview. Studies have shown that people can tell the difference in the letters of a word in the fovea and near-parafovea (the part of the parafovea closest to the fovea), but not in the outer edges of the parafovea. In languages that read from left to right, the word immediately to the right of the fixated word is known as the parafoveal word. Information present in the parafovea can interact with information present in the fovea. The benefit the parafoveal preview has is also mediated by how common the word in the parafovea is, with less common words providing less of a reduction in fixation duration when they reach foveal fixation. As the clarity of information in the parafovea is not as great as in the fovea, the SWIFT model of eye movements in reading, while allowing for parallel processing, accounts for this difference by assigning the parafoveal less processing power the further away it is from the foveal fixation.\n\nEffect on scene perception\nInformation in the parafovea can influence the processing of a scene. In categorization tasks of natural scenes information from the parafovea can be used to determine the gist of a scene well enough for a categorization judgement, though with reduced sensitivity and speed in comparison to foveal vision. An effect of parafoveal preview has also been found for emotional scenes presented in the parafovea, with people more likely shift their fixation point on emotional stimuli than neutral stimuli, when both options are presented parafoveally.\n\nAdditional images\n\nSee also\n Eye movement in reading\n Eye movement in music reading\n Fixation (visual)\n Optical coherence tomography (OCT)\n\nReferences\n\nHuman eye anatomy\nVisual perception",
        "This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \"traditional wisdom\".\n\nSpecifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. \n\nWhile this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \"ignores\" 2 major recent works that are in direct contradictions with many claims of the paper ([a] \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by  Gordo et al. and [b] \"CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples\" by Radenović et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.\n\nHere are some of the misleading claims: \n\n  - \"Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.\"\n  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).\n  \n  - \"the proposed method [...] outperforms the state-of-the-art methods on four typical datasets\"\n  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].\n  \n  - \"Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option.\".\n  This is a questionable opinion. The method exposed in \"End-to-end Learning of Deep Visual Representations for Image Retrieval\" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.\n\n  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.\n\nIn addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). \n\nTo conclude, the paper is one year too late with respect to recent developments in the state of the art.",
        "I'm an anesthesiologist. We monitor body temperature during surgery because anesthesia inhibits your ability to autoregulate temperature. Essentially you are turned into a poikilotherm like a snake, and lose heat to the cold operating room. An inability to contract your muscles prevents you from generating heat. We have a rule of thumb that 1 liter of room temperature intravenous fluids will reduce a patient's body temperature by 0.25 degrees Celsius. We used forced air warming blankets and heated IV fluids to maintain a normal body temperature, which helps the body to metabolize medications predictably and the blood to clot properly. \n\nAfter reading comments I want to add that the reason I brought up anesthesia here is that only when you remove the body's ability to generate heat can you actually measure a reduction in temperature, unless you infuse the fluid very quickly. When we drink cold fluids, the body generates heat to correct the drop in temperature before an appreciable difference can be measured. \n\nFurthermore, there are some interesting studies out there on this. Many involve rapidly administering cold IV fluids in attempt to show that hypothermia is protective against neurologic injury in situations such as cardiac arrest. \n\nHere is one study: \n\nAnn Emerg Med. 2008 Feb;51(2):153-9. Epub 2007 Nov 28.\n\nThey infuse cold and room temperature fluids rapidly in non anesthetized patients and measure a temperature change before compensatory mechanisms (shivering) can restore the body to normal temp. This is better than my rule of thumb as it uses weight-based dosing for IV fluids. Interesting, 30ml/kg of room temp fluid reduced the body core temp by 0.5 Celsius degrees. That would be 3 liters of fluid for a 100kg (220lb) person. Cold fluid reduced the body temp by a full degree Celsius.",
        "Ettore Ciccotti (Potenza, 23 March 1863 - Rome, 20 May 1939) was a historian, lecturer and politician from Italy, member of both the Italian Chamber of Deputies and Italian Senate.\n\nEarly life\nBorn into a liberal family of the lawyer Pasquale Ciccotti, a landowner and several times mayor of Potenza, he studied in the local high school. In 1879 he enrolled at the Law Faculty of the University of Naples. He became a follower of Mazzini and adhered to Italian irredentism. He had a particular interest in both ancient history and for the social problems of Southern Italy, inspired by the example of the historian Giustino Fortunato.\n\nCiccotti, raised in the poor southern region of Basilicata, adhered to the group of socio-political thinker's known as meridionalisti (\"southernists\"), aspiring to solve the economic problems of Southern Italy after the Italian unification. They claimed that the economic policies of the central government of the new state discriminated against the interests of the south while favoring those of north.\n\nAcademic and political activist\nIn 1889, Ciccotti attended the University of Rome and gained a teaching qualification in classical antiquities. He won the competition for the ancient history chair at the Scientific-Literary Academy (Accademia scientifico-letteraria) in Milan in 1891. Meanwhile, he started to cooperate with the socialist Filippo Turati and his journal Critica Sociale. He adhered to the Italian Socialist Party (PSI), where he raised the issue of the underdevelopment of Southern Italy. His political involvement caused him the hostility of the Milanese conservatives, and in 1897 he lost his position at the Academy.\n\nHe was appointed professor of ancient history at the University of Pavia, but his attacks on the government and solidarity for the workers on the occasion of the tragic events in Milan in May 1898 earned him an arrest warrant for subversive incitement. He went into exile, taking refuge in Geneva (Switzerland), hosted by Maffeo Pantaleoni. Here he met Vilfredo Pareto and the German social-democrat August Bebel, and wrote a report on the events in Milan, The revolt of Milan: Notes of a refugee, but lost his job at Pavia.\n\nAs a historian, Ciccotti was the first to give a Marxist economic account of the decline of slavery in the Roman Empire, in contrast to religious-ethical explanations in his book Il tramonto della schiavitu nel mondo antico (The Sunset of Slavery in the Ancient World), published in 1899. Economic changes rendered slavery expensive and inefficient, and doomed it to extinction.\n\nPolitical career\nIn June 1900, he was elected in the Italian Chamber of Deputies (1900-1904) in the Vicaria district of Naples, upsetting the traditional electoral alliance between local politicians and the Camorra. In 1904, he lost his seat due to active campaigning of the Camorra against him. He was re-elected in 1909 and remained a Deputy until September 1919. Meanwhile, Ciccotti initiated the translation in Italian of the major works of the Socialist theorists Karl Marx, Friedrich Engels and Ferdinand Lassalle.\n\nIn the decade before the First World War, he gradually distanced himself more and more from the official line of the Socialist Party, although he kept on contributing to the socialist newspaper Avanti!. He criticized the party, which he considered to be too much focused on the problems of the \"Northern\", industrialist working man, and too little inclined to understand the problems of the impoverished rural populace of the South. Ciccotti defined the prejudices against southern Italians as the anti-Semitism of Italy. In contrast with the PSI, he was in favour of an Italian intervention in the First World War.\n\nSenator and death\nHis pro-war stance and dislike of the liberal Giovanni Giolitti – Ciccotti called Giolittianism the \"death of political life\" –, and in opposition to the post-war revolutionary movements, he looked sympathetically at the rising Fascism an its leader Benito Mussolini. \"In today's evident scarcity of political personalities, Mussolini is the one that more than any other, if not the only one, can deserve this name,\" Ciccotti wrote in September 1922.\n\nIn September 1924, he was rewarded with a seat for life in the Italian Senate. He opposed the move to authoritarian rule of Mussolini, but did not resign. He mainly dedicated his time to study history. In 1931, when demanded to pledge an oath of allegiance to the Fascist regime, Ciccotti initially protested, but eventually took the oath all the same. Finally he opposed, and openly, to the dictatorship of Mussolini when he sensed that the regime was heading to the adventure of a war, which he considered the inevitable conclusion of authoritarian and populist regimes. He died in Rome on 20 May 1939 at the age of 76.\n\nReferences\n\n De Grand, Alexander J. (2001). The hunchback's tailor: Giovanni Giolitti and liberal Italy from the challenge of mass politics to the rise of fascism, 1882-1922, Wesport/London: Praeger,  (online edition)\n Falasca-Zamponi, Simonetta (1997).  Fascist Spectacle: The Aesthetics of Power in Mussolini's Italy, Berkeley: University of California Press, \n Harrill, J. Albert (1995). The Manumission of Slaves in Early Christianity, Tübingen: Mohr, \n Huysseune, Michel (2006). Modernity and Secession: The Social Sciences and the Political Discourse of the Lega Nord in Italy, New York/Oxford: Berghahn Books, \n Snowden, Frank M. (1995) Naples in the Time of Cholera, 1884-1911, Cambridge: Cambridge University Press, \n\n1863 births\n1939 deaths\nPeople from Potenza\nItalian Socialist Party politicians\nDeputies of Legislature XXI of the Kingdom of Italy\nDeputies of Legislature XXII of the Kingdom of Italy\nDeputies of Legislature XXIII of the Kingdom of Italy\nDeputies of Legislature XXIV of the Kingdom of Italy\nMembers of the Senate of the Kingdom of Italy\nPoliticians of Basilicata\nItalian historians",
        "Kengzhyra is a village in Almaty Region, in south-eastern Kazakhstan.\n\nReferences\n\nPopulated places in Almaty Region",
        "  We have carried out a survey toward the central regions of 85 starless cores\nin HCN J = 1-0 to study inward motions in the cores. Sixty-four cores were\ndetected with HCN lines. The infall asymmetry in the HCN spectra is found to be\nmore prevalent, and more prominent than in any other previously used infall\ntracers such as CS J = 2-1, DCO+ J = 2-1, and N2H+ J = 1-0. We found close\nrelation between the intensities of the HCN and N2H+ lines. This implies that\nthe HCN is not much depleted in the central regions of the cores. In some\ncores, the HCN spectra show different sign of asymmetry from other molecular\nlines. A few cores show various signs of asymmetry in individual HCN hyperfine\nlines. The distribution of the velocity shift dV of the HCN profiles with\nrespect to the systemic velocity of the optically thin tracer is found to be\nmore shifted toward bluer side than those of other infall tracers, indicating\nthat the HCN traces inward motions more frequently. The dV distribution of each\nHCN hyperfine line for all sources is similar. Moreover the dV values obtained\nfrom different HCN hyperfine lines for each source are nearly similar. These\nmay mean that most of starless cores are in similar kinematic states across the\nlayers of the cores. We identify 17 infall candidates using all available\nindicators such as the velocity shift dV and the blue to red peak intensity\nratio of double peaked profiles for HCN J = 1-0, CS J = 2-1, J = 3-2, DCO+ J =\n2-1, and N2H+ J = 1-0. Four of them, L63, L492, L694-2, and L1197 are found to\nshow higher blue to red ratio in the HCN hyperfine line along the lower\nopacity, suggesting that infall speed becomes higher toward the center.\n",
        " If you read lots of books that inspire you to write your own stories, then you will have a lot to fire your imagination with.\n\n,, Think about how you can combine the ideas.\n\n, Include everything from the beginning to the end. Keep in mind it's okay to change the ending when you get to it.\n\n, Be sure to use your own words. Ensure that your writing isn't like any of the books you've been inspired by.\n\n, Then present to a friend, family member, etc. and see what they have to say.\n\n, Try finding a picture that has an interesting subject, or is abstract.\n\n, Look at every aspect of the photo.\n\n, Write a list about subjects of the photo, or objects. For example, is the setting in a hot or cold climate? Is the photo still-life, computerized or even a drawing?\n\n, There's no real art to it, just let your hand write and the words flow! Who knows, you may come up with a bestseller!\n\n, You may not see anything worth writing about at first but soon you'll realize that there is inspiration all around you.\n\n, Maybe something exciting has happened in your family that would make a great story with a bit of tweaking.\nAt school. Do you have a nasty teacher you could describe? Or is there a really mean bully that is story-worthy?\nAt a friend's house. Do you have any odd friends with strange families or any spooky houses?\nAt the park. See a strange looking person, goth teenager or an angry business man that you could write about? Or even some interesting wildlife.\nIn the forest. Plenty of amazing nature.\n\n,\nAt a mate's house.\nAt a sporting event of some sort.\nIn your family\n\n,, You'll find out all sorts of interesting things for use in writing stories. Find people, any people, and strike up conversation. Ask them about: And more\n\n\nWhat they did in the weekend\nTheir family\nTheir pets\nWho their idol is\nWhat they watch on television\nWhere they shop\nWhat they love to eat\n\n, Or even the perfect end to your developing novel!\n\n, One by one, go through your old notebooks, checking to see if there are any unfinished stories that could have potential or any scribbled ideas that you assumed were rubbish but are actually golden nuggets.\n\n, Just tweak them to suit you.\n\n, There are plenty of good websites out there for writers block such as:\n\n\nhttp://writingprompts.tumblr.com\nhttp://creativewritingprompts.com\n\nhttp://writers-den.pantomimepony.co.uk/writers-plot-ideas.php.\n\n,",
        " Submit it to various search engines and directories. In addition to major search engines such as Google, Yahoo, Ask, Bing and MSN, it's worth submitting to lesser known web directories. If your website is listed in as many places as possible, it can boost your rankings in major search engines as well.;\n,,, Contact the owners of other websites with your proposal to exchange links. You will add their link to your links page, and they will link to your website in return.\n\n, Visit the online discussion forums where people in your target market like to gather. Post useful responses to people's questions, and include a link to your website at the end of your post next to your name. Although you ultimately want to get your sig file with the link to your site seen as often as possible, don't overdo it with your posts, otherwise other forumites won't respect you, and therefore won't visit your site.\n\n, Visit other people's blogs on the topic related to your website. Most blogs allow you to add comments about the issues being discussed. Add insightful comments about the blog topics, and leave your website link. Your comment and your link will permanently remain on the blog website.\n\n, Set your e-mail program to automatically add your signature to every e-mail that you send.\n\n, There are many types of traffic exchanges but the idea is the same - you team up with other webmasters, and they send you traffic in exchange for you sending them traffic. Search the web for \"traffic exchange\" to find those services.\n\n, It can be a report with useful information that you allow people to give away. It can also be some kind of cool and unusual web-page that people will want to e-mail to all their friends. Of course, the pass-around item will include your advertisement and your link, spreading the word about your website. This is a \"viral\" strategy.\n\n, Submit your articles to article directories, and allow people to publish your article in their newsletters and websites. Your articles will spread around the web like wildfire. People will read them and visit your website. The best part is that people who read your articles will regard you as an expert, and they will be much more likely to buy from you when they go to your website. How's that for a free publicity?\n\n, The point of keywords is that they are the terms most people looking for your article by topic will search by. Check those with free Google terms and use the most popular relevant one in your title - a descriptive title to let them know right away what it is if it's a word with multiple meanings. Drawing blood for a phlebotomist is not the same thing as a comics artist drawing blood in a fight scene. Be very literal and then use the exact keywords several times in the article where they are relevant, as well as synonyms. Don't overdo it, or you look like you're trying to spam Google. Just use the keywords naturally when describing the topic.\n\n\nIf it is a confusing keyword phrase like Drawing Blood, link to the other meaning and mention that in the first paragraph. \"This article is about drawing blood in graphic novels. Phlebotomy is drawing blood for medical tests or donations - here's the other page on \"Drawing Blood.\" Then link to the phlebotomy page in that. By redirecting confused people you leave them a good impression and they might stop to read your article if they also like doing comics art!\n\n, When people read each issue they'll be reminded to revisit your web site. Submit it to all the free e-zine directories on the internet.A newsletter!\n\n, It could be an online message board, e-mail discussion list or chat room. When people get involved in your community, they will regularly return to communicate with others.\n\n, There are many of these on the net. Some of the more popular ones are craigslist, inetgiant, freeadvertisingforum, and gumtree.\n\n, These are lists of subscribers who have opted-in to send and receive emails to each other. You can instantly reach potentially thousands of people by sending out emails, and you don't have to worry about receiving spam complaints either because everyone on the list has opted-in to receive emails.\n\n,,, Your friend's friends will see when they join the fan page or comment on your status.\n\n,,, All this socializing, forum commenting and question answering ought to be on your website's main topic. The more your activity and site content match in topic, the more likely the right people find it. The more targeted your affiliate links are to the topic, the better you'll do with them too. People don't mind advertising that's on topic, it starts to look like a convenience directory rather than irritating commercials.\n\n",
        "The Villanovan culture (c. 900–700 BC), regarded as the earliest phase of the Etruscan civilization, was the earliest Iron Age culture of Italy. It directly followed the Bronze Age Proto-Villanovan culture which branched off from the Urnfield culture of Central Europe. The name derives from the locality of Villanova, a fraction of the municipality of Castenaso in the Metropolitan City of Bologna where, between 1853 and 1855, Giovanni Gozzadini found the remains of a necropolis, bringing to light 193 tombs, of which 179 to cremation and 14 to inhumation.\n\nThe Villanovans introduced iron-working to the Italian Peninsula. They practiced cremation and buried the ashes of their dead in pottery urns of distinctive double-cone shape.\n\nHistory\n\nThe name Villanovan of the early phases of the Etruscan civilization comes from the site of the first archaeological finds relating to this advanced culture, which were remnants of a cemetery found near Villanova (Castenaso, 12 kilometres south-east of Bologna) in northern Italy. The excavation lasting from 1853 to 1855 was done by the scholar and site owner, count Giovanni Gozzadini, and involved 193 tombs, six of which were separated from the rest as if to signify a special social status. The \"well tomb\" pit graves lined with stones contained funerary urns. These had been only sporadically plundered and most were untouched. In 1893, a chance discovery unearthed another distinctive Villanovan necropolis at Verucchio overlooking the Adriatic coastal plain.\n\nThe burial characteristics relate the Villanovan culture to the Central European Urnfield culture (c. 1300–750 BC) and Celtic Hallstatt culture that succeeded the Urnfield culture. It is not possible to tell these apart in their earlier stages. Cremated remains were placed in cinerary urns, specifically in biconical urns and then buried. The urns were a form of Villanovan pottery known as impasto. A custom believed to originate with the Villanovan culture is the usage of hut-shaped urns, which were cinerary urns fashioned like the huts in which the villagers lived. Typical sgraffito decorations of swastikas, meanders, and squares were scratched with a comb-like tool. Urns were accompanied by simple bronze fibulae, razors and rings.\n\nPeriodization\nThe Villanovan culture is broadly divided into Villanovan I from c. 960 BC to c. 801 BC and the Villanovan II from c. 800 BC to 720 BC. The later phase (Villanovan II) saw radical changes, evidence of contact with Hellenic civilization and trade with the north along the Amber Road. This evidence takes the form of glass and amber necklaces for women, armor and horse harness fittings of bronze, and the development of elite graves in contrast to the earlier egalitarian culture. Chamber tombs and inhumation (burial) practices were developed side-by-side with the earlier cremation practices. With the last phase of Villanovan II the Etruscans, in particular Southern Etruria, entered the Orientalizing period. The northernmost areas of the Etruscan world, such as Etruria Padana, continued in their development as Villanovan III (750–680 BC)  and Villanovan IV (680–540 BC).\n\nVillanovan chronology within the Etruscan civilization\n\nMetalwork and trade \nThe metalwork quality found in bronze and pottery demonstrate the skill of the Villanovan artisans. Some grave goods from burial sites display an even higher quality, suggesting the development of societal elites within Villanovan culture. Tools and items were placed in graves suggesting a belief in an afterlife. Men's graves contained weapons, armor, while those for women included weaving tools. A few graves switched or mixed these, indicating the possibility that some women employed tools and that some men made clothing.\n\nDuring the Villanovan period Etruscans traded with other states from the Mediterranean such as Greeks, Balkans, and Sardinia. Trade brought about advancement in metallurgy, and Greek presence influenced Villanovan pottery.\n\nHousing \nBuildings were rectangular in shape. The people lived in small huts, made of wattle and daub with wooden poles for support. Within the huts, cooking stands, utensils and charred animal bones give evidence about the family life of early inhabitants in Italy. Some huts contained large pottery jars for food storage sunk into their floors. There was also a rock cut drain to channel rainwater to communal reservoirs.\n\nVillanovan settlements \nGenerally speaking, Villanovan settlements were centered in the Adriatic Etruria, in Emilia Romagna (in particular, in Bologna and in Verucchio, near Rimini), in Marche (Fermo), and in the Tyrrhenian Etruria, in Tuscany and Lazio. Further south, Villanovan cremation burials are to be found in Campania, at Capua, at the \"princely tombs\" of Pontecagnano near Salerno, at Capo di Fiume, at Vallo di Diano and at Sala Consilina.\n\nSmall scattered Villanovan settlements have left few traces other than their more permanent burial sites, which were set somewhat apart from the settlements—largely because the settlement sites were built over in Etruscan times. Modern opinion generally follows Massimo Pallottino in regarding the Villanovan culture as ancestral to the Etruscan civilization.\n\nGenetics\nA genetic study published in Science in November 2019 examined the remains of a female from the Villanovan culture buried in Veio Grotta Gramiccia, Italy between ca. 900 BC and 800 BC. She carried the maternal haplogroup K1a4. and her autosomal DNA was a mixture of 72.9% Copper Age ancestry (EEF + WHG) and 27.1% Steppe-related ancestry. There was evidence for consanguinity for this sample with another ancient sample (700 BCE - 600 BCE) from the Etruscan necropolis of La Mattonara near Civitavecchia, compatible with being the latter an offspring of third-degree relatives from the former.\n\nSee also\nEtruscans\nProto-Villanovan culture\nPrehistoric Italy\n\nNotes\n\nReferences\n\nSources and further reading\n\n \nGiovanni Gozzadini, La nécropole de Villanova, Fava et Garagnani, Bologna, 1870\nJ. P. Mallory, \"Villanovan Culture\", Encyclopedia of Indo-European Culture, (Fitzroy Dearborn), 1997.\nGilda Bartoloni,  \"The origin and diffusion of Villanovan culture.\" in M. Torelli,   (editor) The Etruscans, pp 53–74. (Milan), 2000.\n Mary E. Moser, The \"Southern Villanovan\" Culture of Campania, (Ann Arbor), 1982.\nDavid Ridgway, \"The Villanovan Cemeteries of Bologna and Pontecagnano\" in Journal of Roman Archaeology 7: pp 303–16 (1994)\nDavid Ridgway, The World of the Early Etruscans, Göteborgs Universitet: The Félix Neubergh Lecture, 2000.\nPerkins, Phil (2017). DNA and Etruscan identity. In: Naso, Alessandro ed. Etruscology. Berlin: De Gruyter, pp. 109–118. URL: https://www.degruyter.com/view/product/128551\n\nExternal links\n\nMuseo Archeologico di Verucchio: Villanovan necropolis (in English)\nAshmolean Museum: Ancient Italy Before the Romans\n Images of Villanovan hut-urn\n\n \n11th-century BC establishments\n7th-century BC disestablishments\n1853 archaeological discoveries\nEtruscans\nArchaeological cultures of Southern Europe\nArchaeological cultures in Italy\nBronze Age cultures of Europe\nIron Age cultures of Europe\nItalic archaeological cultures\nPrehistoric Italy",
        "  We consider a holographic model of QCD from string theory, a la Sakai and\nSugimoto, and study baryons. In this model, mesons are collectively realized as\na five-dimensional \\$U(N_F)=U(1)\\times SU(N_F)$ Yang-Mills field and baryons\nare classically identified as $SU(N_F)$ solitons with a unit Pontryagin number\nand $N_c$ electric charges. The soliton is shown to be very small in the large\n't Hooft coupling limit, allowing us to introduce an effective field ${\\cal\nB}$. Its coupling to the mesons are dictated by the soliton structure, and\nconsists of a direct magnetic coupling to the $SU(N_F)$ field strength as well\nas a minimal coupling to the $U(N_F)$ gauge field. Upon the dimensional\nreduction, this effective action reproduces all interaction terms between\nnucleons and an infinite tower of mesons in a manner consistent with the large\n$N_c$ expansion. We further find that all electromagnetic interactions, as\ninferred from the same effective action via a holographic prescription, are\nmediated by an infinite tower of vector mesons, rendering the baryon\nelectromagnetic form factors completely vector-dominated as well. We estimate\nnucleon-meson couplings and also the anomalous magnetic moments, which compare\nwell with nature.\n",
        "Contemporary literature is literature which is generally set after World War II. Subgenres of contemporary literature include contemporary romance.\n\nHistory\nBelow, contemporary literary movements are listed by decade. The list should not be assumed to be comprehensive.\n\n1930s\n Objectivist poets\n\n1940s\n\n1950s\n Beat Generation\n Black Mountain poets\n Confessional poetry\n New York School\n\n1960s\n British Poetry Revival\n New Wave (science fiction)\n Nouveau roman\n\n1970s\n L=A=N=G=U=A=G=E poets\n\n1980s\n Cyberpunk\n Maximalism\n New Formalism\n Poetry slam\n\n1990s\n Post cyber punk\n\n2000s\n New Weird\n\n2010s\n\n2020s\n\nSee also\n in literature\nModernist literature\nPostmodern literature\n Twentieth-century English literature\n20th century in literature\n2000s in books\n\n \nHistory of literature",
        "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n———\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n————\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (",
        "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. ",
        "Hansard is the traditional name of the transcripts of Parliamentary debates in Britain and many Commonwealth countries. It is named after Thomas Curson Hansard (1776–1833), a London printer and publisher, who was the first official printer to the Parliament at Westminster.\n\nOrigins\nThough the history of the Hansard began in the British parliament, each of Britain's colonies developed a separate and distinctive history. Before 1771, the British Parliament had long been a highly secretive body. The official record of the actions of the House was publicly available, but there was no record of the debates. The publication of remarks made in the House became a breach of Parliamentary privilege, punishable by the two Houses of Parliament. As the populace became interested in parliamentary debates, more independent newspapers began publishing unofficial accounts of them.\n\nThe many penalties implemented by the government, including fines, dismissal, imprisonment, and investigations, are reflective of \"the difficulties faced by independent newspapermen who took an interest in the development of Upper Canada, and who, in varying degrees, attempted to educate the populace to the shortcomings of their rulers\".\n\nSeveral editors used the device of veiling parliamentary debates as debates of fictitious societies or bodies. The names under which parliamentary debates were published include Proceedings of the Lower Room of the Robin Hood Society and Debates of the Senate of Magna Lilliputia. The Senate of Magna Lilliputia was printed in Edward Cave's The Gentleman's Magazine, which was first published in 1732. The names of the speakers were carefully \"filleted\"; for example, Sir Robert Walpole was thinly disguised as \"Sr. R―t W―le\".\n\nIn 1771 Brass Crosby, who was Lord Mayor of the City of London, had brought before him a printer by the name of John Miller who dared publish reports of Parliamentary proceedings. He released the man, but was subsequently ordered to appear before the House to explain his actions. Crosby was committed to the Tower of London, but when he was brought to trial, several judges refused to hear the case and after protests from the public, Crosby was released. Parliament ceased to punish the publishing of its debates as harshly, partly due to the campaigns of John Wilkes on behalf of free speech. There then began several attempts to publish reports of debates. Among the early successes, the Parliamentary Register published by John Almon and John Debrett began in 1775 and ran until 1813.\n\nWilliam Cobbett (1763–1835), a noted radical and publisher, began publishing Parliamentary Debates as a supplement to his Political Register in 1802, eventually extending his reach back with the Parliamentary History. Cobbett's avocation for the freedom of the press was severely punished by the British Government. On 5 June 1810 William Cobbett stood trial for seditious libel for an article he wrote against the British Government which was published by Thomas Curson Hansard. Cobbett was found \"guilty, upon the fullest and most satisfactory evidence\". The court sentence read: \"The court do adjudge that you, William Cobbett pay to our Lord the King a fine of £1000; that you be imprisoned in His Majesty's gaol of Newgate for the space of two years, and that at expiration of that time you enter into a recognizance to keep the peace for seven years—yourself in the sum of £3000, and two good and sufficient sureties in the sum of £1000; and further, that you be imprisoned till that recognizance be entered into, and that fine paid\". The sentence was described by J. C. Trewin as \"vindictive\". The Court argued that Thomas Curson Hansard, who had \"seen the copy before it was printed, ought not to have suffered it to have been printed at all\" and was sentenced to three months imprisonment in the King's Bench Prison.\n\nCobbett's reports were printed by Thomas Curson Hansard from 1809; in 1812, Cobbett's finances ran asunder and he divested himself of his proprietorship of both the Parliamentary Debates and Parliamentary History, which then \"passed into the hands of Hansard in 1812\". Cobbett's Parliamentary Debates became Hansard Parliamentary Debates, \"abbreviated over time to the now familiar Hansard\". From 1829 the name \"Hansard\" appeared on the title page of each issue. Neither Cobbett nor Hansard ever employed anyone to take down notes of the debates, which were taken from a multiplicity of sources in the morning newspapers. For this reason, early editions of Hansard are not to be absolutely relied upon as a guide to everything discussed in Parliament.\n\nHansard outlasted competitors including Almon and Debrett, and the later Mirror of Parliament published by J. H. Barrow from 1828 to 1843; Barrow's work was more comprehensive but he checked each speech with the Member and allowed them to \"correct\" anything they wished they had not said. The last attempt at a commercial rival was The Times which published debates in the 1880s. In 1878 a subsidy was granted to the Hansard press and at that point reporters were employed. Despite hiring contract reporters there were still widespread complaints about the accuracy of the debate reports. In 1889 Henry Hansard, the son of Thomas Hansard, broke the family connection with the debates.\n\nIn the United Kingdom\nThe Hansard of today, a comprehensive account of every speech, began in 1909 when Parliament took over the publication and established its own staff of official Hansard reporters. At the same time the decision was made to publish debates of the two houses in separate volumes, and to change the front cover from orange-red to light blue. A larger page format was introduced with new technology in 1980.\n\nHansard is not a word-for-word transcript of debates in Parliament. Its terms of reference are those set by a House of Commons select committee in 1893, as being a report which, though not strictly verbatim, is substantially the verbatim report with repetitions and redundancies omitted and with obvious mistakes (including grammatical mistakes) corrected, but which, on the other hand, leaves out nothing that adds to the meaning of the speech or illustrates the argument.\n\nOne instance of such an eliminated redundancy involves the calling of MPs to speak in the House of Commons. In that house, the Speaker must call on an MP by name before that member may speak, but Hansard makes no mention of the recognition accorded by the Speaker. Also, Hansard sometimes adds extraneous material to make the remarks less ambiguous. For example, though members refer to each other as \"the hon. Member for Constituency Name rather than by name, Hansard adds, in parentheses, the name of the MP being referred to, the first time that MP is referred to in a speech or debate. When an MP simply points at another whose constituency he or she cannot remember, Hansard identifies him or her.\n\nAny interruption to debate will be marked with the word \"(Interruption)\". This understated phrase covers a variety of situations, ranging from members laughing uproariously to the physical invasion of the chamber. Interjections from seated members, such as heckling during Prime Minister's Questions, are generally only included if the member who is speaking responds to the interjection.\n\nHansard also publishes written answers – known as written ministerial statements – made by government ministers in response to questions formally posed by members. In 1839, Hansard, by order of the House of Commons, printed and published a report stating that an indecent book published by a Mr. Stockdale was circulating in Newgate Prison. Stockdale sued for defamation but Hansard's defence, that the statement was true, succeeded. On publication of a reprint, Stockdale sued again but Hansard was ordered by the House to plead that he had acted under order of the Commons and was protected by parliamentary privilege. In the resulting case of Stockdale v Hansard, the court found that the house held no privilege to order publication of defamatory material. In consequence, Parliament passed the Parliamentary Papers Act 1840 to establish privilege for publications under the house's authority.\n\nSince 1909—and for important votes before then—Hansard has listed how members have voted in divisions. Furthermore, the proceedings and debates in committee are also published in separate volumes. For many years the House of Commons Hansard did not formally acknowledge the existence of parties in the House, except obliquely, with MPs' references to other MPs of the same party as \"hon. Friends\", but in 2003 this changed and members' party affiliations are now identified. The Hansard of the House of Lords operates entirely independently of its Commons counterpart, but with similar terms of reference. It covers parliamentary business in the House of Lords chamber itself, as well as the debates in the Moses Room, known as Grand Committee. Parliamentary written answers and statements are also printed. Emma Crewe notes that \"Editors view reporters in general as a hive of revolution and anti-establishment attitudes, while they perceive themselves as calm and uncomplaining\". The Internet, with the help of volunteers, has made the UK Hansard more accessible. The UK Hansard is currently being digitised to a high-level format for on-line publication. It is possible to review and search the UK Hansard from 1803, with the exception of standing committees.\n\nBecause Hansard is treated as accurate, there is a parliamentary convention whereby if a member of Parliament makes an inaccurate statement in Parliament, they must write a correction in the copy of Hansard kept in the House of Commons library.\n\nIn 2010 historic copies of Hansard were sent to India in its original volume format and was transformed from the original bound versions into plain text by optical character recognition (OCR) and put on the Internet to enable easy research. In July 2018 this digitised Hansard was vastly improved and merged with the rest of Hansard as previously it was available under two websites and now it is a single website. There are still many 'typos' from the OCR process but readers are encouraged to report them when they are spotted.\n\nCanada\n\nHouse of Commons\nAs with the Westminster Hansard, the Canadian version is not strictly verbatim, and is guided by the principle of avoiding \"repetitions, redundancies and obvious errors\". Unlike the UK House of Commons, members are referred to in the House only by the parliamentary ridings they represent (\"The member for Richmond Hill\", etc.) or by their cabinet post. Hansard supplies an affiliation the first time each member speaks in the House on a particular day—\"Mr. Mathieu Ravignat (Pontiac, NDP)\" or \"Hon. Lynne Yelich (Minister of State for Western Economic Diversification, CPC)\"—and by name only when they rise later to speak.\n\nIf interjections give rise to a call for order by the Speaker, they are reported as \"Some hon. members: Oh, oh!\" The details of the approval or negativing of motions and bills are reported in rather baroque detail:\n\nTranslation\nGiven the bilingual nature of the Canadian federal government, two equivalent Canadian Hansards are maintained, one in French and one in English. This makes it a natural parallel text, and it is often used to train French–English machine translation programs. In addition to being already translated and aligned, the size of the Hansards and the fact that new material is always being added makes it an attractive corpus. However, its usefulness is hindered by the fact that the translations, although accurate in meaning, are not always literally exact.\n\nThe Canadian Hansard records make note of the language used by the members of parliament, so as not to misinterpret the words of the person who has the floor. If the member speaks in French, the English Hansard records would state that the member spoke in French and refer the reader to the French Hansard record.\n\nIn one instance, during a Liberal filibuster in the Senate of Canada, Senator Philippe Gigantès was accused of reading one of his books only so that he could get the translation for free through the Hansard.\n\nNewfoundland\nIn Newfoundland the struggle for the free press was much more violent. Henry Winton, editor of Saint John's Ledger, \"had his ears cut off and was left unconscious by thugs who had been lying in wait for him after dark\". The fate of Winton was to be his printer's as well. \nThe Authorities, who were not on friendly terms with the Ledger, made little to no effort to apprehend the culprits. In another case, a \"Gentleman by the name Parsons\", of the Newfoundland Patriot, \"was sentenced to three months imprisonment in another incident\".\n\nNova Scotia\nAs was the case in many early Canadian regions, the newspapers were the first source of the Parliamentary debates. Canada's first newspaper, the Halifax Gazette, was printed on Grafton street in Halifax in 1752. The two most prominent papers in Parliamentary reporting were the Acadian Recorder, founded in 1813 by Anthony Henry Holland, and the Free Press, established in 1816 by Edward Ward. Both newspapers reported the debates of the House of Assembly starting in 1817.\n\nThe Family Compact of Nova Scotia, nicknamed \"the little compact\", \"viewed the admission of reporters to the Assembly with disdain\" and \"were not slow to react whenever they felt the slightest affront\". There are many cases which exemplify the \"struggle to obtain freedom of the press and parliamentary reportings in the Maritimes\" as in the case of William Minns in 1823, who was forced to appear before the bar of the house, and William Milne, who was jailed for not being able to pay his debts.\n\nThe Novascotian newspaper would soon become Nova Scotia's most prominent paper after its launch in 1824, which was highly influenced by George Young who was instrumental in its establishment. George Young sought permission from the Assembly to report its debates. Permission was granted, yet he was not provided with very many privileges in the House. They didn't make it easy for him and didn't allow him a seat in the lower deck.\n\nIn 1827 Joseph Howe bought the Novascotian from Young. \"There was no more powerful an advocate of parliamentary debates than Howe\". In 1835 Joseph Howe was \"prosecuted over a publication of a letter in the Novascotian\". He was charged with libel. This case was infamous and is considered to be a \"cornerstone in the establishment of freedom of the press in Canada\". Howe, who defended himself in court, was found to be Not Guilty. This is why his case is viewed as a milestone in the development of the free press.\n\nOntario\n\nNo official record of the debates in the provincial Legislature was produced before 1944. The debates were reported in various newspapers; the provincial archives clipped and collected these reports in a series of scrapbooks until 1953. The provincial website now posts Hansard online, with records from March 29, 1977, to current.\n\nAlberta\nAlberta adopted a Hansard in 1972. From 1905 to 1971, local newspapers reported on legislative proceedings, and from these articles the Legislature Library has compiled a Scrapbook Hansard, which is available online. News reporters were allowed to take handwritten notes in the Chamber, but they could not make sound recordings, and members of the public were not allowed to take notes.\n\nIn 1965 a recording system was installed in the Chamber. Initially the Clerk's office provided transcription only for special events, such as throne speeches, but requests from MLAs for transcripts increased, and by 1971 all House proceedings were being recorded. On March 8, 1972, the government introduced a motion to create Alberta Hansard, and the following day they brought forward a motion allowing audio and video recording in the Chamber and also permitting visitors to the galleries to take notes. Assembly standing orders 115 and 116 set out the rules for broadcast media in the Chamber and at committee meetings, respectively.\n\nHansard staff verifies the names of individuals and entities mentioned in the House. Like other Hansards, Alberta Hansard follows editorial guidelines established in the 19th century, and transcripts are substantially verbatim. Revisions are limited to \"the correction of grammar, spelling and punctuation, ensuring that the correct parliamentary forms are observed, and minimizing superfluous repetition and redundancies, but no material alterations shall be made, nor any amendments that would in any way tend to change the sense of what has been spoken.\"\n\nTranscripts for Legislative Assembly of Alberta proceedings from 1972 onward are available online, and current issues are usually posted within 12 hours of the day's sitting. A transcript for a regular afternoon Assembly sitting of 4.5 hours contains more than 30,000 words. Also available online are transcripts for meetings of committees of the Legislative Assembly from the 1990s onward, earlier for some committees.\n\nBritish Columbia\n\nNo complete official record of the debates in the British Columbia Legislature was produced until 1972; a partial record was issued beginning in 1970. Unlike the Ottawa Hansard, opposition members and government backbenchers are identified only by initial and last name: \"A. Wilkinson\". Current cabinet ministers have their names prefaced with \"Honourable\": \"Hon. S. Hagen\". Interjections giving rise to a call for order by the Speaker are reported only as \"Interjection\". Other interjections are reported as spoken if they are clearly audible and if they are responded to in some way by the member who has the floor. While the details of approval or negativing of motions and bills closely parallel the House of Commons, the reporting is simplified to a style line (\"Motion approved\" or \"Motion negatived\").\n\nAustralia\nThe Parliament of Australia also keeps record of debates, using the term Hansard. The records are published by the State Law Publisher. The Parliament of South Australia was the first convict free Australian colony to use Hansard; where it became a convention from 1857.  The Parliament of Victoria followed the lead of South Australia by introducing the use of Hansard in 1866. The Parliament of New South Wales commenced its Hansard system on 28 October 1879 with the reporting of the Legislative Council at the opening of the Third Session of the Ninth Parliament.\n\nNew Zealand\n\nOn 9 July 1867 a team of five reporters, led by Chief Reporter C.C.N. Barron, produced the first official report of debates of the New Zealand Parliament. Ever since that day official transcripts of members' speeches in the New Zealand House of Representatives have been continuously published.\n\nToday the New Zealand Hansard is produced by a team of 17 FTE Hansard Editors within the Office of the Clerk of the House of Representatives. Hansard is published on the New Zealand Parliament website each day the House sits, and later indexed bound volumes are produced.\n\nSpeeches are transcribed directly from digital recordings of the debate, with staff present in the debating chamber to monitor the debate by recording the sequence of speakers and any interjections. Interjections are reported only if the member speaking replies to them or remarks on them during the course of his or her speech. Hansard Editors follow strict rules on what changes they can make to the words members use in the chamber. Hansard is as close to verbatim as possible, although Hansard Editors remove repetitions and redundancies and make minor grammatical corrections. Members are provided draft copies of their speeches at the same time that the speeches are first published on the Parliament website. Members can request correction of inadvertent factual inaccuracies but they are unable to significantly change what they said in the House.\n\nList of assemblies using the system\n\n Parliament of the United Kingdom and the UK's devolved institutions\n Parliament of Canada and the Canadian provincial and territorial legislatures\n Parliament of Australia and the Australian state and territory parliaments\n Parliament of South Africa and South Africa's provincial legislatures\n Parliament of Barbados\n East African Legislative Assembly\n New Zealand Parliament\n Legislative Council of Hong Kong\n Parliament of Malaysia and the Malaysian state legislatures\n National Parliament of Papua New Guinea\n Parliament of Singapore\n Legislative Council of Brunei\n Parliament of Sri Lanka\n Parliament of Trinidad and Tobago\n National Assembly of Kenya\n National Assembly of Tanzania\n Parliament of Ghana\n Parliament of Uganda\n Parliament of Mauritius\n Parliament of Jamaica\n National Assembly of Seychelles\n States of Jersey\n States of Guernsey\n Tynwald, the Parliament of the Isle of Man\n National Assembly of Nigeria\n National Assembly of Namibia\n Parliament of Botswana\n Parliament of Zimbabwe\n\nSee also\n List of British colonial gazettes\n Congressional Record, the equivalent for the United States\n Court reporter\n Fuddle duddle\n Hansard Society\n Pepper v Hart\n TheyWorkForYou\n\nReferences\n\nExternal links\n\n \n Hansard from 1803 to 2005\n The records of the House of Lords Official Report (Hansard) are held by the UK Parliamentary Archives\n The records of the Parliamentary Register, 1743-1786 are held by the UK Parliamentary Archives\n Parliamentary Archives, Hansard Publications and Papers\n\nTranscripts of legislative proceedings\nWestminster system",
        "First I would like to apologize for the delay in reviewing.\n\nSummary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.\nThe originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.\n\nIn order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.\n\nThe dataset description in section 2.2, should be moved to section 4 where the other datasets are described.",
        " If your computer isn’t working properly enough to run its own restart, force one by hitting the restart button (if you have one) or by turning the power button off and then back on.;\n, This will open an Advanced Options menu. Tapping the key several times will increase your odds of sending the command at the right moment.\nNote that if you hit F8 too soon on certain operating systems, you may get a keyboard error message and have to restart the computer; if you hit it too late, Windows will simply open normally and you’ll have to try again.\n\n, When you log in, you will see a command window (cmd.exe). This will allow you to modify the computer while still in safe mode.\n\n, (Do not type the quotation marks.) Only do this step if you already know or could easily recognize executable files associated with the virus.\n\n\nGo to the Processes tab to find and end all processes associated with the virus. Simply click on the process and hit End Process at the bottom right-hand corner of the window. If you aren’t sure whether or not a process is associated with the virus, don’t end it.\n\n\n\n\n\n\nClose the Task Manager. This will take you back to the command window.\n\n\n\n\n\n\n\n, (Again, do not type the quotation marks.) This will open the Registry Editor.\n\n, The full directory is “HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon.”\n\n,, A dialogue box should pop up that gives you the value name (Shell) and the value data (C:\\Documents and Settings\\YOUR USERNAME\\desktop\\VIRUS INFO.exe).\n\n, (Again, do not type the quotation marks.) This restores the default value.\n\n, (Ex. “contacts.exe,” “jashla.exe,” “mahmud.exe,” etc.) Make sure that Keys, Values, and Data are all checked in the Find options.\n\n, Hit Find Next to find a registry key containing your virus info, then right-click the name and choose Delete. Do this until there are no more registry keys associated with this virus.\n\n, You should now be back at the command window.\n\n, (Again, do not type the quotation marks.) This will restart your computer in normal mode.\n\n,,,,,,, When the virus reappears after reboot, you know where the virus is hiding\n\n",
        "The paper presents an interesting modification to PoWER algorithm that is well motivated. The main limitation of this paper is the lack of comparison with other methods and on richer problems. The experiments haven't given confidence to show its claimed benefits, generality and scalability over prior methods. Giving this confidence doesn’t necessarily require running your method on all large-scale domains or doing exhaustic hyper-parameter search, but for example it could go beyond current domains. Cartpole only optimizes 5 parameters. Ad targeting task lacks comparison with alternative methods. Since this method is built on PoWER and closely connected with RWR, it is likely there are limits to performance which may become apparent when the method is tried on other domains and with other benchmark methods, e.g. even standard ones like importance sampling-based off-policy learning is known to suffer in high-dimensional or continuous action space; limits of RWR/PoWER-like methods based on their connection with entropy-regularized RL. ",
        " To complete the dungeon you will need bombs, which you can purchase back in town or find around the mountain.\n, Make your way up Mount Crenel and reach the Cave of Flames., Once inside, head through the door leading right. On the upper edge of the room, the wall is cracked. Use a bomb to blow it up and a new path will open, go through it., Once they are on their back kill them with your sword., Once all of the enemies are dead, the chest containing your compass will appear., From the room where you obtained the compass, head left into a room full of rupees. Be careful of the enemy hidden within and make your way through the door on the upper side of the room., Follow the tracks heading to the right., Make your way through until you reach another cracked wall on the left edge of the area. Blow it up with a bomb and pass through into room 7., Pull their masks off with your gust jar then slash them with your sword to kill them., Approach it, shrink down and travel through the door on the upper right side of the room., You'll eventually reach a room full of bats, which contains a heart piece. You can't grab the heart piece while shrunken so simply head down for now., Once inside, travel around to the left side where there is a small gap in the stones. Use it to jump down and reach the portal. Use the portal to grow back to your true size., Head up and climb back up to the upper level to grab a map from the large treasure chest., Head back down then head down across the lava. On the left side you'll see a chest and a button. Open the chest, push the button and continue down to enter the next room., Be careful since the platform will sink like the ones in the previous room., If you hit the armored enemies they will curl into a ball. In their ball form you can maneuver them into holes with your sword to allow you to pass. If you knock one into the hole in front of the treasure chest you will net yourself 50 rupees. Maneuver one into the hole nearest to all of the pots, then climb up to the next floor., Use the tornado in the room to float downward., Land in the next tornado and use it to maneuver yourself around the wall of rocks and reach the chest on the right side. Once you are over there, push the square block aside and push the chest into the hole on the second floor. Inside is a key., You will wind up back in the hallway where the mine cart left you earlier. Now that you have a key, hop back into the mine cart and ride it back to the other side., Follow the tracks and hit the switch with your sword before heading back to the cart for another ride. Once you're back in the hallway, head left until you find a door leading up., For now make your way across the tracks heading right. Blow up the wall on the downward edge of the other half of the map to grab a heart piece, then head through the door going up., You'll be attacked by a group of gray slimes. I recommend using bombs to defeat them, otherwise just hit them when they revert from their spiky appearance., A chest will appear containing the . Grab it and head left., Use your to flip over the spiky looking platform and make your way to the left side of the map., On the other side of the map, use your cane on the hole then jump in. It will launch you up to the upper level. Pressing the switch there will create a teleporter to take you back to the start., Unless you need to return to town for more bombs or something, the teleporter doesn't help you right now. Instead head through the door on the upper level., With it now in riding condition go ahead and hop in., In the new area, push the chest into the hole above and grab the key from within., It will take you back to the door containing the teleporter. Enter that room and open the locked door., In the next room, ignore the switch for now and use the stone portal to shrink. Carefully make your way through the bladed enemies to reach the door on the left side., Travel through the maze to another stone portal. Use it to grow, then carefully dodge the new bladed enemies to make it back to the mine cart. Flip it with the cane and hop in., Once you're back in the room with the green bladed enemies, flip the switch and hop in the cart once more., You should now find yourself in a room containing more of the metal ball enemies. Like before, strike them with your sword to maneuver them into the four holes on the lower-right side., Pull the stone with the switch on it by moving close and holding \"R\" while pressing the d-pad in the opposite direction. Get the stone into the hole on the upper-left side and hit the switch with your sword to clear the way., Jump down using the uppermost gap to claim a kinstone, then use your cane on the closest hole to launch yourself up to another chest. Once you're finished, head right., The next room contains a river of lava with more of those timed platforms. Wait for the spiky one to be headed downward, then quickly hit it with your cane before getting on. Do the same for the next two., Once you're on solid ground again, head to the edge of the path and use your gust jar to clear the pots from the next platform over., Go through and climb the stairs to the next level then hop down through the gap. Turn to the next hole, use your cane and launch up to the upper level. Hop down once more, this time on the left side. Use your cane on the hole and launch yourself into the tornado., Follow the path right and hop into the next tornado. Float in a downward direction and land on the next platform. Follow it until it returns you to the river of lava., Like before, hop over the edge and into the tornado. Float down and use your cane on the hole to launch yourself into the next tornado, float to the burning platform and use your cane on yet another hole., Launch up to the next platform, hop into the next tornado and float from tornado to tornado. On the 3rd, you'll want to float up (left takes you to another kinstone but will force you to repeat quite a few steps). Float using another tornado and land on the platform with the big chest. Within is the boss key., You can jump off of the upper part of the boss-key platform to reach a platform with a switch. Hit it to create another teleporter that will return you to the entrance., Think about going to the fairy fountain on the mountain to catch a fairy in a bottle. The boss battle can be a little hectic and hearts can be tough to come by inside., Use your key and enter the boss room., Now, the boss looks pretty menacing but he is pretty easy once you get the pattern down. Avoid his fire breath while waiting for an opening to shoot your cane at his shell., Once you successfully hit his shell with the cane, his neck will create a bridge allowing you to hit his weak point with your sword. Once he begins to shake quickly run back across his neck before he sinks back into the lava., He will stay submerged for some time just dropping rocks on you. #Rinse and repeat. After 3-4 successful rotations, he should die leaving behind the fire element and a heart container.",
        "Iran participated in the 1970 Asian Games held in the capital city of Bangkok. This country is ranked 4th with 9 gold medals in this edition of the Asiad.\n\nMedal summary\n\nMedal table\n\nMedalists\n\nAthletics\n\n  Gold\n Teymour Ghiassi - Men's high jump\n  Silver\n Jalal Keshmiri - Men's discus throw \n Jalal Keshmiri - Men's shot put\n\nBoxing\n\n  Silver\n Omran Khatami - Men's +81 kg\n  Bronze \n Mohammad Sarehkhani - Men's 71 kg\n Gholamhossein Pakmanesh - Men's 75 kg\n\nCycling\n\nRoad\n  Silver\n Team - Men's team time trial\n Team roster\n Hassan Arianfar\n Asghar Doroudi\n Khosrow Haghgosha\n Hossein Baharloo\n\nTrack\n  Bronze \n Team - Men's team pursuit\n Team roster\n Khosrow Haghgosha\n Manouchehr Daneshmand\n Hossein Baharloo\n Asghar Doroudi\n\nWeightlifting\n\n  Gold\n Mohammad Nassiri - Men's 56 kg\n Nasrollah Dehnavi - Men's 67.5 kg\n  Silver\n Daniel Giorgiz - Men's 75 kg\n  Bronze \n Mohammadreza Nasehi - Men's 52 kg\n Ebrahim Pourdejam - Men's 90 kg\n Houshang Kargarnejad - Men's 110 kg\n\nWrestling\n\nFreestyle\n  Gold\n Ebrahim Javadi - Men's 48 kg\n Mohammad Ghorbani - Men's 52 kg\n Shamseddin Seyed-Abbasi - Men's 62 kg\n Abdollah Movahed - Men's 68 kg\n Dariush Zakeri - Men's 90 kg\n Eskandar Filabi - Men's +100 kg\n  Silver\n Mohammad Farhangdoust - Men's 74 kg\n Ali Hajiloo - Men's 82 kg\n  Bronze \n Abolfazl Anvari - Men's 100 kg\n\nReferences\n\n  Iran Olympic Committee - Asian Games Medalists\n  Iran National Sports Organization - Asian Games Medalists\n\nNations at the 1970 Asian Games\n1970\nAsian Games",
        "No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection.",
        "Lake Sülüklü (), for \"Lake of the Leeches\", is a freshwater lake located at Şehitkamil district in Gaziantep Province, Turkey.\n\nReferences\n\nSuluklu (Gaziantep)\nLandforms of Gaziantep Province\nŞehitkamil District",
        " Make sure you know this by heart to guide you through the tutorial.;\n, The notes are C D E♭ F G A♭ B C\n\n\nNotice that there are two flats: E♭ and A♭. They are both black keys.\nWrite the notes down forward and backwards mentally for memorization.\nSay them out loud forwards and backwards.\n\n,\n\n\nWe'll start with the right hand. The thumb is number 1. The pointing finger is number 2. The middle finger is number 3. The ring finger is number 4, and the pinky is number 5.\nKnow the numbers that indicate fingers on the left hand. It's the same as the right hand. The thumb is number 1. The pointing finger is number 2. The middle finger is number 3. The ring finger is number 4. The pinky is number 5. Here is the diagram, which illustrates this:\n\n, Here is a diagram that will help you memorize what keys each finger is supposed to press.\n\n\nFor the right hand, the fingering is 123 1234 123 12345.\nNow say this, thumb, pointer, middle, thumb, pointer, middle, ring, thumb, pointer, middle, thumb, pointer, middle, pointer, pinky.\nMake sure to remember the numbers corresponding to the fingers. Thumb is number 1, etc.\nFor the left hand, the fingering is 54321 321 4321 321.\nNow say this: pinky, ring, middle, pointer, thumb, middle, pointer, thumb, ring, middle, pointer, thumb, middle, pointer, and thumb.\n\n, Remember, the fingering is 123 1234 123 12345. The first finger, which is the thumb, will play the C note.\n\n,,, Remember, we're on the second set of numbers 1234 in the 123 1234 123 12345 fingering.\n\n,,,, Slide your thumb under to hit the C note again, and repeat the same pattern again for this octave except the last C!\n\n, Remember the 123 1234 123 12345 pattern. The pinky indicates number 5. You finished the right hand!\n\n, This will be different, because the pinky will start off. The pattern is 54321 321 4321 321. The pinky hits the C note.\n\n,,,,, Your fingers will get use to this and you'll get faster. Guaranteed. Just do everything slow!\n\n, We're on the 2 of the second 321 in the 54321 321 4321 321 fingering pattern. Always keep track of the fingering.\n\n, Hit the C note with your thumb.\n\n, The rest of the notes should be self-explanatory. The D is the ring finger. The ring finger is number 4 of the 4321 set of numbers. The 3 is the middle finger, so use the middle finger to hit the E♭ note and continue the fingering until you reach the last C.\n\n, Strike it with number 1(the thumb)! You're finished with the left hand! Now practice this with both the left and right hand until it's memorized by heart. Then you can put both hands together or practice it backward.\n\n",
        "The difference between String Field Theory and String theory is, in the broadest of sense, the same as the difference between Quantum Field Theory and Quantum Mechanics.\n\nQFT supercedes Quantum Mechanics in that it allows particle creation and annihilation dynamically. QM's basic building block is the Schrodinger equation, you're finding how a particle that you suppose already exists evolves when inside a potential that already exists, and never worry about how both of those things are generated. You can infer from that a whole lot more, but you will run into difficulties describing more complicated problems. In QFT, these things are not always fundamental, it allows us to derive a more microscopic view of e.g. the electromagnetic potential of a charged object, you can derive the famous 1/r^2 law.\n\nString Theory is much the same, its basic building block is the action for one string as it moves through space. Again, from knowing that you can infer a whole load of stuff (particle states, anomalies, dimensions of space, existence of branes, etc.) but centrally String theory does not have a proper way of treating strings being created and annihilated dynamically out of the vacuum.  We assume they can, and we can figure out many properties of what happens when it does, but the formalism is somewhat lacking.\n\nString Field theory exists to re-employ the techniques that made QFT an improvement over QM in this circumstance to solve what seems like a similar obstacle.",
        " Be clear on what eye needs what medication and how many drops should be used. It helps to understand what medical condition the eye drops are treating. You will know what to expect when you apply the eye drops.There are many reasons why your child’s doctor could have prescribed the eye drops. Your child may have hay fever or allergies and you will have to treat your child’s itchy eyes off and on throughout the allergy season. Conjunctivitis is an infection of the tissue lining the inside of the eyelids and the sclera, the whites of the eye. You will apply the eye drops for a limited period of time, but you have to be careful not to spread the infection to your child’s other eye or to you. Glaucoma, increased pressure in your child’s eye, is a chronic disease and you likely will have to apply the eye drops for a long period of time.Your child will start to feel better sooner if her eye(s) receive the proper treatment. Your child may be having a problem with only one eye or both eyes. She may not have the same issue in both eyes. You may have to put only one medication in one eye and two medications in the other eye. It will be easier to focus on your child’s comfort when applying the eye drops if you are comfortable with exactly what you need to do.;\n, Eye drops are a medication and they may cause side effects, including an allergic reaction, in your child. You want to recognize the signs so that you can stop the eye drops as soon as possible.The symptoms from an allergic reaction can have a lot of overlap with the symptoms from a side effect. Your child may experience redness, itching, burning, and blurry vision. These are symptoms you child may be having already from her illness; suspect a problem when your child’s symptoms get worse instead of better as time passes. Your doctor will tell you about the specific side effects the eye drops may cause. If you have any concerns about how your child is reacting to the eye drops, call your doctor. She can determine if your child needs a different type of medication., Tell her about all the prescription and over-the-counter medications your child is taking currently. Any medication can interact poorly with the eye drops. Your child’s drug allergies are important information for your child’s doctor to have when she is prescribing the eye drops., Your child may be old enough to wear contacts, but she may still need your help with the eye drops. Follow the doctor’s instructions.A general rule is that you can have your child remove her soft contacts and keep them out for 15 minutes after using preservative-free eye drops. She will have to wear her glasses for several days if the eye drops contain a preservative. If your child wears hard contacts, she can use eye drops with or without a preservative and still keep in her contacts., Every time you use the eye drops from a multi-dose bottle there is a risk of contamination. This could lead to an eye infection in your child.The preservatives used in eye drops discourage the growth of bacteria after the bottle is opened, but there is a limit. A multi-dose bottle should not be used longer than 4 weeks. Write the day and month you opened the bottle on the label to remind you to throw away the eye drops.\nPreservatives are not used in the eye drops contained in single-use vials. These eye drops should be discarded immediately after use; do not save any of remaining fluid for the next dose., Read the medicine label to check the instructions and the expiration date. Shake the bottle and draw up the medication into the dropper to look for any changes the fluid’s appearance.The instructions on the label should be the same as what your child’s doctor told you during the office visit.\nDo not use the eye drops if the expiration date has passed. Do not risk prolonging your child’s recovery time by using medication which may not be at full strength, or may be contaminated with harmful bacteria.\nShaking makes the medication uniform throughout the bottle. Discard the eye drops if you see any crystals forming or if the medication has changed colors. These changes suggest the eye drops have been contaminated. You should be able to examine the eye drops through the clear plastic of the single-dose vial., You want your hands to be free of germs when you are touching the bottle and applying the drops to your child’s eye(s). Contamination of the eye drops and unintentionally causing an infection in your child is always a concern.Use soap and warm water, and scrub your hands for at least 20 seconds (about the time it takes to sing \"Happy Birthday\" twice). Don't forget to get under your nails and between your fingers.\n\n, When the time comes, it will be easier on both you and your child if the eye drops are applied without distractions for your child and with a lot of light for you to see.A room filled with her toys and with the TV or music blaring will make you child want to move around or look all over the place. Your child’s is already a little afraid. Do everything you can to help keep her calm.\n\n, She may be more cooperative if she knows what to expect. Tell her the eye drops will make her feel better in the end, but that drops may make her eyes sting or make her vision blurry for a short time. Do some playacting to get her familiar with how you will apply the eye drops.Show your child the medicine bottle. Explain how you will take out a dropper. Make believe you are putting eye drops in your own eyes or your partner’s eyes first. Then, make believe you are doing it to your child. Praise everyone, especially your child, for staying calm.You can put a drop on the back of your child’s hand to let her see what it feels like. But, you want to be careful not to touch anything with the tip of the dropper., Once you have drawn up the medicine into the dropper, you want to free up your hand. However, you do not want the outside of the bottle to come in contact with dirt or dust.\n\n\nTry not to put down the dropper or the opened single-use vial. The tips must be kept as clean as possible. You must be aware of not contaminating the tip of the dropper.\n\n, It is best if your child has her head back and her eyes looking upward. You may have to try several positions before your child is settled and willing to stay still. It will be easier if you have a partner who can help keep your child calm.You can have your partner cradle the child while she is lying flat on her back. If your child is old enough, ask her to look upwards.Have your child in a seated position so that she can tilt her head back, making her eyes roll upwards, naturally. Your partner may need to hold a younger child’s head in this position, gently.If you are alone, sit on the floor with your child on your lap facing you. After you bend your knees, your thighs become a cradle. Ask your child to lean back or lie on her back so that her head is resting on your knees. Both of your hands are now free., Use a tissue, cotton ball, or cotton swab moistened with warm water. Wipe gently from near the nose towards the ear.An extra layer of pus or hardened eye discharge in and around the eye may prevent the eye drops from being absorbed by the superficial tissue layers of the eye., When the child is looking up, this action creates a sac where you can put the eye drops. Take care not to let the tip of the dropper touch anything, including your child’s eye, eyelashes or face.Use a two-handed approach. Use your non-dominant hand to move the eyelid and your dominant hand to apply the drops.\nYou can encourage your child to look up by having your partner hold up a toy or by placing a toy she likes high up on something and pointing it out to her.If your child will not look up, you may have to use your thumb on the lower eyelid and your pointer finger on the upper eyelid to open her eye., Encourage your child not to squeeze her eye shut. You are giving the eye drops time to bathe the eye and be absorbed by the superficial layers of the eye. While you wait, use a clean tissue to wipe away any medication that drained from your child’s eye.Excessive blinking or tight closing may force medicine out of her eye. However, there is absolutely no way to keep your child from blinking or squeezing if she is unable or unwilling to listen to you.Clear away any excess eye drops that have drained from your child’s eye.\n\n, You want to press gently on the eye near your child’s nose. This step can prevent the medication from becoming systemic and going throughout your child’s entire body.Some children will not tolerate this pressure and it is best not to force the issue.\nYour pressure is meant to block your child’s tear duct and to prevent the eye medication from becoming systemic. The medication in the eye drops is meant to treat your child’s eye only. It is absorbed by the thin layers covering your child’s eye. However, there is a tear duct located in the inside corner of her eye near her nose. Tears flow out of it to lubricate the eye. The eye drops can flow into the tear duct; the small blood vessels in there can carry the medication to other parts of the body., It is best to wait around five minutes. This prevents the second medication from washing away the first before it has time to be absorbed., Your child will enjoy the affection and hearing about how good and brave she is. This is positive reinforcement encourages her to remain calm and cooperative the next time you apply the eye drops., You can keep arms and legs from flying everywhere or stop your child from trying to run away from you. It will help if you have a partner who will help keep your child calm.This technique works best if your child is younger than 3 years old. However, you still can see if your slightly older and upset child will give it a try.Putting drops in open eyes works better, so try the above method first. If it does not work, proceed with this method.\nSwaddling is known to calm infants.A small child will wiggle less and may find the light pressure comforting, especially if your partner is cuddling her as well.\n\n, Use a tissue, cotton ball, or cotton swab moistened with warm water. Wipe gently from near the nose towards the ear.An extra layer of pus or hardened eye discharge in and around the eye may prevent the eye drops from being absorbed by the superficial tissue layers of the eye., It is likely your young or nervous, older child will not be very cooperative. You may have to try several positions. Wrapping your child in the blanket should not prevent you from getting your child into position.You can have your partner cradle the child while she is lying flat on her back.Have your child in a seated position with her head tilted back. Your partner may need to hold your child’s head in this position, gently.If you are alone, sit on the floor with your child on your lap facing you. After you bend your knees, your thighs become a cradle. Ask your child to lean back or lie on her back so that her head is resting on your knees. Both of your hands are now free., If you cannot use the open-eye method (or you have tried it and it didn't work), apply the drops to closed eyes. Use the corner closest to the nose. Make sure you do not touch your child’s eye, eyelashes, or face.This does not work as well as when you place the eye drops in your child’s lower eyelid, but there may not be another option when your child is young or very upset. However, you can try the open-eye method first. Some even very young children will respond well to it., Encourage very young children to open their eyes by showing them a favorite toy or a video on your phone. Normal blinking will let the eye drops flow into the eye. If she is too afraid to open her eye, rub her eyelids gently to bathe the eye. Use a clean tissue to wipe away any excess medication around the eye.Excessive blinking or tight closing may force medicine out of her eye. Help your child to follow your instructions as much as she can.Clean away any excess eye drops that have drained from your child’s eye.\n\n, You want to press gently on the eye near your child’s nose. This step can prevent the medication from becoming systemic and going throughout your child’s entire body.A young or nervous child may not tolerate this pressure, but it is best not to force the issue.\nYour pressure is meant to block your child’s tear duct and to prevent the eye medication from becoming systemic. The medication in the eye drops is meant to treat your child’s eye only. It is absorbed by the thin layers covering your child’s eye. However, there is a tear duct located in the inside corner of her eye near her nose. Tears flow out of it to lubricate the eye. The eye drops can flow into the tear duct; the small blood vessels in there can carry the medication to other parts of the body., It is best to wait around five minutes. This prevents the second medication from washing away the first before it has time to be absorbed., Your child will enjoy the affection and hearing about how good and brave she is. This is positive reinforcement could encourage her to remain calm and cooperative the next time you apply the eye drops., This means washing your hands with soap after you are done using the eye drops. The tip of the dropper should be cleaned with a cotton ball soaked in rubbing alcohol.If your child has an eye infection, you want to prevent the spread of infection throughout your household. Also, you do not want to get the medication, which is not supposed to be swallowed, in your mouth.\nIt's important to keep the dropper tip clean and free from germs for future use. Rinse off the dropper with water to make sure there is no residual alcohol when you use it again.\n\n, This means keeping the bottle out of the reach of your child and other children in house. Ask your pharmacist if the bottle needs to be stored in a specific place like refrigerator or out of direct sunlight to help it maintain its effectiveness.Your child may be very curious about the medicine bottle once you use the eye drops on her. Remind her it is not to be touched.\n\n, Don’t hesitate to call if you feel like something is just not right with your child. You know her best.Call your doctor immediately if your child’s eyelids becoming very red and swollen, she has increasing eye pain, her vision stays blurry for a long time, or if your child begins acting very sick. Most children will play even if they are not well; a child feeling too weak to move around is worrisome.Call your doctor if an infection has not cleared up after 3 days or if your child develops an earache.",
        " Common majors for successful strength and conditioning coaches include Strength and Conditioning, obviously, but also Kinesiology and Exercise Science, among others. By taking classes in these subjects, you will gain the knowledge you'll need to help your athletes improve.\n\n\nYour chances of being hired and paid well in this position will be higher if you hold an associate's or bachelor's degree in one of these or in a related field.\nResearch your local colleges and universities to determine which program is the best fit for you.\nIf attending college full-time is not an option for you, take continuing education classes in those fields. Even if you don't earn a degree, you will still learn the type of information you will need to perform the job.;\n, If you want to work at a high level and make a larger salary, your employer might require certification as a strength and conditioning coach. However, if the position you are trying to attain does not specifically request certification, you may be able to skip it.\n\n, Although there are many agencies for certifying strength and conditioning coaches, the National Strength and Conditioning Association (NSCA) is the most widely respected of these., The National Strength and Conditioning Association's exam costs $260 for NSCA members, and $445 for non-members. Make room for the exam in your budget.\n\n, Look up what information will be cover in your agency of choice's test. Study the format of the exam as well — will it be entirely multiple choice, or do you need to brush up your essay-writing skills?\n\n\nIf you took classes in college relevant to the exam, look at your old class notes to refresh the information in your mind.\nMost reputable certification agencies will provide study materials for people studying to take the exam. Avail yourself of those resources.\n\n, An elementary-aged athlete’s body is very different from a high school-aged athlete’s body, and you cannot treat every age group with the same blanket approach to exercise.With young children (6-11 years old), be aware that girls will develop motor skills at an earlier age than boys.\nWith children, plan activities that will help young athletes develop basic motor skills like jumping, throwing, catching, skipping, climbing, etc., rather than focusing on the specialized motions of the specific sport you’re preparing them for. Older athletes should be encouraged to develop more complex skills like speed, agility, and flexibility.\n\n, Remember to address all the major muscle groups, including calf muscles, hamstrings, quadriceps, shoulders, etc., You cannot use a generic, blanket approach to all athletes. Each sport — and sometimes, each position within a sport — will require athletes to develop specialized skill sets. Make sure you understand the needs of your particular athletes before beginning a training program with them. For example, goalies in ice hockey and catchers in baseball spend much of their time squatting. Their strength and conditioning needs will be very different from a marathon runner, whose needs will be very different from a football running back whose position calls for quick bursts of speed and the agility to make sharp cuts.\n\n, Yoga is a terrific way to strengthen core muscles, as are sit-ups and crunches, low back extensions, Supermans, and exercises with kettle bells. You should also consider activities that will benefit players of specific sports; for example, basketball players will benefit from passing a medicine ball back and forth, while taking golf swings while balanced on one leg will help golfers., Though athletes in all sports should work on their upper bodies, some sports — gymnastics, for example — specifically require a great deal of upper body strength. Bicep curls, push-ups, tricep push-ups, pull-ups, and rope climbing are all good exercises for arm strength.\n\n, If their legs can't support them, athletes will be hard-pressed to win in a competitive setting. Running, either on a track or on a treadmill, not only develops long muscle in athletes' legs, but it also improves their cardiovascular health and endurance, allowing them to perform at a high level without tiring for longer amounts of time. To add power for athletes whose sports require it specifically, leg exercise routines should include squats, leg presses, lunges, and vertical jumps.\n\n, These exercises are meant to maintain, rather than increase, muscle mass.\n\n\nHave your athletes hold each position for a set amount of time; consider their injury and physical ability, and use your discretion in determining the duration of the hold. Add at least 10 seconds onto their best time every two weeks to improve form.\n\n, As a strength and conditioning coach, you will serve in many capacities: teacher, leader, motivator, and support system for the athletes under your guidance. Because you will see your athletes through a wide variety of experiences, you need to be able to adjust your approach from context to context, from day to day.\n\n, Although your athletes will likely be socialized to accept orders from coaches unquestioningly, there may be situations in which you have to alter the manner in which you deliver your message, depending on the emotional state of your audience in that context. Your goal as a coach is not to domineer, but to get the best possible results out of your athletes. This may mean that you have to change the delivery of your message from “an order” to “advice.”\n\n, Ethos is credibility: speak from a place of authority. Most of your communication will be ethically charged orders and commands. Logos is a logical approach: perhaps an athlete feels like giving up because he or she is having a bad day. Reason with them. Instead of speaking from authority, help them understand how logical it is to work through their bad days to reach their goals. Pathos is an emotional approach: on your athletes' worst days, you’ll have to work hard to understand how they’re feeling and help them change negative emotions into positive mindsets.\n\n",
        "  Discovered in 1995 at the Caltech Submillimeter Observatory (CSO), the\nvibrationally-excited water maser line at 658 GHz (455 micron) is seen in\noxygen-rich giant and supergiant stars. Because this maser can be so strong (up\nto thousands of Janskys), it was very helpful during the commissioning phase of\nthe highest frequency band (620-700 GHz) of the Submillimeter Array (SMA)\ninterferometer. From late 2002 to early 2006, brief attempts were made to\nsearch for emission from additional sources beyond the original CSO survey.\nThese efforts have expanded the source count from 10 to 16. The maser emission\nappears to be quite compact spatially, as expected from theoretical\nconsiderations; thus these objects can potentially be used as atmospheric phase\ncalibrators. Many of these objects also exhibit maser emission in the\nvibrationally-excited SiO maser at 215 GHz. Because both maser lines likely\noriginate from a similar physical region, these objects can be used to test\ntechniques of phase transfer calibration between millimeter and submillimeter\nbands. The 658 GHz masers will be important beacons to assess the performance\nof the Atacama Large Millimeter Array (ALMA) in this challenging high-frequency\nband.\n",
        "1s and 0s are typically processed in *chunks*, not individually.\n\nSay you've got a system that works like this: 0000 represents a black pixel, and 0001 represents a white pixel. When the monitor reads 0000, it generates a black pixel, and when it reads 0001, it generates a white pixel in the pixel immediately following that.\n\nNow throw in a few extra commands. Maybe 0100 means \"go to the next line.\" Now you could have a series of chunks of data that looks like this:\n\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0000 0000 0001 0001 0000 0000 0001 0001 0000 0000 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n    0001 0001 0000 0000 0001 0001 0000 0000 0001 0001 0100\n\nTo us, this is just a bunch of 1s and 0s, but the computer will read this and display a checkerboard pattern, like this:\n\n    xx  xx  xx  xx\n      xx  xx  xx\n    xx  xx  xx  xx\n      xx  xx  xx\n\netc.\n\nNow imagine that you add in some extra colors. Maybe 0000 is black, 0001 is white, 0010 is red, 0011 is blue, 0100 is green, 0101 is orange, 0110 is purple, 0111 is yellow, 1000 is pink, 1001 is brown, 1011 is gray, 1100 is beige, etc.\n\nSuddenly you've got the possibility to make a *lot* of different shapes and colors in that grid of 1s and 0s above.\n\nNow, modern games have a *lot* of 1s and 0s in them. Say you've got a game that uses 32-bit color.\n\nIn the example above, I was using *4-bit* color. Each \"bit\" is either a one or a zero. The maximum number of colors I could handle at 4 bits represents all of the different possible combinations of 1s and 0s that can be represented by one bit:\n\nEach bit can be one of two things, and you have four of them, so\n\n2 x 2 x 2 x 2 = 2^4, or 16 possible colors.\n\nWhat *32*-bit color means is:\n\n2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2\n\n= 2^32 = 4294967296 possible colors.\n\nThe incredibly high amount of colors allows for more subtle and realistic shading. A lot of math behind the scenes tells the program how to use these colors, and how they should change in response to everything else.\n\nSo, long story short, video games are *extremely* complex, and 1s and 0s add up!\n\n**edit:** got a little trigger happy copying and pasting twos! I think I counted right this time",
        "The author proposed a simple but yet effective technique in order to regularized neural networks. The results obtained are quite good and the technique shows to be effective when it it applied even on state of the art topologies, that is welcome because some regularization techniques used to be applied in easy task or on a initial configuration which results are still far from the best known results. ",
        " Know if it is sensitive, oily, combination, or dry skin. Then get the appropriate skin care treatment.\n\n\nIf you have acne, you should look for specific products to clear your skin, like medicated facial washes and products containing benzyl peroxide, which is strong but it works, though many are allergic to it.\nRemember to only use products meant for your skin type.\nStart a routine and follow it every day, be consistent!.\nIf it's necessary, see a dermatologist.;\n, Pick appropriate colors that suit you well or look good with your eye color (you could go to makeup counters and the staff could help you chose the right colors) and learn to apply makeup to accentuate that feature.\n\n\nDon't go bold on lips, eyes, and cheeks. You'll appear over-done. Sometimes if you apply too much your parents and/or teachers will make you take it off and you'll get funny looks.\n\n,, Look for what is appropriate for your skin and hair color. Don't just buy the same brands and colors your friends are buying. Also, when buying makeup for the first time, go with your mom or with some friends to get second opinions!\n\n, The initial investment may be more, but you'll be less likely to buy items that are inappropriate. Drug stores won't allow you to sample and try different colors, and the employees aren't experienced to help. Make sure you request they tell you how to do a natural routine, and before you buy the products, make sure it is appropriate and you like it.\n\n, Some teens prefer tinted moisturizer for a sheer, all-over color. You don't want to go overboard. Take off all makeup daily and re-do it every day. Never keep it on when you're sleeping as it can block pores and cause blackheads.\n\n,, Try new colors and looks, but be sure it looks natural. As you find good ones you should also write them down. You'll eventually develop makeup looks for different outfits, events, and seasons.\n\n, Try these out at home before you wear them out. Remember the magazines push products of their advertisers. It may look good on the model but the colors or formula may not look good on you.\n\n, Don't wear soft neutral tones everywhere.\n\n,, These are some common starting points to consider but every face is different.\n\n\nIf you have pale or light skin, try light pink blush or neutral rose blush.\nIf you have medium or olive skin, try coral blush.\nIf you have dark skin, try bolder-colored blush.\nSince every face is different, you should make sure whatever colors you choose are appropriate for your skin type, skin color, hair color, and eye color.\n\n,,\n\n\nRemember to take off mascara after one day. If you forget and don’t apply makeup the next day, it will look horrible. Also, go easy on the eyeliner until you are more experienced with makeup, because it's easy for eyeliner to look bad.\nOne easy way (instead of wearing mascara) is to lightly wing your eyeliner and put it, slowly becoming thicker into the wing, on your top lid just above your eyelashes to make them look longer and fuller.\n\n,, Use something natural, like a tinted gloss or balm. You don't want to look obvious!\n\n",
        "The paper looks at the problem of transferring a policy learned in a simulator to  a target real-world system.  The proposed approach considers using an ensemble of simulated source domains, along with adversarial training, to learn a robust policy that is able to generalize to several target domains.\n\nOverall, the paper tackles an interesting problem, and provides a reasonable solution.  The notion of adversarial training used here does not seem the same as other recent literature (e.g. on GANs).  It would be useful to add more details on a few components, as discussed in the question/response round.  I also encourage including the results with alternative policy gradient subroutines, even if they don’t perform well (e.g. Reinforce), as well as results with and without the baseline on the value function. Such results are very useful to other researchers.",
        "  Using a large galaxy group catalogue based on the Sloan Digital Sky Survey\nData Release 4 we measure three different types of intrinsic galaxy alignment\nwithin groups: halo alignment between the orientation of the brightest group\ngalaxies (BGG) and the distribution of its satellite galaxies, radial alignment\nbetween the orientation of a satellite galaxy and the direction towards its\nBGG, and direct alignment between the orientation of the BGG and that of its\nsatellites. In agreement with previous studies we find that satellite galaxies\nare preferentially located along the major axis. In addition, on scales r < 0.7\nRvir we find that red satellites are preferentially aligned radially with the\ndirection to the BGG. The orientations of blue satellites, however, are\nperfectly consistent with being isotropic. Finally, on scales r < 0.1 \\Rvir, we\nfind a weak but significant indication for direct alignment between satellites\nand BGGs. We briefly discuss the implications for weak lensing measurements.\n",
        "The paper explores the automatic detection of adversarial examples by training a classifier to recognize them. This is an interesting direction, even though they are obviously concerns about training an adversary to circumvent this model. Nonetheless, the experimental results presented in the paper are of interest to the ICLR audience. Many of the initial reviewer comments appear to be appropriately addressed in the revision of the paper.",
        "  We study the evolution of the Aromatic Infrared Bands (AIBs) emitters across\nthe illuminated edge of the Horsehead nebula and especially their survival and\nproperties in the HII region. We present spectral mapping observations taken\nwith the Infrared Spectrograph (IRS) at wavelengths 5.2-38 microns. A strong\nAIB at 11.3 microns is detected in the HII region, relative to the other AIBs\nat 6.2, 7.7 and 8.6 microns. The intensity of this band appears to be\ncorrelated with the intensity of the [NeII] at 12.8 microns and of Halpha,\nwhich shows that the emitters of the 11.3 microns band are located in the\nionised gas. The survival of PAHs in the HII region could be due to the\nmoderate intensity of the radiation field (G0 about 100) and the lack of\nphotons with energy above about 25eV. The enhancement of the intensity of the\n11.3 microns band in the HII region, relative to the other AIBs can be\nexplained by the presence of neutral PAHs. Our observations highlight a\ntransition region between ionised and neutral PAHs observed with ideal\nconditions in our Galaxy. A scenario where PAHs can survive in HII regions and\nbe significantly neutral could explain the detection of a prominent 11.3\nmicrons band in other Spitzer observations.\n",
        "This submission has a couple important contributions and it'd be actually easy to split it into 2 strong papers.\n\nRoughly:\n1. Especially in deep rl, policy gradient methods have suffered from worse sample complexity compared to value-based methods like DQN. Learning a critic to improve sample efficiency for policy gradient methods is a straightforward idea but this is the first convincing demonstration (by carefully combing different elements like Retrace(\\lambda) and experience replay). This represents an important step towards making policy gradient methods more sample efficient and alone, I believe, merits acceptance. It's worth noting that there is another ICLR submission Q-Prop (",
        "This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.\n\n- Do the reported decoding times take into account the vocabulary reduction step?\n- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?\n- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.\n",
        "  There are two promising scenarios that explain the ankle, which is a dip in\nthe spectrum of cosmic rays at $\\sim 10^{19}$ eV. A scenario interprets the\nankle as the transition from Galactic to extragalactic cosmic rays ({\\it\nankle-transition scenario}), while the other is that the dip caused by pair\nproduction on the cosmic microwave background radiation ({\\it proton-dip\nscenario}). In this paper, we consider whether cosmogenic neutrinos can be a\nclue to judge which scenario is favored. We calculated the fluxes of cosmogenic\nneutrinos following these scenarios with plausible physical parameter sets, and\nfound several important features as follows. First of all, the neutrino flux at\n$\\sim 10^{20}$ eV becomes much higher in the ankle-transition scenario as long\nas the maximum energy of the cosmic rays at sources is sufficiently high. On\nthe other hand, the neutrino spectrum has a characteristic peak at $\\sim\n10^{16}$ eV in the proton-dip scenario on the condition that extragalactic\nprotons significantly contribute to the observed cosmic rays down to $10^{17}$\neV. Thus, we conclude cosmogenic neutrinos should give us a clue to judge which\nscenario is favored, unless these features are masked by the neutrino\nbackground coming from possible, powerful neutrino sources such as AGNs and\nGRBs. We also found an interesting feature that the neutrino flux at $\\sim\n10^{18}$ eV depends only on the cosmological evolution of the cosmic ray\nsources. That means cosmogenic neutrinos with the energy bring us information\non the cosmological evolution of the sources of ultra-high energy cosmic rays.\nFinally, we compare the fluxes of cosmogenic neutrinos with the expected\nsensitivity curves of several neutrino detectors, and conclude the detection of\ncosmogenic neutrinos in the near future is promising.\n",
        "Mata Hati Telinga is the third album from Indonesian pop group Maliq & D'Essentials. Released on 8 March 2009, it is the band's first album with guitarist Arya \"Lale\" Aditya, who replaced Satrio Moersid in 2008.\nIt is also the last album to feature Amar Ibrahim as a full member of the band, though he continues to perform live with them whenever available and has contributed trumpet and flugelhorn to subsequent studio albums.\n\nTrack listing\nMusic and lyrics by Widi Puradiredja unless otherwise stated.\n\nPersonnel\nMaliq & D'Essentials\nAngga Puradiredja – vocals\nIndah Wisnuwardhana – vocals\nWidi Puradiredja – drums, Moog\nDendy \"Javafinger\" Sukarno – bass\nIfa Fachir – keyboards\nAmar Ibrahim – trumpet\nArya \"Lale\" Aditya – guitar\n\nAdditional musicians\nRicky Lionardi – orchestra arrangement and orchestration (track 1)\nEugene Bounty – alto saxophone and clarinet (tracks 2 and 3)\nEnggar Widodo – trombone and tuba (tracks 2 and 3)\nReza Jozef \"Rejoz\" Patty – percussion (tracks 2 and 3)\n\nProduction\nEki \"EQ\" Puradiredja – producer\nIndra Lesmana – mixing and mastering\nWidi Puradiredja – engineer\nDendy \"Javafinger\" Sukarno – engineer\n\nReferences\n\n2009 albums\nMaliq & D'Essentials albums",
        "- Strengths:\n\nThe authors focus on a very challenging task of answering open-domain question\nfrom Wikipedia. Authors have developed 1) a document retriever to retrieve\nrelevant Wikipedia articles for a question, and 2) Document retriever to\nretrieve the exact answer from the retrieved paragraphs. \nAuthors used Distant Supervision to fine-tune their model. Experiments show\nthat the document reader performs better than WikiSearch API, and Document\nReader model does better than some recent models for QA.\n\n- Weaknesses:\nThe final results are inferior to some other models, as presented by the\nauthors. Also, no error analysis is provided.\n\n- General Discussion:\n\nThe proposed systems by the authors is end-to-end and interesting. However, I\nhave some concerns below.\n\nDocument Retriever: Authors have shown a better retrieval performance than Wiki\nSearch. However, it is not described as to how exactly the API is used.\nWikiSearch may not be a good baseline for querying \"questions\" (API suits\nstructured retrieval more). Why don't the authors use some standard IR\nbaselines for this?\n\nDistant Supervision: How effective and reliable was distant supervision?\nClearly, the authors had to avoid using many training examples because of this,\nbut whatever examples the authors could use, what fraction was actually \"close\nto correct\"? Some statistics would be helpful to understand if some more\nfine-tuning of distant supervision could have helped.\n\nFull Wikipedia results: This was the main aim of the authors and as authors\nthemselves said, the full system gives a performance of 26.7 (49.6 when correct\ndoc given, 69.5 when correct paragraph is given). Clearly, that should be a\nmotivation to work more on the retrieval aspect? For WebQuestions, the results\nare much inferior to YodaQA, and that raises the question -- whether Wikipedia\nitself is sufficient to answer all the open-domain questions? Should authors\nthink of an integrated model to address this? \n\nOverall, the final results shown in Tables 4 and 5 are inferior to some other\nmodels. While authors only use Wikipedia, the results are not indicative of\nthis being the best strategy.\n\nOther points:\nThe F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and\nTest).\nTable 5: Why not \"No f_emb\"?\nError analysis: Some error analysis is required in various components of the\nsystem. \nAre there some specific type of questions, where the system does not perform\nwell? Is there any way one can choose which question is a good candidate to be\nanswered by Wikipedia, and use this method only for those questions?\nFor WebQuestions, DS degrades the performance further.",
        " There are many to choose from, and, depending on company and features, there are a multitude of programs out there that will help guide you through the rest of the Internet.;\n,,, If there is no such setting by that name, you can easily find it elsewhere in Windows.\n\n, This will open up another dialog box.\n\n,, This should open up a place where, you yourself can set whatever programs you'd like to enable/show/use.\n\n,, And ensure that the other program you'd like to use remains enabled (checked).\n\n,,,, This should list the available options you can choose from, such as My Documents, My Pictures, My Music, etc.\n\n,, This will bring up yet another dialog box.\n\n,\n\n\nIf the option for \"Classic Menu\" is highlighted, you don't need to read any further. Close these boxes and cancel. The top section will work. Open your program and make the browser you want the default. Upon first load, many browsers will ask whether or not you'd like to make the program the default browser.\n\n\n\n\n\n\n\n,,,\n\n\nUpon opening many other browsers for the first time, they'll ask you whether you want to make them the default. This is another place to find this option (of making a browser the default).\nIf not, you'll need to continue onwards with these steps.\n\n, Click the box to open it.\n\n,,\n\n\nIf there is no Ok button, you may just close the window to save your work.\n\n",
        " You will need: two glass beakers, filter paper and potassium nitrate for the salt bridge, aluminium foil and one molar aluminium nitrate solution for the negative electrode, wires and crocodile clips. The materials for the positive electrode vary. You will also need something to act as a circuit load, like a light bulb.\n\nOne molar = one mole per litre.;\n,,,, Bend the filter paper so that it touches the bottom of both beakers.,, The strip should touch the bottom of the beaker. It should not touch the salt bridge. You may wish to bend the top of the strip over the edge of the beaker. This aluminium strip is the electrode.\n\nYou have formed an Al3+/Al half cell.\n\n, You will need: a strip of copper and some one molar copper nitrate solution.,, It should touch the bottom, but not the salt bridge. This is the positive electrode.\n\nThis cell has a voltage of 1.821V.\n\n, You will need: one molar iron (III) nitrate (Fe(NO3)3) solution, one molar iron (II) nitrate (Fe(NO3)2) solution, and a conductive graphite rod.,, It should not touch the salt bridge. This is the positive electrode.\n\nThis cell has a voltage of 2.432V.\n\n, You will need: one molar potassium dichromate solution, one molar nitric acid, one molar chromium nitrate solution, and a conductive graphite rod., One measure should be the largest amount that you can add without the beaker overflowing after everything is added., It should not touch the salt bridge. This is the positive electrode.\n\nThis cell has a voltage of 2.992V.\n\n, You will need: one molar potassium permanganate solution, one molar nitric acid, one molar manganese nitrate solution, and a conductive graphite rod., One measure should be the largest amount that you can add without the beaker overflowing after everything is added., It should not touch the salt bridge. This is the positive electrode.\n\nThis cell has a voltage of 3.172V.\n\n,, The bulb should light up.",
        "The Trojans were losers, but they were very heroic losers. While there are readings of the Iliad that paint the Trojans in a somewhat negative light, I think the most natural readings, and those supported most by later authors, give the Trojans a great deal of glory. And this glorification of a heroic defeat is hardly unique to the Romans--in the debates over English historical identity, for example, the defeated Anglo Saxons won out over the Norman conquerors, and there are rather more statues of Boudicca in Britain than of Suetonius Paulinus.\n\nBut beyond that, the deeper question is \"why the Trojan War?\" After all the Romans were not Greeks, so why wrap so much identity up in a Greek poem? The answer to this goes back quite a bit farther than the conquest of Greece. During early Greek colonization of the western Mediterranean, Greek cultural products became very popular within Italy. For example, one of the most iconic Greek artistic products are the black figure vases, but the great majority of these have actually been found in Italy, and were originally thought to be created by the Etruscans. This importation of material also included an importation of ideas, and in particular Odysseus became a popular founding figure among many of the Etruscan settlements. The Roman adoption of Aeneas is within this Italian context in which the \"Greek\" figures of the Trojan War became heroic figures within Italian communities.\n\nA classic study of the way ethnicity and heroic stories became entangled is *The Returns of Odysseus*, although it is largely from the Greek perspective.",
        "It was all to do with atmospheric oxygen concentration.\\*\n\nArthropods, including insects and crustaceans, more or less lack the active respiratory and closed circulatory systems present in, say, us vertebrates (actively inhaling oxygen, that's then delivered to cells via blood) - rather they use a more passive system, where oxygen enters and is distributed around the body directly by a network of holes (spiracles) and tubes (trachaea) (see [this diagram](_URL_0_)). \n\nAs insects become bigger, this type of semi-passive system becomes increasingly less effective, essentially acting as a limit on size after a certain point. Increasingly large insects simply can't get enough oxygen to keep them alive.\n\nThis is all true, at least, if the atmospheric concentration of oxygen remains around 21%. If you were to increase atmospheric oxygen, arthropod respiration would become more effective and that maximum imposed size would increase - hence bigger creepy crawlies.\n\nAs it happens, during the Carboniferous period some 300 million years ago, it's thought atmospheric oxygen might've peaked at around 35% - hence the [gull-sized dragonflies](_URL_1_) and [millipedes](_URL_3_) as long n' wide as a human. \n\nWhy did it peak? Not too long before, during the early Devonian (~400mya), the land was largely devoid of the plant biomass we're familiar with today. It was during the mid-Devonian through to the Carboniferous (390-300mya) that terrestrial plant life exploded in size, range and diversity - once largely ankle-high lawns of rudimentary shrubs became dense forests covering large swathes of the globe. It's this explosion in terrestrial photosynthesising plant life that sucked so much carbon out of the atmosphere in such a relatively short span of time, converting it into oxygen, that caused the sharp peak in atmospheric oxygen. \n\nOver time, long-term geologic and oxidative processes began to rebalance that concentration to the levels we're more familiar with today, dragging the maximum size of terrestrial arthropods down with it. As such, the most massive terrestrial insects we see today - such as the [*Goliathus* beetles](_URL_7_) - only get as large as a child's fist. Which, I guess, is a good thing - dunno' about you guys, but I quite enjoy not being overrun by [giant, flesh-eating spiders](_URL_4_), 'innit.\n\n__________\n\n^**References:**\n\n[^(Berner, R.A. (1999)^) ^(Atmospheric oxygen over Phanerozoic time. *PNAS*. 96 (20)^), ^10955-10957](_URL_5_)\n\n[^(Kaiser, A., Klok, C.J., Socha, J.J. *et al.* (2007)^) ^(Increase in tracheal investment with beetle size supports hypothesis of oxygen limitation on insect gigantism. *PNAS*. 104 (32)^), ^12198-13203](_URL_8_)\n\n^__*__ ^(Well, perhaps not *completely*. As )[^(other users)](_URL_6_) ^(have mentioned, lack of competition and/or predation from other modern lineages may also have played a part.) [^This ^study](_URL_2_) ^(suggests atmospheric oxygen limited maximum airborne insect size until the early-Cretaceous, when it decoupled in step with the rise of birds.)",
        "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.",
        "A new memory module based on k-NN is presented.\nThe paper is very well written and the results are convincing. \n\nOmniglot is a good sanity test and the performance is surprisingly good.\nThe artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.\nAnd the translation task eventually makes a very strong point on practical usefulness of the proposed model.\n\nI am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.",
        "  We present radio observations and optical spectroscopy of the giant low\nsurface brightness (LSB) galaxy PGC 045080 (or 1300+0144). PGC 045080 is a\nmoderately distant galaxy having a highly inclined optical disk and massive HI\ngas content. Radio continuum observations of the galaxy were carried out at 320\nMHz, 610 MHz and 1.4 GHz. Continuum emission was detected and mapped in the\ngalaxy. The emission appears extended over the inner disk at all three\nfrequencies. At 1.4 GHz and 610 MHz it appears to have two distinct lobes. We\nalso did optical spectroscopy of the galaxy nucleus; the spectrum did not show\nany strong emission lines associated with AGN activity but the presence of a\nweak AGN cannot be ruled out. Furthermore, comparison of the H$\\alpha$ flux and\nradio continuum at 1.4 GHz suggests that a significant fraction of the emission\nis non-thermal in nature. Hence we conclude that a weak or hidden AGN may be\npresent in PGC 045080. The extended radio emission represents lobes/jets from\nthe AGN. These observations show that although LSB galaxies are metal poor and\nhave very little star formation, their centers can host significant AGN\nactivity. We also mapped the HI gas disk and velocity field in PGC 045080. The\nHI disk extends well beyond the optical disk and appears warped. In the HI\nintensity maps, the disk appears distinctly lopsided. The velocity field is\ndisturbed on the lopsided side of the disk but is fairly uniform in the other\nhalf. We derived the HI rotation curve for the galaxy from the velocity field.\nThe rotation curve has a flat rotation speed of ~ 190 km/s.\n",
        "Imagine that you worked in a filing facility - Your job is to take documents provided by various places and store them in your facility.\n\nSince you can't be there 24/7, you have a \"work area\" which consists of a desk where people can drop files off. When people have a document for you to file away, they leave it on this desk, and then when you see it you take it and place it back in storage.\n\nHowever, frequently people need to one: Get their document back, and two: Make changes and then send the file back to storage. This process is quite slow though: The person first has to leave a 'document request' form on the desk telling you what document they need. Then, when you come in and see it, you have to go back into storage, find the document, and leave it on the desk for them. Then they can take the document, make changes, and leave it back on the desk for you to find and store away again.\n\nThe above isn't a huge problem if it's only a few documents, but you quickly find that *lots* of your customers are grabbing the *same document* over and over again, only to make a small change and put it back. This seems like a waste of time to you, because you end-up going to get the document from storage over and over again when-ever they need it. Thus, to remedy the problem, you add a 'fast-access' area to your desk, where you leave the most commonly or recently edited documents from each customer. That way, when a customer needs that document, they can just grab it right from there without having to go back into storage to get it. This all works fine to speed things up, and everybodies happy that the time to get a lot of their most recently used documents has gone from a day to instant access.\n\nBut then a problem happens: The storage facility you're using decided to pack-up and move to a different location. It's not a huge problem, you just open a location for people to go and get their documents over there instead. However, some of your customers start complaining that you don't have all their documents. When you think about it, when the storage facility moved you never took all of the documents in the 'fast-access' area! Thus, you lost all those documents. To remedy the situation, you decide that the facility owner has to 'notify' you if they're going to move so you have time to move all of the documents in the 'fast-access' area into storage to prepare for the move.\n\nThe above is basically how the OS handles your USB drive (the storage facility). USB drive access is fairly slow, with portable hard-drives even slower. To try and speed things up, the OS realizes that you have a few files that you either just edited, or are frequently editing, and keeps those files in memory (fast-access) instead of constantly writing it to your USB drive (which is slow). The problem is that if you remove your USB drive (move the storage facility), you lose the file's currently stored in memory unless you (the storage facility owner) notify the OS (the person handling the storage facility) that you're about to move the USB drive somewhere else.",
        "Med student here. The topvoted answers don't tell the whole truth. There are two reasons as to why pain is delayed/inhibited. \n\nELI5:\n\nThe first is that the inflammatory process takes time. It takes time for pain-inducing chemicals to be produced/released and for these to affect nerves, by activating them.\n\nThe second reason is that your body has a store of molecules that can be released in the spinalcord in stressful situations, which actually have a pain-dampening effect. They block pain-signals travelling to the brain, making you unaware of the pain. When the stressful situation is over and the effect of the inhibitory molecules passes, you will be made aware of the pain\n\n\nNOT ELI5:\n\nReason 1: Inflammatory mediators need to be released and/or created by inflammatory cells, and need to make their way to nerve endings. Histamine for example, also causes the blood vessels to increase in permeability, effectively making exudate from the blood flow into the tissue. The increased swelling in the tissue also presses on nerves (nociceptors - sensory nerves that carry pain signals), activating them which also leads to pain, once the signal reaches the brain.\n\nAs a fight or flight response the following happens:\n\nReason 2: Basically what happens when you dont feel pain at the actual time of injury is your body releasing inhibitory neurotransmittors (endorphins (endogenous morphines), GABA, noradrenaline etc) in the spinal cord which inhibits pain signals travelling up to the brain, and hence less, if any, pain signals reach the brain, making you unaware of the pain. \n\nEdit: Added another reason for pain inhibition. These two reasons combined explain why we don't feel pain at the exact time of injury.\n\nEdit: Added a basic version (ELI5)",
        "Former game dev here,\n\nIt's not impossible, there's just no profit in it for the amount of work involved. The studio would be selling to a tiny niche who would want such a thing, and they can't possibly charge enough to turn a profit at a price the market will tolerate.\n\nOn the technical side, game code for a console is incredibly specific to that console. What you suggest would require a near complete rewrite from scratch of these games, and recreation of all art assets - which would require a full scale dev team to port to a modern platform. That, and not only would the code have to be ported, but redesigned to work with modern graphics on this other, modern platform. And then you would of course require full scale play testing to find and fix bugs and ensure the game experience is faithful to the original.\n\nIt costs ~$10m to make a low end AAA title, and these ports will cost at least as much, likely ~$20m per title. And if you're going to improve the video quality, it's inadequate to go with an incremental improvement, you might as well go all the way, so the effort required would be demanding, escalating the price. Anything less than what is expected for an exalted title on a modern platform would be a complete market flop. And again, this is for a niche market. Buyers don't buy the same experience again, unless you're Nintendo.",
        "Keratin 20, often abbreviated CK20, is a protein that in humans is encoded by the KRT20 gene.\n\nKeratin 20 is a type I cytokeratin. It is a major cellular protein of mature enterocytes and goblet cells and is specifically found in the gastric and intestinal mucosa.\n\nIn immunohistochemistry, antibodies to CK20 can be used to identify a range of adenocarcinoma arising from epithelia that normally contain the CK20 protein. For example, the protein is commonly found in colorectal cancer, transitional cell carcinomas and in  Merkel cell carcinoma, but is absent in lung cancer, prostate cancer, and non-mucinous ovarian cancer. It is often used in combination with antibodies to CK7 to distinguish different types of glandular tumour.\n\nReferences\n\nFurther reading\n\nKeratins",
        "Fireworks Entertainment (originally Skyvision Entertainment) was an independent studio originally founded in 1991 by Brian K. Ross and later bought out by Jay Firestone in 1996 to produce, distribute and finance television shows and feature films.\n\nSkyvision Entertainment was originally operating as a division of John Labatt Entertainment Group.\n\nIn 1993, Orion Pictures inked an agreement with Skyvision Entertainment to handle series rights to the RoboCop franchise. Also that year, it entered into an agreement with Rigel Entertainment for international distribution rights to RoboCop: The Series.\n\nIn 1996, Skyvision Entertainment was purchased by Jay Firestone, former employee of Alliance Communications, and rebranded it to Fireworks Entertainment. The first show under the new name was F/X: The Series, which they acquired from Orion Pictures in 1994.\n\nFireworks was acquired by Canwest Global in May 1998, and was later sold to ContentFilm (production company of The Cooler), a British company, in April 2005. Over the years, Fireworks has amassed a significant catalogue of television shows and movies (under the Fireworks Pictures label)\n\nIn 1998, Peter Hoffman's Seven Arts Pictures formed an alliance with Fireworks to start out the Seven Arts International branding. In 2000, CanWest Films merged with Seven Arts International, another Canwest subsidiary to start the Fireworks Pictures branding to produce theatrical motion pictures. On October 2, 2001, Pliny Porter was hired as head of production and development for the Fireworks Pictures subsidiary, in order to make an effort to continue producing their own feature films.\n\nFrom March 14, 2011, Fireworks International became Content Television under the umbrella Content Media Corporation PLC.\n\nCourt cases \nThe original company was sued by Sony regarding Queen of Swords and by 20th Century Fox regarding Mutant X.\n\nTelevision shows (as Fireworks Entertainment) \nTV shows filmed in widescreen 16:9 from 2000 but generally broadcast in 4:3 pan and scan. The widescreen versions are available on DVD.\n\n 100 Deeds for Eddie McDowd\n 18 Wheels of Justice\n Adventure Inc.\n Andromeda (Gene Roddenberry)\n Black Hole High\n Caitlin's Way\n Even Stevens (co-produced by Disney Channel)\n F/X: The Series\n Highlander: The Raven\n La Femme Nikita (co-produced by Warner Bros. Television)\n Mutant X\n Queen of Swords\n Relic Hunter\nRoboCop: The Series\n RoboCop: Prime Directives (TV miniseries)\nSCTV (distribution only; inherited from WIC during CanWest era)\n Zoe Busiek: Wild Card\n Young Dracula\n\nFilms (as Fireworks Pictures) \n A Wrinkle in Time\n An American Rhapsody\n Better Than Sex\n Coronado\n Faithless\n Greenfingers\n Hardball\n Innocence\n Interstate 60\n Me Without You\n Nola\n Passionada\n Raising Victor Vargas\n Rat Race\n Simon Magus\n Solas\n The Believer\n The Man from Elysian Fields\n Who Is Cletis Tout?\n\nReferences \n\nMass media companies established in 1991\nMass media companies disestablished in 2011\nFilm production companies of Canada\nFilm production companies of the United Kingdom\nFilm production companies of the United States\nFormer Corus Entertainment subsidiaries",
        " Using a pry tool or something thin that is not sharp gently pry around the outside of the phone and slowly lift the broken screen and fold it open towards the top.;\n, Place a suction cup near the home button and pull the screen up. This will require some force. Slip something thin like a guitar pick or spudger into the crack to pry it open carefully.\n\n, Note their positions. Placing the wrong screw in the top right hole will cause the phone to no longer boot correctly and cause permanent logic board damage.\n\n,  There are 3 cables in total.\n\n, Replace this on the new display.\n\n, Don't forget the small white square for the light sensor!\n\n, Remove the LCD shield plate and replace it on the new display.\n\n, Use 3M adhesive the reattach the new frame. Be careful not to use to much heat or you will burn the LCD. 20 seconds on high with a blow dryer about a foot away should be fine.\n\n,, Common problems are:\n\n\nLines across the new screen (this could mean you have not connected it correctly). Push it in ALL the way. Make sure it is perfectly lined up.\nNo visual at all - please ensure once again you check the connections.\n\n, This is common. Just reboot or disconnect and reconnect the battery.\n\n, Reinstall the four screws and close the phone. Line up the frame correctly or it will not close.\n\n, This reduces the chance of the Pentalobe screws being stripped.\n\n",
        "The Woolmer lecture is the flagship lecture of the Institute of Physics and Engineering in Medicine. It takes place annually during the Institute's Medical Physics and Engineering Conference.\n\nDedication \nThe lecture is dedicated to Professor Ronald Woolmer who was the first Director of the Research Department of Anaesthetics at the Royal College of Surgeons. Woolmer convened a meeting at the Royal College of Surgeons, London, to discuss the evolving field of engineering applied to medicine. It was agreed that the group should hold regular meetings and as a result the Biological Engineering Society (BES) was formed with Ronald Woolmer as the first President. Woolmer died two years after the formation of the BES and it was agreed that a memorial lecture would be sponsored in recognition of his achievements.\n\nLecturers \n 2002 Anthony Unsworth\n 2003 Arun Holden\n 2004 Kevin Warwick \n 2005 Henrik Gollee \n 2006 Denis Noble\n 2007 Michael Brady\n 2008 Clive Hahn\n 2009 Martin Birchall\n 2010 Mark Tooley\n 2011 Willi Kalender\n 2012 Lionel Tarassenko\n 2013 Molly Stevens\n 2014 David Keating\n 2015 Anthony Barker\n 2016 Andrew Taylor\n 2017 Josef Käs\n 2018 Alison Noble\n\nSee also\n\n List of medicine awards\n\nReferences \n\nLecture series\nMedicine awards\nBritish science and technology awards",
        "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data.  I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.",
        "We thank all the reviewers for their helpful comments and questions!  Below we clarify a few points raised in the reviews and describe the revisions we have made to the paper.\n\nAll three reviewers, from different perspectives, suggested better positioning of the paper relative to our own prior work and other existing work. We have taken these comments to heart and revised the paper accordingly. In particular, we have further emphasized the following two connections.\n\nMUS-ROVER II vs. MUS-ROVER I: The purpose of the “MUS-ROVER Overview” section was to clarify the connection between the current rover and its earlier version. We have revised it to make the connection clearer. More specifically, we have updated Figure 1’s caption as well as the “(Rule Representation)” paragraph to include a brief explanation of the student’s optimization problem whose formulation is unchanged in the current rover (Reviewer 1). Also, although we had compared the two rovers in terms of their models, and made it clear what is inherited from our prior work and what is new in our current work, we realize that we did not explicitly compare the two rovers in terms of results and/or contributions. So in the revision, we first state the contribution of our prior work, making it clear that the earlier rover was already able to extract basic voice-leading rules such as “Parallel perfect intervals are rare” (Reviewer 2); and then we emphasize that the current rover not only extracts more rules with longer-term dependencies, but more importantly, studies the hierarchies of the extracted rules. Furthermore, we have added a visualization subsection in the end of the “Experiments” section to further compare the results from the two rovers, i.e. hierarchical rule families and subfamilies vs. unstructured rule sequences (Reviewer 3). In addition, we want to clarify that the purpose of this paper is indeed to focus on one aspect of the system (Reviewer 2) rather than to try to explain everything in details for the entire MUS-ROVER system. The aspect that we emphasize in this paper, is the deeper interpretability that is achieved by rule hierarchies and adaptive 2D memory selection. We totally agree with Reviewer 2 that at some point in the future, a comprehensive journal paper will be the best place to fully explain the entire MUS-ROVER system. However, it is apparent that MUS-ROVER II is not the end of this line of research, and we are making progress in continuing the study of building new versions of rovers, each of which emphasizes a different aspect of the system. It is quite common in our area to publish several conference papers with distinct focuses and then to synthesize them in a longer journal paper to unify the framework. This is the path that we plan to follow.\n\nMUS-ROVER vs. Generative models (e.g. LSTMs): Although MUS-ROVER has a generative component that can be used to generate sample pieces, we do not evaluate the rover’s performance based on its generating power. The reasons are twofold. First, the quality (good or bad) of a music piece is not easy to quantify, though there is emerging work in psychology that suggests the Consensual Assessment Technique (CAT) of evaluation by many experts may be useful. Secondly and more importantly, MUS-ROVER is not targeted as an automatic composer, but as an automatic theorist or pedagogue, so the goal is not to generate pieces, but to explain what has been learned through the entire learning process. We have revised several places in the “Introduction” section to try to make this distinction clear: MUS-ROVER, as a pathfinder to Mount Parnassus, cares about the path towards the destination a lot more than the destination per se. Given that the outputs of generative models (such as LSTMs) are generated samples, they are not comparable to outputs of MUS-ROVER, which are instead ways of generating samples (Reviewer 3).\n\nReviewer 3 mentioned that the math section on features, feature-induced partitions, and conceptual hierarchy is an over-complicated way of describing non-overlapping hierarchical clustering. We’d like to clarify that it is absolutely necessary to have this multi-step process: features → partitions → hierarchy, and a simple hierarchical clustering wouldn’t work to achieve our goal. The reasons are twofold. 1) Algorithmically, hierarchical clustering will lose many inter-connections due to its greedy algorithm and its tree structure (conceptual hierarchy on the other hand is a DAG). 2) More importantly, hierarchical clustering will lose the interpretability of the resulting partitions: as we pointed out in the “Feature-Induced Partitions” subsection, “We use a partition to refer to the essence of a concept, and the inducing feature function as a mathematical name to interpret the concept”. So without features, having partitions alone will miss the goal of achieving deeper interpretability. To make this clear, we added a small paragraph in the end of the “Conceptual Hierarchy” subsection, emphasizing the necessity of our approach.\n\nReviewer 3 also mentioned difficulty in understanding the music terms. We apologize if any of the music symbols in the paper cause difficulty for non-musician readers. However, as mentioned above and mentioned in the earlier responses, we do not require readers of this paper to have any music background, and moreover, we do not require the users of MUS-ROVER to have any prior knowledge on music theory either, since the whole purpose of MUS-ROVER is to teach music theory from scratch. The focus of this paper, as opposed to our prior work published in a music venue, is on hierarchies and adaptive 2D memory selection. Taking Table 1 as an example, we do not expect readers to be aware of any underlying music concept so there are no music terms there: all notations are functions, or more precisely, descriptors and windows that are introduced in the “Interpretable Features” subsection. We tried our best to restrict any music-related terms and symbols from the main body of the paper, and put them in the Appendix whenever possible.\n\nLastly, we strongly agree with Reviewer 1, that deploying MUS-ROVER into a real educational environment will be one of the most exciting things to try next. We are actively collaborating with professors from our music department, to make a live and personalized teaching system in the near future.\n",
        ";\n, This will bring up your computer’s Advanced Boot Options screen.\n\n\nIf you are using Windows 8, hold down the Shift key while repeatedly pressing F8 to access the Advanced Boot Options screen.\n\n,,,\n\n\nIf you are using Windows XP, type “C:\\windows\\system32\\restore\\rstrui.exe” and press “Enter.”\n\n, Ideally, you should choose a restore point that occurred a few days earlier than the date on which your computer was infected by the FBI MoneyPak virus.\n\n,, Upon startup, the FBI MoneyPak virus will no longer prevent you from accessing certain applications.\n\n,, Examples of applications that are effective for malware removal and recommended by experts in the security industry are Malwarebytes Anti-Malware, Spybot Search and Destroy, ComboFix, and HijackThis.\n\n,, The software will search for and locate any remaining traces of the FBI MoneyPak virus on your machine.\n\n, The malware program will delete any and all harmful files detected on your machine, including those that may be separate from the FBI virus.\n\n, Going forward, your computer will be guarded more carefully against any malicious cyber threats.\n\n",
        "Mmm, very interesting question.\n\nLet's take \"extinction level event\" to mean a mass extinction like the big 5. \n\nThe closest analogue to this is the [PETM](_URL_0_), the Paleocene-Eocene thermal maximum. This was an event about 50 million years ago that had a short term increase in global temperatures of about 5 degrees C. The best evidence suggests that it was caused by a cascade of methane, leading to global warming and sea level rise that is larger than what we are looking at today. \n\nThis is the best candidate for a historic arctic methane event, and if there were others we might know. So we know that it is possible, but very rare to have this kind of event. We can't tell how long it took, but we know from beginning to end the event was under 10,000 years, but that's just because it's tough to measure short time periods in the geologic record. It could have been much shorter, especially on the release end. \n\nThe next question is -- did the PETM cause a mass extinction? The answer is that no, it did not. The Paleocene/Eocene is associated with a faunal changeover but not a mass extinction. However, this doesn't mean that there wasn't a smaller extinction associated with the PETM! Almost all transitions between geologic periods are defined by changes in fauna of moderate size, and especially in some groups we'd expect to be hit hard by a change in ocean chemistry and temperature like benthic foraminifera. However, even these \"hard hit\" groups didn't really show mass extinction levels of turnover. Alternative is that rates of evolution were higher for many groups, so their old forms disappear from the fossil record while their descendents simply look different -- a \"pseudo-extinction\". It seems very likely that the methane release was the cause of this worldwide shift in biota. \n\nThe next part of answering \"can there be...\" is to think about if the PETM is the worst possible event that could happen... if we could show it's likely that an event could be much worse, we might decide the answer to your question is \"very likely so\". \n\nThe world today is different than it was in Paleocene time. First off, we have massive continental ice shelfs-- there was no continental ice shelfs then. Sudden increase in global temperature interacting with continental ice might cause cascading events that lead to a much worse event. Next, If much larger quantities of methane were locked up in the sea floor, you could potententially have a much larger temperature excursion, including one that would cause a mass extinction. I don't think anyone really knows enough about methane to say if this is plausible or likely. So, we certainly can't exclude the possibility that an arctic methane event could cause a mass extinction. However, it also doesn't seem that likely. We are, after all, talking about a single event from 50 million years ago. The conditions for a massive release clearly aren't common. \n\nTo summarize, methane hydrate release seems to have caused a \"small\" extinction about 50 million years ago, in a very short period of time. While that event didn't cause a truely large extinction, there are reasons to think that event wasn't as bad it could be. We can not exclude arctic methane release as a potential cause of a major extinction, if everything lined up right... but it also doesn't seem very likely.",
        "The reviewers generally liked the application; there were a number of technical points raised that leave doubt about the novelty of the approach. However, this may be an interesting avenue in the future, thus the PCs are accepting it to the workshop track.",
        "Think about the process of how water freezes. \n\n1) We know water expands when it freezes \n2) We know water freezes from the outside-in.\n\nWhen you pour water into an ice cube tray, its liquid. The first thing that freezes is a very thin outer most layer. The water on the inside expands and pushes against that first solid layer but not so much that it breaks that layer. \n\nThis increased inside pressure is in equilibrium with the strength of the outside solid layer.\n\nAgain, freezing from the outside-in, a second layer is formed (inside the first), strengthening the combination of the outside layers. At the same time the liquid pressure inside increasingly pushes against the combination of outside layers because water is still expanding. \n\nFreezing continues with the outside solid water becoming thicker  &  thicker, while liquid inside pressure is increasing more, in equilibrium. Enormous potential energy is created.\n\n-- So, when you drop the ice cube in water, the outer-most layer is melted immediately which breaks the equilibrium. The build up of pressure can not be contained anymore and the potential energy is  released immediately resulting in fracture.\n\nIts not so much a rapid temperature change to the whole cube. What happens rapidly is the outer most layer melts. \n\nThis is a very generalized explanation, and I like it. Its not like we can actually witness layers being formed when water freezes. At a molecular scale, generally, the freezing from the outside-in does occur but it could be irregular around the the body of water that is freezing. \n\nRegardless, it is this equilibrium that is maintained during freezing. And the crack is much like swiftly kicking a leg from under an elephant who has climbed to the top of a mountain.\n\n\nTL;DR;\nWater melts the outside of the ice which breaks the equilibrium of the pressure created during the freezing process, the pressure is released via fracture.",
        "  According to current practice, the results of each run of a radiochemical\nsolar neutrino experiment comprise an estimate of the flux and upper and lower\nerror estimates. These estimates are derived by a maximum-likelihood procedure\nfrom the times of decay events in the analysis chamber. This procedure has the\nfollowing shortcomings: (a) Published results sometimes include negative flux\nestimates. (b) Even if the flux estimate is non-negative, the probability\ndistribution function implied by the flux and error estimates will extend into\nnegative territory; and (c) The overall flux estimate derived from the results\nof a sequence of runs may differ substantially from an estimate made by a\nglobal analysis of all of the timing data taken together. These defects\nindicate that the usual packaging of data in radiochemical solar neutrino\nexperiments provides an inadequate summary of the data, which implies a loss of\ninformation. This article reviews this problem from a Bayesian perspective, and\nsuggests an alternative scheme for the packaging of radiochemical solar\nneutrino data, which is we believe free from the above objections.\n",
        "José Diogo Dalot Teixeira (; born 18 March 1999), known as Diogo Dalot, is a Portuguese professional footballer who plays as a right-back for  club Manchester United and the Portugal national team.\n\nDalot is a product of the Porto youth system and made his professional debut for the club's  B team in January 2017. He made his first-team debut in a Taça de Portugal game in October 2017. After making eight appearances for Porto, he joined Manchester United in June 2018 for a reported fee of €22 million (£19 million). From October 2020 to June 2021, Dalot was loaned to Italian Serie A club Milan.\n\nDalot was a youth international and represented Portugal from under-15 to under-21 level. He was a member of the under-17 squad that won the 2016 UEFA European Under-17 Championship. He made his senior international debut for Portugal at UEFA Euro 2020.\n\nClub career\n\nPorto\nBorn in Braga, Dalot joined Porto's youth system in 2008, aged nine. On 28 January 2017, he made his senior debut with the B team, playing the full 90 minutes in a 2–1 home loss against Leixões for the LigaPro championship.\n\nDalot first appeared with the first-team in a competitive game on 13 October 2017, starting in a 6–0 away win over Lusitano de Évora for the season's Taça de Portugal. He first played in the Primeira Liga on 18 February 2018, coming on as a 75th-minute substitute in a 5–0 home routing of Rio Ave.\n\nManchester United\nDalot signed for Premier League club Manchester United on 6 June 2018 on a five-year contract for a fee of £19 million. Upon his arrival in Manchester, coach José Mourinho said that, considering his young age, he was one of the best right-backs around. He made his debut on 19 September 2018 in an away UEFA Champions League group stage match against Swiss side Young Boys, but was unable to have continuity in the team due to an injury sustained in the previous season. His debut in the Premier League happened on December 1 against Southampton in a 2–2 draw On 26 January 2020. Despite Mourinho's sacking and Ole Gunnar Solskjaer being appointed as the new manager, Dalot remained a valuable option: at the end of the season, he had had 23 appearances, 16 of which in the Premier League. During that season, he had a memorable match in Paris, in the Champions League quarter-finals 2 nd leg against PSG, where Manchester United completed a remarkable comeback thanks to a late penalty, won after a shot by Dalot. He scored the second goal in a 6–0 FA Cup win against Tranmere Rovers; it was his first goal for United.\n\nDuring his first seasons for the club, Dalot struggled with various injuries and following the arrival of Aaron Wan-Bissaka, his performances were severely limited under manager Ole Gunnar Solskjaer.\n\nLoan to AC Milan\nFollowing sporadic use by Manchester United, Dalot was loaned to Italian Serie A side AC Milan for the 2020–21 season. He made his debut for Milan on 22 October, starting in a 3–1 win against Celtic in a UEFA Europa League group stage match. Seven days later, Dalot scored his first goal for Milan and provided an assist for compatriot Rafael Leão in a 3–0 home group stage victory in the UEFA Europa League against Sparta Prague. He made his Serie A debut on 1 November, replacing Davide Calabria in the 71st-minute of a 2–1 away win against Udinese. He made his first start in the league in a 2–2 away draw against Genoa. On 7 March 2021, Dalot score his first Serie A goa in a 2–0 away in at Hellas Verona. \n\nDuring the season, Dalot's versatility enabled him to play either as a right back and left back. This made him an integral part of Stefano Pioli's team, helping Milan secure second place in the 2020–21 Serie A and qualification for the 2021–22 UEFA Champions League after an eight-year absence. During his spell as a Rossonero, he was able to play regularly, making 33 appearances, scoring two goals and provinding three assists. As often said by Dalot himself, in Italy he was able to improve defensively, without losing his ability to attack.\n\nReturn to Manchester United\n\nDuring the summer of 2021, Manchester United were interested in signing another right-back. Meanwhile, Milan, who were impressed with Dalot during his loan move with the club, began negotiations with Manchester United to sign him on permanent basis. After returning to Manchester United, he impressed coach Ole Gunnar Solskjaer with his performances during pre-season. Borussia Dortmund were also interested in signing him on a loan deal, but he decided to remain at United to compete with Aaron Wan-Bissaka for a starting position. On 22 September, Dalot was given his first start of the season, featuring in a 1–0 home loss to West Ham United in the third round of the EFL Cup. Since then, he has had limited opportunities with two starts and three substitute appearances. He played against Villarreal in a Champions League group stage match after Wan-Bissaka was suspended for two games. \n\nOn 2 December, Dalot was given his first start in the league under interim manager Michael Carrick, putting an impressive performance and creating the second goal in a 3–2 home win over United's rivals Arsenal at Old Trafford. Following the arrival of interim manager Ralf Rangnick, Dalot cemented his place as starter for the club, following his solid performances in the club's next two matches against Crystal Palace and Norwich City.\n\nInternational career\n\nYouth\nDalot helped Portugal win the 2016 UEFA European Under-17 Championship, scoring twice in five games in Azerbaijan including once in the final against Spain. The same year, he helped the under-19 team reach the quarter-finals of the same competition.\n\nWith the under-19s, Dalot participated in the 2017 UEFA European Under-19 Championship, helping finishing as runner-up, after losing in the final to England. For his performances throughout the competition, he was named in the \"Team of the Tournament\". Dalot played for Portugal at the 2017 FIFA U-20 World Cup, starting in all the matches in an eventual quarter-final exit.\n\nOn 10 November 2017, he won his first cap for the Portugal under-21s, starting in a 1–1 away draw against Romania for the 2019 UEFA European Championship qualifiers. In March 2021, Dalot took part in the 2021 UEFA European Under-21 Championship. Portugal finished as runners-up after losing in the final 1–0 to Germany.\n\nSenior\nOn 13 June 2021, Dalot was included in Portugal's squad for UEFA Euro 2020 as a replacement for João Cancelo, who withdrew after testing positive for COVID-19. He made his debut ten days later in the final group game – a 2–2 draw with France in Budapest – in which he replaced Nélson Semedo for the final 11 minutes. On  27 June, Dalot made his first start with the senior national team, in a 1–0 loss to Belgium in the round 16.\n\nIn October 2021, he was called up by Portugal and on October 9, he provided two assists, with the first being converted by Cristiano Ronaldo in a 3–0 home win against Qatar.\n\nStyle of play\nDalot is a physically strong defender known for his speed, technique and offensive capabilities. He can play as a full-back or winger on either flank, although he usually plays on the right. He is usually deployed as a wing-back on the right but in a more conventional full-back role on the left. As a left-back, he has been praised for his work ethic and defensive awareness. He has good dribbling skill and is noted for his involvement in counter-attacks by making crosses or long passes. His physique enables him to perform well in aerial duels.\n\nCareer statistics\n\nClub\n\nInternational\n\nHonours\nPorto\nPrimeira Liga: 2017–18\n\nPortugal\nUEFA European Under-17 Championship: 2016\n\nIndividual\nUEFA European Under-17 Championship Team of the Tournament: 2016\nUEFA European Under-19 Championship Team of the Tournament: 2017\n\nReferences\n\nExternal links\n\nPortuguese League profile \nNational team data \n\n1999 births\nLiving people\nSportspeople from Braga\nPortuguese footballers\nAssociation football defenders\nFC Porto B players\nFC Porto players\nManchester United F.C. players\nA.C. Milan players\nPrimeira Liga players\nLiga Portugal 2 players\nPremier League players\nSerie A players\nPortugal youth international footballers\nPortugal under-21 international footballers\nPortugal international footballers\nUEFA Euro 2020 players\nPortuguese expatriate footballers\nExpatriate footballers in England\nExpatriate footballers in Italy\nPortuguese expatriate sportspeople in England\nPortuguese expatriate sportspeople in Italy",
        "  We discuss Subaru and Spitzer Space Telescope imaging and spectroscopy of M87\nin the mid-infrared from 5-35 um. These observations allow us to investigate\nmid-IR emission mechanisms in the core of M87 and to establish that the\nflaring, variable jet component HST-1 is not a major contributor to the mid-IR\nflux. The Spitzer data include a high signal-to-noise 15-35 $\\mu$m spectrum of\nthe knot A/B complex in the jet, which is consistent with synchrotron emission.\nHowever, a synchrotron model cannot account for the observed {\\it nuclear}\nspectrum, even when contributions from the jet, necessary due to the degrading\nof resolution with wavelength, are included. The Spitzer data show a clear\nexcess in the spectrum of the nucleus at wavelengths longer than 25 um, which\nwe model as thermal emission from cool dust at a characteristic temperature of\n55 \\pm 10 K, with an IR luminosity \\sim 10^{39} {\\rm ~erg ~s^{-1}}. Given\nSpitzer's few-arcsecond angular resolution, the dust seen in the nuclear\nspectrum could be located anywhere within ~5'' (390 pc) of the nucleus. In any\ncase, the ratio of AGN thermal to bolometric luminosity indicates that M87 does\nnot contain the IR-bright torus that classical unified AGN schemes invoke.\nHowever, this result is consistent with theoretical predictions for\nlow-luminosity AGNs\n",
        "This paper presents a comparison of several vector combination techniques on\nthe task of relation classification.\n\n- Strengths:\n\nThe paper is clearly written and easy to understand.\n\n- Weaknesses:\n\nMy main complaint about the paper is the significance of its contributions. I\nbelieve it might be suitable as a short paper, but certainly not a full-length\npaper.\n\nUnfortunately, there is little original thought and no significantly strong\nexperimental results to back it up. The only contribution of this paper is an\n'in-out' similarity metric, which is itself adapted from previous work. The\nresults seem to be sensitive to the choice of clusters and only majorly\noutperforms a very naive baseline when the number of clusters is set to the\nexact value in the data beforehand.\n\nI think that relation classification or clustering from semantic vector space\nmodels is a very interesting and challenging problem. This work might be useful\nas an experimental nugget for future reference on vector combination and\ncomparison techniques, as a short paper. Unfortunately, it does not have the\nsubstance to merit a full-length paper.",
        "Great question! To begin with, let's just quickly go over 'what Shakespeare was getting at'. This line is pretty frequently over-interpreted so that it is taken to mean that it is obliquely suggesting a lawyer's position in society is integral to upholding the peace. Our ragamuffin crew are talking about staging a revolution, and some have taken Dick the Butcher's line to suggest that one of the things standing in the way of their proposed revolution is lawyers, thereby praising their role as protecting citizens from upheaval. This is pretty unlikely, given that Shakespeare was as keen as the next person on a good old fashioned lawyer joke.\n\n & nbsp;\n\nCharacterising lawyers as tedious, money-grabbing, pedantic, over-paid and favouring the demands of the rich over genuine justice is not even remotely new, and that appears to be what Shakespeare is doing here. To put the quotation in a bit more context, prior to the conversation about lawyers Jack Cade is talking up his suitability as ruler. He is claiming noble heritage, while Smith and Dick the Butcher snigger at him in asides to the audience:\n\n & nbsp;\n\n**Jack Cade.** My father was a Mortimer,—  \n**Dick the Butcher.** [Aside] He was an honest man, and a good  \nbricklayer.  \n**Jack Cade.** My mother a Plantagenet,—  \n**Dick the Butcher.** [Aside] I knew her well; she was a midwife.  \n**Jack Cade.** My wife descended of the Lacies,—  \n**Dick the Butcher.** [Aside] She was, indeed, a pedler's daughter, and  \nsold many laces.    \n\n & nbsp;\n\nWe then get the conversation about lawyers, as Jack describes the ideal society he will build:\n\n & nbsp;\n\n**Jack Cade.** Be brave, then; for your captain is brave, and vows  \nreformation. There shall be in England seven   \nhalfpenny loaves sold for a penny: the three-hooped   \npot; shall have ten hoops and I will make it felony  \nto drink small beer: all the realm shall be in   \ncommon; and in Cheapside shall my palfrey go to   \ngrass: and when I am king, as king I will be,—  \n\n**All.** God save your majesty!  \n\n**Jack Cade.** I thank you, good people: there shall be no money;  \nall shall eat and drink on my score; and I will   \napparel them all in one livery, that they may agree   \nlike brothers and worship me their lord.  \n\n**Dick the Butcher.** The first thing we do, let's kill all the lawyers.  \n\n\n & nbsp; & nbsp; & nbsp; & nbsp; Henry VI, Act 4, Sc 2.\n\n & nbsp;\n\nThe audience is already primed that Dick is playing for laughs. Presented with Jack's utopian ideal, Dick's first thought about the best way to enact a utopia is to kill all lawyers (badum-tish).\n\n & nbsp;\n\nThis kind of joke turns up elsewhere in Shakespeare's plays, and it is pretty reasonable to assume that 'let's kill all lawyers' is intended to be funny, not serious. Mercutio's 'Queen Mab' speech in *Romeo and Juliet* mentions 'lawyers' fingers, who straight dream on fees', and Act 1 Sc 4 of *King Lear* has Kent say 'this is nothing, Fool' only for the Fool to reply, 'Then ’tis like the breath of an unfee’d lawyer. You gave me nothing for ’t' (i.e. an unpaid lawyer won't say anything).\n\n & nbsp;\n\nAs for whether your average Elizabethan playgoer would get the joke - it seems pretty likely. These kinds of wisecracks about lawyers have cultural relevance outside of Shakespeare. My own area of research is focused on epitaphs, and I have found some contemporary examples of epitaphs copied into manuscripts making jokes at the expense of the legal profession. See for example:\n\n & nbsp;\n\nVpon Mister I. H. a Counsellor of Lincolnes Inne./\n\nHere lyes a Lawyer, who till his tyme of dying  \ndid gayne much mony by his vse of lying  \nLiuing he ly’d; and dead he lyes you see  \nwithin his graue, where let him lye for mee./  \n\n & nbsp; & nbsp; & nbsp; & nbsp; Cambridge University Library, Additional Manuscript 4138, f52r\n\n & nbsp;\n\n'Occupational' epitaphs are a whole subgenre where the deceased is characterised by their profession (it includes one about a locksmith who picks the lock to heaven's gate rather than be let in, and a bellows mender who runs out of breath, for example). The avariciousness and untruthfulness of lawyers is seen by this writer as to be so characteristic that it is the best way to lampoon 'Mister I. H.' (whoever he may be!).\n\n & nbsp;\n\n[EDIT: Thanks /u/chikindiner for pointing out that I had the wrong source here. It's now corrected.] Philip Stubbes' *Anatomy of Abuses* (1583) gives the following description of lawyers in a section entitled 'English Lawyers are Rogues', he writes, \n\n & nbsp;\n\n >  [Lawyers] handle poore mens matters coldly, they execute iustice parcially,  &  they receiue bribes greedily, so that iustice is peruerted, the poore beggared, and many a good man iniured therby. They respect the persons, and not the causes; mony, not the poore; rewards, and not conscience. So that law is turned almost topsie turuie, and therefore happy is he that hath least to doe with them.\n\n & nbsp;\n\nThis type of commentary about lawyers was common enough to not only see print, but was common enough to be copied into personal manuscripts as a joke, and to be used as the foundation of a number of jokes on the Elizabethan stage. The line would likely have gotten a good chuckle out of the audience at the time.\n\n & nbsp;\n\nHope this answers your question, do let me know if you have any questions!  \n\nedit: formatting goof  \nedit again: Thanks so much for the gold, stranger. I love being part of this sub, and it's always so exciting to see other people enthused by this stuff. You've really made my evening :)\nedit again again: Just woke up to double gold! Thank you so much, whoever you are, that's super kind :)",
        " To qualify for law school you will need a four-year bachelor’s degree. Although some schools have “pre-law” majors or concentrations, law schools do not require any particular major. Find a subject that interests you and in which you can do well.\n\nMake sure you attend an accredited undergraduate college or university. To check, visit the U.S. Department of Education’s (“DOE”) Database of Accredited Postsecondary Institutions and Programs.;\n, The ability to speak with anyone is a great skill for a lawyer, including real estate lawyers. Real estate lawyers meet with a variety of people during their workday, from clients, potential clients, opposing counsel, and even judges or arbitrators. You need to be comfortable speaking to diverse constituencies, often off the top of your head.\n\n\nWhile in college, look for opportunities to engage in public speaking. These opportunities can be in debate clubs, public speaking competitions, or even acting as a tour guide for the school.\nAlso look for opportunities to strengthen your research and writing skills. Sign up for upper-level electives that allow you to write long research papers.\n\n, Law school admissions is basically a numbers game, and the two most important numbers are your undergraduate grade point average (GPA) and your score on the Law School Admissions Test (LSAT). To make the strongest admissions case possible, you should try to get the highest grades possible. Admissions committees view a high GPA as an indication that you are motivated and work hard.To get into an accredited law school, you will need a GPA around 3.0 or higher. Applicants admitted into the Top 50 schools generally have a GPA of at least a 3.5.\n\n, Another key element of your application will be letters of recommendations from professors who know you. To get strong letters of recommendation, you should try to work with faculty as a research or teaching assistant. This experience will allow professor to write detailed letters of recommendation in support of your admission to law school.\n\n, You can get an early taste of the life of a real estate lawyer while in college. Intern or work part-time for a real estate lawyer. Many lawyers and law firms need clerical and support staff assistance in the summer but also throughout the year.\n\n\nIt’s never too early to begin building your network. If you do a good job working for a real estate attorney in college, then when you graduate law school you can revive the relationship and potentially get a job.\n\n, The LSAT is offered four times a year, in June, September, December, and February. It is offered on Saturdays. There are special sessions for those who observe a Saturday Sabbath.Create a free account at the Law School Admission Counsel’s (“LSAC”) website.\nFind a test date and location. To do this, start on LSAC’s Law School Admission Counsel’s website Dates and Deadlines page. The last date to take the exam for fall admissions is typically September/October. You may be able to take the December or February test, but by then many law schools will already have filled most of their classes.\n\n, The LSAT is probably the most important factor in your law school application, so take it seriously. It tests reading comprehension, analytical reasoning, and logical reasoning.Test prep companies offer tutoring, but you can also study on your own.\n\n\nYour local library or bookstore should have copies of old LSAT exams. Find the most recent to take as practice exams.\nThe LSAT is scored on a scale from 120-180, with 180 being the highest. To get into an accredited law school, try to get a score around the fiftieth percentile, which is around a 152., The LSAT comprises five multiple choice sections and one unscored essay. Four of the five multiple choice sections count toward your score. The fifth is experimental and does not count toward your score. You will not know in advance which section is experimental.\n\nRead up ahead of time on the test rules so that you can follow them to the letter. Failure to follow the rules could disqualify you from taking the test. A complete set of test day rules can be found on the LSAC’s website on its Day of the test webpage.\n\n, You can take the LSAT more than once. Schools may choose to accept your higher score, or they may choose to average the two. You have to pay each time you take the test.\n\n\nOn average, test takers are able to increase their score only two to three points on a re-take.You may not want to retake the test unless your score was far below what you were scoring on practice exams.\n\n, All law schools use CAS. You will send them your transcripts, letters of recommendation, and evaluation. CAS then creates a packet that it sends to each law school you apply to. The service requires a fee.Make sure to get all documents to CAS in a timely manner. A law school will not move on your application until it has your packet from CAS.\n\n, Ask early. Sometimes professor agree but then forget as they get busy. Only ask professors who you are sure can write a positive letter of recommendation and do not press if faculty are hesitant. A hesitant professor might write a weak letter of recommendation.\n\n\nAlso think of getting letters from employers. If you worked part-time for a real estate attorney, then a detailed letter from your employer could help your application as well.\nSome recommenders may need to be prompted to complete the letter. Send a friendly email reminder, or stop in to chat.\n\n, Law schools require that you write a short (about 500 word) statement, on a topic of your own choosing.You want your personal statement to be engaging, free of errors, and brief.\n\n\nFollow the directions. If the school wants you to write on a specific topic, write on that topic. Also, if they give you a word limit, stick to the limit. Going over, by even a few words, can harm your chances of admission.\nFeel free to write about your interest in real estate law. However, it’s a good idea to only write on the subject if you have something meaningful to say. The best personal statements are often based on anecdotes, so write about real estate law if you have some experience with it.\n\n, An addendum can help explain something that looks bad in your application. It provides context for any information that might raise “red flags” in the eyes of the Admissions Committee members.Red flags include criminal convictions, punishment for cheating or plagiarism, or semesters with very low grades. An addendum might also clarify why one LSAT score is much higher than another. Remember to explain in your addendum, not make excuses.\nYour addendum does not need to be lengthy. For example, you could simply state, “I would like to explain why my first LSAT score is 20 points lower than my second. Two days before the first exam, I contracted the flu. Since I was afraid that cancelling would result in my not being able to apply to law school this year, I went ahead and took the exam although I was sick. During the second exam, I felt much better and scored closer to my averages on practice exams.”\n\n, You can gauge your likelihood of gaining admission to specific schools by using the LSAC calculator. Enter your undergraduate GPA and LSAT score to see your chances at any ABA-accredited law school.\n\n\nIf you have a 3.5 GPA and a 155 LSAT, then you have an 80% chance of getting into Brooklyn Law School but only a 50% chance of getting into Pittsburgh Law School.\n\n, Unless you attend a top 20 school, you most likely will be practicing in the area where you attended law school. Most law schools place their alumni in the local legal community. Accordingly, you should pay attention to the school’s location and make sure you feel comfortable living there.\n\n\nYou should always ask any prospective law school for its job placement statistics. Pay attention to the number of students who get “full-time jobs requiring a JD” after graduation. This is the most relevant statistic. Other statistics, such as “those working full-time,” might include people who are working full-time in a job that doesn’t require a law degree.\n\n, As you compare law schools, you should always pay attention to costs. Law school tuition now exceeds $40,000 a year at many private law schools.Factoring in living expenses, you could be borrowing close to $200,000 to complete a three-year degree.\n\n\nTuition for out-of-state law students is often comparable to the tuition of a private school. If you want to move to a state and hope to qualify as an in-state resident, contact the law school’s admissions office for information.\n\n, A great way to get hands-on legal experience while in law school is to participate in a clinic. Many law schools have clinics where students represent low-income clients while under the supervision of a faculty member. Some law schools offer real estate clinics or have real estate institutes. These schools include Brooklyn Law School and John Marshall Law School.In a real estate clinic, students may represent low-income cooperative boards or other non-profits. Students will assist with loan and co-op unit closings, shareholder meetings, and drafting by-law or lease amendments., The basic curriculum for first year students is pretty much the same at any law school, but after the first year, the classes available might be very different. Many law schools offer a real estate focus or certificate. These schools will offer many upper-level electives in areas such as Land Use Regulation, Basic and Advanced Real Estate, Construction Law, and Municipal Law., Applying to more than one school increases your chances of being accepted. If you don’t get into any school, then you will have to wait a year before applying.\n, Unless you attend an accelerated or part-time program, law school will take three years. In your first year, you will take foundation courses in torts, contracts, property, civil procedure, criminal law, and constitutional law.\n\nYou may end up taking 1L classes with the same people. Get to know your “section” because these people may be the source of career opportunities and contacts down the road.\n\n, Law school can be very isolating, as all students are striving to do the best that they can to master complex, difficult material. You will spend a lot of time alone in the library. Be sure to periodically put the books away and try to meet other people. Join a sports league or a student organization just to unwind.\n\n\nAnother good way to meet people is to join a study group. In addition to the comradery, you will also get help with exam preparation, share notes and outlines, and have a group of people to talk through difficult legal issues with.\nIf you join a study group, however, stick with it. No one likes people who join a group only to drop out after a month.\n\n, Your grades will follow you around your entire career. Though the importance of grades decreases over time, poor grades could keep you locked out of jobs, at least initially.If you want to practice real estate law at a large firm, then doing well in your 1L classes is critical. Large firms will hire summer associates on the basis of your 1L grades. If you want to work at a large firm or in-house at a large corporation, then you should plan on getting grades at the top of your class.\nTo get an idea of how well you need to do to be competitive with large employers, visit your career services office and ask what large firms or corporations come onto your campus to interview. Career Services should also have information on the GPA required to be hired by these large firms.\n\n, Some law schools help place students with a real estate attorney, either at a title insurance company or at a law firm. Students will help out with a variety of tasks, such as title, closing, contract, and other problems.Externships can be taken for credit. If your school does not allow externships for working with a real estate attorney, then you could think about working part-time after your 1L year.\n\n, During the summers you can work for attorneys as a clerk or a summer associate. You should begin looking for these opportunities in the spring semester. Larger firms will advertise through your school’s Career Services. However, you can also send out a copy of your resume and transcript and ask if a position is available.\n\n\nAlthough you may be paid, money should not be the primary purpose of a summer job. Instead, you should begin building your reputation. Be sure to do your best work.\nIf you do good work, then your employer may remember you later when you are looking for a job. Be sure to hang onto contact information, such as phone numbers or emails, and send an occasional email to check up or just say “hi.”\nBe sure to get writing experience in your summer jobs. Employers usually ask for a writing sample when you apply for a job, and it is best to have a “real world” writing sample, such as a contract you helped draft for an employer, than something written for a legal writing class.\n\n, The Multistate Professional Responsibility Examination (MPRE) is required to practice in all but three jurisdictions in the United States. The exam is made up of 60 questions, which test your knowledge of legal ethics.You will take the exam in your third year of law school.\n\n, Each state bar admits its own lawyers and administers its own bar exam. Contact the bar of the state where you wish to practice.They will provide you with a list of the necessary steps to take.\n\n, In almost every state you have to pass a written exam before you can be admitted to the bar. The exam typically includes an essay portion as well as a multiple choice test.The bar exam is typically offered twice a year. There is usually a summer exam (offered in June or July) and a winter exam (offered usually in February). If you have to take the bar exam over, you have to pay each time you take it.\n\n, You will need to prepare extensively for the bar exam. Most people try to take a bar prep course, which usually runs for several months. Costs can reach several thousand dollars.If you can’t afford a full prep course, then you might want to seek out old study guides published by bar prep companies. Many people sell old guides on eBay and other online retailers.\n\n, In addition to passing the bar exam, you also need to pass a character and fitness review.You initiate the review by completing a detailed survey on your employment history, educational history, and criminal/financial history.\n\n\nCommon problems with character and fitness include criminal convictions, financial irresponsibility (such as bankruptcy), and accusations of plagiarism. These may not completely block you from admission, but be prepared to discuss them with the character and fitness committee. If anything looks suspect to the committee, it will call you in for an interview.\nAlways be honest when filling out the background survey. It is better to be upfront than try to hide something and get caught.\n\n, The bar exam is typically a two-day exam. One day consists of a multiple-choice exam covering topics such as contracts, constitutional law, criminal law, evidence, and torts.The other day will be made up of essays on state-specific topics.It will take several months to receive your score. In Illinois, for example, those who take the exam in July will not receive their results until the first two weeks of October., If you want to practice real estate with a large law firm or with a large corporation, then you will probably need to sign up for on-campus interviews (OCI) and interview. Before the start of your 2L year, large law firms and corporations interview students for summer associate positions at their firm (or in-house) for the following summer. If they like you, they will probably extend an offer to join them after you graduate.\n\n\nYour Career Services office will send out detailed requirements for participating in OCI, such as preparing a resume and ordering copies of your transcript. Be sure to follow all policies to the letter, otherwise you could be prohibited from participating in interviews.\n\n, Smaller firms often advertise online. You can check Craigslist, job aggregators like Indeed.com, and with your state bar association, which may have a jobs board. You will be asked to forward a resume, cover letter, and writing sample, so have those ready to go.\n\n\nSmaller firms often want applicants to already have passed the bar exam, so you may not be able to search for these jobs until you have the results of the bar exam.\n\n, Another way to find jobs is to meet with lawyers for informational interviews. After taking the bar exam, you should identify real estate attorneys whose practices you would like to learn more about. Draft a letter (not an email) and introduce yourself. Be sure to mention who gave you their name.\n\n\nIn the letter, explicitly state that you are not asking for a job. You will get a better response this way. The purpose of the interview is to create an initial contact. If you make a good impression, the lawyer may remember you later for a full-time job or for part-time contract work.\nDraft at least five questions about the lawyer’s practice and be engaged during the meeting. Take notes and ask follow-up questions.Ask the attorney if she knows anyone else you can meet with. Be sure to send a thank you note afterwards.\n\n, If you cannot find a job after passing the bar, re-connect with attorneys you worked for during the summer or part-time during the school year. They may have overflow work for you to do, such as research assignments, contract review, or closings to attend.\n\n\nYou can also cold call other real estate attorneys and ask if they have any overflow work. If you do not have a job, you should be most focused on building your reputation and not be picky about how much you get paid. If you do good work for low wages (or even for free), then the attorney may come back to you with additional work.\n\n, Even if real estate law is your dream, you need to start with a first job to gain legal experience (and pay the bills). Ideally, you could work for a general business lawyer, who may do real estate work as one part of his or her practice. Otherwise, you will need some sort of legal job in order to get legal experience.\n\n\nRegardless of your first job, you can try to get real estate law experience in your free time. For example, you can do pro bono work. Volunteer at a local legal aid organization and help non-profits with their contract disputes, real estate closings, and other legal problems.\nYou could also increase your familiarity with real estate issues by writing bar articles on real estate law, offering seminars to small businesses on real estate issues, or sitting on a local government board that handles land use and zoning.\n\n, As your career advances, be sure to raise your profile by offering continuing legal education courses, joining bar association committees, and joining real estate bar associations. Many states, like Illinois, have a Real Estate Lawyers Association. Members are invited to events and seminar, and work together to address the concerns of the profession.\n\n\nYou can also seek board certification in real estate law, if your state offers it. Ohio, for example, offers two specialist certificates in real estate, one in Real Property—Business, Commercial, and Industrial Law and the other in Real Property—Residential Law.To earn the certification, attorneys must show that they devote a significant portion of their practice to real estate law, take advanced courses in the field, and submit references.In many states, they also must pass a written exam.\n\n",
        "Northview High School may refer to:\nNorthview Heights Secondary School, Toronto District School Board, Ontario\nNorthview High School (Brazil, Indiana)\nNorthview High School (Sylvania, Ohio)\nNorthview High School (California), Covina-Valley Unified School District, California\nNorthview High School (Georgia), Johns Creek, Georgia\nNorthview High School (Dothan, Alabama)\nNorthview High School (Century, Florida)\nNorthview High School (Michigan),  Plainfield Township, Kent County, Michigan, a suburb of Grand Rapids",
        "  Epitaxial self-assembled quantum dots (SAQDs) are of interest for\nnanostructured optoelectronic and electronic devices such as lasers,\nphotodetectors and nanoscale logic. Spatial order and size order of SAQDs are\nimportant to the development of usable devices. It is likely that these two\ntypes of order are strongly linked; thus, a study of spatial order will also\nhave strong implications for size order. Here a study of spatial order is\nundertaken using a linear analysis of a commonly used model of SAQD formation\nbased on surface diffusion. Analytic formulas for film-height correlation\nfunctions are found that characterize quantum dot spatial order and\ncorresponding correlation lengths that quantify order. Initial atomic-scale\nrandom fluctuations result in relatively small correlation lengths (about two\ndots) when the effect of a wetting potential is negligible; however, the\ncorrelation lengths diverge when SAQDs are allowed to form at a near-critical\nfilm height. The present work reinforces previous findings about anisotropy and\nSAQD order and presents as explicit and transparent mechanism for ordering with\ncorresponding analytic equations. In addition, SAQD formation is by its nature\na stochastic process, and various mathematical aspects regarding statistical\nanalysis of SAQD formation and order are presented.\n",
        "Section 4.1 Supervision mentioned that the proposed method requires additional supervision by providing the correct graph at each timestep. \n\nI have some questions about whether this is practical: \n\n1. In some applications, human may or may not be able to provide the \"correct graph\" at each timestep. What if the human supervisor can only provide suboptimal graphs during training? How would that affect the results?\n\n2. What was the result of not providing such intermediate supervision? Was the result slightly worse than the current result, or significantly different?\n\n3. Such additional supervision would also require more \"work\" by the human. How can the amount of work be quantified, so the reader understands the implications of having to provide additional supervision?\n\n4. I would appreciate if the need of such strong supervision was mentioned earlier in the paper, perhaps during the introduction or literature review, to give the reader some \"warning\". Some literature review/comparison of other approaches that require strong supervision would also be appreciated.\n\nThanks for the good work, I enjoyed reading it!\n\n\n",
        "  This paper concerns the solution of the self-consistency equation for energy\ngap parameter $\\Delta_{\\bf k}$ in the BCS theory of superconductivity. We show\nthat there exists a well-defined relation between the solution for energy gap\nparameter amplitude $|\\Delta_{\\bf k}|$ for a general interaction $V_{{\\bf\nk},{\\bf k}'}$ and energy gap $\\Delta$ obtained by using the cut-off\napproximation. The relation between $|\\Delta_{\\bf k}|$ and $\\Delta$ indicates\nthat $\\Delta$ is a weighted average over $|\\Delta_{\\bf k}|$ of electronic\nstates within cut-off energy $\\xi_c$ around the Fermi surface. In this\ninterpretation for $\\Delta$, $\\xi_c$ is not a property of $V_{{\\bf k},{\\bf\nk}'}$, but a parameter specifying the energy range within which the weighted\naverage over $|\\Delta_{\\bf k}|$ is taken. We show that the proper choice for\nthe value of $\\xi_c$ is only a few $k_BT_c$ (i.e., $\\xi_c/k_BT_c$ is about 3 or\n4). We also show that the cut-off approximation, even with\n$\\xi_c/k_BT_c=\\infty$, is a good approximation when it is used to calculate\nquantities such as the condensation energy and the specific heat, but it leads\nto significant overestimation for the Josephson critical current density of a\nJosephson junction if $\\xi_c/k_BT_c \\gg 1$ is assumed.\n",
        " You will need to be rested and your patience gauge must be full. Don't expect to do observing in the same night you start collimation if your telescope is way out of alignment, otherwise you'll rush things.;\n, Remember: a LITTLE hole. You can do it using a pointed scissor. Put it into the eyepiece.\n\n, You realize it needs collimation if you don't see the entire secondary mirror through the collimation cap and if you don't see the three locks over the primary mirror (the concave one, in the back of the scope).\n\n, So don't mess around with it unless you're certain of its misalignment! You may need to collimate the secondary if you bought the scope second hand or if you have already messed up with it.\n\n, Place a colored paper behind the secondary mirror, inside the tube of the telescope. For this step, you'll need to open your scope.\n\n, WARNING: keep your scope in a horizontal position, so the secondary mirror won't fall on the primary.\n\n, Now you need the center the reflection of the primary mirror in the secondary. Still using your homemade collimation cap, screw one of the three secondary screws, and start screwing the other two gradually until you see the three locks over the primary mirror. These locks are 120° apart from themselves, so they're easy to spot.\n\n, It is a little easier then the secondary. Most new telescopes come with the exact center of the primary already marked, but if yours is not marked, you can do it by simply sticking a round piece of white paper in the exact center (take your time doing it: it must be the EXACT dead center). It won't trouble your observing as long as your piece of paper is smaller then the secondary mirror's diameter. I recommend using a piece of paper with half inch diameter (or 1 cm).\n\n, Use the screws under the primary to do that. In most telescopes there are six screws, grouped in pairs. In this pair, one them is a locking screw, and the other is the adjusting one. Take your time to learn how they move the mirror.\n\n, If your telescope has what we call a \"secondary offset\", the spider may be off center in the final results. But don't be bothered by that if the reflection of your eye is centered.\n\n, You may stop the collimation process here. But if you want to observe the planets in the Solar System and binary stars, you NEED to do a fine tuning in your collimation, in a process called \"star test\".\n\n, People in the Southern Hemisphere are not as fortunate, because finding a stationary star is way more difficult for them. But worry not: be smart! You can use any bright dot-sized light source to do the fine tuning, like antennae or far (really far) away street lights. This way you can do the fine tuning in cloudy nights and the light source is absolutely stationary. The results are the same.\n\n, Use your highest magnification lens. Focus it. Now continue gradually rotating the focusing knob until you see a pattern that resembles a target (it may be off center) or a Doppler-effect illustration. If your scope is way out of fine alignment, you won't see this effect. But any slight over-focus will do the trick.\n\n, Use the screws in the back of the primary mirror to do that. Be aware that your light source will MOVE A LOT and go out of view. Keep track of where it goes so you can find it again. This process may take a few hours of trial and error, but it is rewarding.\n\n, No need to buy a 50$ piece of plastic or an abhorrently expensive laser collimator (even this laser may need collimation!).\n\n",
        "  We present the results of realistic N-body modelling of massive star clusters\nin the Magellanic Clouds, aimed at investigating a dynamical origin for the\nradius-age trend observed in these systems. We find that stellar-mass black\nholes, formed in the supernova explosions of the most massive cluster stars,\ncan constitute a dynamically important population. If a significant number of\nblack holes are retained (here we assume complete retention), these objects\nrapidly form a dense core where interactions are common, resulting in the\nscattering of black holes into the cluster halo, and the ejection of black\nholes from the cluster. These two processes heat the stellar component,\nresulting in prolonged core expansion of a magnitude matching the observations.\nSignificant core evolution is also observed in Magellanic Cloud clusters at\nearly times. We find that this does not result from the action of black holes,\nbut can be reproduced by the effects of mass-loss due to rapid stellar\nevolution in a primordially mass segregated cluster.\n",
        " Prepare as much as possible while the boat is on the trailer, and on dry land. Prepare the engine, fuel, safety gear, mooring lines, fenders, etc. You should be ready to drive the boat off the trailer right after it's backed down the ramp. Other people want to launch as well, and tempers will flare if you use dock time to do things you could have done in the parking lot. It is a good idea to have your own written checklist.;\n, If bringing along guests, brief them on where to go and what tasks to do. If bringing along children, forbid young children and pets from playing or loitering on a boat launch ramp as this is a roadway that transitions into a waterway. Keep children and pets supervised by an adult in a different area than at the boat launch ramp. NEVER let children or pets play or be unattended at a boat launch ramp.\n\n, Unplug the trailer towing light assembly from the vehicle's electrical system.\n\n, This is very important!\n\n, Undo and stow the hold down straps, but leave the winch hook in the bow eye.\n\n,, This is easiest done with two people: one driving and one as a spotter. With a truck or SUV it can be easier to see when backing up if you put the tailgate down or open the rear hatch/door/window. Drive slowly, making tiny fine-tune steering corrections as you go. How far to back the boat trailer into the water depends on many factors - type of trailer, depth of the water, type of boat, etc. A good rule of thumb is to back in until the water is just above the hubs on the trailer. Be careful about backing in too far otherwise the tow vehicle might become stuck.\n\n\nWhen backing down the ramp, have the vehicle in neutral. This makes it easier to control the vehicle quickly. If something fails, quickly put it in gear and go forward.\n\n,\n\n\nWhen getting out of the vehicle on the ramp, apply the handbrake first, check that it is holding fast, and then put it in park. When the vehicle is sitting in park, the whole weight is sitting on a little piece of metal in the trans (parking pawl). Should this break and the park brake doesn't hold, you launch the whole rig.\n\n,,,,, Make sure the engine is in the water, and turn it over. Once it's running, undo the winch hook from the bow eye, and back the boat out of the trailer. For smaller boats, you can just push the boat out while holding a mooring line.\n\n, Tie the mooring lines to cleats on the dock. Use fenders to prevent the hull from being scratched.\n\n, Boat ramps can be slippery with moss and algae and it is important to not let the vehicle's tires spin. If your tow vehicle is equipped with four-wheel drive, this can be useful if your back tires start spinning. Once the boat is afloat and secured to the dock, drive the vehicle back up the ramp and park it at the landing, in the designated parking area.\n\n, Do not turn the boats steering wheel in the opposite direction of the dock, with the boat a mere few inches from the dock, and just hit the throttle - or your engine's outdrive will push your boat right into the dock, scratching or gouging the gelcoat in the side of the hull. Make sure that you have pushed away several feet from the boat dock before driving away, so that your stern has ample room to turn the boat without scraping the boat alongside of the dock.\n\n",
        "Triadic may refer to:\n\n Triad (music)\n Triadic patent, a series of corresponding patents\n Triadic reciprocal causation, a concept in social psychology\n Triadic relation, a mathematical concept\n p-adic number, where p=3, a mathematical concept\nTriadic System in Psychiatry\n\nSee also\n Tertian",
        "The Via Bona Awards for Philanthropy are annual awards presented by the Via Foundation to acknowledge exemplary philanthropy of companies and individuals who have a history of strong support of the work of non-profit organizations in the Czech Republic. The Via Foundation supports community and philanthropy development in the Czech Republic. Via Foundation focuses on growing the number of people in the Czech Republic working collaboratively to improve their communities and giving to help others. “Every act of charity is a story – which can motivate other people who are thinking about giving back but don’t know how to start,” says Executive Director Jiří Bárta about why Via Foundation presents the Via Bona awards.\n\nThe Via Bona Awards are one of the two major annual awards in the Czech Republic to honor philanthropy and philanthropists.  The other major Czech philanthropy award is the TOP Corporate Philanthropists Award presented by Donors Forum to award companies which have given the highest total monetary amount to charitable causes over the course of one year.\n\nSince 1997, The Via Bona Awards are held each year under the auspices of the Embassy of the USA in Prague and the general partner is Nadace Vodafone Česká republika. The Via Bona Award is given to those who serve as an example to others through their material and financial support, as well as their willingness to tread untried paths.\n\nThe Via Bona Awards are nominated by Czech nonprofit organizations and are chosen by an independent jury.\n\nFor the 19th year in a row, Via Foundation recognized individuals and companies who help improve their communities by giving their time, energy, expertise or money to help others. Via announced the winners of the 2016 Via Bona Philanthropy Awards. Juries selected winners in six categories.\n\nThe 2016 award winners include people such as Zdeňka Mocňáková, who helps disabled people and senior citizens despite being bound to a wheelchair herself. She is the recipient of the Personal Engagement Award.\n\nIn the Young Personal Engagement category, the jury selected two winners: Jakub Trefný, who despite being just nineteen years old, is an experienced volunteer nad helps senior citizens, homeless people and refugees, and Milan Dzuriak, who undertakes “Journeys against Pain” to raise money to make disabled children’s dreams come true.\n\nThe Patron Award was presented to businessman Sanjiv Suri, owner of Zátiší Group, for his numerous philanthropic activities both in the Czech Republic and around the world. \n\nThe Good Company Award went to Dušan Hopp’s company Auto Díly Servis s.r.o., based in Nový Jičín, whose owner and employees helped remodel a Home for Pregnant Women in Crisis.\n\nThe Loyal Donor Award is a new category this year. It went to Asaf Auerbach, a regular donor to children’s charities whose own life was saved by Sir Nicholas Winton during World War II.\n\nThe Bequests Help Award draws attention to bequests, which are still an uncommon form of giving in the Czech Republic. Via Foundation recognized microbiologist Ludmila Šilhánkova in memoriam. She left over CZK 2 million to help disabled children and young people.\n\nThe Public’s Choice Award is a special seventh category presented under the auspices of Czech TV, the main media partner of the Via Bona Awards. Members of the public chose from among the 15 nominated philanthropists on the Via Foundation website. The winner of the voting was Zdeňka Mocňáková.\n\nList of Award Winners\n\n2016 \nPersonal Engagement Award and Public's Choice Award: Zdeňka Mocňáková\n\nYoung Personal Engagement Award: Milan Dzuriak and Jakub Trefný\n\nGood Company Award: Dušan Hopp, Auto Díly Servis s.r.o.\n\nPatron Award: Sanjiv Suri\n\nLoyal Donor Award: Asaf Auerbach\n\nBequests Help Award: Ludmila Šilhánková\n\n2015 \nPersonal Engagement Award: Zdenka Wasserbauerová\n\nYoung Philanthropist Award: High school student Kateřina Vaňáková\n\nAward for a Bequest: Professor Kateřina Šmídková\n\nForging a New Path Award: Vojtěch Sedláček\n\nIndividual Donor Award: Libor Winkler\n\nThe Small and Medium-Sized Company Award: Konzum, obchodní družstvo v Ústí nad Orlicí\n\nLarge Corporation Award: NET4GAS, s.r.o.\n\nThe Public’s Choice Award (public online voting at www.umimetouzoddetstvi.cz): First, third, fourth and 4C classes, Gymnázium Jateční, Ústí nad Labem\n\n2014 \nPersonal Engagement Award: \n Barbora Jindřiška Petrtýlová\n Petr Sýkora\nJournalists' Individual Donor Award: Kvido Štěpánek\n\nYoung Philanthropist Award: Martin Kučera and Vojtěch Paukner\n\nForging a New Path Award: Aleš Jeník\n\nSmall and Medium-Sized Company Award: GALVAMET spol. s.r.o.\n\nLarge Corporation Award: AVAST Software s.r.o.\n\n2013 \nForging a New Path Award: JUDr. Hana Machačová\n\nPersonal Engagement Award: Tomáš Slavata\n\nAcademic Philanthropy Award: students of the Third Faculty of Medicine - Charles University Scott Keel and Petr Oliva\n\nJournalists' Individual Donor Award: Martin Hausenblas\n\nSmall and Medium-Sized Company Award: Z - Trade s.r.o.\n\nLarge Corporation Award: Česká spořitelna\n\nHonorary Award: Olbram Zoubek\n\n2012 \nJournalists' Individual Donor Award: Jana Bečvářová\n\nAcademic Philanthropy Award: students of grammar school Broumov and their teacher Šárka Rambousková\n\nPersonal Engagement Award: Ondřej Horecký\n\nForging a New Path: Adastra, s.r.o.\n\nSmall and Medium-Sized Company Award: MIBCON a.s.\n\nLarge Corporation Award: KPMG Czech Republic\n\nAward for Support Specific Project by Large Corporation: OKD, a.s.\n\n2011 \nJournalists' Individual Donor Award: Břetislav Tůma\n\nPersonal Engagement Award: PhDr. Jaroslav Šturma\n\nLarge Corporation Award: Vodafone Czech Republic\n\nSmall and Medium-Sized Company: LAW CZ\n\nAward for Innovative Project: LMC s.r.o.\n\nAward for Employee Involvement in Philanthropy: KPMG Czech Republic\n\nAward for Long-term partnership with a Nonprofit: Telefónica Czech Republic, a.s.\n\n2010 \nCorporate Award (large companies): Ogilvy Group\n\nCorporate Award (small- and medium- sized companies): College of Economics and Management (VŠEM)\n\nIndividual Award: Lubomír Kohout\n\nAward for Innovative Projects: e|n|w|c Natlacen Walderdorff Concola v.o.s\n\nAward for Employee Involvement in Philanthropy: PricewaterhouseCoopers Česká republika s.r.o.\n\nAward for Long-term Partnership with a Nonprofit: Ogilvy Group\n\nJournalists' Award: Lubomír Kohout\n\n2009 \nCorporate Award (large companies):  Československá obchodní banka, a.s.\n\nCorporate Award (small- and medium- sized companies): Josef Kvapil, a.s.\n\nIndividual Award: Marek Stanzel\n\nAward for Innovative Projects: Jaroslav Sklenář\n\nAward for Employee Involvement in Philanthropy: T-Mobile Czech Republic, a.s.\n\nAward for Long-term Partnership with a Non-profit: Pavel Cindr\n\n2008 \nCorporate Award (large companies): Telefónica O2 Czech Republic\n\nCorporate Award (small- and medium- sized companies): Bisport, spol. s.r.o.\n\nIndividual Award: Mr. Milan and Mrs. Marie Sourada\n\nAward for Innovative Projects: Poštovní spořitelna\n\nAward for Employee Involvement in Philanthropy/Volunteerism: Citibank Europe plc\n\nAward for Long-term Partnership with a Non-profit: Kocián Šolc Balaštík\n\nHonorary Award: Mrs. Marka Bednářová and Mr. Donald Hamer\n\n2007 \nCorporate Award (large companies): Isolit – Bravo, s.r.o.\n\nCorporate Award (small- and medium- sized companies): Orbit, s.r.o.\n\nIndividual Award: Countess Mathilda Nostitzozová\n\nAward for Innovative Projects: Vodafone Czech Republic\n\n2006 \nCorporate Award (large companies): Metrostav a.s.\n\nCorporate Award (small- and medium- sized companies): Hobra - Školník\n\nIndividual Award: Vlastimil Venclík\n\nAward for Innovative Projects: Skanska CZ Corporation\n\nHonorary Award: Vaclav and Marie Hora\n\nHonorable Mentions: Marten,s.r.o; Československa obchodní banka\n\n2005 \nCorporate Award (large companies): Stavební spořitelna České spořitelny, a.s.\n\nCorporate Award (small- and medium- sized companies): Casta a.s.\n\nAward for Innovative Projects: ECE Projektmanagement G.m.b. H & Co. KG\n\nIndividual Award:  MUDr. Eva Hvížďalová\n\nHonorary Award:  Silva and Oldrich Vasicek\n\nHonorable Mentions: Mrs. Marie Šenfeldová, Mr. Luďěk Zýka, Mr. Milan Havránek and Mr. Čeněk Zlatohlávek; Connex Morava, a.s.\n\n2004 \nCorporate Award (large companies): Eurotel Praha, spol. s.r.o\n\nCorporate Support for Regional Activities: Osram Bruntál, s.r.o.\n\nAward for Innovative Projects: Ing. Libor Holub\n\nLong-Term Partnership Award: The Safari Family and the Roesel-Krum Family\n\nHonorary Award: Dr. Alfred Bader\n\n2003 \nCorporate Award (large companies): Nokia ČR\n\nCorporate Support for Regional Activities: Plzeňský Prazdroj, a.s.\n\nAward for Innovative Projects: GlaxoSmithKline, s.r.o.\n\nLong-Term Partnership Award: Mucos Pharma CZ\n\nIndividual Award: Mrs. Lada Martinézová\n\nHonorable Mention: Česká Televize\n\n2002 \nCorporate Award (large companies): Philip Morris, a.s.\n\nCorporate Support for Regional Activities: MEKS - Červenák\n\nAward for Innovative Projects: Kateřina Dubská - Vydavatelství ERA\n\nLong-Term Partnership Award: Jaromír Křivohlavý - Renesance, Třebenice\n\nIndividual Award: Jakub Blatský\n\n2001 \nCorporate Award (large companies): Veba, a.s.\n\nCorporate Support for Regional Activities: MESIT Holding, a.s.\n\nAward for Innovative Projects: Altik, s.r.o.\n\nHonorable Mentions: Ladislav Čerych;  Milan Horvát\n\n2000 \nMain Award: Ostravské kanalizace a vodárny\n\nHonorable mentions:\n Staving Olomous s.r.o. \n Odkolek a.s.\n Mius a.s.\n Pavel Svoboda\n\n1999 \nMain Award (large companies): Dopravní stavby Holding, a.s.\n\nHonorable Mentions: \n Leo Burnett Advertising, Ltd.; \n MEKS - Červenák;  Healthy Food J.O.D.\n Ing. Jiří Rous Pireo\n HK Shoes - Dobříš\n Czech-o-dog Ltd.\n J.O.D. Louny\n\n1998 \nMain Award: Pekárny Jaromíra Středy\n\nHonorary Award:\n Jan Pivečka s.r.o.\n Graddo, a.s.\n Microsoft s.r.o.\nHonorable mentions:\n Preciosa, a.s.\n Setuza, a.s.\n\nReferences\n\nExternal links\n Česká Televize program \"Události, komentáře\"  September 25, 2007 television interview with Via Foundation Executive Director, Jiří Bárta (in Czech).\nHelping Hands October, 2007 Prague Post article about the Via Foundation's Via Bona 2007 Awards for Philanthropy by  Brooke Edge.\nHelping Hands September, 2006 Prague Post article about the Via Foundation's Via Bona Award for Philanthropy by  Kathleen Kralowec.\n\"Metrostav získal Via Bonu\" September 25, 2006  Strategie Magazine article about the Via Foundation's 2006 Via Bona Awards (in Czech).\n Cenu VIA Bona v kategorii malých a středních firem získala Hobra - Školník s.r.o. September 19, 2006  Broumovsko Regional Server article about 2006 Via Bona Awards (in Czech).\n Cenu Via Bona dostal Philip Morris za pomoc obětem domácího násilí October 2, 2002 Radio Praha Český Rozhlasarticle about the 2002 Via Bona Awards by Vilém Faltýnek (in Czech).\nwww.nokia.com\n\nAwards established in 1997\nCzech awards\nHumanitarian and service awards",
        "The authors propose \"information dropout\", a variation of dropout with an information theoretic interpretation. A dropout layer limits the amount of information that can be passed through it, and the authors quantify this using a variational bound. \n\nIt remains unclear why such an information bottleneck is a good idea from a theoretical standpoint. Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation. The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nThe experiments are not convincing. The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.",
        "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008",
        "I have not much to add to my pre-review comments.\nIt's a very well written paper with an interesting idea.\nLots of people currently want to combine RL with NLP. It is very en vogue.\nNobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.\nMost people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.\nHence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL.\nBut many directions need to be explored and maybe eventually they will reach a point where they become relevant.\n\nIt is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.\n\nRegardless, it's an interesting exploration, worthy of being discussed at the conference.\n",
        ",\nfeeler gauge (see above)\n2 new valve cover gaskets\n\n, If you have AC, you may need to slip a shop rag through the bail and give both ends a sharp snap downward to get the bail loose.\n\nThe bail will move downward releasing the tension on the cover and you'll be able to work the cover off. Some oil will drip down at this point, don't worry, it will be a small amount.\nOnce it finishes dripping, use a rag to clean it off the heat exchangers.\n\n,\n\nThe cap is held on by 2 clips, insert the screwdriver toward the top of the clip between the clip and the cap, gently pry outward and the clip will pop off the notches in the cap.\nDon't remove the wires or the rotor inside the cap, just pull the cap up off the distributor and place it to the side suspended by the spark plug wires.\n\n,\n\nIf the belt is loose, it may slip as you turn the bolt, when this happens, grab both sides of the belt as it enters and exits the upper pulley and squeeze them together with one hand.\nUse the wrench to turn the pulley with the other, this will increase the tension enough to turn the engine over by this method.\n\n,, Do this by watching the distributor rotor.\n\nThere is a small notch (oriented at about 4 o'clock) cut into the top edge of the distributor where the cap meets the metal distributor body. Rotate the engine until the rotor is pointing directly at the notch.\nWhen it is lined up, look down at the crankshaft pulley (large pulley at the lower end of the fan belt). There will be a notch cut on the inside edge of the pulley that lines up with the split in the center of the engine case. It's a good idea to paint the notch with a small amount of visible paint (whiteout works for this but doesn't last). Line the notch up with the split, your engine is now at TDC for cylinder #1.\n\n,\n\nSometimes the gauge won't fit between the adjuster and the valve stem, in that case break the lock nut loose on the adjustment screw, loosen the screw one turn and insert the feeler gauge.\nIf it requires more than one turn to get the gauge inserted, double-check TDC.\n\n,,\n\nDo not over tighten the lock nut.\nIt should be tight, not overly tight.\n\n,, At the rear of the car using the 19mm or 21mm wrench, turn the engine counter-clockwise so the crankshaft (large) pulley turns 1/2 turn or 180 degrees.\n\nThere will be another notch on the pulley exactly 180 degrees or halfway around from #1's TDC point.\nLine the notch up with the split, your engine is now at TDC for cylinder #2.\n\n,,,,,,\n\nPlace the new gasket into a valve covers and place the cover on the head.\nLift up the bale till it clicks into the notch in the valve cover, A screwdriver may be required to get it the last of the way on.\n\n,, If it is present, you may have mis-adjusted one of the valves.,",
        " If you are worried about any of the changes or side effects of testosterone, direct your questions to the doctor. You may want to ask:\n\n\nWhat are the side effects of testosterone supplements?\nDo I need to see an endocrinologist?\nHow do my testosterone levels look?\nIf your doctor is not knowledgeable about hormone replacement therapy for female to male transitions, or is not comfortable with prescribing testosterone to you, you may need to find another primary care physician or specialist who does have experience with hormone replacement therapy.\n\n, You’ll also have your blood tested directly for testosterone levels.\n\n\nIf you have any conditions that could be worsened by taking testosterone (like blood clots or hormone-sensitive cancers), you may need additional tests. Your doctor might recommend an alternative treatment.\nYou might also simply need to exercise more or change your diet to correct symptoms like increased fat, decreased bone mass, and feelings of fatigue.\nYour doctor might direct you to see an endocrinologist, a doctor who specializes in hormones.\nIt is illegal to buy, sell, or use testosterone without a prescription from a healthcare professional.\n\n, Be sure to get at least two or three tests to establish your baseline testosterone level before deciding supplements are a good choice.\n\n\nThese tests will probably be performed in the morning, since that is when testosterone levels tend to be lowest.\nA healthy testosterone range for men ranges from 250 – 1,100 nanograms per deciliter of blood.\nTaking additional testosterone will not speed up puberty.\nTestosterone treatments are not appropriate for age-related declines in testosterone levels.\n\n, This might take the form of heart palpitations or arrhythmia, a fluttering or racing heart, or heart valve problems that could produce lightheadedness, chest pain, or general feelings of fatigue. Truly dangerous side effects include heart attacks, strokes, and heart failure.These side effects increase in likelihood with age.\n\n, Common side effects of testosterone supplements include a high red blood cell count, breast enlargement, acne, and increased prostate size.Less common side effects include blood clots or swelling (edema) in the legs and/or ankles, pain in the breast, and sleep apnea (an inability to breathe properly during sleep). Testosterone has also been linked to negative sexual results, including changes in the size of testicles and low sperm counts.You might also experience:\n\n\nIncreased skin oiliness and acne\nUnbalanced emotions (especially at high doses)\nHeadaches and migraines\nAn increased appetite\nWeight gain and redistribution of fat around the abdomen\nUnwanted or excessive body hair growth\nIncreased sweat production and stronger body odor\nA receding hairline or male-pattern baldnessAn imbalance in thyroid function, especially if you have been treated for overactive or underactive thyroid function in the past.\n\n, Some doctors rely on your reports of physical and mental changes to adjust your dosage rather than doing expensive blood tests during every visit, and your comfort level with the changes can be used to determine whether your dosage or prescription should be changed.\n\n\nBefore beginning testosterone therapy, your doctor should test your red blood cell count (hematocrit). Testosterone can cause an increase in hematocrit and hemoglobin, which increases the viscosity of your blood. This can make you at risk for blood clots and stroke.\nFollow up with your doctor for routine exams and blood tests to stay healthy. You will need medical supervision for the entire time you are using hormone replacement therapy and for a while after you stop taking testosterone to detect and control any harmful changes or side effects.\nIf you decide that you do not wish to continue testosterone therapy, you are free to stop at any time, in consultation with your doctor. They can help you through any side effects from stopping hormone replacement therapy, and can help you find other treatments that can lessen your symptoms and help you reach your goals.\n\n",
        "It is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the CIFAR-10 for cross entropy loss and auto encoder for MSE loss. At least empirically, by comparing the observed training loss and taylor loss, the better a particular optimizer performs (training loss statement, not a validation or observed test statement) the smaller the difference between these two. Also shown is the regret loss is satisfied at different scales of the network, by layer, neuron and whole network. \n\nThe Taylor approximation can be used to investigate activation configurations of the network, and used to connect this to difficulty in optimizing  at kinks in the loss surface, along with an empirical study of exploration of activation surface of the SGD/Adam/RMSprop optimizers, the more exploration the better the resulting training loss.\n\nNot that it impacts the paper but the weaker performance of the SGD could be related to the fixed learning rate, if we anneal this learning rate, which should improve performance, does this translate to more exploration and tightening between the actual loss and the Taylor loss? \n\n- It might be useful to use a cross validation set for some of the empirical studies, in the end we would like to say something about generalization of the resulting network\n\n- Is there a reason the subscript on the Jacobian changes to a_l in the ",
        "  Ignition delay times of cyclohexane-oxygen-argon and\ncyclopentane-oxygen-argon mixtures have been measured in a shock tube, the\nonset of ignition being detected by OH radical emission. Mixtures contained 0.5\nor 1 % of hydrocarbon for equivalence ratios ranging from 0.5 to 2. Reflected\nshock waves allowed temperatures from 1230 to 1800 K and pressures from 7.3 to\n9.5 atm to be obtained. These measurements have shown that cyclopentane is much\nless reactive than cyclohexane, as for a given temperature the observed\nautoignition delay times were about ten times higher for the C5 compound\ncompared to the C6. Detailed mechanisms for the combustion of cyclohexane and\ncyclopentane have been proposed to reproduce these results. The elementary\nsteps included in the kinetic models of the oxidation of cyclanes are close to\nthose proposed to describe the oxidation of acyclic alkanes and alkenes.\nConsequently, it has been possible to obtain these models by using an improved\nversion of software EXGAS, a computer package developed to perform the\nautomatic generation of detailed kinetic models for the gas-phase oxidation and\ncombustion of linear and branched alkanes and alkenes. Nevertheless, the\nmodelling of the oxidation of cyclanes requires to consider new types of\ngeneric reactions, and especially to define new correlations for the estimation\nof the rate constants. Ab initio calculations have been used to better know\nsome of the rate constants used in the case of cyclopentane. The main reaction\npathways have been derived from flow rate and sensitivity analyses.\n",
        " Your goal is to make her time with you better through this intimate connection. Prepare to focus entirely on her for at least 30 minutes if not longer. Think about what she likes when she relaxes and use those insights to create the perfect environment for a backrub.;\n, For the massage to work best her back will need to be completely exposed. This means she will probably be topless. You need to make sure the area you choose has plenty of privacy from neighbors and kids. If you have kids in the home, you may consider locking the door.\n\n\nThe bedroom can be a great place for a massage. However, be ready to stay on your knees most of the time. You will probably need lots of towels to ensure the oils do not get on the sheets, pillows, or comforter.\nAvoid giving a massage on the couch. Although convenient it provides little room to maneuver for applying pressure.\n\n, You need a surface where she can lie comfortably without being cold. Somewhere with plenty of support. You will be pushing down on her back so avoid the waterbed.\n\n, If she has a favorite album or band that she likes to listen to when relaxing, go with that. Otherwise find something relaxing. Your local music store will likely have plenty to choose from if you feel stuck. Try to avoid the radio since you can't control what might come on or what commentators might say between songs or commercials.\n\n\nIn a pinch you may be able to find some relaxing playlists online or using a smartphone app. Some premium streaming music services provide highly customizable playlists with no commercials.\n\n, Essential oils used in aromatherapy can provide just the right touch to a room. If you know that your room has unwanted odors or scents you definitely want something that smells good. Orange blossom oil, for example, has a fragrance that can produce a calming sensation.Some essential oils can be incorporated into your massage oil. Lavender oil is particularly relaxing when used this way.\n\n, Electric lighting is usually not intended to calm. Most electric lighting can be pretty harsh on the eyes. Natural lighting from candles will produce enough light to perform the massage but is not so bright as to upset the mood. The room should be calm and inviting. Candle light creates a very warm environment.\n\n\nIf you are using heated essential oils you will want to find non-scented candles to avoid mixing smells. You may chose to go with a scented candle in lieu of essential oils altogether.\n\n, Oil reduces friction on the skin. This allows you to press without dragging or pulling the skin with your fingers or hands. Have plenty of massage oil ready to use on both your hands and on her back.\n\n\nMassage oil bottles will get very messy if you need to add additional oil. Consider wrapping your massage oil bottle with a small washcloth and rubber band or putting your massage oil in a liquid pourer such as a small liquid measuring cup.\n\n, Cloth towels, especially cotton towels, can be used for numerous tasks. You can use them to wipe off excess oil. They can also be used to cover body parts when needed. If your wife is lying face down, she may like a towel under her knees or rolled up to relax her feet on.\n\n\nIf your wife wants to completely undress for the massage, use a towel to cover her buttocks and upper thighs if she feels too exposed.\n\n, You want to stretch the muscles in your hands and wrists before you begin the massage. Do this by putting your open hands together in front of you with fingers extended. With one hand, push the other with your fingertips. Repeat for both hands.\n\n\nAnother stretch is to squeeze a stress ball. This can strengthen your fingers for a harder massage.\n\n, While she is decompressing go ahead and put the finishing touches on the setup. Light the candles and turn on the music. Wait until she is ready to do any of this. If you do this before she has arrived she may not feel she can de-stress. Furthermore you don't want to leave the candles unattended since they can catch other items on fire.\n\n, When she is ready for the massage she will likely do this naturally. If she is uncomfortable with taking off her bra for this you may allow her to lie down before unfastening the strap. Let her know that oil may get on her bra if she does not remove it after lying down.\n\n, Then rub your hands on her back. If you seem to need more, put more on your hands and repeat. Do this until your hands slide effortlessly up and down her back. Be sure to get her shoulders, neck, and sides.\n\n, Herein lies much of her stress. Start by cupping your hands over her shoulders. Gently squeeze your hands to put pressure on the muscles in her shoulders. Apply pressure evenly with your fingers as you massage the muscles. Avoid the collar bone and move left and right from neck to arms and back.\n\n\nOne technique is to place your thumb on the back of her shoulder for pressure. While keeping the fingers in the same place, move the thumb across her back and toward her neck with each squeeze. Reposition your fingers and hand periodically to ensure a complete massage.\nDon't forget the neck and the upper arms. With the neck, simply massage the back and sides gently with your fingertips and thumbs. With the upper arms you will come almost entirely off the shoulder and knead the upper arm. Apply equal pressure with your fingers and thumb and allow your hand to slip away from the arm with each squeeze.\n\n, Place your open hands on each side of her back. with the tips of your fingers and thumbs, begin making small circular movements while slightly opening and closing your hand. As you move down the back, massage with your thumbs the muscles beside the spine while your fingers massage her sides.\n\n\nAvoid the spine or anything that might tickle or cause pain.The spine and muscles do not massage well. You can cause a lot of unwanted pain (or injury) by pressing too hard on them. Furthermore, any tickling, punching, poking, slapping, or scratching should be avoided unless requested. Typically this upsets a calm mood.\nIf you were not making circular movements, your fingers would be moving in and out from your palm about and inch or two.\nKeep your fingers spread apart during this exercise. This provides more coverage and pressure is distributed more evenly. Try to apply pressure evenly throughout the bottom of the finger rather than focused on the tips.\n\n, Slide your open hand, palm down, up her back from the lower back to her shoulders and arms. Then slide your hands back to her lower back. Repeat this motion several times remembering to provide even pressure throughout the hand. If you feel a lot of friction, add more oil promptly.\n\n, Position yourself so that you can use your body weight to apply pressure. Not too much pressure--you don't want to hurt her. But use your body's weight instead of constant squeezing to give your hands a break. This technique is especially useful when sliding up and down her back.\n\n",
        "This work proposes a pattern extraction method to both understand what a trained LSTM has learnt and to allow implementation of a hand-coded algorithm that performs similarly to the LSTM. Good results are shown on one dataset for one model architecture so it is unclear how well this approach will generalize, however, it seems it will be a useful way to understand and debug models.\n\nThe questions in WikiMovies seem to be generated from templates and so this pattern matching approach will likely work well. However, from the experiments it's not clear if this will extend to other types of Q&A tasks where the answer may be free form text and not be a substring in the document. Is the model required to produce a continuous span over the original document?\n\nThe approach also seems to have some deficiencies in how it handles word types such as numbers or entity names. This can be encoded in the embedding for the word but from the description of the algorithm, it seems that the approach requires an entity detector. Does this mean that the approach is unable to determine when it has reached an entity from the decomposition of the output of the LSTM? The results where 'manual pattern matching' where explicit year annotations are used, seem to show that the automatic method is unable to deal with word types.\n\nIt would also be good to see an attention model as a baseline in addition to the gradient-based baseline.\n\nMinor comments:\n- P and Q seem to be undefined.\n- Some references seem to be bad, e.g. in section 5.1: 'in 1' instead of 'in table 1'. Similarly above section 7: 'as shown in 3' and in section 7.1.\n- In the paragraph above section 6.3: 'adam' -> 'Adam'.",
        "The World Inside is an album by Human Drama, released by Triple X in 1992. The album landed three videos, \"Look into a Strangers Eyes\", \"Fascination and Fear\" and \"My Skin\", onto Billboard's Top 40 Indie Video chart.\n\nTrack listing\n \"The World Inside I: Nothing Ever Changes/The Enemy/Here I Will Stay\"\n \"The World Inside II\"\n \"My Skin\"\n \"Tears\"\n \"Look into a Strangers Eyes\"\n \"This Tangled Web\"\n \"Winters Life\"\n \"Fascination and Fear\"\n \"A Million Years\"\n \"Color Me Red\"\n \"Father Sing\"\n \"The Sound of the Rain\"\n \"Voices\"\n \"Fading Away\"\n \"Times Square\"\n\nReferences\n\n1992 albums\nHuman Drama albums",
        "Fatma Lanouar (Arabic: فاطمة لأنور; born March 14, 1978) is a former female middle distance runner from Tunisia. She is best known for twice (2001 and 2005) winning the gold medal at the Mediterranean Games in the women's 1500 metres. Lanouar set her personal best (4:06.91) in the 1,500 metres in 2000. She was also the silver medallist at the 2001 Jeux de la Francophonie.\n\nCompetition record\n\nReferences\n\nExternal links\n\n1978 births\nLiving people\nTunisian female middle-distance runners\nAthletes (track and field) at the 2000 Summer Olympics\nOlympic athletes of Tunisia\nMediterranean Games gold medalists for Tunisia\nMediterranean Games medalists in athletics\nAthletes (track and field) at the 2001 Mediterranean Games\nAthletes (track and field) at the 2005 Mediterranean Games\nAthletes (track and field) at the 1999 All-Africa Games\nAthletes (track and field) at the 2007 All-Africa Games\nAfrican Games competitors for Tunisia",
        "The paper extends PixelCNN to do text and location conditional image generation. The reviewers praise the diversity of the generated samples, which seems like the strongest result of the paper. On the other hand, they are concerned with their low resolution. The authors made an effort of showing a few high-resolution samples in the rebuttal, which indeed look better. Two reviewers mention that the work with respect to PixelCNN is very incremental, and the AC agrees. Overall, this paper is very borderline. While all reviewers became slightly more positive, none was particularly swayed. The paper will make a nice workshop contribution.",
        "The proposed approach consists in a greedy layer wise initialization strategy for a deep MLP model, which is followed by global gradient-descent with dropout for fine-tuning. The initialization strategy uses a first randomly initialized sigmoid layer for dimensionality expansion followed by 2 sigmoid layers whose weights are initialized by Marginal Fisher Analysis (MFA) which learns a linear dimensionality reduction based on a neighborhood graph constructed using class label information (i.e. supervised dimensionality reduction). Output layer is a standard softmax layer.\n\nThe approach is thus to be added to a growing list of heuristic layer-wise initialization schemes.\nThe particular choice of initialization strategy, while reasonable, is not sufficiently well motivated in the paper relative to alternatives, and thus feels rather arbitrary.\nThe paper lacks clarity in the description of the approach:  MFA is poorly explained with undefined notations (in Eq. 4, what is A? It has not been properly defined); the precise use of alluded denoising in the model is also unclear (is there really training of an additional denoting objective, or just input corruption?).\n\nThe question of the (arguably mild) inconsistency of applying a linear dimensionality reduction algorithm, that is trained without any sigmoid, and then passing its learned representation through a sigmoid is not even raised. This, in addition to the fact that sigmoid hidden layers are no longer commonly used (why did you not also consider using RELUs?).\n\nMore importantly I suspect methodological problems with the experimental comparisons: the paper mentions using *default* values for learning-rate and momentum, and having (arbitrarily?) fixed epoch to 400 (no early stopping?) and L2 regularization to 1e-4 for some models. \n*All* hyper parameters should always be properly hyper-optimized using a validation set (or cross-validation) including early-stopping, and this separately for each model under comparison (ideally also including layer sizes). This is all the more important since you are considering smallish datasets, so that the various initialization strategies act mainly as different indirect regularization schemes: they thus need to be carefully tuned. This casts serious doubts as to the amount of hyper-parameter tuning (close to none?) that went into training the alternative models used for comparison. \n\nThe Marginal Fisher Analysis dimensionality reduction initialization strategy may well offer advantages, but as it currently stands this paper doesn’t yet make a sufficiently convincing case for it, nor provide useful insights into the nature of the expected advantages.\n\nI would also suggest, for image inputs such as CIFAR10, to use the qualitative tool of showing the filters (back projected to input space) learned by the different initialization schemes under consideration, as this could help visually gain insight as to what sets methods apart. \n\n",
        "Sataspes is a genus of moths in the family Sphingidae erected by Frederic Moore in 1858. They are mimics of carpenter bees.\n\nSpecies\nSataspes cerberus Semper, 1896\nSataspes infernalis (Westwood, 1847)\nSataspes javanica Roepke, 1941\nSataspes leyteana Brechlin & Kitching, 2009\nSataspes negrosiana Brechlin & Kitching, 2009\nSataspes ribbei Rober, 1885\nSataspes scotti Jordan, 1926\nSataspes tagalica Boisduval, 1875\nSataspes xylocoparis Butler, 1875\n\nReferences\n\n \nSmerinthini\nMoth genera\nTaxa named by Frederic Moore",
        " Is he or she, for instance, coming home from work later than usual on some nights? Sometimes these small changes to a person's routine mean nothing more than life has given them a reason to be unavailable, but they are still something that you want to be aware of.;\n, Sometimes, a cheating lover will displace his or her shame, anger, and guilt onto you by starting a fight, which can then be blamed on you. In short, they need a place to throw off their negative feelings.\n\n, Did your significant other always have things to say, and has suddenly become distant? Your relationship may be losing intimacy because your lover has started a new one with someone else.\n\n, Everyone knows this one is stereotypical but it can also be true. If he or she is coming around smelling like a perfume or aftershave that you don't wear, then he or she has been too close to someone who wears it.\n\n, When someone spends enough time with someone else, they begin to adopt their views, or at least understand them. If your boyfriend or girlfriend is interested in things that they hated before, perhaps they are being influenced by someone else more than usual.\n\n, Does your lover allow you to touch, or look through their phone, emails, etc.? If your boyfriend or girlfriend is hiding their phone from you, or deleting all their messages before letting you have it, then there is something they don't want you to see.\n\n, If you ask whom called or texted them, do they always tell you \"It's nobody?\" These are also important signals, which show that your boyfriend, girlfriend, etc. has something to hide.\n\n, Does he or she become angry when you come over unannounced, or only text you when they are at home, never calling you? These are also signs of betrayal. For some reason, they do not want you at their home at certain times, and they do not want to call you while they are at home.\n\n, If an earring is lying in the passenger side of their car, or a condom is behind their bed, extra time will probably be taken to dispose of those items.\n\n, They may be saying awful things about that person, but pay attention to the fact that they are still talking about them 24/7. They are trying to fool you into believing the person is undesirable, thus throwing the suspicion off them.\n\n, Casually ask a boyfriend or girlfriend where they were such and such day, and let them answer. Remember their answer, and ask again a few days later. If they are lying constantly, then they will have a hard time keeping up. They may start to get angry with these simple questions, another big hint.\n\n, bought for two people. If you keep finding receipts from McDonalds with two different meals on it, or two sodas in the cup holders, or a bill for a bracelet you did not get, then watch out.\n\n, Or begin to, out of nowhere, change their passwords to keep you out of their accounts. Normally, if they didn't give you their password to start with, this is no problem. But if they've suddenly changed it to protect messages they've written, or chats they've had, you should be alert to this possible red flag.\n\n, Have his or her friends begun to act oddly around you? When you casually talk to his friends, do they seem anxious, nervous, or eager to leave? They probably know something you don't know.\n\n, Questions such as \"What would you do if we broke up?\" are key hints. Men and women, both, become guilty. This guilt can easily be forgotten with an action or item for the other person that makes the cheater seem selfless.\n\n, If they voluntarily give you excuses that are detailed and scripted, then they probably are.He or she has most likely projected your questions, based on their indiscretion, and preemptively created a story to cover it all up. Listen and note any details which are unnecessarily significant to their story (names, times, exact locations, etc.) and keep a mental list.\n\n, When they finish telling their story do they shake their head, toss up their hands, or use similar gestures to mimic exhaustion, confusion, and disbelief about their 'crazy evening' or event? Body language is a great indication that he or she is 'acting'.\n\n, Keep it a seemingly harmless detail that won't frighten or alert your partner that you are suspicious (specifics are not what you are looking for). Now ask them an abstract question about the specific detail you picked. Innocently ask when this 'detail' or event happened, or some other simple question. If he or she is not being truthful they will give it away during this question.\n\n, Did they awkwardly get silent or shift before giving you an answer? Did they stutter or become nervous with their words? Did they look like a deer in the headlights? If so, here's why: When someone is telling the truth they don't need to think about the answer for more than a second or two. The details are already there because it truly happened. But, if this person is lying, they will have to stop and mentally go through their story from the beginning. He or she will need to remember where in the story this question happened and that takes thought processing. Since the 'minor' detail you asked them to provide wasn't prominent when they made up their excuse, they most likely won't have a cut and dry answer.\n\n, Keep watching closely. Can you see them thinking it through again? Are they becoming frustrated and starting to snip at your 'dumb questions' and/or ask, \"What does it matter?\" If so, this is a stalling technique they use while making up new details. It's also a handy distraction if they point their finger back at you and make you the bad guy for asking in the first place. How often have we heard, \"What exactly are you insinuating?\"\n\n, Usually your partner will start questioning you and inevitably say, \"What? You don't believe me? Do you think I’m lying to you?”.\n\n, Eventually you will hear the sweet words, \"What? Do you think I'm cheating on you?!\" Bingo!\n\n, Did you steer the story in a direction to make room for interrogation then blatantly accuse them of messing around? Did you actually accuse them of anything at all? By asking simple questions your partner not only revealed what kind of behavior they have been hiding, but they also spun their entire story out of whack, no matter how solid they believed it to be, and became overwhelmed. At this point, if you ask them anything, he/she will probably start to sweat, panic, or lose the capability to accurately recall anything about their story. The discrepancies will flow at this point and soon you will have enough information know if your partner is 'on the level.'\n\n",
        "Phạm Ngũ Lão was a leading general of the Trần Dynasty.\n\nPhạm Ngũ Lão may also refer to:\n Phạm Ngũ Lão Street, a street in Ho Chi Minh City, Vietnam\n , a frigate in commission in the Republic of Vietnam Navy from 1972 to 1975\n , a patrol vessel in commission in the Vietnamese People's Navy in 1975",
        "This paper extends batch normalization successfully to RNNs where batch normalization has previously failed or done poorly. The experiments and datasets tackled show definitively the improvement that batch norm LSTMs provide over standard LSTMs. They also cover a variety of examples, including character level (PTB and Text8), word level (CNN question-answering task), and pixel level (MNIST and pMNIST). The supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration.\n\nThe experiment on pMNIST also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies. I additionally also appreciated the gradient flow insight, specifically the impact of unit variance on tanh derivatives. Showing it not just for batch normalization but additionally the \"toy task\" (Figure 1b) was hugely useful.\n\nOverall I find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting.",
        "  Context. Searching for planets in open clusters allows us to study the\neffects of dynamical environment on planet formation and evolution.\n  Aims. Considering the strong dependence of planet frequency on stellar\nmetallicity, we studied the metal rich old open cluster NGC 6791 and searched\nfor close-in planets using the transit technique.\n  Methods. A ten-night observational campaign was performed using the\nCanada-France-Hawaii Telescope (3.6m), the San Pedro M\\'artir telescope (2.1m),\nand the Loiano telescope (1.5m). To increase the transit detection probability\nwe also made use of the Bruntt et al. (2003) eight-nights observational\ncampaign. Adequate photometric precision for the detection of planetary\ntransits was achieved.\n  Results. Should the frequency and properties of close-in planets in NGC 6791\nbe similar to those orbiting field stars of similar metallicity, then detailed\nsimulations foresee the presence of 2-3 transiting planets. Instead, we do not\nconfirm the transit candidates proposed by Bruntt et al. (2003). The\nprobability that the null detection is simply due to chance coincidence is\nestimated to be 3%-10%, depending on the metallicity assumed for the cluster.\n  Conclusions. Possible explanations of the null-detection of transits include:\n(i) a lower frequency of close-in planets in star clusters; (ii) a smaller\nplanetary radius for planets orbiting super metal rich stars; or (iii)\nlimitations in the basic assumptions. More extensive photometry with 3-4m class\ntelescopes is required to allow conclusive inferences about the frequency of\nplanets in NGC 6791.\n",
        "This paper presents a simple strategy for hyperparameter optimization that gives strong empirical results. The reviewers all agreed that the paper should be accepted and that it would be interesting and useful to the ICLR community. However, they did have strong reservations about the claims made in the paper and one reviewer stated that their accept decision was conditional on a better treatment of the related literature. \n \n While it is natural for authors to argue for the advantages of their approach over existing methods, some of the claims made are unfounded. For example, the claim that the proposed method is guaranteed to converge while \"methods that rely on these heuristics are not endowed with any\n theoretical consistency guarantees\" is weak. Any optimization method can trivially add this guarantee by adopting a simple strategy of adding a random experiment 1/n of the time (in fact, SMAC does this I believe). This claim is true of random search compared to gradient based optimization on non-convex functions as well, yet no one optimizes their deep nets via random search. Also, the authors claim to compare to state-of-the-art in hyperparameter optimization but all the comparisons are to algorithms published in either 2011 or 2012. Four years of continued research on the subject are ignored (e.g. methods in Bayesian optimization for hyperparameter tuning have evolved considerably since 2012 - see e.g. the work of Miguel Hern‡ndez Lobato, Matthew Hoffman, Nando de Freitas, Ziyu Wang, etc.). It's understood that it is difficult to compare to the latest literature (and the authors state that they had trouble running recent algorithms), but one can't claim to compare to state-of-the-art without actually comparing to state-of-the-art.\n \n Please address the reviewers concerns and tone down the claims of the paper.",
        " It is possible that you are too sick for work and should stay home. By staying home you can prevent yourself from getting worse and help prevent the spread of illness. You might also help speed along your recovery so that you are more productive when you do return to work.Think carefully about whether you are better off working or focusing on your recovery.\n\n\nIf you have a high fever (over 101 degrees Fahrenheit) or spots on your throat, you might have to consult your physician. You should also talk to your doctor if you have trouble staying hydrated or if your symptoms do not get better after a few days.Many workers cannot afford to take time off because of illness. If this is the case, then you will have to find ways to take care of yourself even while working.;\n, It might be possible for you to get your work done from home instead of from the office. This option is great for employees (who can focus a bit more on recovery) and for employers (who do not have to worry about the spread of illness). Talk to your workplace to see if this is an option.In order for telecommuting to work, you will likely require a secure laptop and a high-speed internet connection as well as a reliable phone.\n\n, Being expected to work while ill can be a source of stress. However, stress weakens the immune system and might prolong your recovery time.Take a few deep breaths and tell yourself that you will be all right. Even though you are ill, you will be able to be productive as well as recover from your illness. It might not be ideal, but you will get through this illness.\n\n, Sometimes we have a day or two of warning before we get sick. Perhaps you might feel run-down, achy, or sleepy. When you feel a cold or other illness coming on, organize your work tasks so that you do not lose productivity during your illness. Get ahead on as many tasks as you can, and consider taking some work home with you so that you don't have to come into the office., Illness makes it hard to focus and can also reduce your stamina. In order to get work done, approach your work as a series of small, manageable tasks. The Pomodoro technique, where you work in short bursts of 25 minutes and then take a short break, is especially useful when you are sick.For example, rather than putting together an entire presentation, tell yourself that you will simply make one slide at a time. After each slide is complete, allow yourself a break to recover: take a short nap or drink some tea.\n\n, If you can, focus on lower-stakes projects while you are ill. This can help prevent yourself from making silly errors on important tasks. Consider carefully whether it is necessary for you to do crucial, important work when you are feeling poorly. Catch up on busy-work whenever possible.For example, a day when you are ill might be an excellent time to do mundane, mindless tasks like cleaning out your email inbox, filing documents, or putting together next month's calendar. Try to avoid tasks that require high-level thinking, such as writing a crucial research report.\nIt is also a good idea to work on first drafts instead of final drafts of papers and projects.You can reread your drafts when you are feeling more like yourself. This will reduce the likelihood of major errors in the final version.\n\n, Workers who are ill are only 60% as productive as normal.This means that you have to think very carefully about what kinds of work you actually have to accomplish while ill. Examine your deadlines and your calendar in order to prioritize which tasks have to be completed during your sick day., Acknowledge from the outset that you will not be as productive as normal while you are ill. Be kind to yourself and resist the urge to run yourself ragged. If you push yourself too hard while you are ill, you might delay your recovery or you might feel even sicker.Be productive if you have to, but allow yourself some time to relax and recover.\n\n, Sometimes we do not have a choice about what work we have to accomplish. But other times we might be able to rearrange our schedules. If you are sick, think about whether there might be some meetings that would be more productive when you are feeling better. Ask about postponing any meetings that are not time-sensitive or meetings where you will be expected to perform at your highest level., People who are sick need more rest than usual and also need to stay hydrated. Be sure that you are allowing yourself lots of time to rest in between work tasks. Go to the water cooler, get tea at a nearby coffee shop, or simply rest your eyes at your desk for a few minutes.You will be more effective if you do not push yourself too hard, too quickly.\n\n, Reach out to your neighbors, friends, family, and coworkers if you have to work while sick. Perhaps they might help you out around the house, bring you some soup, or be able to lend a hand with editing an important document. Everybody gets sick sometimes, and your loved ones and coworkers will sympathize with your plight.If your coworkers help you with your duties, be sure that you express your gratitude and that you return the favor when your colleagues feel ill.\n\n, It is important to stay hydrated when you are sick. But sometimes we need caffeine in order to get through a workday when we are under the weather. Feel free to indulge in the occasional cup of coffee to get through this tough time, but be sure that you are drinking water as well. Drink 3 cups of water for every cup of coffee you have., If you are working from home, let yourself take a nap every now and again. Use naps as a reward to treat yourself when you accomplish an important task.These naps will inspire you to accomplish more at work and will also help your body begin to fight your illness.\n\n, If you are working from home or only working a half-day while you are sick, take a few minutes to organize your return to full-time work. Make a list of the most important tasks that you will have to accomplish, and begin to envision how you will accomplish these. Set a reasonable schedule to make sure that you catch up on what you might have missed during your illness.\n\n, Use rewards for finishing goals each day. Treat yourself with comfort foods, hot beverages, naps, or your favorite movie to watch while sick. Feel proud that you could accomplish so much, even during your illness\n\n, Perhaps you feel too ill to accomplish your tasks for work or school. Your brain might be too sluggish, or perhaps you cannot even get yourself out of the house. If you are feeling so poorly that you cannot focus on work, let yourself be productive in other ways. Maybe you can catch up on sleep, which will make you more effective when you return to the office.Or maybe you can clean your house or prepare some meals to put in your freezer, leaving yourself more time to work later in the month. Think about other ways that you can be productive, even if you are too ill to focus on your job.\n\n, In order to be effective at work, you will have to be kind to yourself. Try to make yourself feel as good as possible before heading in to work. Alleviating your symptoms might not speed up your recovery time, but you will feel more like yourself. Moreover, you will be more capable of meeting the demands of the day.\n\n, Many of the ways to alleviate your symptoms involve specific medications, supplies, foods, and beverages. You might have to plan a trip to a local drugstore or grocery store in order to stock up on these supplies if you don't have them handy.\n\n\nConsider asking a friend or family member to pick up these supplies for you if you are too under the weather to leave the house.\n\n, The most important key to recovering and feeling better is staying sufficiently hydrated. Keep a bottle of water with you at all times. It is also a good idea to have a good supply of hot tea nearby: hot tea is not only hydrating but it can also soothe a sore throat.Avoid alcohol while you are sick because it can dehydrate you and slow your recovery time.\n\n, An over-the-counter saline nasal spray can help when you have a stuffy nose, sinus headache, or seasonal allergies. Nasal spray helps your body to flush out mucus and allergens, which will help you clear your head. Nasal spray can also help soothe your nose if it feels dry and irritated during a cold.When you use nasal spray, be sure that you keep facial tissues or a handkerchief nearby. You will likely have to blow your nose immediately after using the spray.\n\n, Ice cubes can help numb and soothe a sore throat. They are also an excellent way to keep hydrated if your throat is feeling too ragged to take full swallows., Many symptoms of common illnesses can be soothed with over-the-counter medications. For example, cough drops and syrups, decongestants, pain relievers, and anti-nausea medications can all be purchased without a prescription from your doctor.Don't combine medications to ensure that you don't experience adverse reactions.Be sure that you read the instructions carefully, take only the recommended dosage, and be on the lookout for allergic reactions. Even over-the-counter medications can cause side effects: do not treat them like candy.\n\n, Many illnesses are exacerbated by irritants in the environment such as smoke or chemical scents. Try to stay away from these irritants if you can. For example, do not hang out in a breakroom if smokers use it for their cigarette breaks. Stick to clean, controlled environments., A vaporizer or humidifier can help a sick person breathe normally and can help break up nasal blockages. Breathing in wet, humid air also keeps the mucus membranes lubricated, which allows the body to fight infection more effectively.Use a vaporizer overnight or, if possible, at your work desk in order to breathe and feel better.\n\n, Sometimes illness can make you feel less hungry than usual. However, your immune system requires nutritious food in order to have the energy to fight infection. Try to eat nutritious, comforting foods such as broths and soups. These foods also help keep you hydrated, which is essential when you are sick., Before you head in to work, take a hot, steamy shower. You will ease your aches and cramps, and the steam will help to clear your head. This can be especially important if your illness is caused by a cold or flu virus, sinus blockage, or seasonal allergens., When you are sick, you might feel flushed or chilled at times. Use hot and cold compresses in order to help equalize your body temperature and feel like yourself. These compresses can also help ease the muscle aches and pains caused by certain illnesses such as the flu virus., It is wonderful that there are so many options for alleviating symptoms. However, easing a symptom is not the same thing as a cure or a full recovery. In many cases, a symptom reliever does not shorten recovery time at all. If you cannot shake your illness after seven days, you might want to consult a physician to make sure that you do not require prescription medication in order to recover fully., If you cannot avoid going in to school or the office, do your best to keep your illness contained. Stay well clear of your colleagues in order to minimize their exposure to your illness. Telecommuting is another excellent option for getting work done without exposing your coworkers to infection., When you are sick, it is a good idea to wash your hands more often than normal. Use warm water and soap, and wash your hands for at least 15 seconds to make sure that you are fully cleaning your hands. This will help prevent the spread of infection around the office, such as when you touch doorknobs or computer keyboards., When you cough or sneeze, use your sleeve or elbow to cover your mouth. Sneezes and coughs spread infection easily, and you do not want to expose your colleagues.Covering your mouth with your hand might lead to you spreading germs when you touch doors, computers, or other objects around the office. Your elbow is much safer.\n\n, When you are sick, use disinfectant cloths and sprays to wipe down communal surfaces.Be sure to clean doorknobs, drawer pulls, and the refrigerator door handle. Any surface that both you and your colleagues are likely to touch should be disinfected.\n\n, Do not lend your colleagues your computer, mug, stapler, or pen when you are ill. If they ask to borrow these items, warn them that you are feeling under the weather. It is better for them if they borrow an item from another, healthy colleague., It is terrific to use reusable items most of the time. That is better for the environment and often better for your pocketbook. However, you are allowed to cheat a bit when you are feeling sick and contagious. Splurge on disposable coffee and tea mugs, disposable silverware, and paper plates. This will allow you to throw away your used items to minimize your colleagues' exposure to your infection.",
        "  We present spatially resolved mid-infrared images of the disk surrounding the\nyoung star IRS 48 in the Ophiuchus cloud complex. The disk exhibits a ring-like\nstructure at 18.7 micron, and is dominated by very strong emission from\npolycyclic aromatic hydrocarbons at shorter wavelengths. This allows a detailed\nstudy of the relative distributions of small and large dust grains.\n  Images of IRS 48 in 5 mid-infrared bands from 8.6 to 18.7 micron as well as a\nlow resolution N-band spectrum are obtained with VLT-VISIR. Optical\nspectroscopy is used to determine the spectral type of the central star and to\nmeasure the strength of the Halpha line.\n  The 18.7 micron ring peaks at a diameter of 110 AU, with a gap of ~ 60 AU.\nThe shape of the ring is consistent with an inclination of i = 48 +- 8 degrees.\nIn contrast, the 7.5-13 micron PAH emission bands are centered on the source\nand appear to fill the gap within the ring. The measured PAH line strengths are\n10-100x stronger than those typically measured for young M0 stars and can only\nbe explained with a high PAH abundance and/or strong excess optical/UV\nemission. The morphology of the images, combined with the absence of a silicate\nemission feature, imply that the inner disk has been cleared of micron-sized\ndust but with a significant population of PAHs remaining. We argue that the gap\ncan be due to grain growth and settling or to clearing by an unseen planetary\nor low-mass companion. IRS 48 may represent a short-lived transitional phase\nfrom a classical to a weak-line T Tauri star.\n",
        " Most newspaper articles break down into two types: news articles and feature articles. You will also find opinion pieces, like editorials and book or movie reviews in the pages of your school newspaper. But generally, you will be working on a news article or a feature article.News articles cover the basics of current events. They answer the five key questions: who, what, where, when, and why.\nFeature articles are longer and more in depth than regular news articles. They cover one subject from multiple angles and written in a more creative format.\nBoth types require the same level of research and reporting. If you are writing a feature article, you may have more freedom with the structure of the article. However, news articles always follow the same “triangle” or five part structure.;\n, A newspaper article is written in the form of an inverted triangle, with the most important or key information in the first paragraph (the wide part of the triangle), and then the least important information in the last paragraph (the narrow point of the triangle). A basic news article is made up of five parts:\n\n\nThe headline: Also referred to as the “hed”, this is a short, attention grabbing statement about the event. It will appear at the top of your article.\nThe byline: Tells the reader who wrote the article. If you wrote the article, your name will appear in the byline.\nLead paragraph: This first paragraph contains the who, what, when, where, and why in as few words as possible. You must find all the answers to these questions and place them in the opening 1-3 sentences of the article.\nExplanation: The second-third paragraphs should include any other facts or details the reader should know. This is where the writer answers any other important questions the reader might have after reading the headline and the lead paragraph. This section can include direct quotes from witnesses or bystanders.\nAdditional information: The last paragraph contains the least important information. For example, information about a similar event or occurrence. Your editor may cut this paragraph down if the article is too long for the space it needs to fill in the newspaper.\n\n, These abbreviations are important in the newspaper world. Most editors will ask you for the “dek” and “lede” of your article after you pitch the story to them.The “dek” is a blurb, usually one to two sentences, that tells the reader what the story is going to be about. It will appear under the “hed” or headline of the article. For example, an article about fiber might have the head: “YOU’RE FIBER-ED!” and the dek might be: “Ten Reasons To Eat More Fiber.”The “lede” is the introduction to the news article, usually in the first paragraph of the article. “Lede” is a deliberate misspelling of “lead” to prevent confusion in the days when printing was done with lead type.\nThe “lede” should answer the classic five W’s of journalism. What happened? Who did it? Where did it happen? When did it happen? Why did it happen?Some articles require a \"How\" as well, but this can often be covered by answering the others.\n\n, Most news articles are written from an objective point of view, in the third person. Unlike opinion pieces, or editorials, news articles should not contain any “I” statements, such as “I believe” or “I think”. The main purpose of the article is to inform your readers of all the facts around a story or event. You should maintain a neutral tone throughout the article and cover all sides or perspectives on the story.However, most stories have a particular angle or “slant”. This means the story focuses on a particular aspect or element of the larger story. For example, a story about fireflies could focus on the endangerment of fireflies through the use of pesticides in the air. The “slant” is a valuable way to give the story a clear focus and unique take on an existing topic.Feature articles can use the “I”, or first person point of view. Recently, personal narrative style features, with the “I” and a personal story that colors or informs the news story, have been popular online., Now that you are familiar with the structure and terms of a basic news article, read several example articles to get up close and personal with how to write an article:\n\n\n“Pandemic Level Increased”, a straightforward news article about swine flu.“Harry Potter Opens July 15”, a creatively written news article about the opening of the film Harry Potter and the Half Blood Prince.“Fireflies Count!”, an example of a feature article written from the first person point of view.“Pregnant at Harvard?”, an example of a feature article written in the style of a personal narrative for The Harvard Crimson, a university publication., Look for the headline, the byline, the lead paragraph, the explanation paragraph(s) and the additional information at the end of the articles.\n\n\nFor example, the headline for this article, “Pandemic Level Increased”, has the byline: Daniel Wetter.It also has a lead paragraph that begins with key information: “The swine flu outbreak became a phase 6 pandemic on June 11, declared the World Health Organization (WHO). Officially known as the H1N1 virus, the flu is being transmitted community-wide in both North America and Australia. A pandemic is an epidemic (disease outbreak) over a widespread geographical area.”\nIt then has a fairly long explanation section, with quotes from two doctors, or medical sources.\nIt ends with additional information, or an ending sentence that reinforces the main slant of the article: “Being health conscious and getting vaccinated will make you part of the solution.”\n\n, The “hed” or headline should be pretty obvious by now. The “dek”, or blurb, usually 1-2 sentences, that tells the reader what the story is going to be about, will be underneath the “hed”.\n\n\nFor example, for the article “Harry Potter Opens July 15”, the “dek” is “Magic and mysteries in the air at Harry Potter’s New York premiere.”The “lede” of the article should answer the four W’s and the H of newspaper writing. The Harry Potter premiere article’s “lede” appears in the second paragraph. “At the premiere of the newest Harry Potter movie in New York City July 9, I stood along the red carpet watching as excited Potter fans crammed together behind metal barriers waiting for the stars to arrive. Some sported sorting hats, the signature round rimmed Potter glasses, and other mystical garments from the world of Hogwarts. Some held up signs. One read: “Honk if you love Harry.” And there was honking—and yelling and cheering and chanting. “Snape! Snape! Sirius! Snape!” It was magical!\"\nThe “lede” in the article then extends into the third paragraph: “The crowd ignited with screams and cheers as the actors stepped out of their limos and onto West 54th Street in front of the Ziegfeld Theatre. There was so much excitement in the air you could taste it!”\nThis “lede” answers the who (Harry Potter fans, including the writer of the article), the what (Premiere of the Harry Potter movie), the where (Ziegfeld Theatre, New York City), the when (July 9), and the how (The premiere happened because the newest Harry Potter movie is out and Harry Potter fans are enthusiastic about the new film).\n\n, Tone and point of view are important elements of a news article. Most news articles should maintain a neutral or objective tone. But they should also have a “slant” or particular focus in the article, while still discussing all sides of the story.\n\n\nFor example, the Harry Potter article is written in the first person, by a Harry Potter fan, so it will have a certain slant and point of view. The writer uses statements like: “It was magical!” and “There was so much excitement in the air you could taste it!”These statements are purely opinion based and serve to give the article some personal color and perspective.\nIn contrast, the swine flu article is written in the third person, with no use of the “I” or any personal statements from the writer. It presents the facts and details of the swine flu outbreak and how to prevent spreading the disease to others. It is a more standard news article in tone and voice.In The Harvard Crimson feature story, “Pregnant at Harvard?”, the writer examines her personal struggles with being pregnant at Harvard and uses many personal notes and moments to give the story a unique slant. These types of features are becoming more popular in publications and school newspapers, as they give a story a personal connection or voice., Journalism students often use writing prompts to generate story ideas. These can be prompts such as:“That Time I…”: Think of a life changing experience or moment in your life. It could be that time you almost drowned in your neighborhood swimming pool, that time you ate nothing but ramen noodles for 2 months, or that time you broke up with an abusive boyfriend. Then, consider how these personal experiences could translate to a story idea. For example, pool safety in residential pools, the health issues associated with a diet of ramen noodles, or the steps needed to get out of an abusive relationship.\n“A Day in the Life”: Profile of a person of interest by following them around for a day. This could be someone with an interesting job at your school, a peer or student at your school who is working on a social or political project, or a teacher who is leading a unique class or approach to teaching. This could also involved your school’s star athletes, or athletes who overcame obstacles to be successful.\n“Everyday school topics”: Think about your daily school routine and note anything interesting or unique you come across. This could be your morning commute, with gossip on the school bus that could lead to story ideas, your lunch meal, with its suspicious looking macaroni and cheese, or your classes, with a great teacher or an incompetent teacher. Look for possible issues you encounter during your school day, or conflicts that you might want to delve further into.\n\n, Another good way to take a simple word or one-word idea and turn it into a story idea is to brainstorm possible angles. Choose a current issue, like gay marriage in America or gender identity, and do a brainstorm. Or choose an issue that is related directly to your school, like the school budget for next year or a new class.Write down the word or idea in the center of your paper.\nWrite down other related words or terms around the central idea. Don’t stop to evaluate the items or revise them. Don’t worry if the words or terms seem lame, and don’t cross out or ignore the idea.\nContinue adding words or terms until you feel you have written down enough. Read over the terms and circle or highlight any terms that seem useful or that could lead to a possible angle or slant on the topic.\n\n, Read through other news sources to see what other writers are talking about or discussing. Get back issues of your school’s newspaper and look at what previous articles have addressed before. Are there current topics that might respond to previous articles? Or is there a unique take only you might have on a controversial topic?For example, you may be going through a personal trauma that relates to a current topic like gender identity, abortion rights, same-sex marriage, or police brutality. Or you might know someone, like a family friend or relative, who might be able to offer a unique take on one of these topics. This person could act as a key source for your article.\n\n, Often, if you are a staff writer for the newspaper or have signed up to be a contributing editor to the publication, your editor may assign you story ideas. Your editor may also assign you seasonal articles, like Christmas or Halloween themed stories, that will be in the seasonal issue of the newspaper.\n\n\nMost editors will ask you what topics you are interested in writing about, or if you have a unique angle on a certain topic or issues before they assign story ideas. Maintain an open dialogue with your editor so she knows where your interests lie, and which story ideas might be the right fit for you.\n\n, Once you determine your story idea, you should do some basic research to ensure the story is worth pitching to your editor. You also want to confirm someone else hasn’t already written the story, or from the same angle.\n\n\nDo a basic google search of keywords of your topic. If you’re writing an article about teaching social justice in high school classes, for example, do a basic search of possible courses in your area, state, or country.\nYou should also make a list of potential sources you can contact and interview for the story.\nIf you have been assigned a story idea via your editor, you likely will not need to write a pitch letter for the story. But you will still need to do research before you write the article.\n\n, One of the first things you should do is contact your potential sources and set up in person interviews. You can also conduct phone interviews or email interviews. But if possible, try to talk to your sources in person. Email interview answers tend to be stilted and awkward, as the interview subject is responding by typing, rather than talking.Contact the source via email or by phone. Give the person a brief summary of the article you are writing, the “lede”, and ask them what time works best for an interview. Try to give 45 minutes minimum for the interview, especially if they are a primary source. Confirm the meeting time with the source.\nFor a standard news article, you should have one to two sources minimum. Good sources include people with credentials that relate to your topic, such as a medical doctor or a specialist in your topic. Your sources should have strong knowledge about your topic, on a professional or personal level, and be willing to answer questions on tape.\n\n, You should come to the interview with a list of at least 10 questions to ask the source, as well as two to three backup questions.Focus on open ended questions that will result in expansive answers, as opposed to a yes or no. Rather than start a question with “Do you think”, begin with “How do you think” or “In what ways does”.\nDon’t be afraid to ask dumb questions, such as “How does that work?” or “What do you mean by that term or abbreviation?” This will be useful if you are interviewing experts in a specialized field or about a complex idea and need to simplify it for the general public.\nAsk short questions and then follow up. Most interviewers make the mistake of sticking to the questions on their notepad, rather than having a more free flowing conversation with the source. Start with short, basic questions like “How did you get into teaching social justice at schools?” or “How do you balance athletics with school?” and then build off of the source’s answers. Split up long winded questions into shorter questions so that source does not get overwhelmed or confused by your questions.\nMix lighthearted questions with pointed questions. A good interview will generate quotes from your sources that address your topic and give your unique angle on the topic some weight. But avoid only asking pointed questions that might cause your source to shut down or clam up. Mix in lighthearted or fun questions so the source feels comfortable and relaxed during the interview.\n\n, To get the most accurate quotes possible, use a small digital recorder. Place it on a flat surface and let the interview subject know you are recording the conversation before you turn it on.You can also download a recording app on your phone to record in person interviews or phone interviews.If you are doing a skype interview, you can use a recording app through skype.\n\n, If you have already been assigned a story idea by your editor, you will not need to create or send her a pitch letter. But if you are proposing an original idea to the editor of your school newspaper, you will need to create a pitch letter. Keep the letter short and to the point. Follow this format:Address the letter to the editor of the newspaper, by title or by name. For example: “Dear Chronicle Editor in Chief” or “Dear Mrs. Jenna Smith”.\nCreate an engaging opening sentence. Avoid telling the editor you have a great story or that you, the writer, will blow their socks off. Lead with what is strong and interesting about your topic, as well as the unique angle you might have on the topic. For example: “The World Health Organization has recently declared swine flu a phase 6 pandemic. Yet there is very little knowledge of how to prevent the spread of this disease in the classroom.”\nBody paragraph: Break down your article in more detail. Let the editor know if you plan to speak to certain sources. If you have any personal connection or experience with the topic, note this in the body of the letter. For example: “As a student at Roosevelt High, I think it’s essential that students are aware of what swine flu ir and how to prevent the spread of swine flu. In my article, I will talk to two medical professionals about the dangers of swine flu and prevention techniques. I plan to also look at how students can practice simple, everyday habits to prevent the spread of this disease. ”\nClosing: Finish the letter by letting the editor know if you have done any initial research on the topic and if you have any other experience writing similar articles. For example: “Based on my initial research, swine flu is a major disease that is not nearly as understood by the general population, or by high school students, as much as it should be.” Provide links to clips, or examples of your writing, in other publications. Then, sign off the letter with “Best regards” or “Thank you for your time.”\n\n, Once you submit your pitch letter, give your editor time to review it. Then, ask her if she has any suggestions on other sources or angles for your story. She may also give you a word limit for your story. Most news articles are short, 400-500 words long.\n\n, You want to lead with a sentence that grabs the reader’s attention and intrigues them enough to keep reading. Start with the most important information first.For example: “The swine flu outbreak became a phase 6 pandemic on June 11, declared the World Health Organization (WHO).”or “I was like a seeker in the game of quidditch, but I wasn’t searching for the golden snitch...I was searching for the golden actors who star in the latest Harry Potter movie, Harry Potter and the Half Blood Prince.”The first lead is factual, objective, and to the point. It tells the reader that there is a clear medical issue and gets them to pay attention to the outbreak.\nThe second lead is more personal and uses the first person. It draws the reader in by using familiar terms from the world of Harry Potter and unique language to keep the reader engaged.\n\n, Don’t overuse adverbs or adjectives in your article. Keep the language simple and plain, with strong verbs and nouns. If a word or term is not essential to the story, don’t include it.Using plain language will build trust with your reader, especially if you are discussing a complex medical topic. It will also help your reader follow along with your writing.\nUse sentences that are 25 words or fewer. Focus on plain English, rather than academic or technical jargon.\n\n, Consider who you are writing the article for. If its for the general public, you should assume the reader has no prior knowledge of the topic. Imagine you are explaining the topic or issue to someone who has never heard of the topic. However, if you are writing about a current issue that the general public might be familiar with, such as the latest political scandal, or the big win at the football game on Friday, you can assume some prior knowledge of the events. Your article should then give the reader new information or more information on a major topic from yesterday’s news or the latest hot topics.If you are writing an article for a certain section of the newspaper, like the Arts and Culture section, you may be able to assume your reader will be familiar with certain mainstream artists or current cultural trends.\nYou may also be writing about a topic that is very familiar to most readers, such as Harry Potter. In this case, you could use terms or phrases that are known to avid readers of the topic, such as an article about a Harry Potter film.\n\n, Strong verbs will make your article lively and interesting. Focus on using active verbs like “she stands”, “walks” “runs”, “spots her teammate”, “chats to her coach”. Passive verbs can sound dull and boring to readers.\n\n\nIn most cases, editors advocate for the use of the present tense, rather than the past, to give the article immediacy and an active voice. However, you can use the past tense in your article if your editor says otherwise.\n\n, The reporting “voice” of the article should only contain facts. Any opinions or subjective descriptions should be attributed to a named source. Your article should be supported by quotes from at least two sources. Rather than tell you reader they should be concerned about swine flu, for example, use a quote from an expert to back up with claim.For example: \" ‘We need to be appropriately concerned,’ Dr. Trochet said. It cannot be ignored, but it can be prevented with easy measures, she stressed. Dr. Trochet and Dr. Tom Hopkins, the Chief Medical Correspondent on NBC's Sacramento Station, KCRA, talked recently to the Scholastic Kids Press Corps about swine flu concerns. They also discussed how to prevent getting or spreading the disease.”\nUse “said” or “told me” when attributing a quote to a person, and only use the source’s last name or title and name.\n\n, Make sure your article adheres to the five part structure of an article:\n\n\nThe headline, or the “hed”.\nThe byline.\nThe lead paragraph, or the “lede”. Should answer who, what, when, where, and why in as few words as possible.\nExplanation paragraphs, including direct quotes from sources.\nAdditional information, last paragraph that contains the least important information.\n\n, Look over the article for any spelling or grammatical errors. Ensure your article has a strong \"lede\" and follows the five part news article structure.\n\n\nYou should also format the article based on the publication you are writing for. If it is an online publication, ask the editor if there is any specific formatting you need to do, such as adding links in text.\nYour school newspaper may also have a style guide with rules of certain phrases or terms in the articles they publish. Ask your editor for these guidelines and adjust your article accordingly.\n\n",
        "Post-WWII, exile as a form of judicial punishment would be a violation of international law:\n\n[Article 15 of the Universal Declaration of Human Rights](_URL_6_) states that:\n\n > Everyone has the right to a nationality.  \n\n[The Convention on the Reduction of Statelessness](_URL_2_) further binds member-states not to deprive people of their nationality as a judicial punishment unless they have another nationality, except in fairly limited circumstances^1.  The popular image of exile - ie, of forcing someone to leave the country and never come back - is impractical as a punishment for non-political crimes in the modern age, because states don't want to end up harbouring foreign criminals: \"Ring ring, hello, Germany here.\"  \"Hello, it's the USA, we've got a rapist we want rid of with no connection to Germany, can we send him to live in Hamburg?\" \"Piss off.\"  There is also no unclaimed land left, aside from (debatably) Antarctica, and dropping someone off in Antarctica with no means to survive would effectively be a death sentence - and a rather cruel one at that.\n\nPersons with dual nationality can be forced to leave their adopted countries for committing crimes - but this is deportation, not exile.\n\nBut exile does still happen after WWII, and frequently - its just that it is not formally imposed as a judicial punishment, but is a *de facto* result for individuals who cannot remain in their homeland for some reason.  South American rulers of both ideological sides of the Cold War found themselves [living in each other countries during that era](_URL_7_) - for example, Stroessner (Paraguay) went to Brazil and Torres (Bolivia) went to Argentina (where he was later murdered).  Various categories of people who have fallen out of favour with their national governments ended up fleeing abroad in the 20th century - eg, former Republicans in Francoist Spain, Latin American leftists during Operation Condor, Soviet defectors, German social democrats in the Nazi era, German Nazis to South America and Spain following WWII, etc. These processes can be considered a kind of political exile, as often their national governments are satisfied that these \"subversives\" are out of the country and are content to ignore them.  But not always - see, for example, the Soviet KGB's assassination of Ukrainian nationalists^2 [Lev Rebet and Stepan Bandera](_URL_4_) in the 1950s, Bulgarian intelligence's [murder of Georgy Markov \n](_URL_1_) in London in 1978, and the assasination by carbomb in Washington DC of [Chilean dissident Orlando Letelier in 1976.](_URL_3_) \n\n\n- 1: Such as obtaining nationality by fraud or if the person is entitled to, but has not claimed, another nationality. \n Incidentally, the United States is one of the few countries that does allow its citizens to renounce their nationality without first having another one \"lined up\", so to speak.  Occasionally US citizens domiciled abroad who want to avoid tax renounce their citizenship, which [tends to be](_URL_5_) [a bad move](_URL_0_).\n- 2: And ethnic-cleansing terrorists depending on perspective.  A difficult question for another topic!",
        "This paper is clearly written, but the method isn't demonstrated to solve any problem much better than simpler approaches. To quote one reviewer, \"the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.\"",
        "This is a difficult question to answer. First a minor point of clarification: the term \"troll\" can mean many things depending on the location in Scandinavia. I think it is safe to assume that you are referring to the array of supernatural beings that figure in popular fantasy literature and films and that the trolls you are asking about are the big ogre-like monsters.\n\nOne of the reasons why this is a difficult question to handle is that cultures nearly universally emerge from prehistory with beliefs and stories dealing with a full array of supernatural beings. Finding a point or origin in a prehistoric past is nearly impossible and can only be a matter of speculation.\n\nThe focus of what is certainly the subject of your enquiry is the pre-industrial inheritance of Northern Europe. Medieval Norse literature frequently mentions these sorts of supernatural beings. It is not clear if these entities are identical to what nineteenth-century folklore collectors encountered, but there is enough continuity to suggest that they are either the same or there was cultural drift that at least links the supernatural beings expressed in literature and oral tradition in the two periods. Some of the difference may also be the result of different forms of expression: medieval literate writers as opposed to illiterate nineteenth-century storytellers.\n\nOne the peculiar things about a certain type of supernatural beings is that regardless of the term used, they seem to hang together with a great deal of continuity. These are the elves (but we can also refer to them as fairies, sidhe, huldrefolk, or in parts of Scandinavia they are also called trolls). These entities are distinct in the way they are believed to live in communities with societies that roughly reflect the human condition. Outside of Britain, Ireland, Brittany, and Scandinavia (with some bleeding over into continental Europe and Russia), this idea is virtually non-existent. Supernatural beings playing the same part in similar legends are usually singular, or appear in pairs or threes - but they act singularly. It is reasonable to ask how this concept existed across linguistic lines in a well-defined geographic zone. If the idea diffused, why did it not diffuse elsewhere? Most folklorists would shun the explanation of diffusion because this perception of elves seems to be such a core perception of the supernatural world that it would be difficult for it diffuse: when it comes to folklore, stories travel easily, but they usually adapt to local belief systems rather than carry foreign belief systems with them.\n\nSo by way of not answering your question, we can describe this peculiar aspect of oral tradition related to elves, but we cannot say with authority were it originated. The question of origin for all the supernatural beings at the heart of your question is in a remote past that is difficult to describe or understand.\n\nFor sources: on trolls, a great piece of work is Elisabeth Hartmann, Die Trollvorstellungen in den Sagen und Märchen der Skandinavischen Völker (Tübingen: Eberhard Karls Universität Tübingen, 1936). In general, one of the most respected authorities on the question of Celtic and Scandinavian folklore is the late Bo Almqvist; see his Viking Ale: Studies on Folklore Contacts between the Northern and the Western Worlds (Aberystwyth: Boethius, 1991) edited by Éilís Ní Dhuibhne and Séamas Ó Catháin.",
        "  Following deep astrometric and photometric study of the cluster NGC2682\n(M67), we are able to accurately determine its fundamental parameters. Being an\nold and rich cluster, M67 is a relevant object for the analysis of the Galactic\ndisk evolution. M67 is well studied but the lack of a wide and deep Stromgren\nphotometric study makes our results worthwhile. The brightest stars of the open\ncluster M67 were used as uvby-Hbeta standard stars in our studies of NGC1817\nand NGC2548, and the extension of the field covered, as well as the amount of\nobservations, allowed to obtain the best set of Stromgren data ever published\nfor this cluster. We discuss the results of our CCD uvby-Hbeta\nintermediate-band photometry, covering an area of about 50'x50' down to V 19.\nMoreover, a complete membership segregation based on astrometric and\nphotometric criteria is obtained. The photometric analysis of a selected sample\nof stars yields a reddening value of E(b-y)= 0.03\\pm0.03, a distance modulus of\nV_0-M_V= 9.7\\pm0.2 and [Fe/H]= 0.01\\pm0.14. Through isochrone fitting we found\nan age of log t= 9.6\\pm0.1 (4.2\\pm0.2 Gyr). A clump of approximately 60 stars\naround V= 16, (b-y)= 0.4 could be interpreted as a population of\npre-cataclysmic variable stars (if members), or as a stream of field G-type\nstars placed at twice the distance of the cluster (if non-members).\n",
        "Electrons have a fundamental property called the quantum mechanical spin. This spin can be understood and described as an intrinsic angular momentum.\n\nThe spin creates a magnetic dipole moment with a certain magnitude. In non-interacting electrons, these dipole moments are randomly oriented such that in average all magnetic moments cancel each other and the net magnetization is vanishing. If the electrons are brought inside an external magnetic field, the spins partially align such that a rather small net dipole moment is created which is aligned in the same direction as the external field. This is called paramagnetism. As soon as the external magnetic field is removed, the electrons lose their alignment and the overall magnetization is zero again.\n\nIf the distance between the electrons is reduced they start to interact with each other. Either through their direct magnetic interaction between the dipoles (dipole-dipole interaction) or through a quantum mechanical effect called exchange interaction. This causes the electrons to align with respect to their direct neighbor, either in a parallel or anti-parallel configuration. In the former case (ferromagnetism) the individual magnetic moments add up and a large net magnetization is maintained, even in the absence of an external magnetic field. In the anti-parallel case, it is called antiferromagnetism and the net magetization is cancelled even in the presence of an external magnetic field.\n\nIn ferromagnets, the spins do align only within certain volumes, called the magnetic domains. Between these domains, these large net magnetizations may again be randomly oriented such that the overall magnetization of a piece of ferromagnetic metal is zero. If such a material is brought inside a sufficiently strong magnetic field, the domains rearrange such that all their magnetizations add up. The domains' orientations may be effectively \"locked-in\" so that when the external field is removed, the material maintains a significant amount of net magnetization and a magnet is obtained. This is called persistence.",
        " Maltese do like company and can bark or suffer anxiety related problems if they are left alone for long periods of time. Think about whether you can structure your day so that the Maltese is not left alone for more than 4 hours at a time.\n\n\nBe aware that because Maltese like to be with people, they can easily become over-dependant on human company and suffer from stress or \"separation anxiety\" unless trained from a young age to cope in solitude. To train the dog to do this means deliberately spending small periods of time away from the dog so that it learns to be alone. Be careful not to over pamper the dog as this can make it even more difficult for it when you're not there to fuss it.Most Maltese are friendly, outgoing, and relatively fearless but because they are small (4-7 lbs) and have delicate bones, they are easily injured. As a result, they can become fearful or aggressive. Keep an eye on them around larger animals, and show children how to hold the Maltese properly without squeezing or dropping him. If you have very young children, you might want to consider a sturdier breed like a Shih Tzu, Havanese, or Bichon. ;\n, Be aware that the Maltese feel the cold easily. If your home is chilly or you live in a cold environment, then reconsider your choice of dog. Also be aware they have long silky coats, which needs daily grooming and will need to have their hair trimmed regularly because Maltese do not shed like most other breeds. Although this is an added expense (unless you learn to do it yourself), the upside is that the Maltese (like the Poodle) is a relatively hypoallergenic breed., If you can't commit to such a long term undertaking, or you expect your circumstances to change, then reconsider getting a dog , especially one that can live to be 14 or 15. Adopting a dog is a long-term commitment, not something to take lightly.\n\n, There are thousands of Maltese breeders out there, but there are also plenty of purebreds in animal shelters. If you know someone with a Maltese ask where that person got it. Make sure that before you bring your Maltese home, that it gets along with other pets or children you may have.\n\n\nMake sure you see the Maltese puppy with his mother. If for whatever reason the mother is not present, walk away. This is a give away sign that the puppy is from a puppy mill and such immoral breeding programs should not be encouraged.Make sure your Maltese was socialized well as a puppy, as this helps make it become a confident outgoing adult.\n\n, If you don't want to deal with the upkeep a shorter \"puppy cut\" looks cute on a Maltese. You will need to have him professionally groomed maybe once every six weeks but will still need to brush his hair daily using a nice, soft brush. Remove excess hair and untangle knots while the hair is dry or with a special detangler. Bathing him first will make the tangles worse.\n\n\nYou may need to use a comb to get to the root of any tangles and knots. Remember to gently tease them apart. Take extreme care cutting out knots, as it is all too easy to nick the skin., You may need to bathe a playful puppy once a week, while a calm, older dog will only need a monthly bath.Wash its hair like it's your own, only be very gentle! Don't forget to clean your pup's face with a washcloth.\n\n, Maltese dogs have white hair that is prone to tear staining. You will recognize this as the brown streaks down the face of a white dog. This happens when tears overspill onto the cheek and stay there for a few hours. Substances in the tear fluid oxidizes and change to that rust-colored pigment, which causes the staining.\n\n\nTo avoid this, dry the area around your dog's eyes regularly. The amount of tear overflow varies with each individual dog and is more pronounced when they are teething. If the lower lid area or cheek are constantly wet, then wipe hourly. For dogs with less moisture from the eyes, you only need to wipe twice a day or when you spot gloop in the corner of the eye.\nThere are various commercial products available for wiping eyes, but previously boiled water on cotton wool is perfectly sufficient. Dampen the cotton wool with the cool water and wipe away any moisture or stickiness from the eye area. Then finish with a dry piece of cotton wool to pat the area dry.\n\n, Many people don't do this, but it is very, very important, especially for older dogs. This will slow up tartar formation and protect your dog's dental health. Start at a young age, so the dog accepts having its teeth brushed. Use a soft-bristled toothbrush and toothpaste made for dogs. It can be purchased at any pet store.\n\n\nBe aware the Maltese are especially prone to the formation of plaque and then tartar formation. This irritates the gums leading to inflammation and infection. Ultimately this causes gum recession and the teeth to loosen, become infected, and fall out.\nIf this is your first time, rub the gums and teeth with your finger. Adding some soft dog food can make this more pleasant. Brush your dog's teeth before bed.\nAlternatively speak to your vet about small-sized kibble that has a cleaning action as the dog chews. Be aware that canned food is more likely to build tartar than dry food, and speed up tartar formation.\n\n, Maltese nails should be clipped monthly so they do not overgrow and get infected. Using a regular nail clipper, slowly begin to clip the end of your dog's nails, stopping before you reach the live center of the nail. If you are uncertain how much nail to remove, invest in a lesson from the groomer or vet tech.\n\n\nThe nail has a living center made up of blood vessels and nerves, called the quick. If you clip too much nail away you will cut the quick, which is painful for the dog and causes bleeding. If in doubt, err on the side of caution and remove the minimum amount possible.\nThe dog may be scared and will struggle. If this is the case, try feeding the dog immediately after the nail trim so that it sees its food as a reward for good behavior.\nMany Maltese owners trim their dog's nails when the dog is asleep. However, you need to be very careful or it could really startle the dog awake.\n\n, This can be done professionally, but can also be done at home. Go into the outer ears with a Q-tip, but do not stick it into the inner ear. Just use it to clean the visible part of the ear. If there is wax of any sort, be sure to clean it out.\n\n\nNever put water into the dog's ears, as this softens the skin and makes it more likely to become infected. Instead, use an ear cleaner sold for the purpose. These are designed to dissolve wax and also for an excess to evaporate away, so the ear canal is not left over wet.\n\n, The Maltese has a small stomach, so choose a premium food that is high quality. Take your vet's recommendation and also look for a food that has a named meat listed as the main ingredient.\n\n, Feed him a high quality dog food and treats recommended by your vet. Always have fresh water and dry kibble available for your Maltese (unless the animal is an adult and is overweight). Maltese puppies can suffer from hypoglycemia, which can be life threatening, if food is not readily available.\n\n, It may be that you are giving the dog too much food at meal time or that you are giving too many treats. Consult with your vet if your Maltese is putting on too much weight.He or she may have a suggestions for helping the dog lose weight.\n\n, A Maltese, like all dogs, needs to be well socialized in order to become a well-rounded dog. While it is important to bond with your pup, its also important that the dog is open to meeting new people and animals. Expose your puppy to a wide variety of people and animals when it is young, so that it understands that this is a normal and enjoyable part of life.Maltese dogs that are not socialized can become either fearful or aggressive toward strangers.\n\n, Although Maltese are good apartment dwellers and don't require a great deal of exercise, it is important for the dog's brain, socialization, sensory system, and enjoyment to get out. Even a fifteen to twenty minute walk can be a nice break for you and your dog.Maltese are adaptable when it comes to exercise. They enjoy walks but are also happy to burn energy in a restricted space, by playing games of fetch. Teach your Maltese not to jump off your bed or couch, though, as this can cause injury. The breed is vulnerable to knee injuries (luxating patellas).\nBe aware that a bored Maltese may turn to barking to amuse itself.\n\n, The Maltese is an intelligent dog and loves to be stimulated mentally. Like all dogs, your Maltese needs to be trained to sit, stay, and come when called, in addition to being potty trained. Use positive reinforcement when training, as opposed to punishment when the dog does something wrong. These dogs respond much better to treats and love as incentive to do the right thing.\n\n\nDon't be fooled by their small size, they love the mental challenge of training. Indeed, some Maltese excel at agility training, albeit on a small scale, so don't write this off as an activity for your dog.\n\n, Maltese dogs are beautiful animals that deserve to be treated with love and care. Snuggle with your pup often and show it that you care for it.\n\n, Your veterinarian will be able to advise you as to which diseases are prevalent in the area, and which it is necessary to vaccinate against. Some states also require all pets to be vaccinated against rabies, in which case you must comply.\n\n\nTake your veterinarian's advice on preventative healthcare such as heartworm, and regular worming treatments and use them regularly., If your dog is a pet and not intended for breeding, then speak to your vet about desexing. This helps reduce antisocial behaviors, such as territory marking in the male. It also reduces the risk of certain diseases in the female, such as mammary cancer (if done before the second season) and pyometra (womb infection).\n\n, This is a permanent way of identifying the dog as yours. Should it escape or be stolen, the ID chip allows you to prove ownership, and will help someone who finds your dog locate you (vets can read the chip and find you through the registry).\n\n",
        " Founding a successful scholarship depends upon identifying the reason for awarding it, as well as the target student audience it can potentially help. Normally, your scholarship will carry a name that characterizes its purpose, such as “The Jane Doe Memorial Scholarship,” “The Michigan Rural Community Scholarship,” or “The Leaders of Tomorrow Scholarship.” Common reasons for founding scholarships include:\n\n\nMemorializing a loved one by helping students with a scholarship in that person’s name\nProviding financial aid to students studying in a particular field, such as medicine, writing, or social work\nProviding financial aid to students who attend a particular school or type of school\nHelping students who have financial or personal hardships to overcome\nAwarding students who excel in a sport, activity, or academic field\nHonoring students who have completed an essay on a topic of interest\nHonoring students who have made a significant impact in their community\nHonoring students based on personal characteristics such as ethnic or cultural background, gender, or geographic origin;\n, There is a lot of planning that goes into starting an educational scholarship, and you will need to think ahead.Typically, if you want to award a scholarship by a summer prior to a given academic year, you should begin planning for it the previous summer. Steps to factor into your timeline include:\n\n\nProgram development\nFundraising\nAdvertising\nEvaluating applications\nAwarding the scholarship\n\n, In addition, scholarships may be awarded only during one year, or they may become an ongoing program with application cycles each year, or on some other set cycle. The duration of your scholarship will depend upon your financial abilities and intentions.\n\n, You may decide to award a certain amount, or leave the amount open. The latter method allows you flexibility in determining the number of students to award each year.\n\n, You may decide to fund it entirely on your own, or with the aid of corporate, community, or educationally-based donors.Depending on your scholarship terms, donors may be able to receive tax benefits for making contributions to your scholarship. Contact an accountant or talk to a school’s financial aid office for details.\n\n, If you or someone directly involved with the project cannot finance the scholarship alone, you will need to do some fundraising to get the project off the ground. If you are working with a school to develop the scholarship, it may already have a network of donors to contact. If you will be raising funds on your own, start by making a list of potential donors, such as businesses and community organizations. Draft a flyer, letter, email, or website to let these potential donors know about the purpose of your scholarship. Your promotional materials can also request a donation or let potential donors know about fundraisers you can host, such as:Auctions, where items such as antiques or artwork are sold to raise funds\nOffering perks (such as a free dinner at a restaurant) to those who donate\nRaffles for items or perks (each participant pays a fee to enter the raffle; one or more entries is chosen as the winner of a prize)\nBake sales\nA game night (bowling, board games, etc.) where participants pay a donation to enter\nA crowdfunding campaign, where people from anywhere in the world can donate online to your scholarship fund\n\n, Scholarships can be based on a one-time gift. This award can be divided and distributed across multiple years, if desired, until the funding is depleted. Scholarships may also be based on an endowed fund. In the case of an endowed scholarship fund:\n\n\nA gift of a fairly large amount is set aside as a principal balance and invested.Scholarships are then awarded by drawing from the interest earned on the principal, and any remaining interest is reinvested to grow the principal and the balance that can be awarded for future scholarships. As a reference point, a scholarship awarding $1000 per year may need an endowment fund of $20,000-$25,000.If you choose to create an endowed scholarship, you should talk to a trusted investment broker about creating a sustainable plan for the scholarship.\nYou will also want to talk to a tax advisor, since there are usually restrictions on how awards can be administered.You may also wish to select an advisory board to help with investing and other financial planning required by an endowed scholarship.\n\n, When you advertise a scholarship, you may have many applicants, and you will want to choose criteria that will help you select the most deserving applicant(s) to receive the award. The criteria used for selection will vary based on the purpose of your scholarship, but common ones include:Financial need\nAcademic achievement, shown by grade point average, test scores, etc.\nExtent of community involvement\nLeadership skills\nWork history\nWriting skills, demonstrated by an application essay or otherwise\nAchievements in particular fields (sports, debate, performing arts, etc.)\n\n, You may be able to do all of the work yourself, but selecting a scholarship administrator to assist is very helpful. Your options include:\n\n\nCreating a board of community members.\nLetting a particular school administer the scholarship, with or without your input.If you choose this option, contact a school’s financial aid office or gifts office to create a plan for administering the scholarship.\nUsing a third-party scholarship administration service. These professional services specialize in assisting with scholarships, and can streamline the process. However, most will charge a fee (even if they are non-profit), ultimately cutting into the amount you can potentially award to students.A school’s financial aid office may have information about finding a scholarship administration service, if you would like to choose one.\n\n, You will need to consider:\n\n\nWhere and when will applications be made available?\nWhere should the applications be sent?\nWhen will applications be due?\nWhat will the components of an application be?\nWho will select the recipient(s), and how?\nHow and when will the recipient(s) be notified?\nHow and when will the scholarship funds be distributed?\n\n, You can ask high schools and/or colleges to advertise the scholarship (via email, flyers, financial aid workshops, etc.) by contacting their office of financial aid or scholarship coordinator. You may also advertise the scholarship opportunity via community groups, student workplaces and activity centers, etc.\n\n, Usually, however, a scholarship application has many components. However you design your application, you want to make sure that it will solicit the information you need to select the recipient(s) based on your criteria. Common components of scholarship applications may include:\n\n\nAn essay on a topic relevant to the scholarship purpose\nA list of academic and/or non-academic awards and achievements\nA description of extra-curricular involvement\nAn explanation of financial need\nCopies of high school and/or college transcripts\nLetters of recommendation from teachers, advisors, employers, etc.\n\n, Once you have designed the application, advertised it, and received submissions from applicants, you can begin to select its recipient(s). A committee working together can read applications, determine the best candidate(s), and then vote for approval. You should then notify the recipient(s) and award the scholarship funds according to the timeline you scheduled.\n\n, If your scholarship is a continuing program, it is essential to periodically review it for financial health, and to ensure that it is serving its purpose and help students. An annual report from your scholarship administrator can help with this. Reviewing the scholarship will help you determine if there are any issues to take care of regarding the scholarship fund’s finances (such as raising or lowering the award to continue it) or its purpose (such as changing the application to improve the selection process).\n\n",
        " The ribbons should measure 1/2 inch (1.25 cm) wide. Get 6 yards (5.5 meters) of each ribbon.\n\n, Leave 6 inches (15 cm) loose for finishing touches on the lei.\n\n,, Use your left hand and the bottom ribbon to make a loop 2 inches (5 cm) long. The loose ribbon tail will be under the knot.\n\n,, Pull the upper ribbon tightly behind the first loop and very near the knot.\n\n,, Move the left ribbon toward you between the left thumb on top and the index finger. Loop the ribbon beneath itself and grip with your index finger above and your thumb below with the loose ribbon tail dangling.\n\n, Always keep the loose end on top when moving loops through other loops.\n\n,, This will secure the second loop. If needed, pull on the lower part of the bigger loop to draw the ribbons tight.\n\n,, Shove the third loop through the second loop.\n\n,,, Switch your hands after making every loop to pull the ribbon tight. Alternate the ribbons with every loop.\n\n,, There should be 2 loose ribbon tails.\n\n, Tie these together twice to make a bow.\n\n,,",
        " Pheochromocytoma refers to a medical disorder that affects neuroendocrine cells at various sites in the body and which are called chromaffin cells. This type of cells is spread among the body tissues at various sites. This tumour of chromaffin cells is characterized by its excessive secretion of different types of compounds into the circulation.;\n, The most important of which are the catecholamines and, in particular, epinephrine and norepinephrine. These two compounds are important due to their clinical effect on inducing hypertension in affected individuals. These two compounds are usually secreted by the sympathetic division of the nervous system.\n\n, They mediate a stress situation such as increased heart beat rate and increased perspiration, in addition to increased wakefulness. In addition these two compounds increase the constriction of blood vessels including arteries and veins. Thus it induces a state of increased blood pressure and hypertension.\n\n, In the neuron synaptic cleft they function as neurotransmitters which transmit electric signals between neurons. In the adrenal medulla, they are secreted to the circulation as hormones.\n\n, The most important chromaffin cells are present in the medulla of the adrenal gland. As a result of a tumor to this part of the gland it oversecretes autonomously increased amount of several compounds of which epinephrine and norepinephrine are the most important. As a result a state of hypertension occurs and which does not respond to medications.\n\n, Accompanying symptoms include increased activity of the sympathetic nervous system which includes increased perspiration and heart beat rate. Other stressful symptoms can also occur such as dilatation of the eye pupils.\n\n, Its excessive secretion can lead to distinct symptoms that are typical for dopamine such as psychotic symptoms in addition to increased sexual desire.\n\n,, The clinical picture of this syndrome usually depends on the type of compounds that are secreted by the affected chromaffin cells. Catecholamines which are usually secreted in this type of tumors are derivatives of amino acids and which are by themselves clinically important.\n\n, Its excess as occurs in certain types of pheochromocytomas can predispose to psychosis. In addition its deficiency can cause Parkinsonism as occurs sometimes in people taking dopamine antagonists. An example is the drug risperidone.\n\n, Chromaffin cells associated with pheochromocytoma usually secrete these two compounds in an endocrine fusion exerting their effect on blood vessels as hormones.\n\n,",
        " The more ridiculous, silly, and strange the image, the better!\n\n\nFor example, if you are trying to remember the date 1732, the year in which George Washington was born, every time you think of that date, imagine a small boy wearing a Washington-type wig and chopping down a cherry tree while saying “I cannot tell a lie!”\nSimilarly, you could also imagine a person dressed as George Washington “making it rain” with $1,732 dollars in one-dollar bills (which feature a portrait of Washington on the front).;\n, Pacing while you study, creating hand motions to learn along with certain dates, and even singing out dates can all improve your memory. For instance, you could:\n\n\nRaise your arm up dramatically like a Roman orator when trying to remember the date 44 BC, the year in which the Roman Emperor Julius Caesar was assassinated.\nMemorize dates by singing them to the melody of your favorite song.\n\n, This is because it is very difficult to memorize a set of unconnected bits of information. As you spend time each day practicing the dates you need to remember, look for ways to classify and organize them. For instance:\n\n\nIf you are learning a set of historical dates, make a timeline as you seek to memorize them. This helps to put the dates in some kind of relation to each other, and to show logical connections between them. The more you can contextualize the dates, the more meaning they will have to you; the more meaning they have, the greater your chances of remembering them.\nIf you are learning birthdates of your family members, map them out on a family tree that you draw out. As you practice memorizing the dates each day, you can visualize “climbing” the family tree to recall all of the dates.\n\n, For example, you could memorize the date “1066” (the year of the Battle of Hastings), the date “1215” (when the Magna Carta was drafted), or the date “1776” (the year of the signing of the Declaration of Independence), by associating them with the alphabetic strings “TZGG,” “TNTL,” and “TKKG,” respectively, according to the following scheme:\n\n\n0 = Z, because the word “zero” begins with “z”\n1 = T, because the numeral “1” and the letter “T” are both written with a single downstroke\n2 = N, because if you rotate the letter “N” clockwise ninety degrees, it resembles the numeral “2”)\n3 = M, because if you rotate the letter “M” clockwise ninety degrees, it resembles the numeral “3”)\n4 = R, because the numeral “4” looks like a backwards letter “R” (and the word “four” also ends in the letter “R”)\n5 = L, since “L” is the Roman numeral for “50”\n6 = G, since the numeral “6” and the letter “G” resemble each other\n7 = K, because if you rotate the letter “K” clockwise, it resembles the numeral “7” mirroring itself\n8 = B, since the numeral “8” and the letter “B” resemble each other\n9 = P, since the letter “P” looks like a mirror image of the numeral “9”\n\n, One technique is to use the strings of letters you form using the previous step to develop fun and memorable sentences. For example, imagine:\n\n\nYou are trying to memorize the date “1776,” the year the Declaration of Independence was signed.\nYou form the string of letters “TKKG” using the previous step.\nYou develop the phrase “That Kooky King George,” where the first letter of each word in the phrase corresponds to the string of letters “TKKG”\nThis phrase will help you remember the date because it uses the string of letters associated with “1776,” and because the Declaration of Independence was written to declare the independence of the American colonies from Great Britain, which was ruled at the time by King George III.\n\n, Concentrating in this way can help you remember 20% to 60% more information.There are various practical ways to concentrate while studying. For instance:\n\n\nReduce distractions; try to study in a quiet, stress-free environment.\nIntentionally focus your eyes on a written date that you are trying to memorize. \"Trace\" the date with your eyes.\nWhen you come across a date you need to learn, take a moment and carefully write it out, thinking “I need to remember this” as you do.\nVisualize yourself writing the number any time you think of it. For instance, imagine yourself writing the date on a chalkboard in your mind.\n\n, Since we lose the greatest amount of information within 24 hours of learning something, it is important not just to repeat the information immediately, but frequently. If you keep practicing and going over dates every day, you increase your recall and memory; if you keep trying to memorize them for thirty days, you are likely to remember them for years to come.If you need to learn a set of dates for an exam or other purpose, go over them as frequently as you can, spending at least a few minutes each day reviewing the information.\n\n, If used correctly, they can be a big help.\n\n\nUsing a set of cards (or an electronic flash card program), write each date you need to memorize on one side of a card, and the significance of that date on the other side.\nQuiz yourself by shuffling the cards, looking at the date on each one, and seeing if you can remember the significance of it. You can also reverse the cards and look at the “significance” side, and see if you can recall the date associated with it.\nAs you go through the cards each session, take out the ones that you remember well, and repeat the ones you cannot, until you can remember all of the dates.\nPractice using the flashcards frequently, but work in small bursts of just a few minutes at a time. If you try to memorize too much at one time, the information won’t really stick.\n\n, The more you use the dates you need to learn, the more likely you are to remember them. Talk about the dates you are learning with family and friends, think about them to yourself, and write about them whenever you can. Keep doing this, and you’ll memorize them all!\n\n",
        "Jaghatu is a district in Maidan Wardak province, Afghanistan, 20 km northwest of Ghazni. According to 2019 data, the population is 50,792.\nThe district is within the heartland of the Wardak tribe of Pashtuns. Agriculture is the main source of income. The popular apples in Afghanistan is from this district. Drought has become a serious problem in the whole province. Health and education services are lacking although most of the people in this area have attempted to create schools and for students to attend with their own efforts.\n\nHistory\nJaghatu district has some historical places such as Barghalee, which was the capital of the  empire before Islam. Sultan Dam is located in Jaghatu district although only two villages in the district benefit from the water,  the rest of the water goes towards the Khaja Omary district and Ghazni province.\n\nNotes\n\nDistricts of Maidan Wardak Province",
        "  We present a search for outflows towards 51 submillimetre cores in Perseus.\nWith consistently derived outflow properties from a large homogeneous dataset\nwithin one molecular cloud we can investigate further the mass dependence and\ntime evolution of protostellar mass loss. Of the 51 cores, 37 show broad\nlinewings indicative of molecular outflows. In 13 cases, the linewings could be\ndue to confusion with neighbouring flows but 9 of those sources also have\nnear-infrared detections confirming their protostellar nature. The total\nfraction of protostars in our sample is 65%. All but four outflow detections\nare confirmed as protostellar by Spitzer IR detections and only one Spitzer\nsource has no outflow, showing that outflow maps at this sensitivity are\nequally good at identifying protostars as Spitzer. Outflow momentum flux\ncorrelates both with source luminosity and with core mass but there is\nconsiderable scatter even within this one cloud despite the homogeneous\ndataset. We fail to confirm the result of Bontemps et al. (1996) that Class I\nsources show lower momentum fluxes on average than Class 0 sources, with a KS\ntest showing a significant probability that the momentum fluxes for both Class\n0s and Class Is are drawn from the same distribution. We find that outflow\npower may not show a simple decline between the Class 0 to Class I stages. Our\nsample includes low momentum flux, low-luminosity Class 0 sources, possibly at\na very early evolutionary stage. If the only mass loss from the core were due\nto outflows, cores would last for 10^5-10^8 years, longer than current\nestimates of 1.5-4 x 10^5 years for the mean lifetime for the embedded phase.\nAdditional mechanisms for removing mass from protostellar cores may be\nnecessary.\n",
        " It is best when deciding what branch of philosophy you want to study, by picking one that deals with topics either relevant to your current subjects. By that I mean, if you are a musician or artist it is better to study Aesthetics, as this deals with the value of art and 'does it have any meaning?;\n, Like the previous step choose one relevant to you, especially if this is your first attempt at forming a philosophy from scratch. It can be very difficult to get into and concentrate on something unfamiliar or uninteresting, this might put you off philosophy or lower your confidence if you decide to give up on a topic you don't like. So best to start with something you like.\n\n, By writing them down it'll be easier to keep track of what you think, it will also make it easier for you to spot flaws in you thinking and why you think those things, however you must look at these notes as if they are someone else's, to make yourself truly impartial.\n\n, Make sure you do research of both sides as you'll have a biased opinion from the start and as you try to prove one side or the other you won't have the full facts, probably ending up with an ill defended conclusion. People will immediately see the flaws and tear your philosophy apart.\n\n, A mind map will help you. As stated earlier this will help you spot flaws in what you think and how to remedy these flaws to form or attempt to form your flawless philosophy or truth, which is the philosophers end goal.\n\n,, It is best for you to do some research by asking people's opinions as well, for the reason that they are usually biased. You can use their opinions to form part of your philosophy, in the sense that you show how there ideas are wrong and yours are right. This could be done like this: If there are only three possible answers, X is wrong because ..., Y is wrong because ..., Therefore Z is right because ... X says... Y says... they are wrong... However if it is done like this... then Z is right. Roughly like that, hopefully yours will be Z.\n\n, Can we know? Rip it to pieces and ask why? You can't tell what an atom is made of unless you break it apart, you shouldn't stop until you break the philosophy down into it's constituent parts. A's philosophy is X, X is formed from Y+Z, so A's philosophy is Y+Z. However Y is formed from Q+U but Z is a truth or the base, therefore A's philosophy is (Q+U)+Z or X.\n\n, Making assertions like the previous statement will help you build your argument, by proving it or:\n\n, Counter arguing will help breakdown the philosophy or narrow down the possible answers there could be.\n\n, If you find some truth like Z which is an axiom or self evident truth requiring no proof you're actually getting somewhere, but if you've broken down the philosophy of X by A into: (Q+U)+Z or X, Or X to be true all of it's constituent parts need to be true, like a healthy person must be by definition made up of healthy parts or else he would be a lame man. The rest of his body may be healthy, but if his one lung doesn't work he is not a completely healthy person. Therefore if Q,U and Z are right then X is right however if Z and Q are right but U is false then X must be false; following our lame, healthy man allegory.\n\n, finding other truths based on your first truth. If Q,U and Z are right and X is right then you've wasted a lot of time proving something someone else has already proved however if you prove Q and Z but not U, meaning X is untrue, start looking at what could produce a truth using Q and Z but not U, unless you are looking at the opposite of U to find a truth that is.\n\n,, Remember, though, that philosophy is a continual process of old philosophies either dying and been thrown out or rejuvenating and evolving. Greek thoughts about the universe, like Thales belief that the universe was made out of water has been proved wrong by new technology, try to ensure your philosophy can evolve like Socrates or Kant. It is easier to have a more flexible system in ethics and political theory than in epistemology and metaphysics which compete with medicine and psychology which are sciences not arts like philosophy; they instead based mostly on empiricism.\n\n",
        "All reviewers unanimously praised the novelty and quality of the paper. Minor revisions, following the reviewers' suggestions, will make the paper even better. ",
        "The Anhumas River is a river of São Paulo state in southeastern Brazil.\n\nSee also\nList of rivers of São Paulo\n\nReferences\n\nRivers of São Paulo (state)",
        "Joseph Slogan (born 15 February 1931 at Windsor, Ontario) was a Progressive Conservative party member of the House of Commons of Canada. He was a dentist by career.\n\nHe was first elected at Manitoba's Springfield riding in the 1958 general election after an unsuccessful attempt to win the riding in 1953. Slogan was re-elected there in 1962 and 1963, then defeated in the 1965 election by Edward Schreyer of the New Democratic Party.\n\nExternal links\n \n\n1931 births\nCanadian dentists\nLiving people\nMembers of the House of Commons of Canada from Manitoba\nPoliticians from Windsor, Ontario\nProgressive Conservative Party of Canada MPs",
        "While I'm not a metabolic or GI expert, perhaps I can shed some light.\n\nThe several segments of the GI system are specialized in specific things depending on which segment of the GI you're looking at. Example: stomach, mechanical, acidic and enzymatic digestion of large segments of food.\n\nThe segment you have to really be concerned with are teh jejunum through the small instesti. These segments are specialized of water and nutrient absorption and I cannot emphasize the TREMENDOUS amount of surface area in these tissues that specialize in nutrient absorption. These tissuse are compressed, have vili on the folds that are compressed and then have microvili on the vili which are on those folds. All this contributes to surface area which absorption/diffusion can take place. ([Jejunum](_URL_1_))([Sm Intesti EM](_URL_0_))\n\nThat paired with the immense blood supply to the gut make this system extremely efficient. However, I feel it is not impossible to overload the system. First, absorption of each type of nutrient is a process independent of others (Fat and Carb uptake are mediated through different cellular processes). \n\nThere can be individual differences with the efficiency of each of these with the most variance in fat absorption. It is possible to overload this system by simply loading too many nutrients into the system for DIGESTION to efficiently take place. This can be because of too much material or difficult to digest material. If anyone has ever eaten too many protein shakes in one sitting knows the result: Loose Stool. This is primarily because there is too much nutrient content in the lumen of the large intestine as the content passes through the Lg Intestine is not able to absorb water and dehydrate the stool. \n\nAs far as exact calorie count at which this happens, it's not that simple. Calories come into the diet in different forms and this mix along with individual differences is what dictates the amount absorbed. If you eat 1500 calories from a carb heavy source like potato chips I doubt your GI would have trouble dealing with it. However, 1500 calories of bacon or lard may not do so well and you'll spend the next 2 hours on the toilet. \n\nSources: Alberts Mol Bio of the Cell 5th\n\nEdit, put wrong source initially, whoops.",
        "  We will consider a SUSY-SU(5) with one right-handed neutrino with a large top\nlike Yukawa coupling. Assuming universal soft masses at high scale we compute\nthe low-energy spectrum and subsequently the neutralino LSP relic density\ntaking also into consideration SU(5) as well as the see-saw running effects\nabove the gauge coupling unification scale. We found that there exists no\nviable region in parameter space for $\\tan\\beta \\ler ~35$. The $\\tilde{\\tau}$\ncoannihilation process starts becoming efficient for $\\tan\\beta \\ger 35-40$.\nHowever, this process is significantly constrained by the limited range in\nwhich the stau is lighter than the neutralino. In fact, for a given $\\tan\\beta$\nwe find that there exists an upper bound on the lightest neutralino mass\n($M_{\\chi_1^0}$) in this region. The A-pole funnel region appears at very large\n$\\tan\\beta \\simeq 45-50$, while the focus-point region does not make an\nappearance till large ($m_0,M_{1/2}$), namely a few TeV. Large $A_0$ terms at\nhigh scale can lead to extended regions consistent with WMAP constraints and\nremove the upper bounds in the stau coannihilation regions.\n",
        "Philabundance is a non-profit food bank that serves the Philadelphia and Delaware Valley region of Pennsylvania, United States. It is the largest such organization in the region.\n\nHistory\nThe organization was founded in 1984 by Pam Lawler. In the year 2005, it merged with The Greater Philadelphia Food Bank, and the new entity operates under the name Philabundance.\n\nGoals\nThe organization aims to drive hunger from our communities today and end hunger forever.\n\nOrganization\nPhilabundance serves 90,000 people each week, 30% of whom are children, 15% of whom are seniors, and others served include students, the working poor, veterans and single parents. It has its own programs, as well as a network of 350 agencies, including emergency kitchens, shelters and soup kitchens. As of 2017, 1 in 8 people in the U.S. face hunger while in our area, that number is 1 in 5, making hunger a crisis in the Delaware Valley on which we need all need to collaborate to eradicate.\n\nPhilabundance works with local grocers and farmers to rescue perfectly good food that would otherwise go to waste; in 2016, it rescued 10 million pounds of food for its neighbors in need. The organization also hosts events alongside the Greater Philadelphia Coalition Against Hunger.\n\nTrends\nClients Who Volunteer - Many of Philabundance's 15,000 volunteers are also clients, especially at its Fresh for All program. These volunteers spend 2–3 hours a week distributing food to others, while they themselves need help, and are allowed to take the same amount of food as those attending the program.\n\nHungry Students – Due to rising tuition, and the rules around qualifications for SNAP benefits, many college campuses are seeing a rise in the number of hungry students enrolled. As a result, many schools are opening pantries on campus to serve those in need. Philabundance's partner, the Chester County Food Bank, and other Feeding America food banks have partner with colleges to ensure this population has enough food to survive and thrive in school. Philabundance will soon follow suit.\n\nPrograms\n Fresh For All Program - To help families and individuals in need gain access to fresh vegetables and fruits, Philabundance operates Fresh For All Program at 9 locations in PA and NJ.\n Senior Boxes Program - Gives USDA-sponsored food boxes to approximately 5,000 low-income senior citizens.\n Philabundance Community Kitchen - A culinary, vocational job training program that teaches low-income and unemployed individuals life and job-readiness skills.\n Grocers Against Hunger- a food rescue program that allows participating grocers to donate surplus inventory to Philabundance. The food is later distributed to clients through direct service programs. In 2015, the program saved 10 million pounds of produce that would have otherwise gone to waste.\n KidsBites - The KidsBites initiative provides access to nutritious food in low-income areas for children and their families to help ensure that kids in our service area have the food they need to thrive and grow. Since its inception, KidsBites has provided almost 250,000 pounds of food to children and their families. To meet the needs and size of different communities, Philabundance offers several program models: Mobile pantry; BackPacks; and LunchBoxes.\n\nContributions Received\nA multitude of generous individuals, foundations and corporations support Philabundance. Following are some of the largest annual contributions:\n  Phans Feeding Families: The Philadelphia Phillies and the Citizens Bank, sponsor the annual Phans Feeding Families program, which consists of a night at the ballpark at which there are raffles and fundraisers to benefit Philabundance, as well as food collections at AT&T station and Citizens Bank Park. As part of its support of Phans, Citizen Bank makes an annual donation of $40,000 to support Philabundance' KidsBites initiative.\n SEPTA Stop Hunger at Your Station - The Southeastern Pennsylvania Transportation Authority (SEPTA) supports Philabundance annually, by collecting food at more than 40 SEPTA stations. Stations also collected money, leading to tens of thousands of meals for our neighbors in need.\n 93.3 WMMR's Preston & Steve Camp Out for Hunger – Every year since 1998, Preston & Steve hold a five-day \"Camp Out for Hunger\" event – most recently, at Xfinity Live! This is the largest single-location food drive in the county, raising more than 2.6 million pounds of food and $200,000 in 2019's drive and over 14 million pounds in since its inception. The event was aimed at increasing hunger awareness, as well as collecting food for Philabundance. The event featured popular bands and distinguished guests showing their support for Philabundance. Keeping with tradition, Preston and Steve camp at the spot in a trailer all through the week.\n 6abc's Holiday Food Drive: Each year, 6abc and partners Dunkin Donuts and ACME raise food, funds and awareness of Philabundance from Thanksgiving through the New Year. Elements of the campaign include donations at ACME stores, a 6abc telethon and Dunkin Donuts in-store promotions. \n Toyota Hauls Away Hunger: The Toyota Dealership Association of the Delaware Valley holds a literal food drive each December in which more than 50 Tundras caravan from Philabundance's warehouse in North Philly to Citizen Bank Park parking lot, hauling food to help feed those in need.\n Fraud Street Run: Two Philadelphia runners, Chip and Jeff of the Junk Miles Podcast, organised a charity run inspired by the Four Seasons Total Landscaping press conference. The event had proven more popular than they anticipated, eventually raising nearly .\n\nSee also\n Food salvage\n Food rescue\n Food Waste\n\nReferences\n Notes\n\nExternal links\n Philabundance.org\n\nFood banks in Delaware\nNon-profit organizations based in Philadelphia\nFood banks in Maryland\nFood banks in Pennsylvania",
        "all reviewers agree that the paper is not convincing enough at this stage but needs more work to be ready for ICLR (e.g. missing comparisons to other existing methods).",
        "The Bihari Muslim minority in Bangladesh were subject to persecution during and after the 1971 Bangladesh Liberation War (called the Civil War in Pakistan), experiencing widespread discrimination. Biharis were ethnic Urdu-speakers and largely maintained a pro-Pakistani stance, supported the Pakistan Armed Forces and opposed the independence of Bangladesh and the Bengali Language Movement. Biharis faced reprisals from mukti Bahini and militias and from 500,000 to 550,000 were killed. \nBihari representatives claim a figure of 600,000 Biharis killed.\n\nThe Supreme Court of Bangladesh ruled Biharis eligible for Bangladesh citizenship in 1972, but about 500,000 chose repatriation to Pakistan. Some repatriation was implemented by the Red Cross over a number of years, but in 1978 the Pakistani government stripped Pakistanis remaining in Bangladesh of Pakistani citizenship. Researchers (such as Sumit Sen) maintain that the Pakistani government's denationalisation of the Biharis and reluctance to rehabilitate them in Pakistan are sufficient evidence of persecution to warrant refugee status. The Biharis have also faced institutionalised discrimination linked to their citizenship status, and many live in squalor in refugee camps.\n\nHistory\n\nPartition violence\nBihar (now a state in eastern India) was plagued by communal violence between Muslims and Hindus due to partition, along with the other former territories of British India. More than 30,000 Biharis were killed in October and November 1946, and it is estimated that up to one million migrated to East Pakistan. In the aftermath of the 1946 riot in Bihar, Jinnah said 'I never dreamt that in my lifetime I shall see Pakistan in being, but the tragedy of Bihar has brought it about'. The Muslim League organised the rehabilitation of the Bihari refugees in Sindh. The arrival of Bihari refugees in camps in Sindh and Bengal in 1946 paralleled the later movement of refugees in 1947.\n\nSheikh Mujibur Rahman (then a student leader) toured affected villages in Bihar with his relief team, and was moved to ask Bihari refugees to move to East Bengal in 1947.\n\nMigration from Bihar \nThe 1947 partition of India displaced between 11.6 and 18 million people; millions of Muslims migrated from India to Pakistan while millions of Hindus and Sikhs migrated from Pakistan to India. Adherents of the two-nation theory believe that in addition to Pakistan, Muslims should have an independent homeland in Muslim-majority areas of India; this sparked the mass Muslim migration to the Dominion of Pakistan. According to the 1951 census, 671,000 refugees were in East Bengal; by 1961, the refugee population had reached 850,000. Broad estimates suggest that about 1.5 million Muslims migrated from West Bengal and Bihar to East Bengal in the two decades after partition.\n\nBackground \nOne reason cited for communal violence between Biharis and Bengalis was Bengali opposition to Urdu as a national language, which resulted in the Bengali Language Movement and an economic downturn. The relatively secular attitude of East Pakistan increased tensions between the two communities and the two provinces of the country. In the 1970 general elections Biharis predominantly supported the mostly West Pakistani Muslim League over the Awami League (overwhelmingly supported by Bengalis), and played an active anti-secessionist role in the liberation war.\n\nBiharis supported the Pakistan Armed Forces during the 1971 Bangladesh Liberation War, comprising majorities in armed paramilitary groups such as Al-Shams, Razakars and Al-Badr (held responsible for the genocidal campaign against Bengali nationalists, civilians, religious and ethnic minorities). News outlets such as the BBC have published death-toll estimates by independent researchers varying from 200,000 to 500,000. Scholars such as R. J. Rummel and Matthew White estimate the total Bengali civilian death toll at 1.5 million. The casualty figure estimated by Pakistan is 25,000, as reported by the Hamoodur Rahman Commission.\n\nHaving generated unrest among Bengalis, Biharis became the target of retaliation. The Minorities at Risk project puts the number of Biharis killed during the war at 1,000; however, R. J. Rummel cites a \"likely\" figure of 150,000.\n\nAnother cause of Bengali reprisal could be the collaboration of Biharis with the Pakistan Army, which participated in mass rape of Bengalis during the Bangladesh Liberation War.  Susan Brownmiller has estimated the number of rape victims of the Pakistan Army and its collaborators during the war at 200,000 to 400,000 women and children.\n\nMany scholars have used such events to understate, marginalize and even justify atrocities against non-Bengalis or to suppress the memory of atrocities committed against them.\n\nEvents \nIn early March 1971, 300 Biharis were killed by Bengali mobs in Chittagong. The massacre was used by the Pakistan Army as a justification to launch Operation Searchlight against the Bengali nationalist movement. Biharis were massacred in Jessore, Panchabibi and Khulna (where, in March 1972, 300 to 1,000 Biharis were killed and their bodies thrown into a nearby river).\n\nThe magnitude of anti-Bihari attacks by Bengalis throughout the war are contested. Bengali sources admit the death of a few thousand to 30,000 or 40,000 non-Bengalis. According to a white paper released by the Pakistani government, the Awami League killed 64,000 Biharis and West Pakistanis. R. J. Rummel, a historian with the University of Hawaii, gives a range of 50,000 to 500,000 Biharis killed and concludes at a prudent figure of 150,000 murdered by Bengalis overall. International estimates vary from 20,000 to 200,000. In June 1971, Bihari representatives put forward a figure of 500,000 Biharis killed by Bengalis.\n\nAftermath\n\nMukti Bahini \nAllegations have been made that Mukti Bahini, the Bengali resistance force, backed by Indian government, from East Pakistan, killed non-Bengalis (primarily West Pakistanis and Bihari) in the aftermath of the Bangladesh Liberation War. Sarmila Bose, in her book Dead Reckoning: Memories of the 1971 Bangladesh War, accused Bangladeshi liberation accounts of ignoring atrocities against Urdu-speaking people in East Pakistan. However, Bose's book is considered controversial, and is laced with historical innacuricies and obfuscations. Her book was highly criticized by many historians, journalists and the writers.\n\nRefugee crisis \nThe Bangladesh government announced Presidential Order 149 in 1972, offering citizenship to Biharis. According to government sources 600,000 Biharis accepted the offer, and 539,669 opted to return to Pakistan. But according to historian Partha Ghosh approximately 470,000 Biharis out of a total of 700,000 Biharis opted to be repatriated to Pakistan through the International Red Cross. Several groups in Pakistan have urged their government to accept the Biharis.\n\nSurur Hoda, a Socialist leader, played an active role in solving the refugee crisis. He organized a delegation, headed by British Labour Party politician David Ennals and Ben Whitaker, which encouraged many refugees to return to Pakistan. In a 1974 agreement, Pakistan accepted 170,000 Bihari refugees; however, the repatriation process has since stalled.\n\nOrganisations such as Refugees International have urged both governments to \"grant citizenship to the hundreds of thousands of people who remain without effective nationality\". During his 2002 trip to Bangladesh, Pakistan president Pervez Musharraf said he sympathised with the plight of the Biharis but could not allow them to emigrate to Pakistan. As of 2006, the Office of the United Nations High Commissioner for Refugees (UNHCR) had not addressed the plight of the Biharis. On 19 May 2008, the Dhaka High Court approved citizenship and voting rights for about 150,000 refugees who were minors at the time of Bangladesh's 1971 war of independence. Those born in the country since the war also gained citizenship and the right to vote.\n\nImmigration \n\nDue to their initial pro-Pakistan stance, the Biharis were consistent in their wish to be repatriated to Pakistan. Initially, 83,000 Biharis (58,000 former civil servants and military personnel), members of divided families and 25,000 hardship cases were evacuated to Pakistan. By 1974, 108,000 had been transferred to Pakistan (mainly by air); by 1981, about 163,000. Both countries have signed agreements on the repatriation of stateless people, but only a few hundred have managed to go to Pakistan. Under the supervision of the UN High Commissioner for Refugees over 119,000 Biharis were airlifted to Pakistan. By 1982 Pakistan had received 169,000 Biharis. Some Biharis also entered Pakistan through illegal means. According to the UNHCR report 170,000 Biharis were repatriated after the second Delhi Agreement. In 1977, 4,790 families were repatriated; 2,800 in 1979; 7,000 in 1981; 6,000 in 1984; and 50 families in 1993. A total of approximately 178,069 Biharis were repatriated to Pakistan between 1973 and 1993.\n\nIn 1988, the Organisation of Islamic Cooperation (OIC) raised about $500 million for the repatriation and rehabilitation of Biharis to Pakistan. A special committee, the Rabita (Coordination) Trust Board, was formed by Pakistan President Muhammad Zia-ul-Haq. It received $14 million in 1992, and was requesting additional donations from Saudi Arabia and other Gulf states for the rehabilitation of Biharis.\n\nLand allocated to Biharis in Pakistan in one colony in Mian Channu is now a slum. The Biharis were targeted by the ethnic Sindhi people during the 1980s  Karachi riots. In the Punjab province of Pakistan, ethnic Punjabis forcefully occupied shelters allocated to the Biharis.  These incidents  have prompted some Biharis to return to Bangladesh.\n\nPresent conditions \n\nAlthough many Biharis have assimilated into the Bengali population of Bangladesh, some opt to migrate to Pakistan and are  relocated to refugee camps across Bangladesh. According to one estimate, at least 250,000 Biharis are still in  Bangladesh urban refugee camps. The camps have become slums, the largest of which (known as \"Geneva Camp\", with over 25,000 people) is crowded and undeveloped; families up to 10 people typically live in a single room, one latrine is shared by 90 families and no more than five percent of the population has a formal education. Due to the lack of educational opportunity and poor living conditions, young men in the slums have set up an Urdu Bhashi Jubo Chhatro Shongothon (Urdu-Speaking Young Students Association) to increase educational opportunities in their community. Health and sanitation problems persist due to poor drainage and sewage systems, and the economic condition of Bihari refugees has been described in news reports and academic journals as extremely poor.\n\n2014 Kalshi clashes\nIn 2014, members of the ruling Awami League, aided by police clashed with the members of the Urdu speaking community, in a bid to grab land, in Mirpur. During these clashes, nine people including eight members of a family were burnt alive by Awami League and their local Bengali supporters.\n\nThe Biharis blamed the attacks being directed by Elias Mollah, the ethnic Bengali lawmaker of Mirpur. Elias Mollah denied involvement and blamed a \"vested conspiracy\" against him.\n\nCitizenship and reconciliation efforts \nIn May 2003, a high court ruling in Bangladesh allowed ten Bihari refugees to obtain citizenship and voting rights. The ruling exposed a generation gap among Biharis; younger Biharis tended to be \"elated\", but many older people felt \"despair at the enthusiasm\" of the younger generation and said their true home was in Pakistan. Many Biharis now seek greater civil rights and citizenship in Bangladesh.\n\nOn 19 May 2008, the Dhaka High Court approved citizenship and voting rights for about 150,000 refugees who were minors at the time of Bangladesh's 1971 war of independence. Those born in the country since the war also gained citizenship and the right to vote. Several political parties campaigned in the camps for the Bihari vote during the 2008 general election, and the group was considered important to parties and candidates. Although the court ruling explicitly said that the Biharis are eligible to register to vote in the December 2008 elections, the Election Commission closed its rolls in August 2008 without enrolling them.\n\nSee also \n\n 1971 Bangladesh genocide\n Anti-Bihari sentiment\n Human rights in Bangladesh\n Stranded Pakistanis\n\nReferences\n\nFurther reading \n\nReport Hamoodur-Rahman Commission\n\nAnti-Pakistan sentiment\nBangladesh–Pakistan relations\nEthnic cleansing in Asia\nBiharis\nHuman rights abuses in Bangladesh\nMuhajir history\nRacism in Bangladesh\nTorture in Bangladesh",
        "  We study macroscopic quantum tunneling (MQT) in c-axis twist Josephson\njunctions made of high-T_c superconductors in order to clarify the influence of\nthe anisotropic order parameter symmetry (OPS) on MQT. The dependence of the\nMQT rate on the twist angle $\\gamma$ about the c-axis is calculated by using\nthe functional integral and the bounce method. Due to the d-wave OPS, the\n$\\gamma$ dependence of standard deviation of the switching current distribution\nand the crossover temperature from thermal activation to MQT are found to be\ngiven by $\\cos2\\gamma$ and $\\sqrt{\\cos2\\gamma}$, respectively. We also show\nthat a dissipative effect resulting from the nodal quasiparticle excitation on\nMQT is negligibly small, which is consistent with recent MQT experiments using\nBi${}_2$Sr${}_2$CaCu${}_2$O${}_{8 + \\delta}$ intrinsic junctions. These results\nindicate that MQT in c-axis twist junctions becomes a useful experimental tool\nfor testing the OPS of high-T_c materials at low temperature, and suggest high\npotential of such junctions for qubit applications.\n",
        "This paper proposed a compare-aggregate model for the NLP tasks that require semantically comparing the text sequences, such as question answering and textual entailment.\nThe basic framework of this model is to apply a convolutional neural network (aggregation) after a element-wise operation (comparison) over the attentive outputs of the LSTMs. \nThe highlighted part is the comparison, where this paper compares several different methods for matching text sequences, and the element-wise subtraction/multiplication operations are demonstrated to achieve generally better performance on four different datasets.\nWhile the weak point is that this is an incremental work and a bit lack of innovation. A qualitative evaluation about how subtraction, multiplication and other comparison functions perform on varied kinds of sentences would be more interesting.   \n\n",
        "\nA stepping must be preformed smoothly and with agility to be affective.\nFocus must be on leaping the balance of the body.\nThe movement must be carried out with revolution and self-confidence.;\n,\nFootwork that makes the opponent loses sense and distance.\nDrawing that falsely cause the opponent to think there is something wrong.\nTarget open for attack easily.\nTo avoid being attacked.\nTo launch an accurate and timely attack.\n\n,,, The target, the ball of your front foot, and the heel of your back foot should be on the same line so that the back foot is slightly to the side of the front foot. In Tae Kwon Do, we do this so that we can execute techniques such as front snap kick or roundhouse without having to adjust the foot position first.\n\n, Don't try to stand all the way up on your toes, but this way you can move quickly and pivot without having to adjust your weight.\n\n, Just avoid keeping your knees locked and standing stiffly. Keep the center of your belt just over the middle of both feet so that you know you are standing with the center of your mass in the middle.\n\n, Hands up, shoulders down.\n\n,, A good fighting stance allows you to be ready with out wearing you out. Being stiff makes you rigid and also will make you tired before even starting.\n\n,,",
        "We updated the paper to take into account the reviewers' comments.\nIn particular, the following additions/modifications were made:\n   Section 1 Introduction:\n      - clarified the previous work cited from the arxiv paper, as suggested by AnonReviewer3\n   Section 3:\n      - added a citation to the arxiv paper\n   Section 4:\n      - better explained the decoding scheme (AnonReviewer1)\n      - explained the label embedding use (AnonReviewer1)\n   Section 5:\n      - divided the experiment tasks (AnonReviewer1)\n      - better highlighted the rank information in Table 1 (AnonReviewer1)\n      - added experiments to evaluate the resilience of the decoding scheme to missing at random embedding components\n   Appendix:\n      - added references to the datasets (AnonReviewer3)\n      - added figures for the new experiments\n\nPlease let us know if further modifications are needed.",
        "- Strengths: The paper addresses a relevant topic: learning the mapping between\nnatural language and KB relations, in the context of QA (where we have only\npartial information for one of the arguments), and in the case of having a very\nlarge number of possible target relations.\n\nThe proposal consists in a new method to combine two different representations\nof the input text: a word level representation (i.e. with segmentation of the\ntarget relation names and also the input text), and relations as a single token\n(i.e without segmentation of relation names nor input text). \n\nIt seems, that the main contribution in QA is the ability to re-rank entities\nafter the Entity Linking step.\n\nResults show an improvement compared with the state of the art. \n\n- Weaknesses:\nThe approach has been evaluated in a limited dataset. \n\n- General Discussion:\n\nI think, section 3.1 fits better inside related work, so the 3.2 can become\nsection 3 with the proposal. Thus, new section 3 can be splitted more properly.",
        "  The results of quantitative analysis of word distribution in two fables in\nUkrainian by Ivan Franko: \"Mykyta the Fox\" and \"Abu-Kasym's slippers\" are\nreported. Our study consists of two parts: the analysis of frequency-rank\ndistributions and the application of complex networks theory. The analysis of\nfrequency-rank distributions shows that the text sizes are enough to observe\nstatistical properties. The power-law character of these distributions (Zipf's\nlaw) holds in the region of rank variable r=20 - 3000 with an exponent\n$\\alpha\\simeq 1$. This substantiates the choice of the above texts to analyse\ntypical properties of the language complex network on their basis. Besides, an\napplicability of the Simon model to describe non-asymptotic properties of word\ndistributions is evaluated.\n  In describing language as a complex network, usually the words are associated\nwith nodes, whereas one may give different meanings to the network links. This\nresults in different network representations. In the second part of the paper,\nwe give different representations of the language network and perform\ncomparative analysis of their characteristics. Our results demonstrate that the\nlanguage network of Ukrainian is a strongly correlated scale-free small world.\nEmpirical data obtained may be useful for theoretical description of language\nevolution.\n",
        "No. \n\n1. Light doesn't self-interact. Meaning a photon never directly affects another photon.\n\n2. This boils down to what we mean by light coming in \"particles\" really. Each beam of light is composed of many particles of the same energy. Even if you add the energies of each beam together, all you get is an overall increase in *intensity* (number of particles per second) not an increase in *momentum* of any one particle. - similarly, see the \"photoelectric effect\" for more similar kinds of experiments.\n\n3. There *are* ways to convert invisible light to visible. If light is below visible (like IR), then some crystals can cause \"frequency doubling\" (or tripling, or more), where photons give energy to the crystal which then spits out half as many photons, but at twice the frequency. Green laser pointers are actually an IR laser beam passed through such a crystal. (one of the reasons why you should not disassemble green laser pointers: since your eyes can't see IR, your pupils won't contract to reduce the amount of light entering, and you have an intense IR laser beam entering the eye. Very dangerous. Really.\n\nAnother way, if the light is more energetic than visible (like UV) is to act through fluorescence. Fluorescent lighting is so named. The gas in a fluorescent tube actually predominantly glows in the UV. The frosty coating on the surface of the tube is made of materials that absorb the UV light, and then re-radiate it down spectrum into blues and reds and greens. \n\nBut no, 2 lasers crossed will not change color at their crossing point.\n\n---\n\n***Edit***: a couple comments below have made me second guess my initial particle-physics based assumption. Does anyone know what would happen if we mixed light ala beat frequencies? Suppose you shone two UV lasers at 2 PHz and 2.5 PHz at a spot. Would that spot glow in the visible at 500 THz? I'm not good at classical EM/optics to answer this\n\n---\n\n***Edit 2***: A lot of people are questioning my phrasing about light self-interaction. Let me address what I mean here:\n\nMy background is the strong force. Gluons are the mediating particle, like photons are in EM. Gluons, however, have (strong force) charge. Therefore, a gluon can be attracted to another gluon. A gluon will exchange a gluon with another gluon to attract or repel and generally exchange momentum directly.\n\nA photon is not a charged particle. Photons only interact with charged particles. A photon leaves a charged particle and travels to some other charged particle. Therefore, a photon never is attracted to, nor repelled by other photons. A photon does not exchange photons with other photons to exchange momentum.\n\nThere *are* of course, indirect ways in which photons can interact. Two photons can collide to create particle-antiparticle pairs. Those pairs can annihilate to create two new photons (the originals were destroyed in the collision). Photons do occasionally act like 'virtual' particle-antiparticle pairs, and in that behaviour can, indirectly, exchange a virtual photon from one of their virtual particles to some other photon, and exchange momentum indirectly. But such interactions are *always* mediated by charged-particles at some step. Never directly.\n\nFinally, there are *effects* of photons as well. Classically speaking, their waves may interfere constructively or destructively. One photon may push a particle one way, while another photon pulls it the other at the exact same time. These photons aren't interacting with each other... they're interacting with that \"receiver\" particle. Pushing and pulling at the same time is like destructive interference. Pushing together or pulling together is like constructive interference. \n\nSo that's what I mean by light never self-interacts.\n\n---\n\n***Edit the third***: Sorry, maybe I misinterpreted OP's question re: laser interference. I understand (as mentioned in point 3 above) that there are ways to convert invisible light to visible. I had interpreted the question to mean a medium-free way of mixing lasers. Like shine two lasers through air/vacuum on a single point to create a single spot of visible light.\n\n---\n\n***Edit 4***: **How do materials work to change the frequency of light?** So, the easiest thing, start with this[ post from our FAQ](_URL_0_). Essentially, the relevant thing to note is that *within material*, the effective electromagnetic field behaves differently than it does in vacuum. All the charged particles around interact with each other, so we can come up with a modified electromagnetism to describe the material.\n\nWhen a photon \"enters\" a material (assuming, for the moment it can), The photon, on its own, ceases to exist really. A new propagation through the effective EM field is set up. Well now, since there's a material around, this effective EM field may be able to do different things than the vacuum one can. In the case of certain crystals, two of these propagations can *add* their momenta together, forming a new propagation with new momentum. \n\nAnd when a propagation hits the end of the material it can trade its momentum into a \"bare\" photon that exits the material. (It can also reflect internally and do other things, but we're ignoring that for now). So the new, summed momentum excitation hits the other end of the material and out pops a new photon that has the sum of the momenta of the photons entering the crystal.\n\nWhen I mention above that photons don't self-interact, what I mean *specifically* is that photons have no direct way of giving each other their momentum. They can destroy themselves and become something else and that something else can create new photon(s) with new momentum values. But photons, on their own, in vacuum, don't, generally speaking, do this on their own.",
        "This paper introduces a 'GPU-friendly' variant of A3C which relaxes some synchronicity constraints in the original A3C algorithm to make it more friendly to a high-throughput GPU device. The analysis of the effects of this added latency is thorough. The systems analysis of the algorithm is extensive. \n\nOne caveat is that the performance figures in Table 3 are hard to compare since the protocols vary so much. I understand that DeepMind didn't provide reproducible code for A3C, but I gather from the comment that the authors have re-implemented vanilla A3C as well, in which case it would be good to show what this reimplementation of A3C achieves in the same setting used by DM, and in the setting of the experiment conducted using GA3C (1 day). It would be good to clarify in the text that the experimental protocol differed (top 5 out of 50 vs single run), and clarify why the discrepancy, even if the answer is that the authors didn't have time / resources to reproduce the same protocol. A bit more care would go a long way to establishing that indeed, there is no price to pay for the approximations that were made.\n\nI applaud the authors for open-sourcing the code, especially since there is a relative shortage of properly tested open-source implementations in that general area, and getting these algorithms right is non-trivial.\n\nA disclaimer: having never implemented A3C myself, I have a low confidence in my ability to appropriately assess of the algorithmic aspects of the work.",
        "From 1315 through 1322, northwest Europe was wracked by bad harvest after bad harvest, on top of epidemics ravaging livestock in some years. A generation of Europeans starved to death. But William Chester Jordan's studies of the Great Famine's cultural impact turned up something really interesting: for the first year, it was hardly noticeable. He concludes that the frequency of famine, as attested in chronicles theoughout the Middle Ages, had led medieval people to develop procedures for storing, preserving, and consuming food in patterns that enabled most people above the barest subsistence level to stock and/or acquire food for several seasons to a year of bad harvest.\n\nThe biggest sirprise for modern students of the Middle Ages, I think, tends to be the small role played by fruits and vegetables in the medieval diet. Modern people who have the privilege and luxury to spend time reading about the Middle Ages tend to align pretty closely with the class of people who hear a lot more about overconsumption of food/calories as a problem than under. We have basic knowledge or nutrition, vitamins, and now antioxidants. We *value* vegetables as a way we can eat lots of bulk while saving precious kcal for chocolate. So I'd like us to consider that lettuce, strawberries, even apples are not going to be the medieval peasant's first-choice food for surviving winter, famine, and siege. This is not to say that the peasants cast out of Rouen in the middle of a siege, who spent the winter trapped between fortress walls and the besieging army, did not scrabble and some survive on the greenest, barest roots and grubs they could dig out of the soil. Just, fresh berries and leafy greens were not going to be their choice in better circumstances.\n\nThat said, certain produce did make it through the winter. Noble households in late medieval England, for example, served apples and various dried fruits as a Christmas delicacy. Fruits with a very limited shelf life, most prominently strawberries, were summer treats exclusively.\n\nReally, though, when we talk about diet for survival it comes down to animal protein and grains. From a farming standpoint, the medieval agricultural year actually included multiple growing and harvest seasons. Storage for the year suggested by Jordan would have been a matter of managing consumption and protecting the grain from dampness/mold. But from a consumer standpoint, it's vital to keep in mind that medieval households--even in villages--were not American pioneer-style self-sufficiency. Bread, in particular, was a centralized product at one stage or another, whether it was having one's flour ground at the mill, use of the communal village/town ovens, or simply purchasing bread. I've talked on AH before about the medieval \"moral economy\", in which the price of a bread loaf was fixed at a cost considered to be one affordable by even the poorest day labourers. The *size* of the loaf would vary according to flour prices, but a half penny could always buy at least *something*.\n\nThe role of animal product in medieval diets was also a balance between home production and purchase. For families that could own meat animals, archaeological studies and account books show definite seasonal patterns of consumption of types of animals. Pigs were often slaughtered in the fall, obviously young animals in the spring. As far as dairy consumption, milk with its shorter shelf life was rare, or at least, it doesn't show up in records very often. Instead, we read much more about cheese production, and cheese is that food were molding is sometimes the point.\n\nThe most important animal product by far, for individual Christians and for the wider European economy, was fish. Preserved herring in particular is an enormously large proportion of winter and Lenten diets in the later medieval west. Salt preservation was practiced by that point. Another popular option for cities was to acquire dried fish from Scandinavia, where the northernmost climates were conducive to autopreservation of fish. This was actually a key product that helped knit the North into the expanding western economy!\n\nLocal hunting and fishing were contentious possibilities. Hunting was a noble sport, and nobles often took steps to limit peasants' ability to hunt for food. Fishing rights in rivers underwent similar legal struggles.\n\nMedieval diets were somewhat less diverse than modern ones, it's true, and they certainly became less so in winter, in Lent, and during extreme struggles like famine and siege. For the most part, though, survival was a matter of choosing which food would provide sufficient calories and endure a few months, and saving the money to obtain what else was necessary to fill in the supply gaps--which, over the course of the Middle Ages, was na increasing percentage of the average diet.",
        "  We investigate the evolution of dust that formed at Population III supernova\n(SN) explosions and its processing through the collisions with the reverse\nshocks resulting from the interaction of the SN ejecta with the ambient medium.\nIn particular, we investigate the transport of the shocked dust within the SN\nremnant (SNR), and its effect on the chemical composition, the size\ndistribution, and the total mass of dust surviving in SNR. We find that the\nevolution of the reverse shock, and hence its effect on the processing of the\ndust depends on the thickness of the envelope retained by the progenitor star.\nFurthermore, the transport and survival of the dust grains depend on their\ninitial radius, a_{ini}, and composition: For Type II SNRs expanding into the\ninterstellar medium (ISM) with a density of n_{H,0}=1 cm^{-3}, small grains\nwith a_{ini} < ~ 0.05 micron are completely destroyed by sputtering in the\npostshock flow, while grains with a_{ini}= 0.05--0.2 micron are trapped into\nthe dense shell behind the forward shock. Very large grains of a_{ini} > ~ 0.2\nmicron are ejected into the ISM without decreasing their sizes significantly.\nWe find that the total mass fraction of dust that is destroyed by the reverse\nshock ranges from 0.2 to 1.0, depending on the energy of the explosion and the\ndensity of the ambient ISM. The results of our calculations have significant\nimpact on the abundance pattern of subsequent generation of stars that form in\nthe dense shell of primordial SNRs.\n",
        "The authors investigate the neural GPU model introduced by Kaiser and Sutskever. In section 3 they claim its performance is due to the O(n^2) number of steps it can perform for each example. In the subsequent section they highlight the importance of curriculum training and empirically show that larger models generalize better. In section 5 they construct examples that reveal failure modes. In the last section they compare the performance given different input formats.\n\nThe paper is well written. It contains an exhaustive set of experiments which provide insight into the details of training the neural GPU model. It pushes the boundary of algorithms that can be learned further. On the other hand, the paper seems to lack a coherent message. It also fails to provide any insight into the how and why of the observations made (i.e. why curriculum training is essential and why certain failure modes exist).\n\nThe introduction contains several statements which should be qualified or explained further. As far as I am aware statistical learning theory does not guarantee that empirical risk minimization is consistent when the number of parameters is larger than the number of examples; the generalization performance depends on the VC dimension of the function space instead. Furthermore, the suggested link between adversarial examples and learning algorithms is tenuous, and references or a further explanation should be provided for the contentious statement that deep neural networks are able to match the performance of any parallel machine learning algorithm.\n\nThe authors argue that the neural GPU performs O(n^2) “steps” on each example, which allows it to learn algorithms with super-linear complexity such as multiplication. This analysis seems to overlook the parallel nature of the neural GPU architecture: Both addition and multiplication have O(log n) time complexity when parallelism is used (cf. a carry-lookahead adder and a Wallace tree respectively).\n\nIn section 4 the authors show that their larger models generalize better, which they argue is not self-evident. However, since both training and test error decrease it is likely that the smaller models are underfitting, in which case it is not counter-intuitive at all that a larger model would have better generalization error.\n\nIt is interesting to see that progressively decreasing the number of terms and increasing the radix of the number system works well as a learning curriculum, although it would be nice to have a stronger intuitive or theoretical justification for the latter.\n\nThe final section claims that neural GPUs are cellular automata. Further justification for this statement would be useful, since cellular automata are discrete models, and the equivalence between both models is in no way obvious. The relationship between global operations and changing the input format is circuitous.\n\nIn conclusion, the paper provides some useful insights into the neural GPU model, but does not introduce original extensions to the model and does not explain any fundamental limitations. Several statements require stronger substantiation.\n\nPro:\n\n* Well written\n* Exhaustive set of experiments\n* Learning algorithms with decimal representation\n* Available source code\n\nCons:\n\n* No coherent hypothesis/premise advanced\n* Two or three bold statements without explanation or references\n* Some unclarity in experimental details\n* Limited novelty and originality factor\n\nTypos: add minus in “chance of carrying k digits is 10^k” (section 5); remove “are” from “the larger models with 512 filters are achieve” (section 4); add “a” in “such model doesn’t generalize” (section 4).",
        "  This paper extends earlier work on quantum theory representations of natural\nnumbers N, integers I, and rational numbers Ra to describe a space of these\nrepresentations and transformations on the space. The space is parameterized by\n4-tuple points in a parameter set. Each point, (k,m,h,g), labels a specific\nrepresentation of X = N, I, Ra as a Fock space F^{X}_{k,m,h} of states of\nfinite length strings of qukits q and a string state basis B^{X}_{k,m,h,g}. The\npair (m,h) locates the q string in a square integer lattice I \\times I, k is\nthe q base, and the function g fixes the gauge or basis states for each q. Maps\non the parameter set induce transformations on on the representation space.\nThere are two shifts, a base change operator W_{k',k}, and a basis or gauge\ntransformation function U_{k}. The invariance of the axioms and theorems for N,\nI, and Ra under any transformation is discussed along with the dependence of\nthe properties of W_{k',k} on the prime factors of k' and k. This suggests that\none consider prime number q's, q_{2}, q_{3}, q_{5}, etc. as elementary and the\nbase k q's as composites of the prime number q's.\n",
        " Use this to pack food supplies (I use: 5 min rice, potato flakes, canned refried beans, canned chicken, steri-packed $2 Indian food pouches, PB&J, jerky, trail mix/granola w/dried fruit, fast oatmeal, any of the cream/sugar inclusive cheap instant coffees pouches exported from Vietnam, raisins, lots of apples & bananas, tea bags, and raisins.)\n, On the side, slip a bone saw and hanging triangle with rope (if you own a battery powered sawz-all this can save some time when splitting the spine and quartering)., They hold a lot and still fit in the bottle sleeve on the gear you already carry when out hunting, plus you can tuck them all over the car wherever they'll fit to use up unused crannies and under-seat storage. Platypus & Nalgene roll-up 2L bottles with a drinking hose also work well. Always pack more water than you think you'll use, as you will want to rinse gear that gets bloody when you get a deer and rinse blood from the body cavity when dressing (do towel up the extra moisture inside before hauling or you'll be risking bacterial growth)., This will be both your deer cart and the way you haul your field gear and bow or rifle into the back-country when access roads beyond the campsites are still bike or hike only (especially in bow season). Ask your ranger; they'll help you be sure to follow the rules that aren't posted and are in your favor on a bike. These carts have great vertical and lateral suspension and are over-engineered to protect children. As a result, they'll haul a good sized field-dressed deer, your $30 climbing tree stand from craigslist, airtight hunting clothes bag, daypack with supplies, and a bow or rifle without even flinching., The bag that a bedding set came in: sturdy zipper, thick clear plastic, and gotten freely from many friends/family is a great tool for this job. Pack this bag in the shoe well of the bike cart, then place the cart in the back of the Prius with seats folded down., When you make your own scent spray you can afford to really liberally apply it to everything you have on you. You'll need about 1 liter per trip. Store it in a jug in the fridge.,, This will take a little finagling to get a right fit, so be patient while you learn how to do it.,, (bow should fit on top of the deer cart or rifle beside it) Take care to separate ammunition per your state's laws., If you have any extraneous gear to squeeze in, you can put it in the expansive storage space under the trunk. It's a great place to put a roadside emergency kit, jumper cables, 1 gal fuel jug, tow rope, DC air compressor, etc., You can climb a pretty nasty mountain trail with these little $30 kiddie carts, so prepare to be pleased!, It should now create a full 7' bed for one onto which you can unroll your bedding. Chain your deer/gear cart and bike together next to your car, and if you really have concerns about theft, secure it to one of the tow loops under the Prius. Since you're on public land, you may need to chain your bike/cart to a tree once near your hunting spot and walking your stand into position.) You'll be leaving your biking clothes and shoes in the cart, but folks don't tend to steal that!,, If your shoulder blades are having trouble with the joint between the car seat and top of the chest of drawers, lay that Sterilite lid underneath the Thermarest for better support., Now in the morning you can gear up, heat a pot of water which will warm your hands too and pour some in your oatmeal bowl and some in your coffee cup, and leave a little to rinse stuff when you're done and brush your teeth (You can brush with baking soda for bow hunting mornings). Since your food is in a sealed car, you don't have to worry about bear bags or trash bag issues., The car should remain off until the electric battery runs down and then the internal combustion engine will come on for about 1 minute every 45 minutes to recharge itself. Even if gas is $4/gallon, you've saved so much by getting 50mpg on your drive out that you've got nothing to complain about! If you haven't done the 'charge while off' hack to your Prius, the added bonus of leaving it running is that you can charge your phone/gps that night too., A lot of mosquito mesh can be had on eBay for $5 with free shipping.,\nSkin with a fresh box knife razor or a scalpel, just can't top that for precise work (plan to use up 2 blades). You can usually fit the whole quartered up deer in a double bagged 60 gallon trash bag set on the lid of the large sterilite tub (to catch leaks). Hit your nearest gas station on the way home to set some ice bags on top if needed (don't put it in the meat and make it wet though!).\n\n, If you can't afford a full strength grinder that can handle deer meat, take any grind cuts to your local deer processor to make ground and sausage, however is your custom.,",
        "Modifications to the last revision:\n- Section 2, SampleRNN model. Better model description, changed order of sub-sections, explained the upsampling method more clearly.\n- Added a paragraph (before Section 3.1) detailing the training procedure and hyper-parameters.\n- Added subsection 3.3 Quantifying information retention. (See authors' response to AnonReviewer3 on Dec. 2 titled \"time horizon\")\n- Minor modifications (typo, citation, rephrasing, etc.)",
        " Logan Airport is located in Boston and is an international airport receiving flights daily from all over the world. Several popular airlines fly to this airport, including United Airlines, Southwest Airlines, and US Airways.\n\n\nLogan Airport receives flights directly from several US states, including Las Vegas, Los Angeles, Dallas, and New York.\nLogan Airport receives international flights daily from several countries, including Europe, Asia, and Oceana.\nLook online, at your airport, or with a travel agent to pick the best domestic or international flight bound for Logan airport.\nLogan Airport has the FAA code BOS, so any airplane bound for BOS will get you there.;\n, You will need to pack accordingly depending on the duration of your stay.\n\n\nInternational and domestic airline flights have strict limits on number of bags, weight, and dimensions. Be sure to check these restrictions before packing.\n\n, Ask a friend for a ride, take public transportation, or pay to park your car in the airport parking lot.\n\n\nBring two forms of identification, a passport if traveling internationally, and your flight itinerary to the airport.\nCheck in for your flight.\nBoard your flight bound for Logan Airport.\n\n, Find information about Boston Train and Subway routes at the Massachusetts Bay Transportation Authority Website.Inbound is always toward downtown Boston, and outbound is away from it. In the subway system, inbound is toward four stations: Park Street, State, Downtown Crossing and Government Center.\n\n, Transfer to the Blue Line to Airport Station.Then take the free Massport shuttle bus to all airport terminals.\n\n, Transfer to the free Massport shuttle bus to all terminals.\n\n\nOther options from the northeast include Bus Route Nos.448 and 449 from Marblehead or Bus Route No.459 from Salem, all of which serve Logan Airport Terminal C.\n\n, Transfer to the Silver Line SL1, which serves all airport terminals.\n\n, Transfer to the Orange Line.\n\n\nTake the Orange Line one stop south to State Station. Transfer to the Blue Line to Airport Station.\nThen take the free Massport shuttle bus to any terminal.\nOther options from the northwest include taking the Commuter Rail to South Station and transferring to the Silver Line SL1, which serves all airport terminals.\n\n, Transfer to the Blue Line to Airport Station.\n\n\nThen take the free Massport shuttle bus to any terminal.\nOther options from the south include taking the Commuter Rail, Red Line, or SL4 to South Station and transferring to the SL1, which serves all airport terminals.\n\n, Transfer to the SL1, which serves all airport terminals.\n\n\nOther options from the southeast include taking the MBTA Harbor Express ferries from the Quincy commuter boat terminal located at 703 Washington Street in Quincy or from Pemberton Point in Hull to Logan Airport. Then connect to the free Massport Route 66 Shuttle Bus for service to all airport terminals., This bus goes directly to the airport for about $7.You can access a Logan Express bus terminal in Braintree, Framingham, Woburn, and Peabody.\nEach Logan Express bus terminal has a secure parking lot. , Before boarding, know your bus! The bus's route number and destination can be found on the sign above the windshield. At some stops, all buses on a route stop to pick up passengers, no matter which direction they're going.Take care to board the correct bus by checking the destination sign or asking the operator.\n\n, Most local routes charge 90¢. Bus fare-boxes accept change, passes, and tokens. Avoid paying with dollar bills.\n\n\nAll buses are pay-as-you-board, except routes 71, 72, and 73 when heading outbound. Pay when leaving these routes.\nIf you need to transfer from one route to another, ask for a transfer while paying your face. This might give you another local route for free. Check city bus websites for route information and details.Exit to the rear of the bus.\n\n,\n\n\nOnce you arrive at the dock, take the free Massport Route 66 Shuttle Bus, which services all airport terminals.\nCommuter Boat service from Quincy operates 7-days-a-week.\nCommuter Boat service from Hull operates weekdays only.\nOvernight parking at Quincy is affordable and discounted weekly rates are also available. For more information about MBTA ferries to Logan Airport, please call the carrier at (617) 770-0040., Although taking a taxi will be more expensive, this is one of the best methods to use if you are unfamiliar with the area, in a rush, or nervous about getting lost. Taxi drivers will know how to get you to Logan Airport.When traveling within a 12-mile radius of download Boston, taxis charge a metered rate. Outside the 12-mile radius of downtown Boston, taxis charge a flat rate.\n\n, Uber and Lyft are both alternatives to the traditional taxi and often offer ride share options and cheaper rates. You can schedule a ride with Uber or Lyft by downloading the respective app on your mobile device and linking a payment option., Boston Water Taxis operate on demand from 6:30am to 10pm Monday through Wednesday and 6:30am to 11pm Thursday through Saturday. Rates and details can be found at the Boston Water Taxi website.Boston Water Taxis and are covered and heated for comfort in winter months.\nThe Boston Water Taxi charges $12 for one stop, for one adult. Discounts apply for same-day services.\nMulti-ride passes are available to purchase.\nHours may change by season.\nFind a Boston Water Taxi at any designated stop (find a map of stops at the Boston Water Taxi official website) or call for one at 617-227-4320., Depending on your distance from the airport, the time of day, day of the week, and unpredictable factors like traffic accidents or roadwork, travel time can vary.\n\n\nIf you are traveling to Logan airport to catch a flight, remember to plan to arrive at the airport at least 2 hours early for domestic flights and 3 hours early for international flights.\n\n, There are several routes you can take to get to Logan Airport.\n\n\nFrom the west, take the I-90 East (Mass Pike) to the Ted Williams Tunnel and take the I-90 East Exit 26 to Logan.\nFrom the north, take the I-93 South to Exit 24B to the Callahan Tunnel and Route 1A North to Logan. Or, take Route 1 South, exit at Route 60, and follow the signs to Logan Airport.\nFrom the south, take the I-93 North to Exit 20 to I-90 East and the Ted Williams Tunnel, then take I-90 East Exit 26 to Logan.\n\n, Be cautious of parking rules and prices when leaving your vehicle in an airport parking lot.Parking availability information is updated in real time on the Logan Airport official website. You can use this tool to check status on parking lots for long-term, short-term and economy parking.Short-term parking is suitable for 4 hours or less.\nParking rates vary by length of stay and lot. Current rates can be found at the Logan Airport website., If you are relatively close, you may be able to walk to Logan Airport. Walking is not recommended if you are bringing heavy luggage with you.\n\n, Several designated bike paths can get you to Logan airport. It is not recommended that you ride your bike if bringing heavy luggage.\n\n\nConsult Boston's Bike Maps online for the best route.Bike racks at Logan airport are available in Terminal A, Terminal E, Economy Parking Garage, Rental Car Center, Blue Line Airport Station, Bremen Street Park, and Greenway Connector.\n\n, If you are short on cash or worried about getting to the airport by yourself, ask a friend or family member to give you a ride.\n\n\nLogan Airport has designated pick-up or drop-off waiting lots. These lots are located about five minutes from the terminals and allow drivers to stay for up to 30 minutes.\n\n",
        "The determining factor of the \"amplitude\" of a sound wave on Earth is atmospheric pressure. Sound is a longitudinal wave- that is the wave moves along the direction of propagation, like a slinky (an ocean wave is a transverse wave, where the wave motion is perpendicular to the direction of travel). Thus, really a sound wave is low and high pressure patterns. \n\nTherefore, the loudest a sound wave can be on Earth (here talking about being on land) is when the pressure difference goes from a vacuum to 2 atmospheres (to remain averaged out at 1 atmosphere). This means that standing outside, the loudest sound you could hear is [~194 dB](_URL_0_). \n\n**Edit:** In response to a lot of (valid) responses below: yes, there can be shock waves in which have a greater atmospheric pressure difference than 2 atmospheres. I do not classify shock waves as sound waves, but they do produce a sound- which sounds like a single \"crack\" in the air. If this question is not limited to sinusoidal waves (undistorted sound), then the answer is sound much louder than the answer described here.",
        "I'm sure there are others on here who specialise more closely in stagecraft than I, but I can offer a few insights here. This is a great question which requires a few different perspectives to give a good answer.\n\n & nbsp;\n\nThe first bit of context I would like to attach here is that of the Elizabethan play-going scene. While there were indoor theatres and court performances which would cater specifically to a higher class of patron, the playhouses in London famously took all comers. They were also seated in an environment where a fairly large variety of entertainments were on offer on the South side of the river, some more savoury than others. Brothels, animal baiting, and plays could all be enjoyed a stone's throw away from one another.^1 Fighting and duels were also commonplace - tournaments were held at Smithfield, and public fencing contests between students of London's fencing schools were held in playhouses, open fields, and inns and taverns.^2 Public fighting was very much part of the entertainment scene, so in many ways it isn't surprising to see duels and swordfights appear on stage.\n\n & nbsp;\n\nAs for your question about whether the sword fights were 'for show', that's harder to answer. Elizabethan play-goers were certainly accustomed to sword fighting to some degree, especially given that fencing contests were potentially a part of their entertainment diet, and that it is reasonable to assume that a majority of able-bodied men at this time had some familiarity with swordplay themselves ('stab them with the pointy end!'). We do not know whether sword fights on stage generally attempted to appear convincing and real, or whether stage fighting had its own conventions without any expectation of verisimilitude.^3 Few contemporary sources discuss this at all, let alone tell us whether these stage fights were realistic. We do know that some actors, like Richard Tarlton were talented fencers, but have no way of knowing whether this was a talent that was put to use on stage to enhance realism.^4 It is worth bearing in mind that we do know that animal blood was more than likely used on the stage to produce realistic special effects, so there must have been some expectations of realism from the stage in other areas.^5\n\n & nbsp;\n\nNow to come to Shakespeare. Charles Edelman sees Shakespeare's substantial use of on stage fencing/swordfighting as particularly unusual for the period. He says;\n\n > However prevalent the tradition of fencing in the playhouse may have been at the time Shakespeare began his career as a dramatist, it borders on the remarkable that he was the first to exploit this tradition to any significant extent. While stage comba certainly existed before Shakespeare, it is also true that most poets, at least as far as can be determined from extant texts, preferred much of their fighting to be done offstage.^6\n\nQuite a few contemporary plays call for both blood and gore in copious quantities (Marlowe's *Tamburlaine* plays come to mind, which includes a character braining himself to death on stage), but few combine swordplay and dialogue in the way that Shakespeare does, which is what Edelman regards as the true innovation taking place here.\n\n & nbsp;\n\nI think that covers the key points of cultural context for the staging of swordplay, but I'll spend a little time going into a few specific examples of the way in which Shakespeare makes swordplay an integral and symbolically meaningful part of his drama. *Romeo and Juliet* is probably the most oft-discussed example of Shakespeare's focus on swordplay. For example, Old Capulet calls for his long sword during the brawl in act 1 sc 1, marking him as something of an old fuddy-duddy compared to the young men with their modern rapiers. Baz Lurhmann does a great job with this in the film adaptation, where Capulet reaches for a ~~large, old fashioned shotgun~~ [edit: thanks for correcting me /u/g2petter, it's actually a modern pump-action shotgun he goes for], where all the young men have flashy pistols. The generational divide is exemplified here by his choice of weaponry.\n\n & nbsp;\n\nLater in the play, the fight between Tybalt, Mercutio and Romeo features a number of jibes about the way that Tybalt fences 'by the book', and this indeed refers to a popular genre of fencing manual (The British Library has an interesting copy online [here](_URL_4_)). It can be argued that this very detailed fight scene was a good opportunity to show off some fashionable continental duelling styles on stage.\n\n & nbsp;\n\nThe verbal run-down of the fight is so detailed that Adolph L. Soens reconstructs the fight and the stereotypical assumptions that come with each style of fighting that the men adopt. He describes how Tybalt is represented as 'foreign and affected' compared to Mercutio's representation as a 'rough, honest, downright Englishman' in terms of their clothing and manners, and how Tybalt's mechanical style of Spanish fencing overcomes Mercutio's Italian style (on which much contemporary English fencing was based). We have a clear set-up for 'good guy and bad guy' here, and *welp*, the bad guy wins.^7\n\n & nbsp;\n\nOther plays that I don't have time to discuss in detail today (sorry) but which use sword fighting in a similarly narrative way include *Hamlet*, *King Lear*, and *Macbeth*.\n\n & nbsp;\n\nTo attempt a conclusion - yes, to some degree sword-fighting was included in plays because it was already part of the entertainment scene and known to be popular. Early modern plays were a visual spectacle, often full of blood and gore and it's not surprising that we see it take place on stage, especially as fencing manuals were becoming popular in print. However, the way in which Shakespeare gets a lot of his fighting done on stage and the way he integrates it into his overall symbolic landscape for the play is pretty unusual. Hope that answers at least some of your question, and look forward to see what else others with a closer degree of specialism in the field have to offer!\n\n & nbsp;\n\n^1 Philip Henslowe, the owner of the Rose Theatre (where many of Christopher Marlowe's plays were staged) had business interests in all three of these areas, with one of his theatres set up to accommodate *both* plays and animal-baiting. See [here](_URL_1_)\n\n & nbsp;\n\n^2 McElroy, Mary, and Kent Cartwright. \"Public Fencing Contests on the Elizabethan Stage.\" *Journal of Sport History* 13, no. 3 (1986): 193-211. _URL_2_.\n\n & nbsp;\n\n^3 I recommend reading the introduction to Charles Edelman's *[Brawl Ridiculous: Swordfighting in Shakespeare's Plays](_URL_5_)* (Manchester: Manchester University Press, 1992) for a more detailed discussion of this.\n\n & nbsp;\n\n^4 Thomson, P. 'Tarlton, Richard (d. 1588), actor and clown' in *Oxford Dictionary of National Biography*. Ed.   Retrieved 15 Aug. 2018, from [_URL_3_.](_URL_3_.)\n\n & nbsp;\n\n^5 Farah Karim-Cooper and Lucy Munro, *Shakespeare’s Globe Theatre History Seminar\nStage Blood: A Roundtable*, 13 July 2006. See [here](_URL_0_) (opens pdf).\n\n & nbsp;\n\n^6 Edelman, *Brawl Ridiculous*, p.11.\n\n & nbsp;\n\n^7 Soens, Adolph L. \"Tybalt's Spanish Fencing in Romeo and Juliet.\" *Shakespeare Quarterly* 20, no. 2 (1969): 121-27. doi:10.2307/2868996.\n\n & nbsp;\n\nedit: formatting derp",
        "  In recent years, strong analogies have been established between the physics\nof acoustic perturbations in an inhomogeneous dynamical fluid system, and some\nkinematic features of space-time in general relativity. An effective metric,\nreferred to as the `acoustic metric', which describes the geometry of the\nmanifold in which acoustic perturbations propagate, can be constructed. This\neffective geometry can capture the properties of curved space-time in general\nrelativity. Physical models constructed utilizing such analogies are called\n`analogue gravity models'. Classical analogue gravity effect may be observed\nwhen acoustic perturbations propagate through a inhomogeneous transonic\nclassical fluid. The acoustic horizon, which resembles a black hole event\nhorizon in many ways, is generated at the transonic point in the fluid flow.\nThe acoustic horizon is essentially a null hyper surface, generators of which\nare the acoustic null geodesics, i.e. the phonons. The acoustic horizon emits\nacoustic radiation with quasi thermal phonon spectra, which is analogous to the\nactual Hawking radiation. The temperature of the radiation emitted from the\nacoustic horizon is referred to as the analogue Hawking temperature.\n  It has been demonstrated that, in general, the transonic accretion in\nastrophysics can be considered as an example of the classical analogue gravity\nmodel naturally found in the Universe.\n",
        "It depends, but you're most likely a Qiang slave captured in one of their ongoing wars, so there's not much you can do. You could try running, but the royal palace is a compound surrounded by a 26ft-high wall and rather extensive territory. [1]\n\nHuman sacrifice is relatively widespread throughout history, with well-known examples in the Vikings, Aztecs, Incas, Greeks, etc. However, retainer sacrifice - sacrifice of servants to accompany monarchs after death - is more rare. Perhaps most famous of these come from the First Dynasty of Egypt, but the practice was relatively short-lived. [2]\n\nIn the Shang Dynasty, retainer sacrifice served a similar purpose as in Egypt - to provide the deceased monarch with the luxuries they enjoyed in life, and the Shang are not alone in this practice in China. [3]\n\nRetainer sacrifice goes hand-in-hand with Rénjì 人祭, the practice of killing prisoners - primarily male Qiang captives - in the worship of spirits or, more commonly, male ancestors. [4] The higher the status of the individual, the more people were killed; rulers would usually offer 10 of these sacrifices for \"normal\" worship. [5]\n\nThese sacrifices were quite brutal and included beheading, dismemberment, beating, taking blood, burning, burying alive, drowning, and drying. I'm afraid it's not looking too good for you so far, my friend.\n\nShang rulers were both absolute monarchs and the high priest, overseeing the two most important enterprises of Shang times - war and sacrifice, primarily the aforementioned Rénjì. [6] Their tombs contained many of victims of Rénjì, and one tomb in Anyang uncovered more than 700 skeletons, some decapitated and others buried alive. [7] Large numbers of retainer sacrifices have also been found in the tombs of other kings and nobles. [8]\n\nThese victims were well-organized in various pits and showcase both regular sacrifices and hundreds of skeletons arranged in the king's tomb.\n\nLooks like you're dying, and likely in a brutal manner. However, if you're closer to the end of the Shang dynasty, you might be in luck!\n\nDuring the decline of the Shang dynasty, human sacrifice was drastically reduced. You see, the Shang didn't practice institutionalized slavery, so sacrifice was the only way of using all the prisoners they captured in war. Once slavery became a more attractive option throughout the region, sacrifice of captives decreased. [3]\n\n\n\n**Sources**\n\n[1] Fairbank, John King; Goldman, Merle (2006). China: A New History (2nd ed.). Harvard University Press.\n\n[2] Spencer, A.J. (1982). Death In Ancient Egypt (1st ed). Great Britain: Penguin Books Ltd. pp 68, 139.\n\n[3] Shelach, Gideon. (1996). The Qiang and the Question of Human Sacrifice in the Late Shang Period. Asian Perspectives. Vol. 35, No. 1.\n\n[4] Huáng Zhǎnyuè. (1990) 黄展岳, Zhōngguó gǔdài dē rénshēng rénxùn.中国古代的人牲人殉. Běijīng: Wénwù Chūbănshè. Oracle Bone Inscriptions\n\n[5] Jiǎgǔwén Héjí. (1983). 甲骨文合集. 中华书局影印本. Běijīng: Zhōnghuá Shūjú Yǐngyìnběn. \n\n[6] Smith, Howard (1961). Chinese Religion in the Shang Dynasty. International Review for the History of Religions, Vol 8, Fasc 2. pp 142–150. JSTOR 3269424.\n\n[7] Kuo Mo-Jo (1952). Nu Li Chih Shih Tai. University of Virginia. pp. 68-9, 73-4.\n\n[8] Chang (1980). pp 253.",
        "  A free zero-range process (FRZP) is a simple stochastic process describing\nthe dynamics of a gas of particles hopping between neighboring nodes of a\nnetwork. We discuss three different cases of increasing complexity: (a) FZRP on\na rigid geometry where the network is fixed during the process, (b) FZRP on a\nrandom graph chosen from a given ensemble of networks, (c) FZRP on a dynamical\nnetwork whose topology continuously changes during the process in a way which\ndepends on the current distribution of particles. The case (a) provides a very\nsimple realization of the phenomenon of condensation which manifests as the\nappearance of a condensate of particles on the node with maximal degree. The\ncase (b) is very interesting since the averaging over typical ensembles of\ngraphs acts as a kind of homogenization of the system which makes all nodes\nidentical from the point of view of the FZRP. In the case (c), the distribution\nof particles and the dynamics of network are coupled to each other. The\nstrength of this coupling depends on the ratio of two time scales: for changes\nof the topology and of the FZRP. We will discuss a specific example of that\ntype of interaction and show that it leads to an interesting phase diagram.\n",
        "There is not enough information to answer this question. You need some kind of extra info. Depending on the situation, it could be 0% for red, 50% for red, or 100% for red (or any % really). \n\nFor instance, let's say that you actually number the tokens as they go in, 1,2,3,4,5,6,... \n\n* If you make it so that every odd number is blue and every even number is red. Then the chances of pulling red will be 50%.\n\n* If you make it so that every prime is red and every non-prime is blue, then there will be a 0% chance of pulling red.\n\n* If you make it so that every square number is blue and every non-square number is red, then you have a 100% chance of pulling red.\n\n* If you make it so that every number that has remainder 7 after dividing by 20 is red, and every other number is blue, then there will be a 5% chance of pulling red.\n\nQuantity alone is not enough to say anything about probability. You need to say something about how things are distributed, how *dense* the events are. If you have finitely many possible events, and everything is uniformly dense, then you can resort to counting and then just dividing to find the proportion. If things are infinite, or they are not uniformly dense, then you need to use other methods.\n\nWhat you *can* do, to figure out these proportions, is count proportions of larger and larger handfuls of tokens. Take a handful of size 1, size 2, size 3, size 4, size 5, size 6,... and find the proportion of red in each. Maybe it's 100%, 50%, 66%,  50%, 60%,... and perhaps these percentages, as you grab more and more, even out to 62.5%, then you can say that the chances, in the infinite bags, of pulling a red is 62.5%. This is also how you can get 0%. The percentages could go down like 100%, 50%, 33%, 25%, 20%, 16%, 14%, ... always getting smaller and smaller. So, even though it is never 0%, and even though you *can* choose red tokens, the percentage \"at infinity\" of pulling a red could still be zero.\n\nEDIT: As many have pointed out, it doesn't make sense in a strict mathematical framework to talk about \"randomly picking\" out of an infinite bowl (no uniform random variables on the integers). But there are still ways to make sense of our intuition in this context, and this density description is one of them.",
        "Hoo boy. A better question might be, how did we get to be so lucky and have separate passages for urine, feces, and genetic material(at least in the female). Chordates (vertebrates and their ancestors, including us) develop urogenital structures from the same type of tissue, mesoderm. The shared development results in a [cloaca](_URL_0_) for the vast majority of vertebrates. That is, a shared passage for excretions and reproduction. However, mammals fully separate fecal waste from a urogenital sinus, and in some cases (humans) separate urethra from vaginal canal. The 'plumbing' of human males connects reproduction and urination early on in development, and are necessarily connected anatomically; the seminal vesicles and prostate are located at the base of the bladder. \n\nTL;DR **Eutherian* Mammals (humans especially) are exceptional vertebrates in that we actually have much *more* separation of excretory/reproductive function. Developmentally, urogenital tissues are intimately connected.",
        "Robert Dennison (6 March 1912 – 19 June 1994) was an English professional footballer who made 112 appearances in the Football League playing for Newcastle United, Nottingham Forest, Fulham and Northampton Town. He then went into management, with Northampton Town, Middlesbrough, Hereford United and Coventry City, where he was chief scout and spent three months as caretaker manager after Noel Cantwell's dismissal in 1972.\n\nReferences\n\n1912 births\n1996 deaths\nPeople from Amble\nEnglish footballers\nAssociation football defenders\nNewcastle United F.C. players\nNottingham Forest F.C. players\nFulham F.C. players\nNorthampton Town F.C. players\nEnglish Football League players\nEnglish football managers\nMiddlesbrough F.C. players\nHereford United F.C. players\nCoventry City F.C. players\nEnglish Football League managers\nCoventry City F.C. non-playing staff",
        "NADH:ubiquinone oxidoreductase complex assembly factor 5, also known as Arginine-hydroxylase NDUFAF5, or Putative methyltransferase NDUFAF5, is a protein that in humans is encoded by the NDUFAF5 gene. The NADH-ubiquinone oxidoreductase complex (complex I) of the mitochondrial respiratory chain catalyzes the transfer of electrons from NADH to ubiquinone, and consists of at least 43 subunits. The complex is located in the inner mitochondrial membrane. This gene encodes a mitochondrial protein that is associated with the matrix face of the mitochondrial inner membrane and is required for complex I assembly. A mutation in this gene results in mitochondrial complex I deficiency. Multiple transcript variants encoding different isoforms have been found for this gene.\n\nStructure \nNDUFAF5 is located on the p arm of Chromosome 20 in position 12.1 and spans 36,554 base pairs. The NDUFAF5 gene produces a 30 kDa protein composed of 267 amino acids. The presumed structure of the c-terminal of the protein has been found to resemble that of the known secondary structure of RdmB. NDUFAF5 contains the S-adenosylmethionine dependent methyltransferase domain, which contains the GxGxG signature sequence, and the S-adenosylmethionine-binding motif which are common in most SAM-dependent methyltransferases. This arginine-hydroxylase is involved in the assembly of mitochondrial NADH:ubiquinone oxidoreductase complex (complex I, MT-ND1) at early stages. Complex I is composed of 45 evolutionally conserved core subunits, including both mitochondrial DNA and nuclear encoded subunits. One of its arms is embedded in the inner membrane of the mitochondria, and the other is embedded in the organelle. The two arms are arranged in an L-shaped manner. The total molecular weight of the complex is 1MDa.\n\nFunction \nThe NDUFAF5 gene encodes a mitochondrial protein that is associated with the matrix face of the mitochondrial inner membrane and is required for complex I assembly. Their role is integral to co-factor insertions and in utilizing sub-assemblies for building complex I. It does so by catalyzing the hydroxylation of Arg-73 in the NDUFS7 subunit of human complex I, which occurs before the peripheral and membrane arm juncture formation in the beginning stages of complex I assembly. The NADH-ubiquinone oxidoreductase complex (complex I) of the mitochondrial respiratory chain catalyzes the transfer of electrons from NADH to ubiquinone, and consists of at least 43 subunits. The complex is located in the inner mitochondrial membrane. Though the exact biochemical function of NDUFAF5 is not yet known, mutations in this gene results in mitochondrial complex I deficiency. NDUFAF5 is also known to be a member of the 7β-strand family of SAM-dependent methyltransferases.\n\nClinical significance \nMutations in NDUFAF5 is known to result in mitochondrial diseases and associated disorders. It is majorly associated with a complex I deficiency, a deficiency in the first complex of the mitochondrial respiratory chain. Suppression of the NDUFAF5 gene has been found to lead to the loss of both peripheral and membrane arms of complex I associated with NDUFS7 and ND1. This then leads to the progressive loss of complex I, causing the deficiency. Such disorders involving the dysfunction of the mitochondrial respiratory chain may cause a wide range of clinical manifestations from lethal neonatal disease to adult-onset neurodegenerative disorders. Phenotypes include macrocephaly with progressive leukodystrophy, non-specific encephalopathy, cardiomyopathy, myopathy, liver disease, Leber hereditary optic neuropathy, and some forms of Parkinson disease. Mutations in NDUFAF5 has also been common patients with Leigh syndrome, an early-onset progressive neurodegenerative disorder characterized by the presence of focal, bilateral lesions.\n\nInteractions \nIn addition to co-subunits for complex I, NDUFAF5 has protein-protein interactions with NDUFAF8 (for stabilization), and NDUFS7.\n\nReferences\n\nFurther reading \n\n \n \n \n \n\nHuman proteins\nGenes on human chromosome 20",
        "The problem of utilizing all available information (across modalities) about a product to learn a meaningful \"joint\" embedding is an interesting one, and certainly seems like it a promising direction for improving recommender systems, especially in the \"cold start\" scenario. I'm unaware of approaches combining as many modalities as proposed in this paper, so an effective solution could indeed be significant. However, there are many aspects of the proposed architecture that seem sub-optimal to me:\n\n1. A major benefit of neural-network based systems is that the entire system can be trained end-to-end, jointly. The proposed approach sticks together largely pre-trained modules for different modalities... this can be justifiable when there is very little training data available on which to train jointly. With 10M product pairs, however, this doesn't seem to be the case for the Amazon dataset (although I haven't worked with this dataset myself so perhaps I'm missing something... either way it's not discussed at all in the paper). I consider the lack of a jointly fine-tuned model a major shortcoming of the proposed approach.\n\n2. The discussion of \"pairwise residual units\" is confusing and not well-motivated. The residual formulation (if I understand it correctly) applies a ReLU layer to the concatenation of the modality specific embeddings, giving a new similarity (after dot products) that can be added to the similarity obtained from the concatenation directly. Why not just have an additional fully-connected layer that mixes the modality specific embeddings to form a final embedding (perhaps of lower dimensionality)? This should at least be presented as a baseline, if the pairwise residual unit is claimed as a contribution... I don't find the provided explanation convincing (in what way does the residual approach reduce parameter count?).\n\n3. More minor: The choice of TextCNN for the text embedding vectors seems fine (although I wonder how an LSTM-based approach would perform)... However the details surrounding how it is used are obscured in the paper. In response to a question, the authors mention that it runs on the concatenation of the first 10 words of the title and product description. Especially for the description, this seems insufficiently long to contain a lot of information to me.\n\nMore care could be given to motivating the choices made in the paper. Finally, I'm not familiar with state of the art on this dataset... do the comparisons accurately reflect it? It seems only one competing technique is presented, with none on the more challenging cold-start scenarios.\n\nMinor detail: In the second paragraph of page 3, there is a reference that just says (cite Julian).",
        " Read the news and talk to people to find interesting stories. Think about what phenomena are happening and how you can talk about them in a new and innovative way.;\n, Finding out background information can help you figure out an angle and identify subjects to interview. Doing online research is good, but it may only get you so far. You may also need to consult books to make sure you are fully aware of the issues surrounding a topic. A historical article may require a visit to an archive.\n\n, There are a number of ways to write a feature, depending on what you want to focus on. Some of these include:\n\n\n\nHuman Interest: Many feature stories focus on an issue as it impacts people. They often focus on one person or a group of people.\n\nProfile: This feature type focuses on a specific individual’s character or lifestyle. This type is intended to help the reader feel like they’ve gotten a window into someone’s life. Often, these features are written about celebrities or other public figures.\n\nInstructional: How-to feature articles teach readers how to do something. Oftentimes, the writer will write about their own journey to learn a task, such as how to make a wedding cake.\n\nHistorical: Features that honor historical events or developments are quite common. They are also useful in juxtaposing the past and the present, helping to root the reader in a shared history.\n\nSeasonal: Some features are perfect for writing about in certain times of year, such as the beginning of summer vacation or at the winter holidays.\n\nBehind the Scenes: These features give readers insight into an unusual process, issue or event. It can introduce them to something that is typically not open to the public or publicized.\n\n, As you brainstorm story ideas, think about who will read these stories. Ask yourself questions such as Who will be my readers? and What kinds of angles appeal to these readers? For example, you might write a profile about a pastry chef, but you’ll write differently depending on if your readers are aspiring chefs or if they are wedding planners looking to buy a wedding cake., If you are writing for a magazine or blog with a very specific topic, such as gardening, then you will likely need to tailor your feature article to reflect that interest in some way. A newspaper, on the other hand, is meant for a more general audience and may be more open to varied content.\n\n, Ask your interviewee to tell you when and where the best place is for them to meet. If they give you a choice, ask for a quiet place where you will be relatively undisturbed for the duration of the interview.\n\n\nSchedule about 30-45 minutes with this person. Be respectful of their time and don’t take up their whole day. Be sure to confirm the date and time a couple of days ahead of the scheduled interview to make sure the time still works for the interviewee.\nIf your interviewee needs to reschedule, be flexible. Remember, they are being generous with their time and allowing you to talk with them, so be generous with your responses as well. Never make an interviewee feel guilty about needing to reschedule.\nIf you want to observe them doing a job, ask if they can bring you to their workplace. Asking if your interviewee will teach you a short lesson about what they do can also be excellent, as it will give you some knowledge of the experience to use when you write.\n\n, Do research ahead of time to ensure that you are asking the most compelling questions. Have a long list of questions to keep the conversation flowing. Know your interview subject’s background and experience, as well as their views on the subject that you’re interviewing them about.\n\n, The direction of the interview should not be a surprise to the interviewee. Giving them the questions before the interview will help them be able to give more thoughtful answers.\n\n, Your interviewee’s time is valuable, so you don’t want to waste the appointment rushing in and catching your breath. Get to the interview site early. Set up your audio recording equipment and test it out. Make sure you have extra pens and paper.\n\n, Use an audio recorder for the interview, but take notes throughout as well. There is always the possibility that your recorder will run out of batteries or memory.\n\n\nBe sure to ask your interviewee if it’s okay to audio-record the interview. If you plan to use the audio for any purpose other than for your own purposes writing up the article (such as a podcast that might accompany the feature article), you must tell them and get their consent.\nDon't pressure the interviewee if they decline audio recording.\n\n, You don’t want to write a lengthy feature about a person only to find out that you’ve spelled their name wrong. Make sure you double-check the spelling of their name, as well as other details that are important to the story.\n\n, Questions that rely on yes or no answers will not give you very rich information. Instead, ask questions that start with “how” or “why.” These types of questions give the interviewee a chance to tell a story, relate details or give their opinion.\n\n\nAnother good option is a question that begins Tell me about a time when.... This allows the interviewee to tell you the story that's important to them, and can often produce rich information for your article.\n\n, Listening is a key component of a good interview. Don’t give too many of your own observations, but do react to what they are telling you by smiling or nodding. People are more likely to continue talking when their audience is receptive.\n\n, Part of being a good interviewer is determining when someone is finished talking about a particular subject and when it will be helpful to prompt them for further discussion. You can also use your follow up questions to make connections between ideas.\n\n, Make observations and notes immediately when you’re finished with the interview when it is fresh in your mind. These might be observations about the location, what the person looked like, what they were doing or how they were carrying themselves.\n\n, Transcribing, or typing out the entire interview, can be a tedious task. It is essential for getting quotes correct, however, and it can be very helpful to be able to read what your interviewee said. Do this yourself or pay someone to transcribe for you.\n\n, Thank them for their time, and give them an idea of when to expect the article about them. This is also a chance when you can ask a few follow-up questions if you find you need more information.\n\n, Feature articles do not have a particular formula the way hard news articles do. You don’t need to follow the “inverted pyramid” style of writing that conveys the “who, what, where, when and why” of a news story. Instead, choose a more inventive way to write a story. Some possible formats may include:\n\n\nStart by describing a dramatic moment and then uncover the history that led up to that moment.\nUse a story-within-a-story format, which relies on a narrator to tell the story of someone else.\nStart the story with an ordinary moment and trace how the story became unusual.\n\n, Newspaper feature stories run between 500 and 2,500 words, while magazine features run from 500 to 5,000 words. Blog features run from 250 to 2,500 words.\n\n\nCheck with your editor to see how long they would like your article to be.\n\n, Start piecing together your article by reviewing your notes, selecting quotes, and drafting a structure for the article. Start with your introduction and decide how you want to build the article. What information do you want to reveal first? As you get to the conclusion, think of the overall theme or lasting impression you want to leave with the reader.Consider what you absolutely must have in the story and what can be cut. If you are writing a 500-word article, for example, you will likely need to be very selective about what you include, whereas you have a lot more space to write in a 2,500 word article.\n\n, Your first paragraph is your chance to hook your reader and draw them into your story. If the opening paragraph is dry or difficult to follow, you will lose your reader and they won’t continue on to the rest of your story.\n\n\nStart with an interesting fact, a quote, or an anecdote for a good hook.\nYour opening paragraph should only be about 2-3 sentences.\n\n, While your lead might draw people in, your second paragraph (and subsequent paragraphs) need to start explaining the reason for the story. Why are we reading this story? What is important about it?\n\n, You’ve drafted your article in outline form, which can help you stay on track to building a good feature article. The outline can also help you remember how details connect to each other and how quotes support certain points that you’re making.\n\n\nBe flexible, however. Sometimes when you write, the flow makes sense in a way that is different from your outline. Be ready to change the direction of your piece if it seems to read better that way.\n\n, By writing a feature article, you have the chance to describe people and scenes to the reader.Describe a setting or person so that the reader can clearly envision it in their mind.\n\n, While it can be tempting to include the interviewee’s own words in the story, don’t rely too much on quoting them. Otherwise, this becomes more of a straightforward interview. Write around their quotes to give them context, build the story and help the reader interpret what the interviewee is saying.\n\n, Consider the target audience of the publication for which you are writing and write to their level and interest. Do not assume they are familiar with what you’re talking about, so you may need to explain certain things. Be sure to spell out acronyms and explain jargon or slang. Write in a style that is more conversational, rather than stiff and academic., A feature article is a piece that conveys information and detail about a person or phenomenon. It is not an opportunity for you to give your opinion on a topic. Rather, your personality is conveyed through your writing style., Once you finish writing, put the article away for a day to get some distance from it. Come back to it when you are fresh and read through it all the way. Think of ways to sharpen descriptions, clarify points and streamline explanations. What areas do you need to cut out? What areas need additional information?\n\n, The last thing you want to do is write an article that does not have accurate details or information. Double-check how names are spelled, the order of events, and other pertinent details.\n\n, Not all feature writers do this, and in fact, some may argue that this can detract from the journalistic quality of a piece. But many subjects often want to see their article before it is printed in order to make sure they feel they are represented properly and fairly.\n\n\nYou can choose to incorporate or not incorporate their suggestions.\n\n, Do not detract from your feature article with misspelled words and poor grammar. Consult \"The Elements of Style,\" which is the standard for proper grammar usage.Consult \"The Associated Press Stylebook\" for style guidelines, such as how to format numbers, dates, street names, and so on., Ask a friend or colleague to read through the article. Your editor will also give you feedback. Be open to this feedback and don’t take it personally. They want you to write a good, solid article, and will give you advice on how to change, clarify or expand on what you’ve already written in order to make the best article possible.\n\n, Your publication might write headlines for you, but if you want the initial entry into the article to be reflective of your content, write a headline that does so. The headline is short and to the point, using no more than about 10-15 words, if that. A headline should be action-oriented and should convey why the story is important. It should grab the reader and draw them into the article.If you want to convey slightly more information, write a sub-headline, which is a secondary sentence that builds on the headline.\n\n, Make sure your article is submitted to your editor or to the publication on or before the deadline. Late articles usually do not get printed, and then all of your hard work will either be delayed until the next issue or not published at all.\n\n",
        "  Simple examples are constructed that show the entanglement of two qubits\nbeing both increased and decreased by interactions on just one of them. One of\nthe two qubits interacts with a third qubit, a control, that is never entangled\nor correlated with either of the two entangled qubits and is never entangled,\nbut becomes correlated, with the system of those two qubits. The two entangled\nqubits do not interact, but their state can change from maximally entangled to\nseparable or from separable to maximally entangled. Similar changes for the two\nqubits are made with a swap operation between one of the qubits and a control;\nthen there are compensating changes of entanglement that involve the control.\nWhen the entanglement increases, the map that describes the change of the state\nof the two entangled qubits is not completely positive. Combination of two\nindependent interactions that individually give exponential decay of the\nentanglement can cause the entanglement to not decay exponentially but,\ninstead, go to zero at a finite time.\n",
        "Lameira is a closed halt on the Algarve line in the Silves municipality, Portugal. It is part of the section from Algoz to Poço Barreto, which opened on the 19th of March 1900.\n\nReferences\n\nRailway stations in Portugal\nRailway stations opened in 1900",
        "Table 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\n\nThanks,\n\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).",
        " There are a number of medicines that are available to purchase over the counter that will help stomach aches. However it is important that you take the right medicine for the right symptoms. Before purchasing a medicine, consult a doctor or pharmacist and follow labels exactly. Child versions of medications are safest for teenagers as well.\n\n\nNote that if you experience daily stomach pain for several days in a row you should call your doctor and schedule an appointment to be seen. Prolonged stomach pain could be a sign of a serious health problem.;\n, Aspirin-based pain relievers can be harsh on the stomach and even cause bleeding, so avoid taking aspirin specifically. Ibuprofen and naproxen can be irritating as well. Instead of these options, take acetaminophen to relieve stomach aches.\n\n\nFor general stomach pain, call your doctor if it persists for several days or begins to worry you.\nAspirin should never be given to children or teenagers unless prescribed by your doctor due to the risk of Reye's syndrome, which can be dangerous.\n\n, Heartburn will feel like a painful burning sensation in the chest. It will usually occur after eating or while you are lying down. It is caused by acid build up in the stomach. An over the counter antacid or acid reducer will cure most cases of heartburn.If you continue to experience heartburn for more than two weeks while on over the counter medicine, or if your pain is severe, you have vomiting, or you are unable to eat due to pain, call your doctor to schedule an appointment.\nNote that antacids containing aluminum can cause constipation. Also, antacids containing magnesium can cause diarrhea.\nRead labels carefully. Adult Pepto Bismol, Kaopectate, bismatrol, and and some other drugs marketed for heartburn contain bismuth subsalicylate. Like aspirin, this drug can cause a dangerous condition called Reye's syndrome in children and teens. Do not take it without a prescription, especially if you might have the flu or another virus., Generally this means less than three bowel movements per week. Constipation is relatively common, but for some it can cause stomach pain and discomfort. A laxative or stool softener can help relieve discomfort.Check with your doctor or pharmacist on which medication to try.\n\n\nIf your constipation continues for three weeks or longer call your doctor to schedule an appointment. You should also call your doctor if you begin to lose weight or see blood in your stool., Chose one of these medicines and begin taking it, as instructed on the bottle, as soon as bleeding or cramping starts.If these medicines do not work, your doctor will be able to prescribe a stronger medicine for you.\n\n, There are several options to chose from. You can drink a cup of herbal tea after each meal to help settle an upset stomach. The following three types are worth trying:\n\n\nChamomile tea contains an anti-inflammatory agent that can help settle an upset stomach. You can pick up chamomile tea at almost any supermarket. Try having a cup of tea after meals to settle your stomach. You should drop the tea bag in hot, but not boiling, water, so as not to destroy chamomile's active ingredient.\nMint tea is a useful remedy for gas, bloating, and indigestion because it works to relax stomach muscles. Peppermint tea is available at most supermarkets, but you can also use fresh mint leaves. Simply drop the leaves in hot water and let them sit for five to 10 minutes. Enjoy this drink after meals for best effect.\nMake yourself a rice tea. Rice tea is simply rice, water and honey. Boil half a cup of rice in six cups of water for 15 minutes. Next strain the rice out of the water, saving the water in a bottle. Add a small amount of sugar or honey to the water and drink warm. Rice tea has been shown to help settle upset stomachs., Yogurt can help speed up digestion because it contains active cultures. Mix yogurt with fruit juice for a healthy snack that will aid digestion. Try one part yogurt, one part juice.\n\n\nCarrot, apple, and peach drinks work well for indigestion. Avoid acidic fruits such as orange juice, as they can be harsh on an upset stomach.\nYogurt labels will note if they contain active cultures. Be sure to only buy those with active cultures if you are using it to help with a stomach ache.\n\n, Try mixing a tablespoon of apple cider vinegar with one cup of warm water and a tablespoon of honey. This will help to reduce cramping, gas, and even heartburn., Ginger has been used for thousands of years to settle stomachs. Studies have shown that it is the anti-inflammatory properties in ginger that are so effective. Ginger can be taken fresh, in ginger capsules, ginger chews, or as ginger ale., For maximum effectiveness the pad or bottle should be around 104 °F or 40 °C. The heating pad or hot water bottle works by activating heat receptors deep in the body which, in turn, cause your body to not feel as much pain.This treatment is particularly recommended for menstrual pain.\n\n, Each body is different, so it can be difficult to give general advice about what foods should be avoided. When you eat a certain food, pay attention to how you feel afterwards. By doing this you will soon be able to pinpoint what food or foods are causing problems. Talk to your doctor to see if you may have an allergy to a certain food, gluten sensitivity, or celiac disease. Be particularly mindful about the following foods:\n\n\nProcessed foods, including fast food, white bread, sausage, donuts, hamburgers, and potato chips.\nDairy products can cause stomach aches in some people, especially if they are unknowingly lactose intolerant. Try staying off dairy products for a week to see if there is improvement, or try a soy based milk.Spicy and greasy food can irritate the stomach and should be avoided if you are experiencing stomach pains.\n\n, The best foods to help with a stomach ache are high in fiber. Your stomach aches may actually be caused by a lack of fiber in your diet. It is also important that you drink water, approximately two to three liters per day (nine to 13 cups) is the recommended amount.Foods high in fiber include fruits like bananas, vegetables like broccoli, and many whole grains. Prunes, cherries, raisins, and apricots are particularly effective. These foods will help with regular bowel movements and prevent constipation.\n\n, Healthy foods like beans, broccoli, cabbage, and yogurt make gas in your stomach and can cause stomach pain. Eat these foods in moderation. To help prevent gas chew these foods (and others) thoroughly, and do not swallow too quickly.\n\n\nDrinking ginger ale can relieve stomach aches caused by gas. After you drink, you can try to burp or pass gas to relieve pressure. Over the counter Gas-X may also help.\n\n, Overeating can cause discomfort and stomach pain, even if you eat too much of a healthy food. Try not to get all of your calories in one or two big meals; instead, spread out your calories in three meals and one to two healthy snacks. To ease the load on your stomach, here is a detailed breakdown of how many calories should be consumed by teenagers daily.14–16 year old male should have 3,100 when active or 2,300 when not active. Females should have 2,350 and 1,750, respectively.\n17–18 year old male should have 3,300 when active or 2,450 when not active. Females should have 2,400 and 1,750, respectively.\n\n, Teenagers should not be drinking alcohol, but if you are it could be a cause of stomach aches. Alcohol increases the amount of acid your stomach produces, and can lead to ulcers, acid reflux and other issues. Alcohol can also cause vomiting and diarrhea., Stomach aches can be caused by stress, anxiety, or depression. Try to reduce your stress level. Try exercising for 30 minutes each day with a long walk or jog. You can also reduce your caffeine and sugar intake, which can both lower anxiety and help your stomach feel better.\n\n\nConsider talking with a counselor if you are dealing with a lot of stress or anxiety.\n\n, If your stomach pains are caused by menstrual cramps, you will want to have plenty of rest. In addition, you will want to avoid alcohol, caffeine and smoking., The use of medicines, herbal remedies and/or lifestyle changes are not substitutes for medical care. Given that stomach aches can point to serious problems, it is vital that you know which symptoms to take seriously and know when to see a doctor.\n\n, If you are experiencing severe stomach pain that makes you unable to sit still, or if you need to curl into a ball to find relief, you need to visit the emergency room. This is especially true if the pain is on the right side of your abdomen. You should also visit the emergency room or see your doctor right away if you have any of the following symptoms:\n\n\nStomach pain with bloody stool, persistent nausea and vomiting, skin that appears yellow, swelling in your abdomen, or tenderness in your abdomen.If you have stomach pain after an injury or car accident.\nSee your doctor right away if you have stomach pain and suspect you may be pregnant.a\n\n, If your stomach ache persists for a few days or begins to worry you, it is time to see a doctor. You should also call your doctor is you have heartburn that lasts for several weeks with no improvement from over the counter medicine. Also, call your doctor if the stomach pain is accompanied with fever and headache, poor appetite, weight loss, or pain with urination.\n\n, You should also call your doctor if cramping is severe.",
        "This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.\n\nThe paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.\n\nThis paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. \n\nAlthough I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\n\nMinor comments:\n\nI find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?\n\nIn the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?\n\n\n\n\n\n\n\n\n ",
        "  New high-resolution observations reveal that small-scale magnetic flux\nconcentrations have a delicate substructure on a spatial scale of 0.1''. Its\nbasic structure can be interpreted in terms of a magnetic flux sheet or tube\nthat vertically extends through the ambient weak-field or field-free atmosphere\nwith which it is in mechanical equilibrium. A more refined interpretation comes\nfrom new three-dimensional magnetohydrodynamic simulations that are capable of\nreproducing the corrugated shape of magnetic flux concentrations and their\nsignature in the visible continuum. Furthermore it is shown that the\ncharacteristic asymmetric shape of the contrast profile of facular granules is\nan effect of radiative transfer across the rarefied atmosphere of the magnetic\nflux concentration. I also discuss three-dimensional radiation\nmagnetohydrodynamic simulations of the integral layers from the top of the\nconvection zone to the mid-chromosphere. They show a highly dynamic\nchromospheric magnetic field, marked by rapidly moving filaments of stronger\nthan average magnetic field that form in the compression zone downstream and\nalong propagating shock fronts. The simulations confirm the picture of flux\nconcentrations that strongly expand through the photosphere into a more\nhomogeneous, space filling chromospheric field. Future directions in the\nsimulation of small-scale magnetic fields are indicated by a few examples of\nvery recent work.\n",
        "  We replace the ice Ansatz on matrix solutions of the Yang-Baxter equation by\na weaker condition which we call \"rime\". Rime solutions include the standard\nDrinfeld-Jimbo R-matrix. Solutions of the Yang--Baxter equation within the rime\nAnsatz which are maximally different from the standard one we call \"strict\nrime\". A strict rime non-unitary solution is parameterized by a projective\nvector. We show that this solution transforms to the Cremmer-Gervais R-matrix\nby a change of basis with a matrix containing symmetric functions in the\ncomponents of the parameterizing vector. A strict unitary solution (the rime\nAnsatz is well adapted for taking a unitary limit) is shown to be equivalent to\na quantization of a classical \"boundary\" r-matrix of Gerstenhaber and\nGiaquinto. We analyze the structure of the elementary rime blocks and find, as\na by-product, that all non-standard R-matrices of GL(1|1)-type can be uniformly\ndescribed in a rime form.\n  We discuss then connections of the classical rime solutions with the Bezout\noperators. The Bezout operators satisfy the (non-)homogeneous associative\nclassical Yang--Baxter equation which is related to the Rota-Baxter operators.\n  We classify the rime Poisson brackets: they form a 3-dimensional pencil. A\nnormal form of each individual member of the pencil depends on the discriminant\nof a certain quadratic polynomial. We also classify orderable quadratic rime\nassociative algebras.\n  For the standard Drinfeld-Jimbo solution, there is a choice of the\nmultiparameters, for which it can be non-trivially rimed. However, not every\nBelavin-Drinfeld triple admits a choice of the multiparameters for which it can\nbe rimed. We give a minimal example.\n",
        "\"Poor Little Fool\" is a song written by Sharon Sheeley and first recorded by Ricky Nelson in 1958.\n\nBackground\nSheeley wrote the song when she was 15 years old. She had met Elvis Presley, and he encouraged her to write. It was based on her disappointment following a short-lived relationship with Don Everly of The Everly Brothers. Sheeley sought Ricky Nelson to record the tune. She drove to his house, and claimed her car had broken down. He came to her aid, and she sprang the song on him. Her version was at a much faster tempo than his recording.\n\nThe song was recorded by Ricky Nelson on April 17, 1958, and released on Imperial Records through its catalog number: 5528. The recording features the background vocals of the Jordanaires.  On August 4, 1958, it became the first number-one song on Billboard magazine's then-new Hot 100 chart, replacing the magazine's Jockeys and Top 100 charts. It spent two weeks at the number-one spot. It also reached the top 10 on the Billboard Country and Rhythm and Blues charts. Following its success, Sheeley worked with Eddie Cochran.\n\n\"Poor Little Fool\" became a radio hit when it was released as part of a four-song extended-play 45 rpm disc, which was excerpted from the artist's second LP, Ricky Nelson. Responding to the buzz, Lew Chudd, the founder and head of Imperial Records, rushed out a single version (on both 45 and 78 rpm). Nelson objected, however, believing that the move would hurt sales of the EP. Under his contract with Imperial, the singer had approval rights for all picture-sleeve art, so to express his displeasure with Chudd's decision, he chose not to select a photograph for the \"Poor Little Fool\" single. As a result, \"Poor Little Fool\" was the only Ricky Nelson single released by Imperial to be issued in the United States without a photo in a plain-label, cut-out sleeve.\n\nCharts\n\nOther versions\nThe \"Dodgers\" and Johnny Angel released a cover version of the song in 1958 on Skyway 45-119-AA. \nThe Fleetwoods recorded it in 1962.  \nTerry Black released a version of the song in 1965 on his debut album, Only 16, and it reached number six in Canada.\nFrank Mills released a version in 1972 in Canada.  It was one of  two of his mainly instrumental recordings to include his own vocals.\n\nSee also\nList of Billboard Hot 100 number-one singles from 1958 to 1969\n\nReferences\n\n1958 songs\n1958 singles\n1965 singles\nSongs written by Sharon Sheeley\nRicky Nelson songs\nThe Fleetwoods songs\nTerry Black songs\nBillboard Hot 100 number-one singles\nImperial Records singles",
        "Control is the fourth studio album released by Dutch band Kensington. It was released on October 28, 2016, by Universal Music.\n\nRecording \nThe band hired American producer Michael Beinhorn to assist with the recording and production of the album.\n\nTrack listing\n\nCharts\n\nWeekly charts\n\nYear-end charts\n\nCertifications\n\nReferences \n\n2016 albums",
        " Find a font on Microsoft Word or another word processing program, print out the letters, and begin tracing them. Practice tracing these letters on a blank piece of paper.First trace the letters on a lined sheet of paper and then move to a blank sheet.\nSome pens work better than others. Try gel ink pens and calligraphy pens, however, if you cannot find those then a pencil will work just fine but will not have quite the same.\nA light box will be very useful for tracing the letters.;\n, Instead of printing, try writing your daily notes, homework and other handwriting in a distinctive cursive style. Emphasize the loops in letters and try not to jumble words together. Take your time and really concentrate on making your cursive look pretty.\n\n, It might help your writing if you practice copying something that you enjoy reading. In your elegant cursive, write what you are reading into your journal or notebook. Take your time and emphasize your style. You can also use line paper if you are still trying to master your style.\n\n, Elegant handwriting can be thicker or thinner depending on the type of writing utensil used. Markers are going to give you a larger font, while pens and pencils will be skinnier. Try out different writing tools and find one that works for you.\n\n\nYour art teacher or an employee at an art supply store should be able to recommend different writing tools for you to use.\n\n, Buy a lined paper that has an extra-large space for writing. Think of the kind of paper that you learned to write on when you were little. Slowly write your elegant letters, being sure to fill up the entire space. With practice, you will eventually be able to do this without the lines.\n\n\nTake your time and really concentrate on filling the space.\n\n, By practicing this visual art, you will learn how to write multiple styles of elegant handwriting. Calligraphy teaches you how to write elegantly with different tools in different medium. You will also develop the skills produce your own unique styles and fonts. If you stick with it, you will eventually be able to print, paint or chisel beautiful words on almost anything. There are workbooks and classes that you can take online to learn calligraphy.\nYour local community center or YMCA may also offer classes in calligraphy.\n\n, The muscles in the arm, shoulder, chest and back should be conducting all of the action while you write. Your forearm, hand and fingers should be still. Writing from your shoulder results in movement that is more intricate and gives you greater stamina. It also creates smoother and cleaner looking writing.\n\n\nThis movement will likely feel unnatural at first but it is crucial to developing your writing.\nAvoid writing with your fingers. The majority of people “draw” their letters by moving the pen across the paper with their hand. This method is more tiresome and gives you less control over your writing.\nBe conscious of writing with your shoulder, making it a part of your writing practice.\n\n, Concentrate on writing using your shoulder and not your hand. Practice this until it feels natural. As you become more comfortable with the movement, make the letters smaller.You can also practice this method on a marker or chalk board.\n\n, Start with big letters and gradually move to smaller ones. Again, remember to avoid writing with your fingers and concentrate on your shoulder.\n\n, Start making Xs and ///s and OOOs and overlapped OOOs and spirals and | | | |s. Work on making this figures smoothly, uniformly, and evenly spaced. Practice drawing these figures daily emphasizing your strokes and movement. Repetition is important so practice these movements daily.\n\n, Focus on making clear, well-formed letters and words. Hurriedly writing results in messy or illegible writing. Remember that you want to write elegantly and that takes time., If you begin to get sore or fatigued, your writing will suffer. Get out of your chair and walk around. Stretch the muscles in your shoulder and hand. , There is not magic bullet for developing elegant handwriting. Daily practice is the only way to hone your skill and, luckily, there are many strategies for doing this.Set aside at least 20 minutes a day to write. Begin with five minutes and work your way up.\nFollow the curriculum laid out in a handbook or online lesson. These tend to have daily exercises as part of their instruction.\nWrite things out by hand more. Hand-write your notes and grocery lists. Start a journal and send written letters to friends and family. Keep a pocketbook.\n\n, Hold it near the writing end. It is important to have the proper grip on your writing utensil. Think of your hand as the vice that holds the pen in place while you write. Try not to rest it between the thumb and index finger with the barrel on the middle finger.\nAvoid placing the pen between thumb and the index and middle fingers with the pen resting on you ring finger.\n\n, Relax your hand while you hold the pen. Your grip needs to be firm but not so tight that your hand is tense. Having a loose grip helps relax your arm and give it greater dexterity. Imagine that the pen is made of soft rubber and that squeezing it too tightly will cause ink to blot everywhere.\n\n, Do not curl your hand over and write to the left of your palm. That position can leading to hand cramping and discomfort.This is an important tip for lefties to remember since they are more likely to curl their hand.\n\n, This will help you steady yourself and provide better control to your dominant hand. It will also keep the paper from moving around. Be sure you are practicing on a surface that has space to balance yourself with your other arm.\n\n\nThink about your dominant hand and position yourself in a way that gives you room to both balance and write.\n\n, The paper position ensures the correct slant of your letters. Your letters should ideally have a 35-degree slant.If you are right-handed, the bottom-left corner of the paper should be aligned with the top-right.\nIf you are left-handed, the top-left corner should be aligned with the bottom-right corner.\n\n, Keep your spine straight and be sure to have enough space to move your shoulder freely. Try to stay loose and not be too tense while writing. Find a comfortable hard-backed chair to sit on.Avoid sitting on a couch or a recliner.\n\n, It needs to feel comfortable to hold and move across the paper smoothly without requiring you to push it too hard on the paper. Depending on how elegant you want your handwriting to look, you may want to invest in different writing utensils.Fountain pens are believed to be the superior tool for handwriting. The expenses associated with a fountain pen are slightly higher due to the need to buy ink.\nMechanical pens and pencils are also great choice. They allow you to try out different sized nibs and thicknesses, as well as colors.\nIf you are left-handed, there are special pens designed just for you.\nFor those looking for a more rustic look, a quill might be worth looking into.\n\n, For practicing, you will want grid-lined paper because it will help you write lower and upper case letters, as well as track your progress. There are colored and raised papers for the visual perceptual challenged., If you have access to it, a drawing or drafting table is ideal for handwriting. Experts recommend writing at a 45-degree angle; however, a kitchen table or office desk will also work.Depending on the type of handwriting you are doing you might want to invest in a light box, which will help you write on paper without a grid., These workbooks have daily exercises to help you refine your writing. Depending on your skill level, there are also advanced books teach specific styles and scripts. You can take handwriting courses online and some tablets have tools that will help you practice your writing.\n\n",
        "Probably yes (at least most important to be aware of, since it tends to be skipped most often), although most studies only research breakfast as part of a continous, healthy diet where you eat at least three times a day. This means that lunch skipping may be equally bad.\n\n[This study](_URL_0_) summarized the results of 47 studies.\n\n* Breakfast eaters generally consumed more daily calories yet were **less likely to be overweight**, although not all studies associated breakfast skipping with overweight. \n\n\n* Evidence suggests that breakfast consumption may **improve cognitive function** related to memory, test grades, and school attendance. \n\n\n* Breakfast as part of a healthful diet and lifestyle can positively impact children’s health and well-being. \n\n\n* We advocate consumption of a healthful breakfast on a daily basis consisting of a variety of foods, especially high-fiber and nutrient-rich whole grains, fruits, and dairy products.\n\n[Another study](_URL_1_) suggests an inversely associaten between breakfast frequency and obesity and chronic disease.\n\nEDIT: foreword",
        "  The cycling operation is a special kind of conjugation that can be applied to\nelements in Artin's braid groups, in order to reduce their length. It is a key\ningredient of the usual solutions to the conjugacy problem in braid groups. In\ntheir seminal paper on braid-cryptography, Ko, Lee et al. proposed the {\\it\ncycling problem} as a hard problem in braid groups that could be interesting\nfor cryptography. In this paper we give a polynomial solution to that problem,\nmainly by showing that cycling is surjective, and using a result by Maffre\nwhich shows that pre-images under cycling can be computed fast. This result\nalso holds in every Artin-Tits group of spherical type.\n  On the other hand, the conjugacy search problem in braid groups is usually\nsolved by computing some finite sets called (left) ultra summit sets\n(left-USS), using left normal forms of braids. But one can equally use right\nnormal forms and compute right-USS's. Hard instances of the conjugacy search\nproblem correspond to elements having big (left and right) USS's. One may think\nthat even if some element has a big left-USS, it could possibly have a small\nright-USS. We show that this is not the case in the important particular case\nof rigid braids. More precisely, we show that the left-USS and the right-USS of\na given rigid braid determine isomorphic graphs, with the arrows reversed, the\nisomorphism being defined using iterated cycling. We conjecture that the same\nis true for every element, not necessarily rigid, in braid groups and\nArtin-Tits groups of spherical type.\n",
        "Nice work! I am curious about the SSL experiments: since ",
        "This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what \"unsupervised\" means here since the \"causality prior\" uses reward signals for training. This is reinforcement learning, not unsupervised learning.\n\nThe experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.\n\nIn conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.\n\nOverall this paper is confusing and premature.",
        "\"Mind-wandering\", an extremely common phenomenon, but understudied subject in cognitive and educational psychology. MW can take the form of task-related (e.g., imagining scenarios utilizing current material) or task-unrelated (e.g., dwelling on relationship issues, dinner planning, etc.). MW can be mitigated by reaffirming your internal motivations (e.g., interest in the topic) or external motivations (e.g., upcoming exams, being paid to absorb material). However, MW has also been correlated to higher creativity measures, so it ain't all bad.\nSource: cognitive neuroscience researcher.\n\nEDIT: Man_Fred_Beardman 's comment below is insightful for those interested in the cognitive mechanisms at play during mind-wandering:\n\"To expand a little on why you don't remember what you read, you have to understand memory is divided into three categories: Short-Term Memory (which only lasts approximately 4-7 seconds), Long-Term Memory (memories beyond 7 seconds), and Working Memory (memories being processed from short-term to long-term memory).\nWhen you're reading and your mind becomes preoccupied with something else (like day dreaming/mind wandering) your working memory kind if shuts down (or at least focuses on processing your daydream into long-term memory instead of what you just read). So you end up remembering your day dream, but not what you read. Your short-term memory is still working, so you end up remembering the last few words you read, but nothing else.\"\n\nEDIT 2: Thank you kind stranger for the gold!",
        "A New Testament minuscule is a copy of a portion of the New Testament written in a small, cursive Greek script (developed from Uncial).\n\nLegend\n The numbers (#) are the now standard system of Caspar René Gregory, often referred to as the Gregory-Aland numbers.\n Included among the cataloged minuscules are the following types of manuscripts, color coded:\n\n Dates are estimated to the nearest 100 year increment where specific date is unknown.\n Content generally only describes sections of the New Testament: Gospels, The Acts of the Apostles (Acts), Pauline epistles, and so on. Sometimes the surviving portion of a codex is so limited that specific books, chapters or even verses can be indicated. Linked articles, where they exist, generally specify content in detail, by verse.\n Digital images are referenced with direct links to the hosting web pages, with the exception of those at the INTF. The quality and accessibility of the images is as follows:\n\n† Indicates the manuscript has damaged or missing pages.\nP Indicates only a portion of the books were included. \nK Indicates manuscript also includes a commentary.\nS Indicates lost portions of manuscript replaced via supplement of a later hand.\nabs (abschrift) Indicates manuscript is copy.\n[ ] Brackets around Gregory-Aland number indicate the manuscript belongs to an already numbered manuscript, was found to not be a continuous text manuscript, was found to be written in modern Greek versus Koine Greek, was proved a forgery, or has been destroyed.\n Minuscules 2501–2600\n\nSee also \n\n List of New Testament papyri\n List of New Testament uncials\n List of New Testament minuscules (1–1000)\n List of New Testament minuscules (1001–2000)\n List of New Testament minuscules (2001–)\n List of New Testament minuscules ordered by Location/Institution\n List of New Testament lectionaries\n\nReferences\n\nBibliography \n \n \n\n2501\nNew Testament-related lists\nGreek New Testament manuscripts\nLiterature lists",
        " Nobody wants to hang out with some poser scene kid. Listen to the music while following the steps below.\n\n\nIf you like metal, listen to Death-core/Metal-core bands like Bring Me the Horizon, Suicide Silence, Chelsea Grin, Asking Alexandria, and the Acacia Strain.\nIf you're more interested in pop, listen to something still electronic-based and danceable but edgier than a soccer mom would listen to like Panic At The Disco, Hellogoodbye, Lady Gaga (Yes, really) and deadmau5 if you like Dubstep.\nWant something mellow/soft? Try indie like Death Cab For Cutie, Ellie Goulding, LIGHTS, and Foxtrot.\nIf you're looking for something in between, try post-hardcore & Screamo. Examples include A Sky lit Drive, Sky Eats Airplane, Escape the Fate, Breathe Carolina (electronic), Attack Attack, etc.\nA good website to go on to find bands is www. taste-kid.com. It has you type in a band you already like, and it'll give you even more suggestions.;\n, The most important part of the scene look is the hair. You can get away with any clothes as long as your hair is perfect. The typical scene hairstyle is unisex and works for all lengths/hair types. Just make sure you have short layers at the top and side bangs. Cut them yourself if you can. Girls can also have straight bangs. Don't forget to get your hair thinned out.\n\n, Straighten your hair completely flat. If your hair seems frizzy, add shine serum. If you're a boy with longer hair, stop here because you're done. Everyone else, grab those teasing combs. Only tease the uppermost shortest layers if you have short hair. If you have long hair, tease the upper half of your head. Everybody also don't forget to hairspray! Obviously, don't tease your bangs.\n\n\nGuys should never ever wear a ponytail. Short haired girls are allowed to skip the teasing step and wear a deer tail.Girls with long hair can wear a ponytail with all of the hair tied to either side of you, directly behind your ear, and your bangs remaining in their original place. Girls with any hair length can wear pigtails.\nWhen styling hair, it is important that all hair on the front part of your head remain flat. Use your ear as a guide. Nothing in front of it should be moved when creating your hairstyles.\n\n, Sure, you can go ahead and dye your whole head green but it's a lot easier/healthier/trendier to add a few streaks of any color into your hair using extensions. Fashion core needs to be up to the minute, so this is the way to go. At the moment neon is in style, but check out hair on the runways to see what's in the next season. Also, some people add coontails to their hair, which will probably be on the outs by the time you'll read this. Either go to your local hair supply store and buy the extensions or go to Sally's or even buy them on Myspace. You can glue them to your hair or clip them in. Just make sure to trim the extension to the length of your actual hair when doing streaks. Make it look real. Its not that difficult.\n\n, Buy tightest pants possible. Add tight graphic tee, band tee, or v neck. Go to next step. Ladies have the real options. Here are a few looks a scene girl could try out:\n\n\nA casual look can be skinny jeans in a black/light wash/ripped/neon variety with a tight graphic or band tee.\nYou could substitute t-shirts with other tops like beater tanks or regular tanks in the summer. Try one white or black tank with a neon color underneath.\nAnother idea for the summer is to swap out the skinny jeans for shorts or Bermudas in black and white with prints.\nA more glam look can be leggings with an oversize graphic or band t shirt. Just make sure the shirt is large enough to cover your crotch but not so large that it is actually a dress. Find the perfect medium between thug tee and baby doll.\nDresses are also becoming more popular in the 2010/11 scene. The most popular at the moment can be found at Rue 21 and Delia's.\n\n, This is a unisex look.\n\n\nIn colder climates, bomber jackets and fur puffy vests are OK.\nLimit hats because they crush the hair. If one's needed, choose something bright, beanies, or berets. Scarves and earmuffs are acceptable.\n\n, Seriously, no guy-liner unless you're actually a rock star. You will still need to be flawless. Drink 8 glasses of water each day. It really does clear up skin, even better than Proactiv. (You should still wash your face every night!) Wear colorful nail polish. Really, anything will do. Black is acceptable but overused and expected. Guys should not paint their nails as that is way too 1992 to function. Girls should follow the following makeup guidelines:\n\n\nFor the casual look, start with whatever you usually use to cover blemishes. Add blush and cheek highlighter. Smooth a white eyeshadow underneath your eyebrow and above your top lash. Then apply a black eyeliner on top and bottom and follow with 2 coats of mascara.\nWhen getting dressed up, use a color eyeshadow on your lids and follow the rest of the directions for the casual look. The eyeshadow can be black or neon. Use whatever colors compliment your eye color. Use blue for blue eyes, pink for hazel, lavender for brown, and purple for green eyes. Or wear any color, as long as you look amazing!\nWhen leaving the house without makeup, it is mandatory you wear sunglasses to cover this fact up.\n\n, Unisex shoes are great for casual looks. These include Converse, (although Converse have become far to much of a fad - it's best to steer clear of them.) Vans, Reebok Freestyles, and Nike Dunks. Girls can wear ballet flats or pumps with any outfit and probably get away with it. Flats are getting more and more popular. It's mandatory to own at least one pair of black flats. It's also good to have silver or white. Collect tons of different colors to wear with different outfits. TOMS shoes and moccasins are also becoming more popular. Don't wear boots unless they're flat.\n\n, Girls can wear these for a casual look as well.\nGirls also wear necklaces with over sized pendants as well as bangle bracelets. Light, dressy scarves are acceptable as well. Pearl necklaces and silver necklaces with random pendants on them are great too. Don't wear those gummy or rubber band bracelets as they are truly passe. An actual rubber band that really does tie things is an acceptable piece of jewelry for both sexes.\nGirls should also wear a hair bow or a headband. Tiaras are for birthdays only.\nGirls can wear those 3 in 1 belts if they want to glam up. Only wear the ones with snaps instead of the ones that loop through a buckle.\nAlso, if you need to wear glasses, they should be plastic. The current trend is to get your grandma's old huge plastic frames and get them filled with your prescription. If you don't have those, get prescription Ray Bans.\n\n, You could do something creative with your pictures like give yourself porcelain skin or purple eyes.\n\n, Facial piercings, plugs, and tattoos are scene too. When getting a tattoo, just make sure not to get any tribal tattoos. Nothing in this step is mandatory though.\n\n, Try not to do the hold-the-camera pose, the \"oops\" face, or the peace sign. Using a self-timer, take pictures of yourself outside, in looking out of a window, or write your best friend's name on your palm. Upload! Get a high-quality camera instead of just using the camera on your phone.\n\n\nCreate your own Myspace and be famous with lots of friends. Create proof of yourself using a picture or video. For example, you could have a photo of yourself holding a piece of paper with your URL or a video telling your URL.\n\n, Have you added everyone in your area in your age range? That's a good start, but if you want fame, which you DO, you're going to need a higher friend count than that. Go to friend adding sites. Add kids who look scene. Myspace only lets you add 400 people a day, so hop to it! Strive for a six figure friend list. Nothing in this step is mandatory. Fashion-core is about fashion. NOT about having a bazillion friends on Myspace. This actually has NOTHING to do with it, but it may help.\n\n, Add all the bands you liked on Myspace and find what dates they are coming to your town. Go to every show you can even if its local. You need to be seen to be Scene. Make sure to post bulletins telling people what show you're at and tell people to say hi to you if they see you. Bring a friend so you don't look lame and make sure you look your absolute best. Learn to do the dances you see at the shows. If you have a band, talk to the band after the show. They probably know people who can get you gigs. Again, nothing in this step is mandatory. If you're not overly obsessed with music and going to shows, you don't have to. Be yourself. Don't try to be something you're not, or you will be labeled as a poser.\n\n, An iPod touch works well too. Decorate it by adding small rhinestones, using Sharpies, and adding cell phone charms. The more cell phone charms, the better!\n\n, Doing this ensures that everyone knows where all the action is at. After a fun night, post a blog detailing all the upscale things you did. They don't need to be upscale, just make them seem like they are. For example, if you go to McDonald's, say you went to a restaurant with friends. Don't divulge every detail of your life. Make yourself seem mysterious.\n\n, If you do it too often or too little, people will find you boring. Try to post pictures with your hot friends, but just make sure you are the best looking one in the picture.\n\n, Try not to watch television if possible. Go to coffee shops, techno clubs, shows, the mall, the movies, whatever. Just don't be boring.\n\n, Also read NYLON and Miss Behave magazines.\n\n, Just because it's scene to listen to a certain band does NOT mean you have to if you don't want to.\n\n",
        ";\n, Choose a sturdy table or counter in a dark room that is free of noise, vibration, air currents, and small movements (creaky floors, etc.). If you don't have a sturdy table, a concrete basement floor works well.\n\n, Place the subject securely on the sturdy table. If you have a computer mouse pad or tray of sand, place the subject on top of that. This helps minimize any vibrations.\n\n, Place it about 30 centimeter (11.8 in) away the subject by bracing it with a clothespin (UK clothes peg) and then sticking the clothespin into a cup of salt or sugar.\n\n\nIf your diode laser has an adjustable lens, take off that lens and position the laser so that its beam spreads out horizontally in an elliptical shape (it looks similar to that of a loaf of bread).\nIf your laser doesn't have an adjustable lens, secure an optical diverging lens to another clothespin and cup of salt so that the laser beam shines through it to spread the light.\n\n,, You can use a nightlight placed under the table or even slightly crack open the door to see in the darkened room. Block any direct light from reaching the holography system. The room should be dark enough that one cannot read.\n\n, The book will serve like a shutter of a camera.\n\n, Carefully lean it against the object. Wait 10-20 seconds to let the plate settle against the object.\n\n,,,, This process takes about 3-5 minutes total. For making basic holograms, the process of developing is actually quite simple:\nMix the dried powder photochemicals with distilled bottle water to form two solutions: the developer and the bleach\n\n\n\n\n\n\nDip and wiggle plate in developer for 20 seconds\n\n\n\n\n\n\nRinse in water for 30 seconds\n\n\n\n\n\n\nDip and wiggle plate in bleach for 20 seconds\n\n\n\n\n\n\nRinse in water for 30 seconds\n\n\n\n\n\n\nDry with hair dryer\n\n\n\n\n\n\n\n\nThe dried chemical powder photochemicals can be individually put together by going to a photochemist, but is more expensive since only 1 gram of several chemicals is needed, and few photochemists sell in such small amounts. The JD-4 kit recommended in this article puts all the chemicals into a kit and is happens to be the only such kit available. It is preferred by beginners.\n\n, A simple way to do this is by placing the plate on a paper towel and lean it against a wall. If time is limited, you can carefully blow warm air across the holographic plate using a hair dryer from at least foot (30cm) away. Avoid high heat.\n\n, Shine the spotlight from the same angle your laser beam shone on the plate during exposure. You can't use diffused light sources such as frosted bulbs and fluorescent lamps.\n\n",
        " For health and safety reasons don't give out any personal information unless you 100% sure. Don't send pictures of yourself unless you TRULY know the person. Call them to see if their voice is right, because it could be an impersonation.;\n, There are some people who will use fake pictures and even create a whole identity pretending to be that person. One of the easy ways to spot this is if the person has very few pictures of themselves. The more difficult fake profiles to spot are the people who will actually use pictures of friends they know or pictures they find off the internet. Don't assume that just because the person has pictures of a night out with their friends or family that it is 100 percent that person. Until you see them on webcam at the very least, there is always that chance you could be misled.\n\n, Let them know that you're thinking of them at random times. Be wary of giving out any information to the person that they can use to find you before you both decide to meet up in person. Texting allows you to keep communication open with the person while at the same time you're letting them know that you're not just sitting at home waiting for them to go on Facebook or MSN during your every spare minute. Have fun with the texting, send each other pictures of yourselves.\n\n,, Even if its online you will be able to flirt but not to the full capacity as you would in real life.\n\n, People use Facebook for different reasons. Ask the person if they mind if you write flirty messages on their wall. Some people have co-workers or family members that may not be so understanding to read about how much you love hearing how their voice gets 'breathless' when you chat on the telephone for example.\n\n, This could make it easier to help make your decision and get to know the him features like laugh, smile and voice.\n\n, For example make your messages be more emotional, or show him/her that your interested in them.\n\n,, You don't want to waste your time getting romantic feelings for someone who never plans on meeting you in person at a later date because you both live too far apart.\n\n, If you're not able to use something like Skype or MSN and you want to do it on Facebook, try inbox a message that will show that you really care and love for him/her. Or you can do this on the chat but you will need to build the conversation up before asking them out because this can be make you look weird and it can possibly make it awkward.\n\n, Remember, you don't want to feel like you're dating someone you've never even met.\n\n",
        "The paper presents a clean framework for optimizing for the network size during the training cycle. While the complexity of each iteration is increased, they argue that overall, the cost is significantly reduced since we do not need to train networks of varying sizes and cross-validate across them. The reviewers recommend acceptance of the paper and I am in agreement with them.",
        "The reviewers agree that the paper pursues an interesting direction to explore active example selection for CNN training, but have unanimously raised serious concerns with regards to overall presentation which needs further improvement (I still see spelling/grammatical errors/sloppy notation in the latest draft). Some sections in the paper are hard to follow. With regards to technical motivation, the link between depth and need for active example selection is alluded to, but not properly explained in the paper. The PCs think that this paper has too many areas in need of improvement to be accepted to the conference.",
        "Review, ACL 2017, paper 256:\n\nThis paper extends the line of work which models generation in dialogue as a\nsequence to sequence generation problem, where the past N-1 utterances (the\n‘dialogue context’) are encoded into a context vector (plus potential\nother, hand-crafted features), which is then decoded into a response: the Nth\nturn in the dialogue. As it stands, such models tend to suffer from lack of\ndiversity, specificity and local coherence in the kinds of response they tend\nto produce when trained over large dialogue datasets containing many topics\n(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce\ndiverse responses using the decoder, e.g. through word-by-word beam search\n(which has been shown not to work very well, even lose crucial information\nabout grammar and valid sequences), or via a different objective function (such\nas in Li et. al.’s work) the authors introduce a latent variable, z, over\nwhich a probability distribution is induced as part of the network. At\nprediction time, after encoding utterances 1 to k, a context z is sampled, and\nthe decoder is greedily used to generate a response from this. The evaluation\nshows small improvements in BLEU scores over a vanilla seq2seq model that does\nnot involve learning a probability distribution over contexts and sampling from\nthis.\n\nThe paper is certainly impressive from a technical point of view, i.e. in the\napplication of deep learning methods, specifically conditioned variational auto\nencoders, to the problem of response generation, and its attendant difficulties\nin training such models. Their use of Information-Retrieval techniques to get\nmore than one reference response is also interesting. \n\nI have some conceptual comments on the introduction and the motivations behind\nthe work, some on the model architecture, and the evaluation which I write\nbelow in turn:\n\nComments on the introduction and motivations…. \n\nThe authors seem not fully aware of the long history of this field, and its\nvarious facets, whether from a theoretical perspective, or from an applied one.\n\n1. “[the dialogue manager] typically takes a new utterance and the dialogue\ncontext as input, and generates discourse level decisions.” \n\n        This is not accurate. Traditionally at least, the job of the dialogue\nmanager is to select actions (dialogue acts) in a particular dialogue context.\nThe                    action chosen is then passed to a separate generation\nmodule\nfor\nrealisation. Dialogue management is usually done in the context of task-based\nsystems which are goal driven. The dialogue manager is to choose actions which\nare optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few\nsteps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer\nand colleagues, and various publications from Steve Young, Milica Gasic and\ncolleagues for an overview of the large literature on Reinforcement Learning\nand MDP models for task-based dialogue systems.\n\n2. The authors need to make a clear distinction between task-based,\ngoal-oriented dialogue, and chatbots/social bots, the latter being usually no\nmore than a language model, albeit a sophisticated one (though see Wen et. al.\n2016). What is required from these two types of system is usually distinct.\nWhereas the former is required to complete a task, the latter is, perhaps only\nrequired to keep the user engaged. Indeed the data-driven methods that have\nbeen used to build such systems are usually very different. \n3. The authors refer to ‘open-domain’ conversation. I would suggest that\nthere is no such thing as open-domain conversation - conversation is always in\nthe context of some activity and for doing/achieving something specific in the\nworld. And it is this overarching goal, the overarching activity, this\noverarching genre, which determines the outward shape of dialogues and\ndetermines what sorts of dialogue structure are coherent. Coherence itself is\nactivity/context-specific. Indeed a human is not capable of open-domain\ndialogue: if they are faced with a conversational topic or genre that they have\nnever participated in, they would embarrass themselves with utterances that\nwould look incoherent and out of place to others already familiar with it.\n(think of a random person on the street trying to follow the conversations at\nsome coffee break at ACL). This is the fundamental problem I see with systems\nthat attempt to use data from an EXTREMELY DIVERSE, open-ended set of\nconversational genres (e.g. movie subtitles) in order to train one model,\nmushing everything together so that what emerges at the other end is just very\ngood grammatical structure. Or very generic responses. \n\nComments on the model architecture:\n\nRather than generate from a single encoded context, the authors induce a\ndistribution over possible contexts, sample from this, and generate greedily\nwith the decoder. It seems to me that this general model is counter intuitive,\nand goes against evidence from the Linguistic/Psycholinguistic literature on\ndialogue: this literature shows that people tend to resolve potential problems\nin understanding and acceptance very locally - i.e. make sure they agree on\nwhat the context of the conversation is - and only then move on with the rest\nof the conversation, so that at any given point, there is little uncertainty\nabout the current context of the conversation. The massive diversity one sees\nresults from the diversity in what the conversation is actually trying to\nachieve (see above), diversity in topics and contexts etc, so that in a given,\nfixed context, there is a multitude of possible next actions, all coherent, but\nleading the conversation down a different path.\n\nIt therefore seems strange to me at least to shift the burden of explaining\ndiversity and coherence in follow-up actions to that of the\nlinguistic/verbal/surface contexts in which they are uttered, though of course,\nuncertainty here can also arise as a result of mismatches in vocabulary,\ngrammars, concepts, people’s backgrounds etc. But this probably wouldn’t\nexplain much of the variation in follow-up response. \n\nIn fact, at least as far as task-based Dialogue systems are concerned, the\nchallenge is to capture synonymy of contexts, i.e. dialogues that are distinct\non the surface, but lead to the same or similar context, either in virtue of\ninteractional and syntactic equivalence relations, or synonymy relations that\nmight hold in a particular domain between words or sequences of words (e.g.\n“what is your destination?” = “where would you like to go?” in a flight\nbooking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon,\n2016 - the latter use a grammar to cluster semantically similar dialogues.\n\nComments on the evaluation:\n\nThe authors seek to show that their model can generate more coherent, and more\ndiverse responses. The evaluation method, though very interesting, seems to\naddress coherence but not diversity, despite what they say in section 5.2:\n\nThe precision and recall metrics measure distance between ground truth\nutterances and the ones the model generates, but not that between the generated\nutterances themselves (unless I’m misunderstanding the evaluation method).\nSee e.g. Li et al. who measure diversity by counting the number distinct\nn-grams in the generated responses.\n\nFurthermore, I’m not sure that the increase in BLEU scores are meaningful:\nthey are very small. In the qualitative assessment of the generated responses,\none certainly sees more diversity, and more contentful utterances in the\nexamples provided. But I can’t see how frequent such cases in fact are.\n\nAlso, it would have made for a stronger, more meaningful paper if the authors\nhad compared their results with other work, (e.g. Li et. al) that use very\ndifferent methods to promote diversity (e.g. by using a different objective\nfunction). The authors in fact do not mention this, or characterise it\nproperly, despite actually referring to Li et. al. 2015.",
        "This paper revives a classic idea involving regularization for purposes of compression for modern CNN models on resource constrained devices. Model compression is hot and we're in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early 90s to obtain competitive results on standard benchmarks.\n\nThere's not too much to say here: this study is an instance of a simple idea applied effectively to an important problem, written up in an illuminating manner with appropriate references to classic approaches. The addition of the filter visualizations enhances the contribution.",
        "This work proposes to augment normal gradient descent algorithms with a \"Data Filter\", that acts as a curriculum teacher by selecting which examples the trained target network should see to learn optimally. Such a filter is learned simultaneously to the target network, and trained via Reinforcement Learning algorithms receiving rewards based on the state of training with respect to some pseudo-validation set.\n\n\nStylistic comment, please use the more common style of \"(Author, year)\" rather than \"Author (year)\" when the Author is *not* referred to or used in the sentence.\nE.g. \"and its variants such as Adagrad Duchi et al. (2011)\" should be \"such as Adagrad (Duchi et al., 2011)\", and  \"proposed in Andrychowicz et al. (2016),\" should remain so.\n\nI think the paragraph containing \"What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered.\" should be clarified. What is \"seeing\"? That is, you should mention explicitly that you do the forward-pass first, then compute features from that, and then decide for which examples to perform the backwards pass.\n\n\nThere are a few choices in this work which I do not understand:\n\nWhy wait until the end of the episode to update your reinforce policy (algorithm 2), but train your actor critic at each step (algorithm 3)? You say REINFORCE has high variance, which is true, but does not mean it cannot be trained at each step (unless you have some experiments that suggest otherwise, and if so they should be included or mentionned in the paper).\n\nSimilarly, why not train REINFORCE with the same reward as your Actor-Critic model? And vice-versa? You claim several times that a limitation of REINFORCE is that you need to wait for the episode to be over, but considering your data is i.i.d., you can make your episode be anything from a single training step, one D_t, to the whole multi-epoch training procedure.\n\n\nI have a few qualms with the experimental setting:\n- is Figure 2 obtained from a single (i.e. one per setup) experiment? From different initial weights? If so, there is no proper way of knowing whether results are chance or not! This is a serious concern for me.\n- with most state-of-the-art work using optimization methods such as Adam and RMSProp, is it surprising that they were not experimented with.\n- it is not clear what the learning rates are; how fast should the RL part adapt to the SL part? Its not clear that this was experimented with at all.\n- the environment, i.e. the target network being trained, is not stationnary at all. It would have been interesting to measure how much the policy changes as a function of time. Figure 3, could both be the result of the policy adapting, or of the policy remaining fixed and the features changing (which could indicate a failure of the policy to adapt).\n- in fact it is not really adressed in the paper that the environment is non-stationary, given the current setup, the distribution of features will change as the target network progresses. This has an impact on optimization.\n- how is the \"pseudo-validation\" data, target to the policy, chosen? It should be a subset of the training data. The second paragraph of section 3.2 suggests something of the sort, but then your algorithms suggest that the same data is used to train both the policies and the networks, so I am unsure of which is what.\n\n\nOverall the idea is novel and interesting, the paper is well written for the most part, but the methodology has some flaws. Clearer explanations and either more justification of the experimental choices or more experiments are needed to make this paper complete. Unless the authors convince me otherwise, I think it would be worth waiting for more experiments and submitting a very strong paper rather than presenting this (potentially powerful!) idea with weak results.\n",
        "Thomas F. Reid is an American pastor and evangelist who pastored and helped build, alongside his father, one of the largest Christian churches in the world and the Full Gospel Tabernacle located in Orchard Park, New York. Currently he resides in Colden, New York.\n\nEarly life and career\nReverend Reid was born in Buffalo, New York to pastor Al Reid and his wife, Yolanda.  After surviving polio as a young child, Tommy started full-time ministry in 1953, and travelled with his father across the world to Seoul in South Korea.  There, he assisted pastor Paul Cho at the Yoido Full Gospel Church.\n\nMinistry in the Philippines\nAfter his success in Seoul, in 1959, at the age of 26, Reid was named the senior pastor and leader of the Cathedral of Praise, then the Bethel Temple of Manila, the largest pentecostal church in the world.\n\nReturn to Buffalo\nUpon returning to Buffalo, Tommy Reid became the senior pastor and leader of the Full Gospel Tabernacle, a small church located in South Buffalo of about 120 people. During this time, the Charismatic movement swept through Buffalo and the Full Gospel Tabernacle grew from a small 120 member congregation to a massive 800 member congregation in one week. The church quickly became the largest pentecostal church in the United States, which prompted Reid to author his book, The Exploding Church.  In 1977, the Buffalo School of the Bible was founded on the Tabernacle campus, training young people on how to become ministers and religious leaders He was named as Bishop of the Tabernacle, a title he would hold until 2013, when he appointed Eagles Wings Ministries founder Robert Stearns as his successor.  Reid then took the title of Bishop emeritus.\n\nTommy befriended many evangelists during his time as senior leader of the Full Gospel Tabernacle, including Paula White, Paul Crouch, Jim Bakker, Oral Roberts, Jack Hayford and Benny Hinn.  Hinn used to minister monthly at the Tabernacle\n\nA short film, entitled \"How to Live Out a Dream\" was made based on Reid's book of the same name.  Pastor Paul Crouch did the introduction to the film.\n\nPersonal life\nBishop Reid is married to Wanda, and has one child, Aimee Sych, who is the worship pastor at the Full Gospel Tabernacle.\n\nHe resides in Colden, New York, a small town outside of Buffalo.\n\nReferences\n\nLiving people\nAmerican evangelists\nPentecostal religious workers\nPeople from Buffalo, New York\nAmerican bishops\n1932 births",
        "  Mammoths lived in Arctic East Siberia. In this region there is not sufficient\nsunlight over the year for the growth of the plants on which these animals\nfeed. Therefore the latitude of this region was lower before the end of the\nPleistocene. As the cause of this geographic pole shift, we postulate a massive\nobject, which moved in an extremely eccentric orbit and was hot from tidal work\nand solar radiation. Evaporation produced a disk-shaped cloud of ions around\nthe Sun. This cloud partially shielded the solar radiation, producing the cold\nand warm periods that characterize the Pleistocene. The shielding depends on\nthe inclination of Earth's orbit, which has a period of 100'000 years. The\ncloud builds up to a density at which inelastic particle collisions induce its\ncollapse The resulting near-periodic time dependence resembles that of\nDansgaard-Oeschger events. During cold periods fine grained inclusions were\ndeposited into the ice. The Pleistocene ended when the massive object had a\nclose encounter with the Earth, which suffered a one per mil stretching\ndeformation. While the deformation relaxed to an equilibrium shape in one to\nseveral years, the globe turned relative to the rotation axis: The North Pole\nmoved from Greenland to the Arctic Sea. The massive object was torn to pieces,\nwhich evaporated.\n",
        "  The structural properties of L-alanine amino acid in aqueous solution and in\ncrystalline phase have been studied by means of density-functional\nelectronic-structure and molecular dynamics simulations. The solvated\nzwitterionic structure of L-alanine (+NH3-C2H4-COO-) was systematically\ncompared to the structure of its zwitterionic crystalline analogue acquired\nfrom both computer simulations and experiments. It turns out that the\nstructural properties of an alanine molecule in aqueous solution can differ\nsignificantly from those in crystalline phase, these differences being mainly\nattributed to hydrogen bonding interactions. In particular, we found that the\nlargest difference between the two alanine forms can be seen for the\norientation and bond lengths of the carboxylate (COO-) group: in aqueous\nsolution the C-O bond lengths appear to strongly correlate with the number of\nwater molecules which form hydrogen bonds with the COO- group. Furthermore, the\nhydrogen bond lengths are shorter and the hydrogen bond angles are larger for\nL-alanine in water as compared to crystal. Overall, our findings strongly\nsuggest that the generally accepted approach of extending the structural\ninformation acquired from crystallographic data to a L-alanine molecule in\naqueous solution should be used with caution.\n",
        "My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.",
        "Rehal Kalandrian is situated in Bishnah tehsil and located in Jammu district of Jammu and Kashmir. It is one of 116 villages in Bishnah Block along with villages like Rehal Dhamalian and Poondowal.\n\nReferences\n\nExternal links\nThe Rehal Kalandrian\n\nVillages in Jammu district",
        "The Yunnan box turtle (Cuora yunnanensis) is a species of turtle in the family Geoemydidae (formerly Bataguridae). It is believed to be  endemic to Yunnan, China (in Kunming and Huize) and was suspected to be extinct since the early 20th century; the last verified specimen was collected in 1940.\n\nIn 2004, a living female appeared from the pet trade in Kunming; one year later, a male from the same source and again one year later another female were found there. The validity of these specimens was doubted and many believed they were intentionally produced hybrids, a common technique in China to produce turtles that get high prices.\n\nIn 2007, He et al. sampled the three living specimens and gave the genetic proof that all three living specimens are indeed C. yunnanensis and not hybrids. In 2008, Kadoorie Conservation China, a department of Kadoorie Farm and Botanic Garden, together with the Kunming Institute of Zoology and Chinese Academy of Sciences, discovered a small wild population. The distribution of this species remains unclear, but due to its value, it is heavily sought after. Protection measures are needed to save this probably highly endangered species from its return onto the IUCN list of extinct animals.\n\nReferences\n\n Blanck, T., Zhou, T. & W. P. McCord (2006): The Yunnan box turtle, Cuora yunnanensis (BOULENGER 1906); historical background and an update on the morphology, distribution and vulnerabilities of the only known living specimens. SACALIA 13 (4), 2006: 14-35\n Blanck, T. (2005): Cuora yunnanensis (BOULENGER, 1906), the Yunnan Box Turtle, Rediscovered after One-hundred Years? CUORA Special - RADIATA 14 (2), 2005: 10-33\n He J.,T. Zhou, Rao D.-Q. & Y.-P. Zhang.2007 Molecular identification and phylogenetic position of Cuora yunnanensis. Chinese Science Bulletin.52(17):2085-2088(in Chinese)\n PARHAM, J. F., B. L. STUART, R. BOUR & U. FRITZ. 2004. Evolutionary distinctiveness of the extinct Yunnan box turtle (Cuora yunnanensis) revealed by DNA from an old museum specimen. – Proceedings of the Royal Society of Biology (Supplement), Biological Letters, London, 271: 391-394 + Electronic Appendix A, 6 pp.\n ZHOU, T. & E. ZHAO (2004): On the occurrence of living Cuora yunnanensis since fifty-eight years and its description. . Sichuan J. Zool., Chengdu 23 (4): 325-327 + 1 Plate(in Chinese)\n ZHOU, T. (2005): Discovery of a Living Male Yunnan Box Turtle, Cuora yunnanensis BOULENGER, 1906. Sichuan J. Zool., Chengdu 24 (3): 345-346 + 1 Plate(in Chinese)\n\nExternal links\n World Chelonian Trust: Cuora Gallery. Contains C. yunnanensis museum specimen photos. Retrieved 2007-SEP-01.\n\nCuora\nEndemic fauna of Yunnan\nReptiles of China\nReptiles described in 1906\nTaxonomy articles created by Polbot",
        "  We observed 34 comets using the 24 micron camera on the Spitzer Space\nTelescope. Each image contains the nucleus and covers at least 10^6 km of each\ncomet's orbit. Debris trails due to mm-sized or larger particles were found\nalong the orbits of 27 comets; 4 comets had small-particle dust tails and a\nviewing geometry that made debris trails impossible to distinguish; and only 3\nhad no debris trail despite favorable observing conditions. There are now 30\nJupiter-family comets with known debris trails, of which 22 are reported in\nthis paper for the first time. The detection rate is >80%, indicating that\ndebris trails are a generic feature of short-period comets. By comparison to\norbital calculations for particles of a range of sizes ejected over 2 yr prior\nto observation, we find that particles comprising 4 debris trails are typically\nmm-sized while the remainder of the debris trails require particles larger than\nthis. The lower-limit masses of the debris trails are typically 10^11 g, and\nthe median mass loss rate is 2 kg/s. The mass-loss rate in trail particles is\ncomparable to that inferred from OH production rates and larger than that\ninferred from visible-light scattering in comae.\n",
        "The word \"starboard\" actually precedes \"right\" and \"left\" in English usage, while \"port\" is newer. The first entry the OED has for \"starboard\" is this: \n\n >  eOE   *Acct. Voy. Ohthere  &  Wulfstan* in tr. Orosius *Hist.* (BL Add.) (1980) i. i. 14: \n\n >  He..let him ealne weg þæt weste land on ðæt steorbord  &  þa widsæ on ðæt bæcbord þrie dagas.\n\nwhereas \"right\" and \"left\" both seem to appear about 1200-1225: \n\n >  c1225  (?c1200)    *Sawles Warde* (Bodl. 34) (1938) 22   \n\n >  Þe middel sti bituhhe riht  &  luft.\n\n(If you're confused about that, sound it out -- \"the middle is between right and left\") \n\n >  a1225 (?OE)    MS Lamb. in R. Morris *Old Eng. Homilies* (1868) 1st Ser. 141   \n\n >  Þer stod a richt halue and a luft alse an castel wal.\n\nIt's generally thought that \"starboard\" is a word formed from some version of \"steering-board,\" that is, the place where one would steer a ship; we think that steering oars on e.g. Norse and northern European ships were on the right (starboard) side of the boat. \n\nWhat's interesting about that top entry is that you can see that the starboard (\"steorbord\") is opposed here to \"bæcbord,\" or back-board; by the time we get to Middle English, \"laddeborde\" has supplanted \"bæcbord\" and will slowly drift into \"larboard\": \n\n >  c1400  (▸?c1380)    *Patience* l. 106   Þay layden in on laddeborde  &  þe lofe wynnes.\n\n >  1495   in M. Oppenheim *Naval Accts.  &  Inventories Henry VII* (1896) 203  \n\n >   Devettes..j a sterbord an other a latebord.\n\n > 15..   Sir A. Barton in *Surtees Misc.* (1888) 69   \n\n >  A larborde wher Sir Andrewe lay.\n\n >  1582   R. Stanyhurst tr. Virgil *First Foure Bookes Æneis* i. 4   \n\n > Theire ships too larboord doo nod.\n\n >  1591   W. Raleigh *Rep. Fight Iles of Açores* sig. B2,   \n\n >  Two on her larboord, and two on her starboord.\n\n >  1598   in R. Hakluyt *Princ. Navigations* (new ed.) I. 4   \n\n > Vpon his steereboord alwayes the desert land, and vpon the leereboord the maine Ocean.\n\n >  a1600   Sir A. Barton in *Surtees Misc.* (1888) 68   \n\n >  Ethere bye lerbord or by lowe That Scootte would overcome yowe.\n\n >  1667   Milton *Paradise Lost* ii. 1019  \n\n >  When Ulysses on the Larbord shunnd Charybdis.\n\nThe issue with \"starboard\" and \"larboard\" is that if you're trying to quickly distinguish between them, especially with giving orders in say a storm or other place where you can't hear or be heard well, they can sound a great deal like one another. The word \"port\" starts being used in opposition to \"starboard\" around the Tudor era. Again from the OED: \n\n >  1543–4   *High Court of Admiralty Exam. 11 Jan.* (Rypper's Depos.) 92   \n\n > The sayd [ship] mighte have layed his helme a porte.\n\n >  1633   T. Stafford *Pacata Hibernia iii. viii.* 312   \n\n >  With two takles hee might steere the Hoy either to Starboard, or to Port.\n\n >  1644   H. Mainwaring *Sea-mans Dictionary*   \n\n >  To Port. Is a word used in Conding the Ship,..they will use the word steddy a-Port, or Steddy a Star-boord, the Ship heeles to Port: bring things neere to port, or the like.\n\nThe substitution of \"port\" for \"larboard\" is made official in 1844: \n\n >  1844   *Admiralty Order* 22 Nov.   \n\n >  The word ‘Port’ is frequently..substituted..for the word ‘Larboard’, and as..the distinction between ‘Starboard’ and ‘Port’ is so much more marked than that between ‘Starboard’ and ‘Larboard’, it is their Lordships direction that the word ‘Larboard’ shall no longer be used.",
        "Not at all! \n\nThe image of her as oblivious to the suffering around her is incorrect. She was charitable, and sympathetic to the plight of the poor, and she tried to raise her children to be, too. The memoirs of Madame Campan have more details. [Link to part 1](_URL_2_), and [part 2](_URL_1_).\n\nShe wasn't very interested in subjects traditionally seen as intellectual, and her early education was lacking. According to Antonia Frasers biography, she didn't like to be around people who were overtly intelligent. However, that doesn't mean that she was stupid. She was deeply involved with the politics of the court, and did what she could to influence events in Europe in line with her interests, corresponding secretly with other European monarchs and using her position to gain access to military secrets that would help them in the war against France. During her trial, she showed both her wits and her strength of character, cleverly answering the questions asked without either implicating herself or betraying her beliefs. \n\nMost people here probably know that the \"let them eat cake\" quote isn't accurate, and instead attribute it to revolutionary slander. However, I've never seen any mention of this quote in any source from the time, or any mention of it being connected to her during her lifetime. Republicans were more likely to attribute her behavior to  counterrevolutionary plotting than to stupidity. The act of accusation against her ([link](_URL_0_) in French) says\n\n > [...] the public prosecutor has raised this accusation against Marie Antoinette, in her interrogation naming herself de Lorraine d'Autriche, widow of Louis Capet, for having maliciously, and by design, \n > \n > 1) in concert with the brothers of Louis Capet and the infamous minister Calonne, squandered the finances of France in a horrifying manner, and for having sent incalculable sums to the emperor (i.e. her brother, either Joseph II or Leopold II), and for having thus dried out the national treasury. \n\nEarlier on in the same document we're told that part of this terrifying spending was to satisfy her \"disorderly pleasures\", but taken all in all I think what Fouquier-Tinville is trying to show here has more to do with plots against the state, and less to do with just really wanting more pretty dresses. Especially since her personal expenditures are always mentioned in connection with her sending money to France's enemies. Nowadays, most people don't know about her *actual* counterrevolutionary plots, which makes the Stupid Spoiled Princess trope an easier explanation for her spending. \n\nIn reality, her spending wasn't very noteworthy for a queen at the time, and considering that the king never had a mistress to spend money on, she could have afforded even more expensive habits. It certainly didn't put an actual dent in the state finances. Only 6% of the budget was spent by the court, while over 50% went to managing the debt. (Georges Lefebvre, 1789) Which is a lot of money! But most of it wasn't Marie Antoinette's choice. The court spent a lot to keep the nobles happy with their post-Louis XIV lack of political power with pensions and well-payed positions in the king's household, and the king couldn't have cut down on that without getting into trouble. The problem was also that the extravagance of the court served a political purpose, to glorify the king by displaying the wealth of the nobility and royalty. When public opinion turned against both, people were angered by their waste, instead of dazzled by diamonds and paniers. \n\nThank you so much for the gold!",
        "  Given the extensive application of classical random walks to classical\nalgorithms in a variety of fields, their quantum analogue in quantum walks is\nexpected to provide a fruitful source of quantum algorithms. So far, however,\nsuch algorithms have been scarce. In this work, we enumerate some important\ndifferences between quantum and classical walks, leading to their markedly\ndifferent properties. We show that for many practical purposes, the\nimplementation of quantum walks can be efficiently achieved using a classical\ncomputer. We then develop both classical and quantum graph isomorphism\nalgorithms based on discrete-time quantum walks. We show that they are\neffective in identifying isomorphism classes of large databases of graphs, in\nparticular groups of strongly regular graphs. We consider this approach to\nrepresent a promising candidate for an efficient solution to the graph\nisomorphism problem, and believe that similar methods employing quantum walks,\nor derivatives of these walks, may prove beneficial in constructing other\nalgorithms for a variety of purposes.\n",
        "The Haymarket Conspiracy: Transatlantic Anarchist Networks is a 2012 book by historian Timothy Messer-Kruse on the Haymarket affair and the origins of American anarchism.\n\nReferences\n\nExternal links \n\n \n\n2012 non-fiction books\nHistory books about anarchism\nEnglish-language books\nWorks about the Haymarket affair\nUniversity of Illinois Press books\nHistory books about the United States\nHistory books about the 20th century",
        "I think the reviewers evaluated this paper very carefully and were well balanced. The reviewers all agree that the presented comparison between human vision and DNNs is interesting. At the same time, none of the reviewers would strongly defend the paper. As it stands, this work seems a little too premature for publication as the analysis does not go too much beyond what we already know. We encourage the authors to deepen the investigation and resubmit.",
        "The responses to the pre-review questions are not strong; especially w.r.t. the question about dataset density and why the dataset had to be subsampled, the authors responded that subsampling is common in recommender systems work, including the papers cited. This isn't a particularly strong justification of why subsampling is a good idea, and in particular doesn't answer the question of \"how would the results look without subsampling,\" which I think is a question that could easily have been answered directly.\n\nEspecially given that the goal of dealing with the cold-start issue is so heavily emphasized in the paper, in seems odd to sample the data to reduce sparsity.\n\nOther than that, the pre-review questions seem to have been answered satisfactorily.\n\nThe contribution of the paper is to propose user and item embedding methods, as a means of learning complex non-linear interactions between users and items. This is fairly similar to recent work on deep RS, though the network formulation has some differences.\n\nOverall this is an reasonably put together paper that makes a contribution in an important area, though there are still some shortcomings that should be addressed, namely:\n1) The evaluation is unusual. Recall@M is the only result reported, though this is not usually an evaluation seen in recommender systems research. At the very least other performance measures (rmse or AUC) should be reported for completeness, even if the results are not strong\n2) Given that the contribution is fairly simple (i.e., the \"standard\" recommender systems task, but with a new model) it's a shame that unusual data samples have to be taken. This should be a case where it's possible to report results against competing methods using *exactly* the same data they used, and exactly the same error measure, for the fairest comparison possible.\n\nWithout the above it's hard to tell how much the performance improvements are really due to the method being better, versus the choice of datasets and the choice of loss functions.",
        " Everyone loves to sleep in in the mornings from time to time. If for whatever reason you're trying to make a day disappear faster, you can shave off some time from it by hitting the snooze button and sleeping in a bit. Sleeping is by and far the most effective way of making time speed up. If you're already in bed and want the morning to disappear, sometimes the best thing to do is nothing at all., A shower is one of the most physically pleasant stages of your day. Most people try to get in an out of the shower in record time, but if you have some time to kill, taking things more slowly can be great. Let the water rush over you; appreciate the warmth and temporary peace.\n\n, Sitting down with a cup of coffee and enjoying yourself at the kitchen table sounds sort of like a cliche at a point where everyone seems to rush from place to place, but slowing down to enjoy yourself strangely makes time feel that much faster.\n\n, There is probably a regular time you like to leave to go to work, school, or wherever else you typically need to be. If this is the case, consider leaving ten or fifteen minutes earlier than usual. Your brain tends to chunk memories into chapters, and leaving early will streamline that particular stage of your day. Unless you're leaving early, you may actually enjoy the extra time you have to prepare at the start of your day., Do something you've been putting off, whether it's an unfinished project or responding to emails. Your goals are already laid out for you, and you'll feel less stressed after you attend to them.\n\n\nBeing busy is your greatest potential ally when it comes to speeding up time. With that in mind, you should be doing everything you can to immerse yourself in the tasks at hand.\nWork may not be fun per se, but keeping yourself focused will make the day go by faster.\n\n, New projects are always on the go, and the slowest periods of work are perfect to latch onto something new. New projects can actually be a lot of fun at first to organize. Look around yourself at work; there's got to be something that could be done to improve your work environment.\n\n\nTeaming up with someone can add a pleasant social aspect to the experience, if you're extroverted and enjoy interacting with co-workers.\nIf you can't start something on your own accord, ask your superior for something new to do. Being kept busy will make the hours fly past, and you'll also be thought of as a hard worker who shows initiative without being asked.\n\n, Listening to music will keep you at least slightly entertained while you work, saving you from total boredom while you work.\n\n, If work is painfully slow and you're having a really hard time making the time pass, going for frequent breaks will give you ample time to relax and let your perception of time regulate itself. Go have a coffee. Go to the bathroom and relieve yourself. While these activities are unproductive and relying on them too heavily will have the opposite effect you're looking for.\n\n\nFrequent breaks make a distraction from work, but when it comes to passing time, the biggest benefit will be the relaxation it brings you. Of course, this won't have such a great effect if you're worried about falling behind in work.\n\n, People willingly lose hours of their life checking up on the lives of others, and most of the time, they're not even trying to burn time. Social media is a great tool to speed up time while you're at work. You don't want to make an overly frequent habit of it however, as it may compromise your work situation.\n\n\nAs a rule, relying too heavily on distractions at work can make the day feel slower. Ultimately, the best way to get through work as seemingly fast as possible is to immerse yourself in tasks.\n\n, Naps are good ways to kill some time if you're bored and could use the shuteye. If you really have nothing better to do, sleep will give your body some precious time to heal itself and patch up. Going to sleep in the middle of the day may be harder than at night or if you're already in bed in the morning, but it is the fastest way you can go about speeding up time if your body will allow you.\n\n\nNot to mention, when you wake up, you may feel more motivated to be productive and let the day come to you as it will.\n\n, Getting lost in leisurely activities is a brilliant way to speed up time, as we're never really considering how long we've been doing something when we're having a lot of fun. Reading one of your favourite books will take your mind off the passage of time, possibly even to the point where you'll wish there had been more hours in the day to fit in more reading.\n\n\nThe choice of book is very important in this case. A boring or poorly written book will actually have the opposite effect you're looking for.\n\n, So-called 'binge watching' takes up an exorbitant amount of time, and shows like Game of Thrones or Breaking Bad are all too eager to eat up whatever time you'll allot to them. If you have a spare day you want to see over quickly, put on a show and let yourself relax. If it's a show you really like, you'll probably lose track of how much time has really gone by.\n\n, If you've got expertise in something, you may be a candidate to write a wikiHow article! Writing a step-by-step guide on a subject you're fascinated by can be a lot of fun, and like any writing project, you'll find the time slips away like nothing once you get into the thick of planning and realizing your article.If you're not the writing type, you could find a how-to article on any random subject that interests you and learn a new skill. Learning is a great way to pass time, as your mind will become too occupied to worry about what time it is.\n\n, There's nothing like curling up to watch a movie at the end of an overly long day. Unless a movie is dreadfully boring or their bladders are tempting them elsewhere, people aren't usually thinking of the time while they're watching a movie. Instead, they're immersed in what's on the screen. Watching one of your favourite movies during or after dinner can be an incredibly rewarding experience.\n\n\nCurling up somewhere comfortable like a bed or sofa is a must in this case. If your body's relaxed, you'll have an easier time letting the time slip you by.\n\n, When you're learning something new, time seems to speed up. This is because the mind is too occupied with the fresh tasks to be as concerned with the passage of time. Your stomach will thank you for the new experience, and if you like the recipe enough, you can use it again on future occasions.\n\n\nOn the other hand, some studies have actually found that time speeds up when you're doing things that you're already very familiar with. This would include recipes. The bottom line is that you want to be engaging with something., Sleep is the best way to accelerate time. No one is keeping track of the passage of time when they're dozing away. Getting a head start on your sleep will give you a big boost for the coming day, which may be a good thing if your current day hasn't been great.\n\n, People who are looking to make their days go by faster typically fall into one of two categories. The first group wants time to go by in anticipation of some upcoming event. The second are merely bored and cannot decide on how to effectively use their time. If you're trying to speed up the day due to a legitimate stressor, it is understandable and possibly worth doing. If you're bored, you're probably wanting time to speed up only because you're not engaged by anything at the moment.\n\n\nIf you end up finding something that interests you (possibly even from the suggestions in this article!) you'll probably stop caring about wanting the day to speed up.\n\n, In contrast, if you're trying to do something you're very familiar with will have the opposite effect. Routine is your friend. Your mind will speed up time and run on autopilot if you're regaling yourself with the familiar.\n\n, Speeding up your perception of time can only be approached indirectly.It ultimately doesn't matter whether you're doing something new or familiar, or even who you're doing it with; the passage of time ultimately depends on how busy you are. If you are occupied with something, no matter how trivial it may seem, you won't be thinking of the time.\n\n, To combat this, just make sure you're relaxed and feeling relatively decent. This will make it easier to focus on activities as well.\n\n\nThis includes seeing to things like solving headaches or migraines. A migraine can make even an enjoyable experience feel like a test of endurance.\n\n, Ultimately, the way you can speed up the impression of time in your head is to ignore the passage of time. Looking at a clock will remind you exactly how much time has passed, making you more aware of it generally. If you are dead set on speeding up your day as much as possible, you should make a point of avoiding any clocks. Don't even think about how much time has passed.\n\n",
        "This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \"roll-in\" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.\n\nThis is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).\n\nIt's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.\n\nI have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).",
        "  The local, uncorrelated multiplicative noises driving a second-order, purely\nnoise-induced, ordering phase transition (NIPT) were assumed to be Gaussian and\nwhite in the model of [Phys. Rev. Lett. \\textbf{73}, 3395 (1994)]. The\npotential scientific and technological interest of this phenomenon calls for a\nstudy of the effects of the noises' statistics and spectrum. This task is\nfacilitated if these noises are dynamically generated by means of stochastic\ndifferential equations (SDE) driven by white noises. One such case is that of\nOrnstein--Uhlenbeck noises which are stationary, with Gaussian pdf and a\nvariance reduced by the self-correlation time (\\tau), and whose effect on the\nNIPT phase diagram has been studied some time ago. Another such case is when\nthe stationary pdf is a (colored) Tsallis' (q)--\\emph{Gaussian} which, being a\n\\emph{fat-tail} distribution for (q>1) and a \\emph{compact-support} one for\n(q<1), allows for a controlled exploration of the effects of the departure from\nGaussian statistics. As done before with stochastic resonance and other\nphenomena, we now exploit this tool to study--within a simple mean-field\napproximation and with an emphasis on the \\emph{order parameter} and the\n``\\emph{susceptibility}''--the combined effect on NIPT of the noises'\nstatistics and spectrum. Even for relatively small (\\tau), it is shown that\nwhereas fat-tail noise distributions ((q>1)) counteract the effect of\nself-correlation, compact-support ones ((q<1)) enhance it. Also, an interesting\neffect on the susceptibility is seen in the last case.\n",
        "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.",
        " Consider what kinds of situations cause you to resort to hair-pulling. Do you only do it when you're depressed? Angry? Confused? Frustrated? Understanding what triggers your hair-pulling can help you find other, more positive ways of coping.\n\n\nOver two weeks, jot down every time you catch yourself pulling your hair. Document what happened just prior to the hair-pulling, as well as your feelings.;\n, When learning the triggers, try to pin down what could be reinforcing the behavior. If you pull hair when you are anxious and this relieves the anxiety, then the hair pulling is positively reinforced by the feelings of relief.Take stock of how you feel during and immediately after you pull your hair.\n\n\nKnowing this can help you cope because the next time you feel anxious, you can try to find another coping strategy that brings you relief and work to make that your conditioned response to anxiety or your go-to coping strategy rather than hair pulling.\nThere are three distinct phases for sufferers of trichotillomania. Not all sufferers go through each of the three phases. You may experience one or more of these phases:1.You initially experience tension accompanied by a desire to pull out some hair.\n2. You start pulling out hair. It feels really good, like a sense of relief, as well as some excitement.\n3. Once the hair is pulled, you might feel guilt, remorse, and shame. You might attempt to cover the bald patches with scarves, hats, wigs, etc. But eventually the bald patches become obvious to everyone and you tend to start hiding at this point. You might start to feel intensely humiliated.\n\n\n\n, Do you pull hair because you don’t like certain kinds of hair? For example, one person might pull hair compulsively when they find grey hairs because they don’t like grey hair and “all greys must go.” One way to work on this trigger is to re-frame your perceptions of those hairs. No hair is inherently bad--all hair serves a purpose. Attempting to change your thought patterns about these hairs can help reduce the urge to pull.\n\n, The initial cause of trichotillomania could be genetic and/or environmental. Researchers see similarities with the triggers for obsessive-compulsive disorder and consider that chaotic, distressing childhood experiences or disturbed early relationships with parents or care-givers might be behind the development of this disorder.\n\n\nOne study has shown that over two-thirds of sufferers had experienced at least one traumatic event in their lives, with a fifth of them diagnosed with post-traumatic stress disorder. This has led to speculation that it is a form of self-soothing for some sufferers, a way to cope.\n\n, When tracing the source of your trichotillomania, look at whether you have a family history of hair pulling, obsessive compulsive disorder, or anxiety disorders. There is a significantly increased risk of developing trichotillomania if there is a family history of this disorder., The \"Notice, Interrupt, and Choose Plan\" is one strategy that may help you stop pulling your hair. This consists of noticing when you feel like pulling your hair, interrupting the chain of feelings and the urge to pull your hair through listening to positive reminders in your head. Then, you can choose to do something else instead, something that will relax you and calm you.\n\n, Through writing you can get a good idea of the times, the triggers, and the impact of your hair-pulling. Record the date, time, location, and number of hairs you pull and what you used to pull them. Write down your thoughts or feelings at the time, as well. This is a good way of getting the shame out, and of expressing how the hair-pulling is impacting your life in general.\n\n\nWhen you tally up the amount of hair you've pulled out, this can serve as a reality check on how much hair you're removing; is the result surprising to you? What about the amount of time spent on it, was it more than you thought?, Once you have identified the warning signs and triggers, write a list of alternative behaviors you can do instead of hair pulling. Whatever the alternative behavior is, it should be easy to do and easy to access. Some suggestions for alternative ways of expressing your emotions and feelings include: Taking a few minutes to clear your mind.\nDrawing or scribbling on paper\nPainting\nListening to music that relates to your emotions\nCalling a friend\nVolunteering\nCleaning\nPlaying video games.\nStretching\n\n, If you’re pulling your hair unintentionally, you may need a physical reminder to make yourself stop the activity. For a physical barrier, consider wearing ankle weights on the arm that pulls, or a rubber glove, to discourage pulling.\n\n\nYou might even have Post-It notes placed in areas where you tend to pull your hair a lot. These can act as other physical reminders to stop.\n\n, While it’s likely not possible to eliminate all triggers that compel you to pull your hair, you may be able to reduce some of your exposure. Is your girlfriend the cause behind most of your episodes? Perhaps it's time to reconsider your relationship. Is it your boss who's causing you all this stress? Maybe it's time to find a new career opportunity.\n\n\nOf course, for many, the triggers aren't as simple to identify or get away from; for some, change of schools, abuse, newly realized sexuality, family conflict, the death of a parent, or even pubertal hormonal changes are behind compulsive hair-pulling. These triggers are very hard - if not impossible - to get away from. If it is the case that you can’t get away from a trigger for any of the above or other reasons, continue to work on self-acceptance, retraining your habits and enlisting social support to help you cope with your disorder.\n\n, Use an all-natural oil to soothe the follicles and reduce itching, but more importantly to modify behavior from picking and pulling to stroking and rubbing. Make sure to use all natural products such as a mix of essential oils and castor oil. Never use chemical-based products.\n\n\nAlso, beware of products that promise quick fixes. Treatments that promise instant results or cures should not be trusted, as trichotillomania has no overnight cure.\nYou can also talk to your doctor about a prescription numbing cream to use on your head. This may be useful if one of your triggers is an “itchy” or strange feeling in your hair. In a case study of a 16-year-old girl, it was found that temporary use of numbing cream in combination with psychotherapy was successful in eliminating hair pulling behaviors., Hair pulling often results from a refusal to sit and be present with uncomfortable feelings or negative emotions.Use mindfulness techniques to help yourself become more accepting of these negative or uncomfortable emotions as a natural part of the human experience. They don’t necessarily need to be avoided.When the insistent urge to avoid discomfort abates, hair pulling will also decrease.\n\n\nTo do a mindfulness exercise, sit in a quiet, comfortable spot. Take deep breaths. Breathe in for a count of four, hold for a count of four, and exhale for a count of four. As you continue to breathe, your mind will likely wander. Acknowledge these thoughts without judgment and let them go. Return your attention to your breath.\n\n, Many individuals who are affected by this disorder also have low self-confidence or are low in self-esteem. In order to build self-esteem and self-acceptance, use Acceptance and Commitment Therapy (ACT), a therapeutic approach. This approach can help an individual clarify her values and focus on her life goals. Building self-esteem is an important part of recovery.\n\n\nRemember, you are a wonderful and unique person. You are loved, and your life is precious. No matter what anyone else tells you, you should love yourself.\n\n, Negative thoughts about yourself can deflate your self-esteem quickly and can make you feel like pulling your hair. Put-downs, fear of failure, and other negative thinking will keep you feeling as though you are not enough. Start changing these mental habits to begin to build yourself up and increase your confidence. Here are some example of how you can begin to change how you think about yourself:Say you have a thought such as, “I don’t have anything interesting to say, so I can see why people think I’m pathetic.” Catch unkind thoughts like this and make a conscious effort to change these thoughts by correcting yourself. Tell yourself: “Sometimes I don’t have much to say, and that is okay. I don’t have to keep others entertained or take on the entire responsibility for this conversation.”\nReplace critical thoughts with productive thoughts. For example, here is a critical thought: “There is no way I am meeting everyone for dinner. Last time I went, I was so embarrassed at my off-topic comment. I am so stupid.” Replace this with a productive thought: “I was so embarrassed at the last dinner, but I know that I make mistakes and that is okay. I am not stupid. I just made an honest mistake.”\nAs you practice catching these thoughts and changing them, you will notice that your self-esteem will increase along with your confidence., Another way to start accepting your emotions and improving self-esteem is to write down a list of your accomplishments and strengths.Refer to this often.\n\n\nIf you’re having trouble coming up with a list, talk with a trusted friend or family member. This person can brainstorm some ideas with you. No accomplishment is too small for this list. Keep adding to the list.\n\n, Practicing better self-assertion techniques can help you to overcome situations in which you feel challenged by other people. For example:\n\n\nLearn to say no. If people are making requests of you that you don’t want to fulfill, assert your own needs and wants by saying no.\n\nDon’t be a people pleaser. Don’t do things just to secure someone else’s approval. Figure out what is really important to YOU. Ask for what you want.\nUse “I” statements. These types of statements help you convey responsibility for your own emotions and reactions. For example, instead of saying, “You never listen to me,” you can say, “I feel ignored when you are looking at your phone when we talk.”\n\n, Many sufferers find that stress triggers the desire to pull hair. Do whatever you can do reduce stress in your life and learn how to manage the stress you do encounter with better coping techniques.\n\n\nMake a list of the things that stress you out. These can be large things, such as money or work, or they can be small things, like long lines at the grocery store. While you can’t avoid everything that causes you stress, you can minimize your exposure to some things.\n\n, You can reduce stress that you’re feeling by using progressive muscle relaxation. This type of relaxation reduces muscle tension, sending a signal your body to begin relaxing. By tensing and then releasing the tension in your muscles, you can slowing bring your body back to a calm state.\n\n\nTighten your muscles for six seconds and then release for six seconds. Pay close attention to how each muscle is relaxing.\nWork from your head to your toes until you feel your body begin to relax.\n\n, Meditation can be helpful in reducing stress. A regular meditation regimen, even 10 minutes a day, can help clear your head and refocus your energy into a positive space.\n\n\nTo meditate, find a quiet spot and sit or lie down. Begin breathing deeply, taking slow breaths. You might even try guided visualization, wherein you imagine a calm place such as a beach, a rippling creek, or a woodsy area.\n\n, Ensure that you have a regular sleep pattern and that you're getting enough sleep every night. Aim for at least seven or eight hours of sleep every night.\n\n\nIf you have trouble falling asleep, try listening to some soft music. Stop using any screen devices at least 15 minutes before you go to sleep.\n\n, Studies show that stress can be reduced considerably with a regular exercise regimen.Your body will increase its production of endorphins, which contribute to you feeling more positive.\n\n\nYou don’t have to pound the pavement for an hour every day. You can participate in exercising that you enjoy. This might include yoga, martial arts, or other activities. Even gardening can give you an energy boost.\n\n, Find someone you trust and tell him or her about your trichotillomania. If you aren't able to talk about it out loud, write a letter or an e-mail. If you are afraid of talking about your struggle with this disease, at least talk to this person about your feelings.\n\n\nYou might also tell your friends and family what your triggers are. This way, they can help remind you when you may be at risk of pulling your hair. They can also help you find an alternative behavior.\nAsk your friends and family to provide positive reinforcement when they see you successful engaging in a healthy alternative to hair pulling., A counselor or therapist can help you find ways of coping with your disorder. This person can also address any depression or other problems that may be contributing to your self-injury.\n\n\nIf you visit one counselor or therapist and you feel you are not being helped, find another one. You are not chained to one doctor or counselor. It’s important to find someone you feel a connection with, and who you feel is helping you.\nThe types of therapy that may be of benefit to you include behavioral therapy (especially habit-reversal training), psychotherapy, psychodynamic psychotherapy, hypnotherapy, cognitive-behavioral psychology, and possibly anti-depressant medication. , Several medications have been shown to be effective in treating trichotillomania. Fluoxetine, Aripiprazole, Olanzapine, and Risperidone are medications that have been used for treating cases of trichotillomania. These drugs help regulate the chemicals in the brain to reduce symptoms of anxiety, depression, and other emotions that can trigger hair pulling., If you don’t have immediate access to counseling, there are other sources you can access. The Trichotillomania Learning Center has online support groups.\n\n\nSeven Counties Services, Inc. has a free Trichotillomania support hotline you can call. The number is 800-221-0446., Trichotillomania is officially classified as an impulse control disorder, along the lines of pyromania, kleptomania, and pathological gambling. If you suffer from trichotillomania, you may act or react in certain ways when hair-pulling. These might include:\n\n\nChewing or eating pulled-out hair.\nRubbing pulled-out hair across your lips or face.\nAn increasing sense of tension immediately before pulling out the hair or when resisting the behavior.\nPleasure, gratification, or relief when pulling out the hair.\nCatching yourself pulling hair without even noticing (this is called “automatic” or unintentional hair-pulling).\nKnowing that you’re pulling hair deliberately (this is called “focused” hair-pulling).Using tweezers or other tools to pull out hair., There are some tell-tale signs that a person may be suffering from trichotillomania. These include:\n\n\nNoticeable hair loss caused by recurrent pulling out of the hair.\nPatchy bald areas on the scalp or other areas of the body.\nSparse or missing eyelashes or eyebrows.\nInfected hair follicles.\n\n, Some hair pullers may find that they nail bite, thumb suck, head bang, and compulsively scratch or pick at their skin.\n\n\nKeep track of these types of behaviors over several days to see if they are habitual. Notice when you’re doing them and how often you’re doing them.\n\n, Determine if trichotillomania is the only disorder that's affecting you. Compulsive hair pullers may suffer from depression, obsessive-compulsive disorder, Tourette's disorder, bipolar disorder, phobias, personality disorders, and in some cases, exhibit suicidal tendencies. Visiting your doctor or mental health professional can be helpful in determining whether you have other disorders.\n\n\nHowever, it is complicated to say which disorder is causing which. Is the loss of hair causing the depression through the desire to isolate yourself from others and avoiding enjoyable activities because you feel deep shame? Often, successful recovery from Trichotillomania requires treatment for any co-existing disorders as well. , Someone who believes that she suffers from Trichotillomania should be examined by a qualified doctor to rule out other hair follicle disorders. Some disorders include alopecia or tinea capitis, both of which can cause hair loss. When a doctor examines you, he will look for evidence of irregularly broken hairs, coiled hairs, and other hair abnormalities as signs of trichotillomania. , The first thing to realize is that this can be treated; it is a disorder, not something due to willpower or lack thereof. The disorder arises as a result of genetic makeup, moods, and your background. When it kicks in, it's a condition in need of treating, not something to beat yourself up over.\n\n\nBrain scans have shown that people with trichotillomania have differences in their brain from persons not suffering from the disorder. , Don't convince yourself that nothing is wrong; that your hair pulling is \"normal.” Trichotillomania can be considered a form of self-harm, even though it isn't as talked about as other forms of self-injury. Like all forms of self-harm, trichotillomania can become an addictive behavior. With time, it becomes harder and harder to stop; that is why it's best to bring it under control as soon as possible.\n\n",
        "We thank the reviewers for their insightful comments. We have conducted supplementary experiments and incorporated comments into our updated submission. The major changes are:\n\n1. Additional qualitative evidence for the role of affine transformation (Section 6.7): Our submission already contained full generation results on MNIST-ONE, and intermediate decomposition results on CUB-200 and CIFAR-10 to compare our model with and without the transformation. We have now added full generation results on CUB-200 and CIFAR-10, as well as results on LFW. We have placed images generated by the model without transformation next to the those generated by the model with transformation for easier inspection (Fig 20)\n\n2. Quantitative evidence for the role of affine transformation (Section 6.9): We also have quantitative analysis of the learned transformation parameters that confirms that the model does in fact rely on the transformation (Fig 22)\n\n3. Importance of modeling shapes (Section 6.8): We establish importance of the mask generator through an ablation of our model without it (Fig 21).\n\nWe also conducted several additional experiments and analyses that are described in our responses to reviewers below.\n\nPlease note that the sections in Appendix have been reordered to increase readability.\n",
        "The paper proposes a method for future frame prediction based on transformation of previous frame rather than direct pixel prediction.\n\nMany previous works have proposed similar methods. The authors in their responses state that previous work is deterministic, yet the proposed model also does not handle multimodality.\n\nFurther, i asked if they could test their method using 2 RGB frames as input and predicting the transformation as output, to be able to quantify the importance of using transformations both as input and output, since this is the first work that uses transformations as input also. The authors dismissed the suggestion by saying \"if we were to use RGB frames as input and ask the model to output future frames it would produce very blurry results\", that is, misunderstanding what the suggestion was. So, currently, it does not seem to be a valid novel contribution in this work compared to previous works.",
        "Acer Aspire One is a line of netbooks first released in July 2008 by Acer Inc.\n\nMany characteristics of a particular model of Acer Aspire One are dictated by the CPU platform chosen.  Initial models were based on the Intel Atoms.  Later, models with various AMD chips were introduced.  Newer versions of the Atom were adopted as well.\n\nEarly versions were based on the Intel Atom platform, which consists of the Intel Atom processor, Intel 945GSE Express chipset and Intel 82801GBM (ICH7M) I/O controller, and is available in several shell colors: seashell white, sapphire blue, golden brown, onyx black, and coral pink.\n\nHigher end models were released in June 2010 consisting of the AMD Athlon II Neo processor and ATI Radeon HD 4225 graphics controller. These are available in onyx black, antique brass, or mesh black shells depending on model.  Also released was a version of the Aspire One 521 with an AMD-V105 processor running at 1.2 GHz, an ATI Radeon 4225 graphics controller, and equipped with a HDMI port.\n\nA range of later models were powered by AMD Brazos APUs (combined CPU/GPU chips).  The AMD chips had much more powerful video capabilities but consumed more power.\n\nIts main competitor in the low-cost netbook market was the Asus Eee PC line.\n\nIn January 2013, Acer officially ended production of their Aspire One series due to declining sales as a result of consumers favoring tablets and Ultrabooks over netbooks.\n\nHistory\n\nThe line was originally manufactured for Acer Inc. by Quanta Computer.; Quanta was phased out as a supplier to Acer, and production of the Acer Aspire One line shifted to other manufacturers in 2009. Also starting in 2009, eSobi Inc. partnered with Acer to preload the eSobi News Center on Acer Aspire One netbooks beginning in the first quarter of 2009.\n\nOperating systems\n\nWindows\n\nWindows XP Home Edition SP3 is installed on the models with a name ending in X, or ending in B followed by another letter denoting color.\n\nIt is also possible to install and run Windows Vista or Windows 7 on the earlier model laptop. In high-end versions appearing during 2009, Windows Vista is pre-installed. The lack of a DVD-ROM drive requires creating a bootable USB flash drive (the on-board card reader slots are not bootable) using a USB external DVD drive or PXE boot network install.\n\nWindows 7 Starter is installed by default on models with a name starting in D such as D255E and D257 as well as other later Aspire models.\n\nLinux\n\nModels with names starting in L, or ending in A followed by a letter for color, are shipped with Linpus Linux Lite, which is based on Fedora 8.  This offers a simplified user interface, with default applications like the Firefox 2 browser, OpenOffice.org 2.3, Acer One Mail and Acer One Messenger available directly on the main screen. The default desktop environment has been designed to hide advanced features from the user and to prevent modification. It is possible to modify the system to present a more traditional Xfce 4 desktop, enable more advanced features such as context menus, or install additional software.\n\nIt is possible to install and run other Linux distributions on the Acer Aspire One, and some specially customised Linux distributions have been designed to offer out-of-the-box functionality. These include:\nJoli OS\nUbuntu Netbook Edition (UNE)\nLinux4One\nKuki Linux\nArchOne\nMoblin\nSlitaz\nOther distributions of the Linux operating system will also run, such as :\nArch Linux\nFedora\nCentOS\nDebian\nCrunchBang\nBunsenLab\nantiX\nMandriva Linux\nUbuntu, Ubuntu Studio\nEeebuntu\nopenSUSE\nSlackware\nLinux Mint\nPCLinuxOS\n MeeGo\n Puppy Linux\n Peppermint Linux\n Lubuntu - which also features a netbook specific desktop environment.\n Xubuntu\n Parrot Security OS\n MX Linux\n\nMac OS X\n\nThrough the OSx86 project, an Aspire One can boot and run a modified version of Mac OS X, including iAtkos, iDeneb, \"XxX\" and Kalyway distributions. This procedure is not supported by Apple or Acer.\n\nFreeBSD\n\nFreeBSD v. 8.2 is known to run on the Acer Aspire One, although some limitations apply: lid close not starting a hibernate, and power management which works to a very limited degree.\n\nOpenBSD\n\nOpenBSD releases ranging from 4.4 to 6.4 are known to run on some of the many models of Aspire One, with some limitations based on BIOS and other variations.\n\nAndroid\n\nSome models are dual boot (e.g. D250, D260), with Android 1.6 and Windows 7 Starter.  The computer boots up into Android first, with a tab to select Windows.  The Android-x86 OS has limited apps available to it.\n\nFreeDOS\n\nIt is possible to install FreeDOS on even the earliest models. Some peripherals, like a USB mouse or Ethernet port, will not be detected by the standard setup and may require manual tweaking.\n\nStorage\n\nSolid state drives\n\nThe A110 model ships with an 8 GB or 16 GB solid-state drive (SSD), although some models do not come with one. Early 8 GB models come with the Intel Z-P230, model SSDPAMM0008G1. This SSD has been criticized for its slow read and write speed. Intel lists the drive's maximum speeds as 38 MB/s read and 10 MB/s write. Later models come with the slightly faster Samsung P-SSD 1800.\n\nHard disks\n\nThe hard disk is a regular 2.5-in 5400 rpm SATA drive with 80, 120, 160, 250, 320, 500 or 750 GB. A number of different drives from different manufacturers have been reported to be included. Newer-model Aspire Ones take a 7 mm thick drive, as opposed to the usual 9.5 mm thickness that makes up most 2.5-inch form factor hard drives and SSDs.\n\nExpansion slot\n\nThere is also an SD/SDHC storage expansion slot on all models for additional storage (the 533 model does not support SDHC as verified by Acer support UK).  On Linux versions this automatically expands the space of the SSD or HDD using aufs.  Windows XP models treat it as a normal removable drive.\n\nSome models have a second slot that functions as a standard multi-in-1 flash memory card reader.  The 110 BIOS does not allow one to boot an operating system from this slot, but the 150 BIOS is capable of booting from an SDHC card in the slot.  (Note: with Linux, it is possible essentially to boot from HD or USB by using a /boot partition on the regular boot device and an initrd that loads the real OS from the slot).\n\nPower management\n\nThe Intel Atom platform has a specified maximum TDP of 11.8 W. Individual figures are 2.5 W for the N270 processor, 6 W for the 945GSE chipset and 3.3 W for the 82801GBM I/O controller.  The AUO B089AW01 LCD panel is rated at a maximum power consumption of 3 W.  Typical read–write power consumption for the SSD is around 0.3 W, and 0.01 W when idle. The different HDDs are rated at about 1.5–2.5 W for read–write operations and around 0.7 W when idle.\n\nThe official ratings for the battery are up to 3 hours for the three cell, and up to 8 hours for the six cell.  Linpus Linux Lite has been optimized by Acer for lower power consumption. Battery life is shorter on HDD configurations with Windows XP, at approximately 2.5 hours for the three cell.  Although the standard three cell battery is 2.2 Ah, some users have reported 2.4 Ah and 2.9 Ah batteries from the factory.  Various suppliers online now carry aftermarket batteries, including the six cell. Aftermarket nine cell batteries are available though they are quite heavy, and also protrude out of the back, reducing the aesthetics but improving airflow.\n\nSpecifications\n\nAn Acer AOD model (10.1\" screen) product comparison guide, in the form of a spreadsheet file download, is available from increa.com.\n\nAdditional hardware\n\nSince November 2008 the 3G-enabled model Aspire One A150X-3G is available in Europe, while models with 3G modems began shipping in the United Kingdom in December of the same year and were denoted by the letter G in their model number. The first generation Aspire One webcam is an Acer Labs International M5608 camera controller with attached 0.3 MP SuYin or 1.3 MP LiteOn CMOS sensor.\n\nSome models of the Aspire One use an Intel 945GSE chipset which only supports 2 GB of RAM. Installing memory modules larger than 2 GB has caused the Aspire to fail the power-on self-test.  Model 522, featuring the AMD Fusion C-50 chip, has been reported to work with 4 GB installed (although the included Windows 7 Starter edition has an artificially imposed 2 GB limit).\n\nAO751h (751h)\n\nThe AO751h has the larger 11.6\" screen with an LED backlit display and a 1366x768 native resolution. It includes a 1 GB/667 MHz DDR2 533 MHz SDRAM memory option (2 GB being the maximum), a 160 GB HDD option, Bluetooth option, Intel northbridge US15W, and an OS option for Windows Vista Home Basic edition or Windows XP Media Center Edition. All AO751h units are powered by an Intel Atom Z520 processor running at 1330 MHz (or 1240 MHz in first version). The US15W system controller incorporates a GMA500 video core. The AO751h has a dual power (AC/DC) option. The six cell battery provides the working time of about 8 hours. Besides the mentioned specifications of the AO751h it supports 10/100 Mbit/s Ethernet interface, 802.11g Wi-Fi card Atheros, Bluetooth 2.1, standard VGA-out jack, 3 USB 2.0 ports, a Memory Card reader 5:1 (xD-Picture Card, SD card), as well as Microphone In jack 3.5 mm (1/8\" Mini), Headphone Out jack 3.5 mm (1/8\" Mini). One of the cons is the relatively small TouchPad and mouse buttons. Large and comfortable keyboard is one of the distinguishing features of the device.\n\nThere have been reports of some AO751h units randomly freezing, which has led to recalls in Denmark.  Some users have reported that having their motherboards replaced by Acer solved the problem and Acer officially reporting that the fix is to install an updated BIOS.\n\nAOA150 speaker vibration and other issues\nThe 8.9\" models have an improperly designed speaker location that causes vibration to the internal hard drive, causing it to be problematic. The right speaker is especially prone to this. Audio frequencies around 1 kHz cause the hard drive to almost stop responding. Full volume MP3/audio playback easily causes these models to run extremely slowly, or crash because of unresponsive disk I/O. This problem also causes bad sectors, crashed hard drives and corrupt Windows partitions in the long term. Even sound from an external speaker with 1 kHz tone test causes this hard drive behaviour. SSD drives do not suffer from this problem. Workarounds identified are: listening to music at a lower volume, using only the left speaker, using a software equalizer to tone down the 1 kHz frequency, replacing the hard drive with an SSD, and trying to install soft sound dampening material around the speakers and the hard drive.\n\nAspire One Happy\n\nThe Acer Aspire One Happy is a 10.1 inch netbook with different operating systems: (Android 2.1 and Windows 7). It was launched by Acer Inc. in November 2010. The computer is nearly identical to the Acer Aspire One D255. The main difference is that the Happy case comes in several different color options (comes in Candy pink, Lavender purple, Lime green and Hawaii blue). It is powered by Intel Atom N450 or dual-core Intel Atom N550, with up to 2 GB RAM and Intel Graphics Media Accelerator 3150. There is also a 3G version for proper portability, with each model measuring 24 mm thick and weighing 1.25 kg with a six-cell battery pack.\n\nIn mid–2011, the \"Acer Aspire One Happy 2\" line was released. This model may have bluetooth on board.  It seems that some countries ship this newer model with bluetooth while others do not.\nThis seems to depend which wireless card they shipped with, some can in fact be replaced as the BIOS should not be card specific.\nThe functionality uses an internal USB bus on miniPCIe slot specification but not all have these pins connected.\nA good way to check is find a miniPCIe to USB3 card and see if the device is recognized or not.\n\nAcer Aspire One D270\n\nThe Acer Aspire One D270 netbook is the first 10-inch Acer netbook to feature a 1.6 GHz Intel Atom N2600 dual core processor and running Windows 7 Starter 32-bit. The AOD270-1186, the white models, feature an Intel Atom N2600 dual core processor with 1 MB L2 cache, 1.6 GHz processor and Hyper Threading technology. The AOD270-1410, the black model, features a 10.1 inch WSVGA LED-backlit display with CrystalBrite technology with a 17:10 aspect ratio and 1024 x 600 resolution display, 1 GB DDR3 SDRAM memory, a 320 GB 5400 rpm hard drive and integrated Intel GMA 3600 graphics with 64 MB dedicated memory.\n\nIn Europe, the Acer Aspire One D270-26Ckk NU.SGAED.011 is running Linpus Linux and comes in an \"espresso black\" color. It also includes 2 GB of RAM, and a 320 GB hard drive. With Linux, this netbook becomes a bit faster than its Windows 7 counterpart.\n\nAlthough Intel specifies the maximum RAM capability of the N2600 as 2 GB, numerous users have reported a 4 GB SODIMM works well in the D270, with 2.99 GB reported usable by Windows 7 Home Premium 32 bit (after upgrading from Starter).\n\nSome models of the Acer Aspire One D270 have Windows 7 Home Basic pre-installed and have an Intel Atom N2800 1.86 GHz processor.\n\nSome more recent AOA's can be upgraded to 4GB 1.5V RAM if they use DDR3 and the 64 bit CPU; though this is not a recommended upgrade path it  works on the D270 -26Dbb and the modification was medium term stable on the test machine running x64 Windows 7 Home using SDR#. It is believed that many other inexpensive VT64x equipped DDR3 netbooks have the same capability but disabled for licensing reasons or due to the clock circuitry being unable to handle certain modules correctly resulting in system instability, that can be overcome using spdtool and slightly altering timing parameters.\n\nThere are known issues with the WiFi in the D270 over time, which sometimes occur after the card is changed. Putting the OEM one back in does not resolve this problem and symptoms include the Fn + F3 not working. Further investigation suggests that the issue may be BIOS corruption and replacing or reflashing the chip may well resolve it but as of yet this is untested. The D270 is known to have SPECTRE/Meltdown vulnerabilities but this may be patched at a later date with a third party BIOS.\n\nReception\n\nCNET editors' review has been positive, although held reservations as the Aspire One is early to market and many netbooks are planned for the near future.\n\nA The Tech Report (Techreport.com) review of the 751h, and the similar Gateway LT3103U, was generally positive, praising the notebooks' battery life but criticizing the large number of third party software products preinstalled and running by default on the computers.\n\nSee also\n Gateway netbooks\n Comparison of netbooks\n\nReferences\n\nExternal links\n \n\nComputer-related introductions in 2008\nLinux-based devices\nNetbooks\nAndroid (operating system) devices\nAcer Inc. laptops",
        "i'm 30 and started smoking when i was 12.  by the time i was 15 i was a full fledged pack a day smoker.  over the years i have witnessed many people fall prey to the habit.\n\nyou start by smoking here and there, maybe you don't inhale at first, maybe you just like to blow it out your nose or smoke it like a cigar.  it's only on certain occasions.  \n\nif you are fiddling with cigarettes, you likely have friends that are smoking, also.  so you are hanging out, maybe having a few beers, and you see them smoking and decide to be social with them, and also have a cig, but alas, you have none.  so you bum a few off a friend for the night.  you kind of like it.  it just feels good to have one with some drinks or just when you're talking and laughing.  soon you do this every time you get together for a drink/movie/whatever.  it's not a big deal, it's just once in a blue moon.\n\nafter a while, your friends start complaining that you are always asking for cigarettes but you never seem to have your own, you cheap son of a bitch.  those shits are expensive.  maybe next time you could come prepared and let *other* people bum them.  so you buy a pack prior to the next gathering.  it feels weird to order them at the counter, but whatever.  it's only for the night and you feel like an asshole smoking everyone's cigarettes all night.  you see how they act when they run low.  you don't want to be that guy.\n\nso now you have your own pack of cigarettes.  it's just for the socialness of it.  but at the end of the night you have leftovers.  you leave them for a friend because what the hell do you need them for?  the night is over.  it's not like you're going to smoke them tomorrow.\n\neventually there comes a time where you decide you may as well keep the cigarettes you bought.  they *are* expensive and you're sick of having to buy them every time you go out, so you'll just save them for next time.\n\nnow, a weird thing happens when you have your own pack of cigarettes.  maybe you'll get the idea to just smoke one after a shitty day, maybe you won't.  maybe you'll have your own 6 pack while you watch the game.  if you have a few beers, you will notice a strange urge to have one of those cigarettes.  for the past few months, every time you've had drinks with friends you've had a few cigarettes.  it's like they go together.  it's just what you do.  but there's no one else there, it's just you.\n\nso you have a little debate with yourself.  you don't *need* the cig, but you sure would like to have it.  it's not going to ruin your night either way.\n\nif you decide to have that cigarette, you have fucking failed.  you are doomed.  *doomed*.  what you have done is solidified an association with having drinks and smoking cigarettes.  it is no longer a social thing.  you love the little buzz you get.  you love playing with it as you sip your drink.  you love trying to make smoke rings or whatever other cutesy shit you do to amuse yourself while you smoke it.  you don't realize it, but you now have a habit.  an itsy bitsy manageable habit, but a habit nonetheless.  \n\nyou may now find yourself looking forward to outings with friends because you can't wait to have an occasion to smoke a cig or two.  you might notice a feeling of \"nakedness\" if you have beer or two with dinner but no cigarette.  you might hang out with your smoker friends on non drinking occasions and feel that same sense of something missing.  then you see someone light a cig and it hits you.  you *want* that fucking thing.  shit.  you have another internal debate with yourself about whether or not to have a cigarette without the drinks.  you don't even have cigs on you.  if you decide to bum one now, you are officially screwed.  you gave in.  now you've solidified an association between social occasions and smoking.  you will come to expect this at gatherings.  going out to dinner?  let me join you for a cigarette.  cookout?  you don't mind if i have one of those, do you?  i didn't bring any because i'm not drinking.\n\nso now you smoke at social drinking occasions, you might smoke when drinking at home, and you also smoke when just hanging out.  your friends again chastise you for bumming their cigarettes.  buy your fucking own if you want one that bad, they say.  you promise to bring your own next time...\n\nnow you smoke often enough to expect a cigarette after certain occasions.  after dinner? smoke.  movie's over?  smoke.  drinks at bob's?  smoke.  you dun goofed, and it's all downhill from here.  you've accidentally built cigarettes into your life.\n\n**tl:dr** a chain of events will lead to your expecting cigarettes on certain occasions, and if you buy them to fulfill this expectation, you're screwed."
    ]
}